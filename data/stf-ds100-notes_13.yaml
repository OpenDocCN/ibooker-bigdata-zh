- en: 12  Ordinary Least Squares
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12  普通最小二乘法
- en: 原文：[https://ds100.org/course-notes/ols/ols.html](https://ds100.org/course-notes/ols/ols.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://ds100.org/course-notes/ols/ols.html](https://ds100.org/course-notes/ols/ols.html)
- en: '*Learning Outcomes* ***   Define linearity with respect to a vector of parameters
    \(\theta\).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习成果* ***   定义关于参数向量 \(\theta\) 的线性性。'
- en: Understand the use of matrix notation to express multiple linear regression.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解使用矩阵表示法来表达多元线性回归。
- en: Interpret ordinary least squares as the minimization of the norm of the residual
    vector.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释普通最小二乘法为残差向量的范数的最小化。
- en: Compute performance metrics for multiple linear regression.**  **We’ve now spent
    a number of lectures exploring how to build effective models – we introduced the
    SLR and constant models, selected cost functions to suit our modeling task, and
    applied transformations to improve the linear fit.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算多元线性回归的性能指标。**  **我们现在已经花了很多讲座时间来探讨如何构建有效的模型 - 我们介绍了 SLR 和常数模型，选择了适合我们建模任务的成本函数，并应用了转换来改进线性拟合。
- en: Throughout all of this, we considered models of one feature (\(\hat{y}_i = \theta_0
    + \theta_1 x_i\)) or zero features (\(\hat{y}_i = \theta_0\)). As data scientists,
    we usually have access to datasets containing *many* features. To make the best
    models we can, it will be beneficial to consider all of the variables available
    to us as inputs to a model, rather than just one. In today’s lecture, we’ll introduce
    **multiple linear regression** as a framework to incorporate multiple features
    into a model. We will also learn how to accelerate the modeling process – specifically,
    we’ll see how linear algebra offers us a powerful set of tools for understanding
    model performance.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些情况下，我们考虑了一个特征的模型 (\(\hat{y}_i = \theta_0 + \theta_1 x_i\)) 或零个特征的模型 (\(\hat{y}_i
    = \theta_0\))。作为数据科学家，我们通常可以访问包含 *许多* 特征的数据集。为了建立最佳模型，考虑所有可用的变量作为模型的输入将是有益的，而不仅仅是一个。在今天的讲座中，我们将介绍
    **多元线性回归** 作为将多个特征合并到模型中的框架。我们还将学习如何加速建模过程 - 具体来说，我们将看到线性代数为我们提供了一组强大的工具，用于理解模型性能。
- en: 12.1 Linearity
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 线性
- en: An expression is **linear in \(\theta\)** (a set of parameters) if it is a linear
    combination of the elements of the set. Checking if an expression can separate
    into a matrix product of two terms – a **vector of \(\theta\)** s, and a matrix/vector
    **not involving \(\theta\)** – is a good indicator of linearity.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个表达式是 **关于 \(\theta\)** (一组参数) 是线性组合，那么它是线性的。检查一个表达式是否可以分解为两个项的矩阵乘积 - 一个
    **\(\theta\)** 向量，和一个不涉及 **\(\theta\)** 的矩阵/向量 - 是线性的一个很好的指标。
- en: For example, consider the vector \(\theta = [\theta_0, \theta_1, \theta_2]\)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑向量 \(\theta = [\theta_0, \theta_1, \theta_2]\)
- en: '\(\hat{y} = \theta_0 + 2\theta_1 + 3\theta_2\) is linear in theta, and we can
    separate it into a matrix product of two terms:'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(\hat{y} = \theta_0 + 2\theta_1 + 3\theta_2\) 在 theta 上是线性的，我们可以将其分解为两个项的矩阵乘积：
- en: \[\hat{y} = \begin{bmatrix} 1 \space 2 \space 3 \end{bmatrix} \begin{bmatrix}
    \theta_0 \\ \theta_1 \\ \theta_2 \end{bmatrix}\]
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat{y} = \begin{bmatrix} 1 \space 2 \space 3 \end{bmatrix} \begin{bmatrix}
    \theta_0 \\ \theta_1 \\ \theta_2 \end{bmatrix}\]
- en: \(\hat{y} = \theta_0\theta_1 + 2\theta_1^2 + 3log(\theta_2)\) is *not* linear
    in theta, as the \(\theta_1\) term is squared, and the \(\theta_2\) term is logged.
    We cannot separate it into a matrix product of two terms.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(\hat{y} = \theta_0\theta_1 + 2\theta_1^2 + 3log(\theta_2)\) 在 theta 上 *不*
    是线性的，因为 \(\theta_1\) 项是平方的，而 \(\theta_2\) 项是对数的。我们无法将其分解为两个项的矩阵乘积。
- en: 12.2 Terminology for Multiple Linear Regression
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 多元线性回归的术语
- en: There are several equivalent terms in the context of regression. The ones we
    use most often for this course are bolded.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归的背景下有几个等效的术语。我们在本课程中最常用的是加粗的。
- en: \(x\) can be called a
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(x\) 可以被称为
- en: '**Feature(s)**'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征**'
- en: Covariate(s)
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协变量
- en: '**Independent variable(s)**'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自变量**'
- en: Explanatory variable(s)
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释变量
- en: Predictor(s)
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测变量
- en: Input(s)
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入
- en: Regressor(s)
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归器
- en: \(y\) can be called an
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(y\) 可以被称为
- en: '**Output**'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**'
- en: Outcome
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果
- en: '**Response**'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**响应**'
- en: Dependent variable
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因变量
- en: \(\hat{y}\) can be called a
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\hat{y}\) 可以被称为
- en: '**Prediction**'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测**'
- en: Predicted response
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测响应
- en: Estimated value
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计值
- en: \(\theta\) can be called a
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\theta\) 可以被称为
- en: '**Weight(s)**'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**'
- en: '**Parameter(s)**'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数**'
- en: Coefficient(s)
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系数
- en: \(\hat{\theta}\) can be called a
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\hat{\theta}\) 可以被称为
- en: '**Estimator(s)**'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**估计器**'
- en: '**Optimal parameter(s)**'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最佳参数**'
- en: A datapoint \((x, y)\) is also called an observation.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个数据点 \((x, y)\) 也被称为一个观测。
- en: 12.3 Multiple Linear Regression
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 多元线性回归
- en: 'Multiple linear regression is an extension of simple linear regression that
    adds additional features to the model. The multiple linear regression model takes
    the form:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 多元线性回归是简单线性回归的扩展，它将额外的特征添加到模型中。多元线性回归模型的形式为：
- en: \[\hat{y} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:...\:+\:\theta_p
    x_{p}\]
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat{y} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:...\:+\:\theta_p
    x_{p}\]
- en: Our predicted value of \(y\), \(\hat{y}\), is a linear combination of the single
    **observations** (features), \(x_i\), and the parameters, \(\theta_i\).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 \(y\) 的预测值 \(\hat{y}\) 是单个 **观测** (特征) \(x_i\) 和参数 \(\theta_i\) 的线性组合。
- en: We can explore this idea further by looking at a dataset containing aggregate
    per-player data from the 2018-19 NBA season, downloaded from [Kaggle](https://www.kaggle.com/schmadam97/nba-regular-season-stats-20182019).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看从 2018-19 NBA 赛季下载的包含每个球员数据的数据集来进一步探讨这个想法，数据来自 [Kaggle](https://www.kaggle.com/schmadam97/nba-regular-season-stats-20182019)。
- en: <details><summary>Code</summary>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE0]</details> <details><summary>Code</summary>'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE0]</details> <details><summary>代码</summary>'
- en: '[PRE1]</details>'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1]</details>'
- en: '|  | Player | Pos | Age | Tm | G | GS | MP | FG | FGA | FG% | ... | FT% | ORB
    | DRB | TRB | AST | STL | BLK | TOV | PF | PTS |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | 球员 | 位置 | 年龄 | Tm | G | GS | MP | FG | FGA | FG% | ... | FT% | ORB | DRB
    | TRB | AST | STL | BLK | TOV | PF | PTS |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | Álex Abrines\abrinal01 | SG | 25 | OKC | 31 | 2 | 19.0 | 1.8 | 5.1 |
    0.357 | ... | 0.923 | 0.2 | 1.4 | 1.5 | 0.6 | 0.5 | 0.2 | 0.5 | 1.7 | 5.3 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 亚历克斯·阿布里内斯\abrinal01 | SG | 25 | OKC | 31 | 2 | 19.0 | 1.8 | 5.1 | 0.357
    | ... | 0.923 | 0.2 | 1.4 | 1.5 | 0.6 | 0.5 | 0.2 | 0.5 | 1.7 | 5.3 |'
- en: '| 2 | Quincy Acy\acyqu01 | PF | 28 | PHO | 10 | 0 | 12.3 | 0.4 | 1.8 | 0.222
    | ... | 0.700 | 0.3 | 2.2 | 2.5 | 0.8 | 0.1 | 0.4 | 0.4 | 2.4 | 1.7 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 昆西·埃西\acyqu01 | PF | 28 | PHO | 10 | 0 | 12.3 | 0.4 | 1.8 | 0.222 | ...
    | 0.700 | 0.3 | 2.2 | 2.5 | 0.8 | 0.1 | 0.4 | 0.4 | 2.4 | 1.7 |'
- en: '| 3 | Jaylen Adams\adamsja01 | PG | 22 | ATL | 34 | 1 | 12.6 | 1.1 | 3.2 |
    0.345 | ... | 0.778 | 0.3 | 1.4 | 1.8 | 1.9 | 0.4 | 0.1 | 0.8 | 1.3 | 3.2 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 杰伦·亚当斯\adamsja01 | PG | 22 | ATL | 34 | 1 | 12.6 | 1.1 | 3.2 | 0.345
    | ... | 0.778 | 0.3 | 1.4 | 1.8 | 1.9 | 0.4 | 0.1 | 0.8 | 1.3 | 3.2 |'
- en: '| 4 | Steven Adams\adamsst01 | C | 25 | OKC | 80 | 80 | 33.4 | 6.0 | 10.1 |
    0.595 | ... | 0.500 | 4.9 | 4.6 | 9.5 | 1.6 | 1.5 | 1.0 | 1.7 | 2.6 | 13.9 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 史蒂文亚当斯\adamsst01 | C | 25 | OKC | 80 | 80 | 33.4 | 6.0 | 10.1 | 0.595
    | ... | 0.500 | 4.9 | 4.6 | 9.5 | 1.6 | 1.5 | 1.0 | 1.7 | 2.6 | 13.9 |'
- en: '| 5 | Bam Adebayo\adebaba01 | C | 21 | MIA | 82 | 28 | 23.3 | 3.4 | 5.9 | 0.576
    | ... | 0.735 | 2.0 | 5.3 | 7.3 | 2.2 | 0.9 | 0.8 | 1.5 | 2.5 | 8.9 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 班·阿德巴约\adebaba01 | C | 21 | MIA | 82 | 28 | 23.3 | 3.4 | 5.9 | 0.576
    | ... | 0.735 | 2.0 | 5.3 | 7.3 | 2.2 | 0.9 | 0.8 | 1.5 | 2.5 | 8.9 |'
- en: 5 rows × 29 columns
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 5行×29列
- en: Let’s say we are interested in predicting the number of points (`PTS`) an athlete
    will score in a basketball game this season.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有兴趣预测一名运动员本赛季在篮球比赛中将得分的数量（`PTS`）。
- en: Suppose we want to fit a linear model by using some characteristics, or **features**
    of a player. Specifically, we’ll focus on field goals, assists, and 3-point attempts.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要通过使用球员的一些特征或**特征**来拟合一个线性模型。具体来说，我们将专注于投篮命中、助攻和三分球出手。
- en: '`FG`, the number of (2-point) field goals per game'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FG`，每场比赛的（2分）投篮命中数'
- en: '`AST`, the average number of assists per game'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AST`，每场比赛的平均助攻数'
- en: '`3PA`, the number of 3-point field goals attempted per game'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`3PA`，每场比赛尝试的三分球数'
- en: <details><summary>Code</summary>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE2]</details>'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE2]</details>'
- en: '|  | FG | AST | 3PA | PTS |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | FG | AST | 3PA | PTS |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | 1.8 | 0.6 | 4.1 | 5.3 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.8 | 0.6 | 4.1 | 5.3 |'
- en: '| 2 | 0.4 | 0.8 | 1.5 | 1.7 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.4 | 0.8 | 1.5 | 1.7 |'
- en: '| 3 | 1.1 | 1.9 | 2.2 | 3.2 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.1 | 1.9 | 2.2 | 3.2 |'
- en: '| 4 | 6.0 | 1.6 | 0.0 | 13.9 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 6.0 | 1.6 | 0.0 | 13.9 |'
- en: '| 5 | 3.4 | 2.2 | 0.2 | 8.9 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 3.4 | 2.2 | 0.2 | 8.9 |'
- en: Because we are now dealing with many parameter values, we’ve collected them
    all into a **parameter vector** with dimensions \((p+1) \times 1\) to keep things
    tidy. Remember that \(p\) represents the number of features we have (in this case,
    3).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们现在处理的是许多参数值，我们已经将它们全部收集到了一个维度为\((p+1) \times 1\)的**参数向量**中，以保持整洁。记住\(p\)代表我们拥有的特征数量（在这种情况下是3）。
- en: \[\theta = \begin{bmatrix} \theta_{0} \\ \theta_{1} \\ \vdots \\ \theta_{p}
    \end{bmatrix}\]
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: \[\theta = \begin{bmatrix} \theta_{0} \\ \theta_{1} \\ \vdots \\ \theta_{p}
    \end{bmatrix}\]
- en: 'We are working with two vectors here: a row vector representing the observed
    data, and a column vector containing the model parameters. The multiple linear
    regression model is **equivalent to the dot (scalar) product of the observation
    vector and parameter vector**.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用两个向量：一个表示观察数据的行向量，另一个包含模型参数的列向量。多元线性回归模型**等同于观察向量和参数向量的点（标量）积**。
- en: \[[1,\:x_{1},\:x_{2},\:x_{3},\:...,\:x_{p}] \theta = [1,\:x_{1},\:x_{2},\:x_{3},\:...,\:x_{p}]
    \begin{bmatrix} \theta_{0} \\ \theta_{1} \\ \vdots \\ \theta_{p} \end{bmatrix}
    = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:...\:+\:\theta_p x_{p}\]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: \[[1,\:x_{1},\:x_{2},\:x_{3},\:...,\:x_{p}] \theta = [1,\:x_{1},\:x_{2},\:x_{3},\:...,\:x_{p}]
    \begin{bmatrix} \theta_{0} \\ \theta_{1} \\ \vdots \\ \theta_{p} \end{bmatrix}
    = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:...\:+\:\theta_p x_{p}\]
- en: Notice that we have inserted 1 as the first value in the observation vector.
    When the dot product is computed, this 1 will be multiplied with \(\theta_0\)
    to give the intercept of the regression model. We call this 1 entry the **intercept**
    or **bias** term.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经在观察向量中插入了1作为第一个值。当计算点积时，这个1将与\(\theta_0\)相乘，得到回归模型的截距。我们称这个1条目为**截距**或**偏差**项。
- en: 'Given that we have three features here, we can express this model as: \[\hat{y}
    = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:\theta_3 x_{3}\]'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们这里有三个特征，我们可以将这个模型表示为：\[\hat{y} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2
    x_{2}\:+\:\theta_3 x_{3}\]
- en: Our features are represented by \(x_1\) (`FG`), \(x_2\) (`AST`), and \(x_3\)
    (`3PA`) with each having correpsonding parameters, \(\theta_1\), \(\theta_2\),
    and \(\theta_3\).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的特征由\(x_1\)（`FG`）、\(x_2\)（`AST`）和\(x_3\)（`3PA`）表示，每个特征都有对应的参数\(\theta_1\)、\(\theta_2\)和\(\theta_3\)。
- en: In statistics, this model + loss is called **Ordinary Least Squares (OLS)**.
    The solution to OLS is the minimizing loss for parameters \(\hat{\theta}\), also
    called the **least squares estimate**.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，这个模型+损失被称为**普通最小二乘法（OLS）**。OLS的解是参数\(\hat{\theta}\)的最小损失，也称为**最小二乘估计**。
- en: 12.4 Linear Algebra Approach
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4 线性代数方法
- en: We now know how to generate a single prediction from multiple observed features.
    Data scientists usually work at scale – that is, they want to build models that
    can produce many predictions, all at once. The vector notation we introduced above
    gives us a hint on how we can expedite multiple linear regression. We want to
    use the tools of linear algebra.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道如何从多个观察特征生成单个预测。数据科学家通常会进行大规模工作 - 也就是说，他们希望构建可以一次产生多个预测的模型。我们上面介绍的向量表示法为我们提供了如何加速多元线性回归的线索。我们想要使用线性代数的工具。
- en: Let’s think about how we can apply what we did above. To accomodate for the
    fact that we’re considering several feature variables, we’ll adjust our notation
    slightly. Each observation can now be thought of as a row vector with an entry
    for each of \(p\) features.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑如何应用上面所做的事情。为了适应我们正在考虑多个特征变量的事实，我们将稍微调整我们的符号。现在，每个观察可以被认为是一个行向量，其中每个特征都有一个条目。
- en: '| ![observation](../Images/93c3cd4962af19c4ef98b8452f9ebf52.png) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ![observation](../Images/93c3cd4962af19c4ef98b8452f9ebf52.png) |'
- en: To make a prediction from the first observation in the data, we take the dot
    product of the parameter vector and first observation vector. To make a prediction
    from the *second* observation, we would repeat this process to find the dot product
    of the parameter vector and the *second* observation vector. If we wanted to find
    the model predictions for each observation in the dataset, we’d repeat this process
    for all \(n\) observations in the data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要从数据中的第一个观测中进行预测，我们取参数向量和第一个观测向量的点积。要从*第二个*观测中进行预测，我们将重复这个过程，找到参数向量和*第二个*观测向量的点积。如果我们想要找到数据集中每个观测的模型预测，我们将对数据中的所有\(n\)个观测重复这个过程。
- en: \[\hat{y}_1 = \theta_0 + \theta_1 x_{11} + \theta_2 x_{12} + ... + \theta_p
    x_{1p} = [1,\:x_{11},\:x_{12},\:x_{13},\:...,\:x_{1p}] \theta\] \[\hat{y}_2 =
    \theta_0 + \theta_1 x_{21} + \theta_2 x_{22} + ... + \theta_p x_{2p} = [1,\:x_{21},\:x_{22},\:x_{23},\:...,\:x_{2p}]
    \theta\] \[\vdots\] \[\hat{y}_n = \theta_0 + \theta_1 x_{n1} + \theta_2 x_{n2}
    + ... + \theta_p x_{np} = [1,\:x_{n1},\:x_{n2},\:x_{n3},\:...,\:x_{np}] \theta\]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat{y}_1 = \theta_0 + \theta_1 x_{11} + \theta_2 x_{12} + ... + \theta_p
    x_{1p} = [1,\:x_{11},\:x_{12},\:x_{13},\:...,\:x_{1p}] \theta\] \[\hat{y}_2 =
    \theta_0 + \theta_1 x_{21} + \theta_2 x_{22} + ... + \theta_p x_{2p} = [1,\:x_{21},\:x_{22},\:x_{23},\:...,\:x_{2p}]
    \theta\] \[\vdots\] \[\hat{y}_n = \theta_0 + \theta_1 x_{n1} + \theta_2 x_{n2}
    + ... + \theta_p x_{np} = [1,\:x_{n1},\:x_{n2},\:x_{n3},\:...,\:x_{np}] \theta\]
- en: Our observed data is represented by \(n\) row vectors, each with dimension \((p+1)\).
    We can collect them all into a single matrix, which we call \(\mathbb{X}\).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的观测数据由\(n\)个行向量表示，每个向量的维度为\((p+1)\)。我们可以将它们全部收集到一个称为\(\mathbb{X}\)的单个矩阵中。
- en: '| ![design_matrix](../Images/5043df8076e502eded32ecaf0735e6dd.png) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ![design_matrix](../Images/5043df8076e502eded32ecaf0735e6dd.png) |'
- en: The matrix \(\mathbb{X}\) is known as the **design matrix**. It contains all
    observed data for each of our \(p\) features, where each **row** corresponds to
    one **observation**, and each **column** corresponds to a **feature**. It often
    (but not always) contains an additional column of all ones to represent the **intercept**
    or **bias column**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵\(\mathbb{X}\)被称为**设计矩阵**。它包含了我们\(p\)个特征的所有观测数据，其中每一**行**对应一个**观测**，每一**列**对应一个**特征**。它通常（但并非总是）包含一个额外的全为1的列来表示**截距**或**偏置列**。
- en: 'To review what is happening in the design matrix: each row represents a single
    observation. For example, a student in Data 100\. Each column represents a feature.
    For example, the ages of students in Data 100\. This convention allows us to easily
    transfer our previous work in DataFrames over to this new linear algebra perspective.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾设计矩阵中发生的情况：每一行代表一个单独的观测。例如，数据100中的一个学生。每一列代表一个特征。例如，数据100中学生的年龄。这个约定使我们能够轻松地将我们在数据框中的先前工作转移到这种新的线性代数视角。
- en: '| ![row_col](../Images/a5e2159bef9d82fd836e8d5f1f57d8d7.png) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| ![row_col](../Images/a5e2159bef9d82fd836e8d5f1f57d8d7.png) |'
- en: 'The multiple linear regression model can then be restated in terms of matrices:
    \[ \Large \mathbb{\hat{Y}} = \mathbb{X} \theta \]'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 多元线性回归模型可以用矩阵的术语重新表述：\[ \Large \mathbb{\hat{Y}} = \mathbb{X} \theta \]
- en: Here, \(\mathbb{\hat{Y}}\) is the **prediction vector** with \(n\) elements
    (\(\mathbb{\hat{Y}} \in \mathbb{R}^{n}\)); it contains the prediction made by
    the model for each of the \(n\) input observations. \(\mathbb{X}\) is the **design
    matrix** with dimensions \(\mathbb{X} \in \mathbb{R}^(n \times (p + 1))\), and
    \(\theta\) is the **parameter vector** with dimensions \(\theta \in \mathbb{R}^{(p
    + 1)}\)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，\(\mathbb{\hat{Y}}\)是具有\(n\)个元素的**预测向量**（\(\mathbb{\hat{Y}} \in \mathbb{R}^{n}\)）；它包含模型对每个\(n\)个输入观测的预测。
    \(\mathbb{X}\)是维度为\(\mathbb{X} \in \mathbb{R}^(n \times (p + 1))\)的**设计矩阵**，\(\theta\)是维度为\(\theta
    \in \mathbb{R}^{(p + 1)}\)的**参数向量**。
- en: 'As a refresher, let’s also review the **dot product (or inner product)**. This
    is a vector operation that:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个复习，让我们也回顾一下**点积（或内积）**。这是一个向量运算，它：
- en: Can only be carried out on two vectors of the **same length**
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只能在两个**相同长度**的向量上进行
- en: Sums up the products of the corresponding entries of the two vectors
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对应向量的乘积求和
- en: Returns a single number
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回一个单一的数字
- en: 'While this is not in scope, note that we can also interpret the dot product
    geometrically:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不在范围内，但请注意我们也可以几何地解释点积：
- en: 'It is the product of three things: the **magnitude** of both vectors, and the
    **cosine** of the angles between them: \[\vec{u} \cdot \vec{v} = ||\vec{u}|| \cdot
    ||\vec{v}|| \cdot {cos \theta}\]'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是三个因素的乘积：两个向量的**大小**和它们之间的**角度**的**余弦**：\[\vec{u} \cdot \vec{v} = ||\vec{u}||
    \cdot ||\vec{v}|| \cdot {cos \theta}\]
- en: 12.5 Mean Squared Error
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.5均方误差
- en: We now have a new approach to understanding models in terms of vectors and matrices.
    To accompany this new convention, we should update our understanding of risk functions
    and model fitting.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个新的理解模型的方法，以向量和矩阵为基础。为了配合这个新的约定，我们应该更新我们对风险函数和模型拟合的理解。
- en: 'Recall our definition of MSE: \[R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i -
    \hat{y}_i)^2\]'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们对MSE的定义：\[R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\]
- en: At its heart, the MSE is a measure of *distance* – it gives an indication of
    how “far away” the predictions are from the true values, on average.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上，MSE是一种*距离*的度量 - 它指示了预测值与真实值之间的“距离”平均有多远。
- en: 'When working with vectors, this idea of “distance” or the vector’s **size/length**
    is represented by the **norm**. More precisely, the distance between two vectors
    \(\vec{a}\) and \(\vec{b}\) can be expressed as: \[||\vec{a} - \vec{b}||_2 = \sqrt{(a_1
    - b_1)^2 + (a_2 - b_2)^2 + \ldots + (a_n - b_n)^2} = \sqrt{\sum_{i=1}^n (a_i -
    b_i)^2}\]'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理向量时，这种“距离”或向量的**大小/长度**的概念由**范数**表示。更确切地说，向量\(\vec{a}\)和\(\vec{b}\)之间的距离可以表示为：\[||\vec{a}
    - \vec{b}||_2 = \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \ldots + (a_n - b_n)^2}
    = \sqrt{\sum_{i=1}^n (a_i - b_i)^2}\]
- en: The double bars are mathematical notation for the norm. The subscript 2 indicates
    that we are computing the L2, or squared norm.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 双竖线是范数的数学表示。下标2表示我们正在计算L2范数，或平方范数。
- en: The two norms we need to know for Data 100 are the L1 and L2 norms (sound familiar?).
    In this note, we’ll focus on L2 norm. We’ll dive into L1 norm in future lectures.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要了解Data 100的两种范数是L1和L2范数（听起来熟悉吗？）。在这篇笔记中，我们将专注于L2范数。我们将在未来的讲座中深入探讨L1范数。
- en: For the n-dimensional vector \[\vec{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots
    \\ x_n \end{bmatrix}\], the **L2 vector norm** is
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于n维向量\[\vec{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}\]，**L2向量范数**是
- en: \[||\vec{x}||_2 = \sqrt{(x_1)^2 + (x_2)^2 + \ldots + (x_n)^2} = \sqrt{\sum_{i=1}^n
    (x_i)^2}\]
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: \[||\vec{x}||_2 = \sqrt{(x_1)^2 + (x_2)^2 + \ldots + (x_n)^2} = \sqrt{\sum_{i=1}^n
    (x_i)^2}\]
- en: The L2 vector norm is a generalization of the Pythagorean theorem in \(n\) dimensions.
    Thus, it can be used as a measure of the **length** of a vector or even as a measure
    of the **distance** between two vectors.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: L2向量范数是\(n\)维中勾股定理的推广。因此，它可以用作矢量的**长度**的度量，甚至可以用作两个矢量之间的**距离**的度量。
- en: 'We can express the MSE as a squared L2 norm if we rewrite it in terms of the
    prediction vector, \(\hat{\mathbb{Y}}\), and true target vector, \(\mathbb{Y}\):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将MSE表示为平方L2范数，如果我们用预测向量\(\hat{\mathbb{Y}}\)和真实目标向量\(\mathbb{Y}\)来重新表达它：
- en: \[R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n} ||\mathbb{Y}
    - \hat{\mathbb{Y}}||_2^2\]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: \[R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n} ||\mathbb{Y}
    - \hat{\mathbb{Y}}||_2^2\]
- en: 'Here, the superscript 2 outside of the norm double bars means that we are *squaring*
    the norm. If we plug in our linear model \(\hat{\mathbb{Y}} = \mathbb{X} \theta\),
    we find the MSE cost function in vector notation:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，范数双条之外的上标2表示我们正在*平方*范数。如果我们插入我们的线性模型\(\hat{\mathbb{Y}} = \mathbb{X} \theta\)，我们会发现MSE成本函数的向量表示：
- en: \[R(\theta) = \frac{1}{n} ||\mathbb{Y} - \mathbb{X} \theta||_2^2\]
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: \[R(\theta) = \frac{1}{n} ||\mathbb{Y} - \mathbb{X} \theta||_2^2\]
- en: Under the linear algebra perspective, our new task is to fit the optimal parameter
    vector \(\theta\) such that the cost function is minimized. Equivalently, we wish
    to minimize the norm \[||\mathbb{Y} - \mathbb{X} \theta||_2 = ||\mathbb{Y} - \hat{\mathbb{Y}}||_2.\]
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数的视角下，我们的新任务是拟合最佳参数向量\(\theta\)，使得成本函数最小化。等价地，我们希望最小化范数\[||\mathbb{Y} -
    \mathbb{X} \theta||_2 = ||\mathbb{Y} - \hat{\mathbb{Y}}||_2.\]
- en: 'We can restate this goal in two ways:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用两种方式重新表述这个目标：
- en: Minimize the **distance** between the vector of true values, \(\mathbb{Y}\),
    and the vector of predicted values, \(\mathbb{\hat{Y}}\)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化真实值向量\(\mathbb{Y}\)和预测值向量\(\mathbb{\hat{Y}}\)之间的**距离**
- en: 'Minimize the **length** of the **residual vector**, defined as: \[e = \mathbb{Y}
    - \mathbb{\hat{Y}} = \begin{bmatrix} y_1 - \hat{y}_1 \\ y_2 - \hat{y}_2 \\ \vdots
    \\ y_n - \hat{y}_n \end{bmatrix}\]'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化**残差向量**的**长度**，定义为：\[e = \mathbb{Y} - \mathbb{\hat{Y}} = \begin{bmatrix}
    y_1 - \hat{y}_1 \\ y_2 - \hat{y}_2 \\ \vdots \\ y_n - \hat{y}_n \end{bmatrix}\]
- en: 12.6 Geometric Perspective
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.6 几何视角
- en: To derive the best parameter vector to meet this goal, we can turn to the geometric
    properties of our modeling setup.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得出最佳参数向量以实现这一目标，我们可以利用我们建模设置的几何特性。
- en: Up until now, we’ve mostly thought of our model as a scalar product between
    horizontally stacked observations and the parameter vector. We can also think
    of \(\hat{\mathbb{Y}}\) as a **linear combination of feature vectors**, scaled
    by the **parameters**. We use the notation \(\mathbb{X}_{:, i}\) to denote the
    \(i\)th column of the design matrix. You can think of this as following the same
    convention as used when calling `.iloc` and `.loc`. “:” means that we are taking
    all entries in the \(i\)th column.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们大多把我们的模型看作是观测值和参数向量水平堆叠的标量积。我们也可以将\(\hat{\mathbb{Y}}\)看作是特征向量的**线性组合**，由**参数**缩放。我们使用符号\(\mathbb{X}_{:,
    i}\)来表示设计矩阵的第\(i\)列。您可以将其视为在调用`.iloc`和`.loc`时使用的相同约定。“:”表示我们正在取第\(i\)列中的所有条目。
- en: '| ![columns](../Images/74e86045d3dd8feda6f41044271c9418.png) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| ![columns](../Images/74e86045d3dd8feda6f41044271c9418.png) |'
- en: \[ \hat{\mathbb{Y}} = \theta_0 \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}
    + \theta_1 \begin{bmatrix} x_{11} \\ x_{21} \\ \vdots \\ x_{n1} \end{bmatrix}
    + \ldots + \theta_p \begin{bmatrix} x_{1p} \\ x_{2p} \\ \vdots \\ x_{np} \end{bmatrix}
    = \theta_0 \mathbb{X}_{:,\:1} + \theta_1 \mathbb{X}_{:,\:2} + \ldots + \theta_p
    \mathbb{X}_{:,\:p+1}\]
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\mathbb{Y}} = \theta_0 \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}
    + \theta_1 \begin{bmatrix} x_{11} \\ x_{21} \\ \vdots \\ x_{n1} \end{bmatrix}
    + \ldots + \theta_p \begin{bmatrix} x_{1p} \\ x_{2p} \\ \vdots \\ x_{np} \end{bmatrix}
    = \theta_0 \mathbb{X}_{:,\:1} + \theta_1 \mathbb{X}_{:,\:2} + \ldots + \theta_p
    \mathbb{X}_{:,\:p+1}\]
- en: This new approach is useful because it allows us to take advantage of the properties
    of linear combinations.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这种新方法很有用，因为它使我们能够利用线性组合的性质。
- en: Recall that the **span** or **column space** of a matrix \(\mathbb{X}\) (denoted
    \(span(\mathbb{X})\)) is the set of all possible linear combinations of the matrix’s
    columns. In other words, the span represents every point in space that could possibly
    be reached by adding and scaling some combination of the matrix columns. Additionally,
    if each column of \(\mathbb{X}\) has length \(n\), \(span(\mathbb{X})\) is a subspace
    of \(\mathbb{R}^{n}\).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下矩阵\(\mathbb{X}\)的**范围**或**列空间**（表示为\(span(\mathbb{X})\)）是矩阵列的所有可能线性组合的集合。换句话说，范围代表着可能通过添加和缩放矩阵列的某些组合到达的空间中的每一点。另外，如果\(\mathbb{X}\)的每一列的长度为\(n\)，\(span(\mathbb{X})\)是\(\mathbb{R}^{n}\)的子空间。
- en: Because the prediction vector, \(\hat{\mathbb{Y}} = \mathbb{X} \theta\), is
    a **linear combination** of the columns of \(\mathbb{X}\), we know that the **predictions
    are contained in the span of \(\mathbb{X}\)**. That is, we know that \(\mathbb{\hat{Y}}
    \in \text{Span}(\mathbb{X})\).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因为预测向量\(\hat{\mathbb{Y}} = \mathbb{X} \theta\)是\(\mathbb{X}\)的列的线性组合，我们知道**预测包含在\(\mathbb{X}\)的范围内**。也就是说，我们知道\(\mathbb{\hat{Y}}
    \in \text{Span}(\mathbb{X})\)。
- en: The diagram below is a simplified view of \(\text{Span}(\mathbb{X})\), assuming
    that each column of \(\mathbb{X}\) has length \(n\). Notice that the columns of
    \(\mathbb{X}\) define a subspace of \(\mathbb{R}^n\), where each point in the
    subspace can be reached by a linear combination of \(\mathbb{X}\)’s columns. The
    prediction vector \(\mathbb{\hat{Y}}\) lies somewhere in this subspace.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图是对\(\text{Span}(\mathbb{X})\)的简化视图，假设\(\mathbb{X}\)的每一列都有长度\(n\)。注意\(\mathbb{X\)的列定义了\(\mathbb{R}^n\)的子空间，子空间中的每个点都可以通过\(\mathbb{X}\)的列的线性组合到达。预测向量\(\mathbb{\hat{Y}}\)位于这个子空间的某个位置。
- en: '| ![span](../Images/81320b1542a3c56472f8f9b54d009ccc.png) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ![span](../Images/81320b1542a3c56472f8f9b54d009ccc.png) |'
- en: 'Examining this diagram, we find a problem. The vector of true values, \(\mathbb{Y}\),
    could theoretically lie *anywhere* in \(\mathbb{R}^n\) space – its exact location
    depends on the data we collect out in the real world. However, our multiple linear
    regression model can only make predictions in the subspace of \(\mathbb{R}^n\)
    spanned by \(\mathbb{X}\). Remember the model fitting goal we established in the
    previous section: we want to generate predictions such that the distance between
    the vector of true values, \(\mathbb{Y}\), and the vector of predicted values,
    \(\mathbb{\hat{Y}}\), is minimized. This means that **we want \(\mathbb{\hat{Y}}\)
    to be the vector in \(\text{Span}(\mathbb{X})\) that is closest to \(\mathbb{Y}\)**.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 检查这个图，我们发现了一个问题。真实值向量\(\mathbb{Y}\)理论上可以位于\(\mathbb{R}^n\)空间中的*任何位置*——它的确切位置取决于我们在现实世界中收集的数据。然而，我们的多元线性回归模型只能在\(\mathbb{X}\)张成的\(\mathbb{R}^n\)空间的子空间中进行预测。记住我们在前一节建立的模型拟合目标：我们希望生成预测，使得真实值向量\(\mathbb{Y}\)和预测值向量\(\mathbb{\hat{Y}}\)之间的距离最小化。这意味着**我们希望\(\mathbb{\hat{Y}}\)是\(\text{Span}(\mathbb{X})\)中离\(\mathbb{Y}\)最近的向量**。
- en: Another way of rephrasing this goal is to say that we wish to minimize the length
    of the residual vector \(e\), as measured by its \(L_2\) norm.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种重新表述这个目标的方式是，我们希望最小化残差向量\(e\)的长度，即其\(L_2\)范数。
- en: '| ![residual](../Images/f693c4ac3d1e4a696594cecb76c3902b.png) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ![residual](../Images/f693c4ac3d1e4a696594cecb76c3902b.png) |'
- en: The vector in \(\text{Span}(\mathbb{X})\) that is closest to \(\mathbb{Y}\)
    is always the **orthogonal projection** of \(\mathbb{Y}\) onto \(\text{Span}(\mathbb{X}).\)
    Thus, we should choose the parameter vector \(\theta\) that makes the **residual
    vector orthogonal to any vector in \(\text{Span}(\mathbb{X})\)**. You can visualize
    this as the vector created by dropping a perpendicular line from \(\mathbb{Y}\)
    onto the span of \(\mathbb{X}\).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在\(\text{Span}(\mathbb{X})\)中距离\(\mathbb{Y}\)最近的向量始终是\(\mathbb{Y}\)在\(\text{Span}(\mathbb{X})\)上的**正交投影**。因此，我们应该选择参数向量\(\theta\)，使得**残差向量与\(\text{Span}(\mathbb{X})\)中的任何向量正交**。你可以将这个想象成从\(\mathbb{Y}\)到\(\mathbb{X}\)的跨度上垂直投影线创建的向量。
- en: 'How does this help us identify the optimal parameter vector, \(\hat{\theta}\)?
    Recall that two vectors \(a\) and \(b\) are orthogonal if their dot product is
    zero: \({a}^{T}b = 0\). A vector \(v\) is **orthogonal** to the span of a matrix
    \(M\) if and only if \(v\) is orthogonal to **each column** in \(M\). Put together,
    a vector \(v\) is orthogonal to \(\text{Span}(M)\) if:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这如何帮助我们确定最佳参数向量\(\hat{\theta}\)？回想一下，如果两个向量\(a\)和\(b\)正交，它们的点积为零：\({a}^{T}b
    = 0\)。如果向量\(v\)正交于矩阵\(M\)的张成空间，当且仅当\(v\)正交于\(M\)中的**每一列**。综合起来，向量\(v\)对于\(\text{Span}(M)\)正交，如果：
- en: \[M^Tv = \vec{0}\]
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: \[M^Tv = \vec{0}\]
- en: Note that \(\vec{0}\) represents the **zero vector**, a \(d\)-length vector
    full of 0s.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，\(\vec{0}\)代表**零向量**，一个全为0的\(d\)长度向量。
- en: Remember our goal is to find \(\hat{\theta}\) such that we minimize the objective
    function \(R(\theta)\). Equivalently, this is the \(\hat{\theta}\) such that the
    residual vector \(e = \mathbb{Y} - \mathbb{X} \theta\) is orthogonal to \(\text{Span}(\mathbb{X})\).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们的目标是找到\(\hat{\theta}\)，使得我们最小化目标函数\(R(\theta)\)。等价地，这就是使得残差向量\(e = \mathbb{Y}
    - \mathbb{X} \theta\)与\(\text{Span}(\mathbb{X})\)正交的\(\hat{\theta}\)。
- en: 'Looking at the definition of orthogonality of \(\mathbb{Y} - \mathbb{X}\hat{\theta}\)
    to \(span(\mathbb{X})\) (0 is the \(\vec{0}\) vector), we can write: \[\mathbb{X}^T
    (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \vec{0}\]'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 观察\(\mathbb{Y} - \mathbb{X}\hat{\theta}\)与\(span(\mathbb{X})\)正交的定义（0是\(\vec{0}\)向量），我们可以写成：\[\mathbb{X}^T
    (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \vec{0}\]
- en: 'Let’s then rearrange the terms: \[\mathbb{X}^T \mathbb{Y} - \mathbb{X}^T \mathbb{X}
    \hat{\theta} = \vec{0}\]'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们重新排列项：\[\mathbb{X}^T \mathbb{Y} - \mathbb{X}^T \mathbb{X} \hat{\theta} =
    \vec{0}\]
- en: 'And finally, we end up with the **normal equation**: \[\mathbb{X}^T \mathbb{X}
    \hat{\theta} = \mathbb{X}^T \mathbb{Y}\]'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到了**正规方程**：\[\mathbb{X}^T \mathbb{X} \hat{\theta} = \mathbb{X}^T \mathbb{Y}\]
- en: Any vector \(\theta\) that minimizes MSE on a dataset must satisfy this equation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 任何最小化数据集上均方误差的向量\(\theta\)必须满足这个方程。
- en: 'If \(\mathbb{X}^T \mathbb{X}\) is invertible, we can conclude: \[\hat{\theta}
    = (\mathbb{X}^T \mathbb{X})^{-1} \mathbb{X}^T \mathbb{Y}\]'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果\(\mathbb{X}^T \mathbb{X}\)是可逆的，我们可以得出结论：\[\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1}
    \mathbb{X}^T \mathbb{Y}\]
- en: 'This is called the **least squares estimate** of \(\theta\): it is the value
    of \(\theta\) that minimizes the squared loss.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为\(\theta\)的**最小二乘估计**：它是使平方损失最小化的\(\theta\)的值。
- en: Note that the least squares estimate was derived under the assumption that \(\mathbb{X}^T
    \mathbb{X}\) is *invertible*. This condition holds true when \(\mathbb{X}^T \mathbb{X}\)
    is full column rank, which, in turn, happens when \(\mathbb{X}\) is full column
    rank. We will explore the consequences of this fact in lab and homework.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，最小二乘估计是在假设\(\mathbb{X}^T \mathbb{X}\)是*可逆*的条件下推导出来的。当\(\mathbb{X}^T \mathbb{X}\)是满列秩时，这个条件成立，而这又发生在\(\mathbb{X}\)是满列秩时。我们将在实验和作业中探讨这个事实的后果。
- en: 12.7 Evaluating Model Performance
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.7 评估模型性能
- en: Our geometric view of multiple linear regression has taken us far! We have identified
    the optimal set of parameter values to minimize MSE in a model of multiple features.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对多元线性回归的几何视图已经有了很大的进展！我们已经确定了最小化多个特征模型中的均方误差的参数值的最佳集合。
- en: Now, we want to understand how well our fitted model performs. One measure of
    model performance is the **Root Mean Squared Error**, or RMSE. The RMSE is simply
    the square root of MSE. Taking the square root converts the value back into the
    original, non-squared units of \(y_i\), which is useful for understanding the
    model’s performance. A low RMSE indicates more “accurate” predictions – that there
    is a lower average loss across the dataset.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要了解我们的拟合模型的表现如何。模型性能的一个度量是**均方根误差**，即RMSE。RMSE只是MSE的平方根。取平方根将值转换回\(y_i\)的原始、非平方单位，这对于理解模型的性能很有用。较低的RMSE表示更“准确”的预测-在整个数据集中有更低的平均损失。
- en: \[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}\]
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}\]
- en: When working with SLR, we generated plots of the residuals against a single
    feature to understand the behavior of residuals. When working with several features
    in multiple linear regression, it no longer makes sense to consider a single feature
    in our residual plots. Instead, multiple linear regression is evaluated by making
    plots of the residuals against the predicted values. As was the case with SLR,
    a multiple linear model performs well if its residual plot shows no patterns.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理SLR时，我们生成了残差与单个特征的图表，以了解残差的行为。在多元线性回归中使用多个特征时，考虑在残差图中只有一个特征不再有意义。相反，多元线性回归通过制作残差与预测值的图表来进行评估。与SLR一样，如果多元线性模型的残差图没有模式，则表现良好。
- en: '| ![residual_plot](../Images/81a01631c2d875fcd03ab4ce6e93d581.png) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| ![residual_plot](../Images/81a01631c2d875fcd03ab4ce6e93d581.png) |'
- en: For SLR, we used the correlation coefficient to capture the association between
    the target variable and a single feature variable. In a multiple linear model
    setting, we will need a performance metric that can account for multiple features
    at once. **Multiple \(R^2\)**, also called the **coefficient of determination**,
    is the **proportion of variance** of our **fitted values** (predictions) \(\hat{y}_i\)
    to our true values \(y_i\). It ranges from 0 to 1 and is effectively the *proportion*
    of variance in the observations the **model explains**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SLR，我们使用相关系数来捕捉目标变量和单个特征变量之间的关联。在多元线性模型设置中，我们将需要一个性能度量，可以同时考虑多个特征。**多元\(R^2\)**，也称为**决定系数**，是我们的**拟合值**（预测）\(\hat{y}_i\)的**方差比例**到真实值\(y_i\)。它的范围从0到1，实际上是模型解释观察中方差的*比例*。
- en: \[R^2 = \frac{\text{variance of } \hat{y}_i}{\text{variance of } y_i} = \frac{\sigma^2_{\hat{y}}}{\sigma^2_y}\]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: \[R^2 = \frac{\text{variance of } \hat{y}_i}{\text{variance of } y_i} = \frac{\sigma^2_{\hat{y}}}{\sigma^2_y}\]
- en: Note that for OLS with an intercept term, for example \(\hat{y} = \theta_0 +
    \theta_1x_1 + \theta_2x_2 + \cdots + \theta_px_p\), \(\mathbb{R}^2\) is equal
    to the square of the correlation between \(y\) and \(\hat{y}\). On the other hand
    for SLR, \(\mathbb{R}^2\) is equal to \(r^2\), the correlation between \(x\) and
    \(y\). The proof of these last two properties is out of scope for this course.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于具有截距项的OLS，例如\(\hat{y} = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots
    + \theta_px_p\)，\(\mathbb{R}^2\)等于\(y\)和\(\hat{y}\)之间的相关性的平方。另一方面，对于SLR，\(\mathbb{R}^2\)等于\(r^2\)，即\(x\)和\(y\)之间的相关性。这两个属性的证明超出了本课程的范围。
- en: Additionally, as we add more features, our fitted values tend to become closer
    and closer to our actual values. Thus, \(\mathbb{R}^2\) increases.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着我们添加更多的特征，我们的拟合值倾向于越来越接近我们的实际值。因此，\(\mathbb{R}^2\)增加。
- en: Adding more features doesn’t always mean our model is better though! We’ll see
    why later in the course.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，增加更多的特征并不总是意味着我们的模型更好！我们将在课程后面看到原因。
- en: 12.8 OLS Properties
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.8 OLS属性
- en: When using the optimal parameter vector, our residuals \(e = \mathbb{Y} - \hat{\mathbb{Y}}\)
    are orthogonal to \(span(\mathbb{X})\).
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最优参数向量时，我们的残差\(e = \mathbb{Y} - \hat{\mathbb{Y}}\)与\(span(\mathbb{X})\)正交。
- en: \[\mathbb{X}^Te = 0 \]
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathbb{X}^Te = 0 \]
- en: '*Proof:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：'
- en: The optimal parameter vector, \(\hat{\theta}\), solves the normal equations
    \(\implies \hat{\theta} = \mathbb{X}^T\mathbb{X}^{-1}\mathbb{X}^T\mathbb{Y}\)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最优参数向量\(\hat{\theta}\)解决了正规方程\(\implies \hat{\theta} = \mathbb{X}^T\mathbb{X}^{-1}\mathbb{X}^T\mathbb{Y}\)
- en: \[\mathbb{X}^Te = \mathbb{X}^T (\mathbb{Y} - \mathbb{\hat{Y}}) \]
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathbb{X}^Te = \mathbb{X}^T (\mathbb{Y} - \mathbb{\hat{Y}}) \]
- en: \[\mathbb{X}^T (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \mathbb{X}^T\mathbb{Y}
    - \mathbb{X}^T\mathbb{X}\hat{\theta}\]
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathbb{X}^T (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \mathbb{X}^T\mathbb{Y}
    - \mathbb{X}^T\mathbb{X}\hat{\theta}\]
- en: Any matrix multiplied with its own inverse is the identity matrix \(\mathbb{I}\)
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何矩阵与其逆矩阵相乘都是单位矩阵\(\mathbb{I}\)
- en: \[\mathbb{X}^T\mathbb{Y} - (\mathbb{X}^T\mathbb{X})(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}
    = \mathbb{X}^T\mathbb{Y} - \mathbb{X}^T\mathbb{Y} = 0\]*  *2.  For all linear
    models with an **intercept term**, the **sum of residuals is zero**.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathbb{X}^T\mathbb{Y} - (\mathbb{X}^T\mathbb{X})(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}
    = \mathbb{X}^T\mathbb{Y} - \mathbb{X}^T\mathbb{Y} = 0\]*  *2. 对于所有具有**截距项**的线性模型，**残差的总和为零**。
- en: \[\sum_i^n e_i = 0\]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: \[\sum_i^n e_i = 0\]
- en: '*Proof:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：'
- en: For all linear models with an **intercept term**, the average of the predicted
    \(y\) values is equal to the average of the true \(y\) values. \[\bar{y} = \bar{\hat{y}}\]
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有具有**截距项**的线性模型，预测的\(y\)值的平均值等于真实\(y\)值的平均值。\[\bar{y} = \bar{\hat{y}}\]
- en: Rewriting the sum of residuals as two separate sums, \[\sum_i^n e_i = \sum_i^n
    y_i - \sum_i^n\hat{y}_i\]
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将残差总和重写为两个单独的总和，\[\sum_i^n e_i = \sum_i^n y_i - \sum_i^n\hat{y}_i\]
- en: Each respective sum is a multiple of the average of the sum. \[\sum_i^n e_i
    = n\bar{y} - n\bar{y} = n(\bar{y} - \bar{y}) = 0\]*  *3.  The Least Squares estimate
    \(\hat{\theta}\) is **unique** if and only if \(\mathbb{X}\) is **full column
    rank**.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个相应的和是平均和的倍数。\[\sum_i^n e_i = n\bar{y} - n\bar{y} = n(\bar{y} - \bar{y}) =
    0\]*  *3. 最小二乘估计\(\hat{\theta}\)是**唯一**的，当且仅当\(\mathbb{X}\)是**满列秩**的。
- en: '*Proof:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：'
- en: We know the solution to the normal equation \(\mathbb{X}^T\mathbb{X}\hat{\theta}
    = \mathbb{Y}\) is the least square estimate that fulfills the prior equality.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们知道正规方程\(\mathbb{X}^T\mathbb{X}\hat{\theta} = \mathbb{Y}\)的解是满足先前相等的最小二乘估计。
- en: \(\hat{\theta}\) has a **unique** solution \(\iff\) the square matrix \(\mathbb{X}^T\mathbb{X}\)
    is **invertible**.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\hat{\theta}\) 有一个**唯一**的解 \(\iff\) 方阵 \(\mathbb{X}^T\mathbb{X}\) 是**可逆**的。
- en: The **column** rank of a square matrix is the number of linearly independent
    columns it contains.
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方阵的**列**秩是它包含的线性独立列的数量。
- en: An \(n\) x \(n\) square matrix is deemed full column rank when all of its columns
    are linearly independent. That is, its rank would be equal to \(n\).
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 \(n\) x \(n\) 的方阵被认为是完整的列秩当且仅当它的所有列都是线性独立的。也就是说，它的秩等于 \(n\)。
- en: \(\mathbb{X}^T\mathbb{X}\) has shape \((p + 1) \times (p + 1)\), and therefore
    has max rank \(p + 1\).
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mathbb{X}^T\mathbb{X}\) 的形状是 \((p + 1) \times (p + 1)\)，因此最大秩为 \(p + 1\)。
- en: \(rank(\mathbb{X}^T\mathbb{X})\) = \(rank(\mathbb{X})\) (proof out of scope).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(rank(\mathbb{X}^T\mathbb{X})\) = \(rank(\mathbb{X})\)（证明超出范围）。
- en: 'Therefore, \(\mathbb{X}^T\mathbb{X}\) has rank \(p + 1\) \(\iff\) \(\mathbb{X}\)
    has rank \(p + 1\) \(\iff \mathbb{X}\) is full column rank.*  *To summarize:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，\(\mathbb{X}^T\mathbb{X}\) 的秩为 \(p + 1\) \(\iff\) \(\mathbb{X}\) 的秩为 \(p
    + 1\) \(\iff \mathbb{X}\) 是完整的列秩。* *总结：
- en: '|  | Model | Estimate | Unique? |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 | 估计 | 唯一？ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Constant Model + MSE | \(\hat{y} = \theta_0\) | \(\hat{\theta_0} = mean(y)
    = \bar{y}\) | **Yes**. Any set of values has a unique mean. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 常数模型 + MSE | \(\hat{y} = \theta_0\) | \(\hat{\theta_0} = mean(y) = \bar{y}\)
    | **是**。任何一组值都有唯一的均值。 |'
- en: '| Constant Model + MAE | \(\hat{y} = \theta_0\) | \(\hat{\theta_0} = median(y)\)
    | **Yes**, if odd. **No**, if even. Return the average of the middle 2 values.
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 常数模型 + MAE | \(\hat{y} = \theta_0\) | \(\hat{\theta_0} = median(y)\) | **是**，如果是奇数。**否**，如果是偶数。返回中间2个值的平均值。
    |'
- en: '| Simple Linear Regression + MSE | \(\hat{y} = \theta_0 + \theta_1x\) | \(\hat{\theta_0}
    = \bar{y} - \hat{\theta_1}\hat{x}\) \(\hat{\theta_1} = r\frac{\sigma_y}{\sigma_x}\)
    | **Yes**. Any set of non-constant* values has a unique mean, SD, and correlation
    coefficient. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 简单线性回归 + MSE | \(\hat{y} = \theta_0 + \theta_1x\) | \(\hat{\theta_0} = \bar{y}
    - \hat{\theta_1}\hat{x}\) \(\hat{\theta_1} = r\frac{\sigma_y}{\sigma_x}\) | **是**。任何一组非常数*值都有唯一的均值、标准差和相关系数。
    |'
- en: '| **OLS** (Linear Model + MSE) | \(\mathbb{\hat{Y}} = \mathbb{X}\mathbb{\theta}\)
    | \(\hat{\theta} = \mathbb{X}^T\mathbb{X}^{-1}\mathbb{X}^T\mathbb{Y}\) | **Yes**,
    if \(\mathbb{X}\) is full column rank (all columns are linearly independent, #
    of datapoints >>> # of features). |*****'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '| **OLS**（线性模型 + MSE） | \(\mathbb{\hat{Y}} = \mathbb{X}\mathbb{\theta}\) |
    \(\hat{\theta} = \mathbb{X}^T\mathbb{X}^{-1}\mathbb{X}^T\mathbb{Y}\) | **是**，如果
    \(\mathbb{X}\) 是完整的列秩（所有列线性独立，数据点的数量 >>> 特征的数量）。 |'
