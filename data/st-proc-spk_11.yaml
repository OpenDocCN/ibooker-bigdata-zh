- en: Chapter 7\. Introducing Structured Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。介绍结构化流处理
- en: 'In data-intensive enterprises, we find many large datasets: log files from
    internet-facing servers, tables of shopping behavior, and NoSQL databases with
    sensor data, just to name a few examples. All of these datasets share the same
    fundamental life cycle: They started out empty at some point in time and were
    progressively filled by arriving data points that were directed to some form of
    secondary storage. This process of data arrival is nothing more than a *data stream*
    being materialized onto secondary storage. We can then apply our favorite analytics
    tools on those datasets *at rest*, using techniques known as *batch processing*
    because they take large chunks of data at once and usually take considerable amounts
    of time to complete, ranging from minutes to days.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据密集型企业中，我们发现许多大型数据集：来自面向互联网服务器的日志文件、购物行为表以及带有传感器数据的NoSQL数据库，这些都是一些例子。所有这些数据集共享相同的基本生命周期：它们在某个时间点为空，并逐渐被到达的数据点填充，这些数据点被导向某种形式的次要存储。这个数据到达的过程不过是将*数据流*实体化到次要存储上。然后，我们可以使用我们喜爱的分析工具对这些*静态*数据集进行分析，使用称为*批处理*的技术，因为它们一次处理大块数据，并且通常需要大量时间来完成，从几分钟到几天不等。
- en: The `Dataset` abstraction in *Spark SQL* is one such way of analyzing data at
    rest. It is particularly useful for data that is *structured* in nature; that
    is, it follows a defined schema. The `Dataset` API in Spark combines the expressivity
    of a SQL-like API with type-safe collection operations that are reminiscent of
    the Scala collections and the `Resilient Distributed Dataset` (RDD) programming
    model. At the same time, the `Dataframe` API, which is in nature similar to Python
    Pandas and R Dataframes, widens the audience of Spark users beyond the initial
    core of data engineers who are used to developing in a functional paradigm. This
    higher level of abstraction is intended to support modern data engineering and
    data science practices by enabling a wider range of professionals to jump onto
    the big data analytics train using a familiar API.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Spark SQL*中的`Dataset`抽象是分析静态数据的一种方式。它特别适用于*结构化*的数据；也就是说，它遵循了定义好的模式。Spark中的`Dataset`
    API结合了类似SQL的API的表达能力和类型安全的集合操作，这些操作类似于Scala集合和`Resilient Distributed Dataset`（RDD）编程模型。同时，`Dataframe`
    API与Python Pandas和R Dataframes类似，扩展了Spark用户的范围，超越了最初的数据工程师核心，这些人习惯于在函数范式中开发。这种更高级的抽象旨在通过使用熟悉的API，支持现代数据工程和数据科学实践，让更广泛的专业人士加入到大数据分析的行列中。
- en: What if, instead of having to wait for the data to "*settle down*,” we could
    apply the same `Dataset` concepts to the data while it is in its original stream
    form?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不必等待数据“*安定下来*”，而是可以在数据保持原始流形式时应用相同的`Dataset`概念，会怎样？
- en: 'The Structured Streaming model is an extension of the `Dataset` SQL-oriented
    model to handle data on the move:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流模型是`Dataset`SQL导向模型的扩展，用于处理运动中的数据：
- en: The data arrives from a *source* stream and is assumed to have a defined schema.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据来自*source*流，并假定具有定义的模式。
- en: The stream of events can be seen as rows that are appended to an unbounded table.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件流可以看作是附加到无界表的行。
- en: To obtain results from the stream, we express our computation as queries over
    that table.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要从流中获取结果，我们将计算表达为对该表的查询。
- en: By continuously applying the same query to the updating table, we create an
    output stream of processed events.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过不断地将相同查询应用于更新的表，我们创建一个处理后事件的输出流。
- en: The resulting events are offered to an output *sink*.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果事件被提供给一个输出*sink*。
- en: The *sink* could be a storage system, another streaming backend, or an application
    ready to consume the processed data.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sink*可以是存储系统、另一个流后端或准备消耗处理后数据的应用程序。'
- en: In this model, our theoretically *unbounded* table must be implemented in a
    physical system with defined resource constraints. Therefore, the implementation
    of the model requires certain considerations and restrictions to deal with a potentially
    infinite data inflow.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模型中，我们理论上*无界*的表必须在具有定义资源限制的物理系统中实现。因此，模型的实现需要考虑和限制来处理潜在的无限数据流入。
- en: To address these challenges, Structured Streaming introduces new concepts to
    the `Dataset` and `DataFrame` APIs, such as support for event time, *watermarking*,
    and different output modes that determine for how long past data is actually stored.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，结构化流引入了对`Dataset`和`DataFrame` API的新概念，例如支持事件时间、*水印*和确定存储过去数据时间长度的不同输出模式。
- en: Conceptually, the Structured Streaming model blurs the line between batch and
    streaming processing, removing a lot of the burden of reasoning about analytics
    on fast-moving data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，结构化流处理模型模糊了批处理和流处理之间的界限，大大减少了处理快速移动数据分析的负担。
- en: First Steps with Structured Streaming
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用结构化流处理的第一步
- en: In the previous section, we learned about the high-level concepts that constitute
    Structured Streaming, such as sources, sinks, and queries. We are now going to
    explore Structured Streaming from a practical perspective, using a simplified
    web log analytics use case as an example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了构成结构化流处理的高级概念，如数据源、数据汇和查询。现在，我们将从实际角度探索结构化流处理，以简化的网络日志分析用例为例进行说明。
- en: Before we begin delving into our first streaming application, we are going to
    see how classical batch analysis in Apache Spark can be applied to the same use
    case.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始深入研究我们的第一个流处理应用程序之前，我们将看看如何将 Apache Spark 中的经典批处理分析应用到相同的用例中。
- en: 'This exercise has two main goals:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习有两个主要目标：
- en: First, most, if not all, streaming data analytics start by studying a static
    data sample. It is far easier to start a study with a file of data, gain intuition
    on how the data looks, what kind of patterns it shows, and define the process
    that we require to extract the intended knowledge from that data. Typically, it’s
    only after we have defined and tested our data analytics job, that we proceed
    to transform it into a streaming process that can apply our analytic logic to
    data on the move.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，大多数，如果不是全部，流数据分析都是从研究静态数据样本开始的。从一个数据文件开始研究会更容易，可以直观地了解数据的外观，显示的模式类型以及定义我们从数据中提取所需知识的过程。通常，只有在定义和测试我们的数据分析作业之后，我们才会将其转换为可以将我们的分析逻辑应用于正在移动的数据的流处理过程。
- en: Second, from a practical perspective, we can appreciate how Apache Spark simplifies
    many aspects of transitioning from a batch exploration to a streaming application
    through the use of uniform APIs for both batch and streaming analytics.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，从实际角度来看，我们可以欣赏 Apache Spark 如何通过统一的 API 简化从批量探索到流处理应用程序的过渡的许多方面。
- en: This exploration will allow us to compare and contrast the batch and streaming
    APIs in Spark and show us the necessary steps to move from one to the other.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种探索将使我们能够比较和对比 Spark 中批处理和流处理 API，并向我们展示从一个模式转换到另一个模式所需的必要步骤。
- en: Online Resources
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线资源
- en: For this example, we use Apache Web Server logs from the public 1995 NASA Apache
    web logs, originally from [*http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html*](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此示例，我们使用来自公共 1995 年 NASA Apache 网络日志的 Apache Web 服务器日志，最初来源于[*http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html*](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html)。
- en: For the purpose of this exercise, the original log file has been split into
    daily files and each log line has been formatted as JSON. The compressed `NASA-weblogs`
    file can be downloaded from [*https://github.com/stream-processing-with-spark*](https://github.com/stream-processing-with-spark).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 出于本练习的目的，原始日志文件已被拆分为每日文件，并且每个日志行都已格式化为 JSON。压缩的 `NASA-weblogs` 文件可以从[*https://github.com/stream-processing-with-spark*](https://github.com/stream-processing-with-spark)下载。
- en: Download this dataset and place it in a folder on your computer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下载这个数据集，并将其放在计算机上的一个文件夹中。
- en: Batch Analytics
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批处理分析
- en: Given that we are working with archive log files, we have access to all of the
    data at once. Before we begin building our streaming application, let’s take a
    brief *intermezzo* to have a look at what a classical batch analytics job would
    look like.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们正在处理归档日志文件，我们可以一次性访问所有数据。在我们开始构建流处理应用程序之前，让我们简要地*插曲*一下，看看经典批量分析作业的样子。
- en: Online Resources
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线资源
- en: For this example, we will use the `batch_weblogs` notebook in the online resources
    for the book, located at [*https://github.com/stream-processing-with-spark*](https://github.com/stream-processing-with-spark)][[*https://github.com/stream-processing-with-spark*](https://github.com/stream-processing-with-spark)].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此示例，我们将在书籍的在线资源中使用 `batch_weblogs` 笔记本，位于[*https://github.com/stream-processing-with-spark*](https://github.com/stream-processing-with-spark)][[*https://github.com/stream-processing-with-spark*](https://github.com/stream-processing-with-spark)]。
- en: 'First, we load the log files, encoded as JSON, from the directory where we
    unpacked them:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从我们解压缩它们的目录中加载以 JSON 编码的日志文件：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we declare the schema of the data as a `case class` to use the typed
    `Dataset` API. Following the formal description of the dataset (at [NASA-HTTP](http://bit.ly/2YwMAhG)
    ), the log is structured as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们声明数据的模式为一个`case class`，以使用类型化的`Dataset` API。按照数据集的正式描述（在[NASA-HTTP](http://bit.ly/2YwMAhG)）中，日志结构如下：
- en: 'The logs are an ASCII file with one line per request, with the following columns:'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 日志是一个ASCII文件，每个请求一行，包含以下列：
- en: ''
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Host making the request. A hostname when possible, otherwise the Internet address
    if the name could not be looked up.
  id: totrans-34
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发出请求的主机。如果可能，是主机名，否则是互联网地址（如果未查找到名称）。
- en: ''
  id: totrans-35
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-36
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Timestamp in the format “DAY MON DD HH:MM:SS YYYY,” where DAY is the day of
    the week, MON is the name of the month, DD is the day of the month, HH:MM:SS is
    the time of day using a 24-hour clock, and YYYY is the year. The timezone is –0400.
  id: totrans-37
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间戳格式为“DAY MON DD HH:MM:SS YYYY”，其中DAY是星期几，MON是月份名称，DD是月份中的日期，HH:MM:SS是使用24小时制的时间，YYYY是年份。时区为–0400。
- en: ''
  id: totrans-38
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Request given in quotes.
  id: totrans-40
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给出的请求用引号括起来。
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-42
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: HTTP reply code.
  id: totrans-43
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP回复代码。
- en: ''
  id: totrans-44
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-45
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Bytes in the reply.
  id: totrans-46
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回复中的字节数。
- en: 'Translating that schema to Scala, we have the following `case class` definition:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将该模式转换为Scala，我们有以下`case class`定义：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We use `java.sql.Timestamp` as the type for the timestamp because it’s internally
    supported by Spark and does not require any additional `cast` that other options
    might require.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`java.sql.Timestamp`作为时间戳的类型，因为它在Spark内部得到支持，并且不需要其他选项可能需要的任何额外`cast`。
- en: 'We convert the original JSON to a typed data structure using the previous schema
    definition:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用先前的模式定义，将原始JSON转换为类型化数据结构：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we have the data in a structured format, we can begin asking the questions
    that interest us. As a first step, we would like to know how many records are
    contained in our dataset:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将数据结构化后，可以开始询问我们感兴趣的问题了。作为第一步，我们想知道我们的数据集中包含多少条记录：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A common question would be: “what was the most popular URL per day?” To answer
    that, we first reduce the timestamp to the day of the month. We then group by
    this new `dayOfMonth` column and the request URL and we count over this aggregate.
    We finally order using descending order to get the top URLs first:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的问题是：“每天最受欢迎的URL是什么？”为了回答这个问题，我们首先将时间戳缩减到月份中的某一天。然后我们按照这个新的`dayOfMonth`列和请求URL进行分组，并在此聚合上进行计数。最后我们按降序排序以获取前面的top
    URLs：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Top hits are all images. What now? It’s not unusual to see that the top URLs
    are images commonly used across a site. Our true interest lies in the content
    pages generating the most traffic. To find those, we first filter on `html` content
    and then proceed to apply the top aggregation we just learned.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 前几位的都是图片。现在怎么办？看到前几位的URL是站点上常用的图片并不罕见。我们真正感兴趣的是生成最多流量的内容页面。为了找到这些页面，我们首先过滤出`html`内容，然后继续应用我们刚学到的top聚合。
- en: 'As we can see, the request field is a quoted sequence of `[HTTP_VERB] URL [HTTP_VERSION]`.
    We will extract the URL and preserve only those ending in *.html*, *.htm*, or
    no extension (directories). This is a simplification for the purpose of this example:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，请求字段是引用的`[HTTP_VERB] URL [HTTP_VERSION]`序列。我们将提取URL，并仅保留以*.html*、*.htm*结尾或无扩展名（目录）的URL。这是为了本例的简化：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With this new dataset that contains only *.html*, *.htm*, and directories,
    we proceed to apply the same *top-k* function as earlier:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个仅包含*.html*、*.htm*和目录的新数据集，我们继续应用与之前相同的*top-k*函数：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can see that the most popular page that month was *liftoff.html*, corresponding
    to the coverage of the launch of the Discovery shuttle, as documented on the [NASA
    archives](https://go.nasa.gov/2Q9HBQX). It’s closely followed by `countdown/`,
    the days prior to the launch.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到那个月份最受欢迎的页面是*liftoff.html*，对应于探索发现航天飞机发射的报道，详细记录在[NASA档案](https://go.nasa.gov/2Q9HBQX)中。紧随其后的是`countdown/`，即发射前几天的倒计时页面。
- en: Streaming Analytics
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流分析
- en: In the previous section, we explored historical NASA web log records. We found
    trending events in those records, but much later than when the actual events happened.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们探索了历史上NASA的网络日志记录。我们发现这些记录中的趋势事件发生在实际事件之后很久。
- en: One key driver for streaming analytics comes from the increasing demand of organizations
    to have timely information that can help them make decisions at many different
    levels.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 流分析的一个关键驱动因素是组织对及时信息的增加需求，这些信息可以帮助他们在多个不同层次上做出决策。
- en: We can use the lessons that we have learned while exploring the archived records
    using a batch-oriented approach and create a streaming job that will provide us
    with trending information as it happens.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用我们在使用面向批处理方法探索归档记录时学到的经验，创建一个流作业，将在发生事件时提供趋势信息。
- en: The first difference that we observe with the batch analytics is the source
    of the data. For our streaming exercise, we will use a TCP server to simulate
    a web system that delivers its logs in real time. The simulator will use the same
    dataset but will feed it through a TCP socket connection that will embody the
    stream that we will be analyzing.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到与批量分析不同的第一个区别是数据的来源。对于我们的流处理练习，我们将使用TCP服务器来模拟一个实时传递其日志的网络系统。模拟器将使用相同的数据集，但将其通过TCP套接字连接供给，这将成为我们分析的流。
- en: Online Resources
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线资源
- en: For this example, we will use the notebooks `weblog_TCP_server` and `streaming_weblogs`
    found in the online resources for the book, located at [*https://github.com/stream-processing-with-spark*](https://github.com/stream-processing-with-spark).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此示例，我们将使用书籍在线资源中的笔记本`weblog_TCP_server`和`streaming_weblogs`，位于[*https://github.com/stream-processing-with-spark*](https://github.com/stream-processing-with-spark)。
- en: Connecting to a Stream
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接到流
- en: 'If you recall from the introduction of this chapter, Structured Streaming defines
    the concepts of sources and sinks as the key abstractions to consume a stream
    and produce a result. We are going to use the `TextSocketSource` implementation
    to connect to the server through a TCP socket. Socket connections are defined
    by the host of the server and the port where it is listening for connections.
    These two configuration elements are required to create the `socket` source:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您回顾本章的介绍，结构化流定义了源和接收端概念作为消费流和生成结果的关键抽象。我们将使用`TextSocketSource`实现来通过TCP套接字连接到服务器。套接字连接由服务器的主机和它监听连接的端口定义。这两个配置元素是创建`socket`源所必需的：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note how the creation of a stream is quite similar to the declaration of a static
    datasource in the batch case. Instead of using the `read` builder, we use the
    `readStream` construct and we pass to it the parameters required by the streaming
    source. As you will see during the course of this exercise and later on as we
    go into the details of Structured Streaming, the API is basically the same `DataFrame`
    and `Dataset` API for static data but with some modifications and limitations
    that you will learn in detail.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，创建流与静态数据源声明非常相似。不再使用`read`构建器，而是使用`readStream`构造，并向其传递流源所需的参数。在进行此练习的过程中以及稍后深入了解结构化流的细节时，API基本上与静态数据的`DataFrame`和`Dataset`
    API相同，但有一些您将详细了解的修改和限制。
- en: Preparing the Data in the Stream
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备流中的数据
- en: The `socket` source produces a streaming `DataFrame` with one column, `value`,
    which contains the data received from the stream. See [“The Socket Source”](ch10.xhtml#ss-socket-source)
    for additional details.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`socket`源生成一个带有一列`value`的流`DataFrame`，该列包含从流接收的数据。有关详细信息，请参见[“Socket源”](ch10.xhtml#ss-socket-source)。'
- en: In the batch analytics case, we could load the data directly as JSON records.
    In the case of the `Socket` source, that data is plain text. To transform our
    raw data to `WebLog` records, we first require a schema. The schema provides the
    necessary information to parse the text to a JSON object. It’s the *structure*
    when we talk about *structured* streaming.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在批量分析案例中，我们可以直接加载数据作为JSON记录。对于`Socket`源，数据是纯文本。为了将原始数据转换为`WebLog`记录，我们首先需要一个模式。模式提供了将文本解析为JSON对象所需的信息。当我们谈论*结构化*流时，它就是*结构*。
- en: 'After defining a schema for our data, we proceed to create a `Dataset`, following
    these steps:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在为数据定义模式之后，我们继续创建一个`Dataset`，按以下步骤操作：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](Images/1.png)](#co_introducing_structured_streaming_CO1-1)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_introducing_structured_streaming_CO1-1)'
- en: Obtain a schema from the `case class` definition
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从`case class`定义中获取模式
- en: '[![2](Images/2.png)](#co_introducing_structured_streaming_CO1-2)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_introducing_structured_streaming_CO1-2)'
- en: Transform the text `value` to JSON using the JSON support built into Spark SQL
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark SQL内置的JSON支持将文本`value`转换为JSON
- en: '[![3](Images/3.png)](#co_introducing_structured_streaming_CO1-3)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_introducing_structured_streaming_CO1-3)'
- en: Use the `Dataset` API to transform the JSON records to `WebLog` objects
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Dataset` API将JSON记录转换为`WebLog`对象
- en: As a result of this process, we obtain a `Streaming Dataset` of `WebLog` records.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个过程，我们得到了一个`WebLog`记录的`Streaming Dataset`。
- en: Operations on Streaming Dataset
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对流数据集的操作
- en: The `webLogStream` we just obtained is of type `Dataset[WebLog]` like we had
    in the batch analytics job. The difference between this instance and the batch
    version is that `webLogStream` is a streaming `Dataset`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚获得的`webLogStream`是类型为`Dataset[WebLog]`的流，就像我们在批处理分析作业中看到的一样。这个实例与批处理版本的不同之处在于`webLogStream`是一个流式`Dataset`。
- en: 'We can observe this by querying the object:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查询对象来观察到这一点：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'At this point in the batch job, we were creating the first query on our data:
    How many records are contained in our dataset? This is a question that we can
    easily answer when we have access to all of the data. However, how do we count
    records that are constantly arriving? The answer is that some operations that
    we consider usual on a static `Dataset`, like counting all records, do not have
    a defined meaning on a `Streaming Dataset`.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在批处理作业的这一点上，我们正在对我们的数据创建第一个查询：我们的数据集中包含多少记录？当我们可以访问所有数据时，这是一个我们可以轻松回答的问题。然而，如何计算不断到达的记录数？答案是我们认为在静态`Dataset`上通常执行的一些操作，如计算所有记录数，对于`Streaming
    Dataset`来说并没有明确定义的含义。
- en: 'As we can observe, attempting to execute the `count` query in the following
    code snippet will result in an `AnalysisException`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们可以观察到的，尝试在以下代码片段中执行`count`查询将导致`AnalysisException`：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This means that the direct queries we used on a static `Dataset` or `DataFrame`
    now need two levels of interaction. First, we need to declare the transformations
    of our stream, and then we need to start the stream process.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们在静态`Dataset`或`DataFrame`上使用的直接查询现在需要两个层次的交互。首先，我们需要声明流的转换，然后我们需要启动流处理过程。
- en: Creating a Query
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建查询
- en: What are popular URLs? In what time frame? Now that we have immediate analytic
    access to the stream of web logs, we don’t need to wait for a day or a month (or
    more than 20 years in the case of these NASA web logs) to have a rank of the popular
    URLs. We can have that information as trends unfold in much shorter windows of
    time.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是流行的网址？在什么时间范围内？现在，我们可以立即分析访问网站日志的流，不再需要等待一天或一个月（甚至在NASA网站日志的情况下超过20年），以获得流行网址的排名。随着趋势的展示，我们可以在更短的时间窗口内获得这些信息。
- en: First, to define the period of time of our interest, we create a window over
    some timestamp. An interesting feature of Structured Streaming is that we can
    define that time interval on the timestamp when the data was produced, also known
    as *event time*, as opposed to the time when the data is being processed.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了定义我们感兴趣的时间段，我们在某个时间戳上创建一个窗口。结构化流的一个有趣特性是，我们可以在数据生成时的时间戳上定义时间间隔，也称为*事件时间*，而不是数据被处理时的时间。
- en: Our window definition will be of five minutes of event data. Given that our
    timeline is simulated, the five minutes might happen much faster or slower than
    the clock time. In this way, we can clearly appreciate how Structured Streaming
    uses the timestamp information in the events to keep track of the event timeline.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的窗口定义将是五分钟的事件数据。鉴于我们的时间线是模拟的，这五分钟可能比实际时钟时间快得多或慢得多。通过这种方式，我们可以清楚地看到结构化流如何使用事件中的时间戳信息来跟踪事件时间线。
- en: 'As we learned from the batch analytics, we should extract the URLs and select
    only content pages, like *.html*, .*htm*, or directories. Let’s apply that acquired
    knowledge first before proceeding to define our windowed query:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从批处理分析中学到的那样，我们应该提取URL并仅选择内容页面，比如*.html*、.*htm*或目录。在继续定义窗口查询之前，让我们先应用这些获得的知识：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We have converted the request to contain only the visited URL and filtered
    out all noncontent pages. Now, we define the windowed query to compute the top
    trending URLs:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将请求转换为仅包含访问的URL并过滤掉所有非内容页面。现在，我们定义窗口查询来计算热门趋势URL：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Start the Stream Processing
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动流处理
- en: All of the steps that we have followed so far have been to define the process
    that the stream will undergo. But no data has been processed yet.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所遵循的所有步骤都是为了定义流将经历的过程。但是目前还没有处理任何数据。
- en: 'To start a Structured Streaming job, we need to specify a `sink` and an `output
    mode`. These are two new concepts introduced by Structured Streaming:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动结构化流作业，我们需要指定一个`sink`和一个`output mode`。这是结构化流引入的两个新概念：
- en: A `sink` defines where we want to materialize the resulting data; for example,
    to a file in a filesystem, to an in-memory table, or to another streaming system
    such as Kafka.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sink`定义了我们想要将结果数据实体化的位置；例如，可以是文件系统中的文件，内存中的表，或者是其他流系统如Kafka。'
- en: 'The `output mode` defines how we want the results to be delivered: do we want
    to see all data every time, only updates, or just the new records?'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output mode`定义了我们希望交付结果的方式：我们是否希望每次看到所有数据，仅更新，还是只看到新记录？'
- en: These options are given to a `writeStream` operation. It creates the streaming
    query that starts the stream consumption, materializes the computations declared
    on the query, and produces the result to the output `sink`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选项提供给`writeStream`操作。它创建了开始流消费的流查询，实现在查询上声明的计算，并将结果生成到输出`sink`中。
- en: We visit all these concepts in detail later on. For now, let’s use them empirically
    and observe the results.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后详细讨论所有这些概念。现在，让我们凭经验使用它们并观察结果。
- en: For our query, shown in [Example 7-1](#query-write-stream), we use the `memory`
    `sink` and output mode `complete` to have a fully updated table each time new
    records are added to the result of keeping track of the URL ranking.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在[示例 7-1](#query-write-stream)中展示的查询，我们使用`memory` `sink`和输出模式`complete`，每次向结果添加新记录时都会有一个完全更新的表，以跟踪URL排名的结果。
- en: Example 7-1\. Writing a stream to a sink
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-1 编写流到 `sink`
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The memory sink outputs the data to a temporary table of the same name given
    in the `queryName` option. We can observe this by querying the tables registered
    on `Spark SQL`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`memory sink`将数据输出到与给定的`queryName`选项中相同名称的临时表中。我们可以通过查询`Spark SQL`注册的表来观察到这一点：'
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the expression in [Example 7-1](#query-write-stream), `query` is of type
    `StreamingQuery` and it’s a handler to control the query life cycle.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 7-1](#query-write-stream)中的表达式中，`query`的类型是`StreamingQuery`，它是一个处理查询生命周期的处理程序。
- en: Exploring the Data
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索数据
- en: Given that we are accelerating the log timeline on the producer side, after
    a few seconds, we can execute the next command to see the result of the first
    windows, as illustrated in [Figure 7-1](#url-rank).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们在生产者端加速日志时间线，几秒钟后，我们可以执行下一个命令来查看第一个窗口的结果，如[图 7-1](#url-rank)所示。
- en: 'Note how the processing time (a few seconds) is decoupled from the event time
    (hundreds of minutes of logs):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意处理时间（几秒钟）与事件时间（数百分钟的日志）是如何分离的：
- en: '[PRE15]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![spas 0701](Images/spas_0701.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0701](Images/spas_0701.png)'
- en: 'Figure 7-1\. URL ranking: query results by window'
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1 URL 排名：按窗口查询结果
- en: We explore event time in detail in [Chapter 12](ch12.xhtml#ss-event-time).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第 12 章](ch12.xhtml#ss-event-time)中详细探讨事件时间。
- en: Summary
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In these first steps into Structured Streaming, you have seen the process behind
    the development of a streaming application. By starting with a batch version of
    the process, you gained intuition about the data, and using those insights, we
    created a streaming version of the job. In the process, you could appreciate how
    close the *structured* batch and the streaming APIs are, albeit we also observed
    that some usual batch operations do now apply in a streaming context.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入结构化流处理的初始阶段，您已经了解到流应用程序开发背后的过程。通过从过程的批处理版本开始，您对数据有了直观的理解，并利用这些见解，我们创建了作业的流处理版本。在这个过程中，您可以体会到结构化批处理和流处理API之间的密切联系，尽管我们也观察到在流处理上下文中一些通常的批处理操作不适用。
- en: With this exercise, we hope to have increased your curiosity about Structured
    Streaming. You’re now ready for the learning path through this section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个练习，我们希望增加您对结构化流处理的好奇心。您现在已经准备好通过本节的学习路径了。
