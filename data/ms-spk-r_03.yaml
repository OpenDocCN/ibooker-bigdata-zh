- en: Chapter 2\. Getting Started
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章. 入门
- en: I always wanted to be a wizard.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我一直想成为一个巫师。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Samwell Tarly
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ——萨姆威尔·塔利
- en: After reading [Chapter 1](ch01.html#intro), you should now be familiar with
    the kinds of problems that Spark can help you solve. And it should be clear that
    Spark solves problems by making use of multiple computers when data does not fit
    in a single machine or when computation is too slow. If you are newer to R, it
    should also be clear that combining Spark with data science tools like `ggplot2`
    for visualization and `dplyr` to perform data transformations brings a promising
    landscape for doing data science at scale. We also hope you are excited to become
    proficient in large-scale computing.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读[第一章](ch01.html#intro)后，您现在应该熟悉 Spark 可以帮助您解决的问题类型。并且应该清楚，当数据无法在单台机器上容纳或计算速度太慢时，Spark
    通过利用多台计算机来解决问题。如果您是 R 的新手，结合数据科学工具如 `ggplot2` 进行可视化和 `dplyr` 进行数据转换，与 Spark 结合使用为进行大规模数据科学带来了一个充满希望的前景。我们也希望您能够兴奋地成为大规模计算的专家。
- en: In this chapter, we take a tour of the tools you’ll need to become proficient
    in Spark. We encourage you to walk through the code in this chapter because it
    will force you to go through the motions of analyzing, modeling, reading, and
    writing data. In other words, you will need to do some wax-on, wax-off, repeat
    before you get fully immersed in the world of Spark.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将带您了解成为 Spark 熟练使用者所需的工具。我们鼓励您逐步阅读本章的代码，因为这将强迫您通过分析、建模、阅读和写入数据的过程。换句话说，在您完全沉浸于
    Spark 的世界之前，您需要进行一些“上蜡，下蜡，反复”操作。
- en: 'In [Chapter 3](ch03.html#analysis) we dive into analysis followed by modeling,
    which presents examples using a single-cluster machine: your personal computer.
    Subsequent chapters introduce cluster computing and the concepts and techniques
    that you’ll need to successfully run code across multiple machines.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](ch03.html#analysis)中，我们深入探讨分析，然后是建模，展示了在单一集群机器（即您的个人计算机）上使用的示例。随后的章节介绍了集群计算以及您将需要成功在多台机器上运行代码的概念和技术。
- en: Overview
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概览
- en: From R, getting started with Spark using `sparklyr` and a local cluster is as
    easy as installing and loading the `sparklyr` package followed by installing Spark
    using `sparklyr`; however, we assume you are starting with a brand new computer
    running Windows, macOS, or Linux, so we’ll walk you through the prerequisites
    before connecting to a local Spark cluster.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 从 R 开始使用 `sparklyr` 和本地集群来启动 Spark 是非常简单的，只需安装和加载 `sparklyr` 包，然后使用 `sparklyr`
    安装 Spark；不过，我们假设您是从运行 Windows、macOS 或 Linux 的全新计算机开始，因此在连接到本地 Spark 集群之前，我们将为您讲解必备的先决条件。
- en: Although this chapter is designed to help you get ready to use Spark on your
    personal computer, it’s also likely that some readers will already have a Spark
    cluster available or might prefer to get started with an online Spark cluster.
    For instance, Databricks hosts a [free community edition](http://bit.ly/31MfKuV)
    of Spark that you can easily access from your web browser. If you end up choosing
    this path, skip to [“Prerequisites”](#starting-prerequisites), but make sure you
    consult the proper resources for your existing or online Spark cluster.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章旨在帮助您准备在个人计算机上使用 Spark，但也有可能一些读者已经有了 Spark 集群或更喜欢从在线 Spark 集群开始。例如，Databricks
    提供了一个[免费社区版](http://bit.ly/31MfKuV)的 Spark，您可以轻松从 Web 浏览器访问。如果您选择这条路径，请直接跳至 [“先决条件”](#starting-prerequisites)，但确保为现有或在线
    Spark 集群咨询适当的资源。
- en: Either way, after you are done with the prerequisites, you will first learn
    how to connect to Spark. We then present the most important tools and operations
    that you’ll use throughout the rest of this book. Less emphasis is placed on teaching
    concepts or how to use them—we can’t possibly explain modeling or streaming in
    a single chapter. However, going through this chapter should give you a brief
    glimpse of what to expect and give you the confidence that you have the tools
    correctly configured to tackle more challenging problems later on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，在完成先决条件后，您将首先学习如何连接到 Spark。然后，我们介绍您在本书的其余部分中将使用的最重要的工具和操作。我们不太强调教授概念或如何使用它们——在单独的一章中我们无法详尽解释建模或流处理。但是，通过本章的学习，您应该对未来的挑战性问题有一个简要的了解，并且可以自信地确保正确配置了工具以处理更具挑战性的问题。
- en: The tools you’ll use are mostly divided into R code and the Spark web interface.
    All Spark operations are run from R; however, monitoring execution of distributed
    operations is performed from Spark’s web interface, which you can load from any
    web browser. We then disconnect from this local cluster, which is easy to forget
    to do but highly recommended while working with local clusters—and in shared Spark
    clusters as well!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用的工具主要分为R代码和Spark Web界面。所有Spark操作都从R运行；然而，监视分布式操作的执行是从Spark的Web界面进行的，您可以从任何Web浏览器加载。然后断开与本地集群的连接，这很容易忘记但强烈建议在使用本地集群和共享Spark集群时执行！
- en: We close this chapter by walking you through some of the features that make
    using Spark with RStudio easier; more specifically, we present the RStudio extensions
    that `sparklyr` implements. However, if you are inclined to use Jupyter Notebooks
    or if your cluster is already equipped with a different R user interface, rest
    assured that you can use Spark with R through plain R code. Let’s move along and
    get your prerequisites properly configured.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过向您介绍`sparklyr`实现的RStudio扩展功能来结束本章。然而，如果您倾向于使用Jupyter Notebooks或者您的集群已经配备了不同的R用户界面，可以放心使用纯R代码通过RStudio更轻松地使用Spark。让我们继续并正确配置您的先决条件。
- en: Prerequisites
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先决条件
- en: R can run in many platforms and environments; therefore, whether you use Windows,
    Mac, or Linux, the first step is to install R from [r-project.org](https://r-project.org/);
    detailed instructions are provided in [“Installing R”](app01.html#appendix-install-r).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: R可以在许多平台和环境中运行；因此，无论您使用Windows、Mac还是Linux，第一步是从[r-project.org](https://r-project.org/)安装R；详细说明请参见[“安装R”](app01.html#appendix-install-r)。
- en: Most people use programming languages with tools to make them more productive;
    for R, RStudio is such a tool. Strictly speaking, RStudio is an *integrated development
    environment* (IDE), which also happens to support many platforms and environments.
    We strongly recommend you install RStudio if you haven’t done so already; see
    details in [“Installing RStudio”](app01.html#appendix-install-rstudio).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人使用带有提高生产力工具的编程语言；对于R语言来说，RStudio就是这样一款工具。严格来说，RStudio是一个*集成开发环境*（IDE），它也支持多个平台和环境。如果您还没有安装RStudio，我们强烈建议您安装；详细信息请参见[“安装RStudio”](app01.html#appendix-install-rstudio)。
- en: Tip
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: When using Windows, we recommend avoiding directories with spaces in their path.
    If running `getwd()` from R returns a path with spaces, consider switching to
    a path with no spaces using `setwd("path")` or by creating an RStudio project
    in a path with no spaces.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Windows时，建议避免路径中带有空格的目录。如果从R运行`getwd()`返回带有空格的路径，请考虑使用`setwd("path")`切换到没有空格的路径，或者通过在没有空格的路径中创建RStudio项目来解决。
- en: 'Additionally, because Spark is built in the Scala programming language, which
    is run by the Java Virtual Machine (JVM), you also need to install Java 8 on your
    system. It is likely that your system already has Java installed, but you should
    still check the version and update or downgrade as described in [“Installing Java”](app01.html#appendix-install-java).
    You can use the following R command to check which version is installed on your
    system:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，由于Spark是使用Scala编程语言构建的，而Scala是由Java虚拟机（JVM）运行的，因此您还需要在系统上安装Java 8。您的系统可能已经安装了Java，但您仍应检查版本并根据[“安装Java”](app01.html#appendix-install-java)中的描述进行更新或降级。您可以使用以下R命令检查系统上安装的版本：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can also use the `JAVA_HOME` environment variable to point to a specific
    Java version by running `Sys.setenv(JAVA_HOME = "path-to-java-8")`; either way,
    before moving on to installing `sparklyr`, make sure that Java 8 is the version
    available for R.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用`JAVA_HOME`环境变量通过运行`Sys.setenv(JAVA_HOME = "path-to-java-8")`来指向特定的Java版本；无论哪种方式，在安装`sparklyr`之前，请确保Java
    8是R可用的版本。
- en: Installing sparklyr
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装`sparklyr`
- en: 'As with many other R packages, you can install `sparklyr` from [CRAN](http://bit.ly/2KLyaoE)
    as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多其他R包一样，您可以按照以下步骤从[CRAN](http://bit.ly/2KLyaoE)安装`sparklyr`：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The examples in this book assume you are using the latest version of `sparklyr`.
    You can verify your version is as new as the one we are using by running the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的示例假设您正在使用最新版本的`sparklyr`。您可以通过运行以下命令验证您的版本是否与我们使用的版本一样新：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Installing Spark
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Spark
- en: 'Start by loading `sparklyr`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先加载`sparklyr`：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This makes all `sparklyr` functions available in R, which is really helpful;
    otherwise, you would need to run each `sparklyr` command prefixed with `sparklyr::`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这样可以在 R 中使所有 `sparklyr` 函数可用，这非常有帮助；否则，你需要在每个 `sparklyr` 命令之前加上 `sparklyr::`。
- en: 'You can easily install Spark by running `spark_install()`. This downloads,
    installs, and configures the latest version of Spark locally on your computer;
    however, because we’ve written this book with Spark 2.3, you should also install
    this version to make sure that you can follow all the examples provided without
    any surprises:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行 `spark_install()` ，你可以轻松安装 Spark。这会在你的计算机上下载、安装并配置最新版本的 Spark；但是，因为我们是在
    Spark 2.3 上编写本书的，你也应该安装这个版本，以确保你能够顺利地跟随所有提供的示例，没有任何意外：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can display all of the versions of Spark that are available for installation
    by running the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以运行以下命令显示所有可安装的 Spark 版本：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can install a specific version by using the Spark version and, optionally,
    by also specifying the Hadoop version. For instance, to install Spark 1.6.3, you
    would run:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过指定 Spark 版本和可选的 Hadoop 版本安装特定版本。例如，要安装 Spark 1.6.3，你可以运行：
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can also check which versions are installed by running this command:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过运行此命令检查已安装的版本：
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The path where Spark is installed is known as Spark’s *home*, which is defined
    in R code and system configuration settings with the `SPARK_HOME` identifier.
    When you are using a local Spark cluster installed with `sparklyr`, this path
    is already known and no additional configuration needs to take place.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 安装路径被称为 Spark 的*主目录*，在 R 代码和系统配置设置中使用 `SPARK_HOME` 标识符定义。当你使用通过 `sparklyr`
    安装的本地 Spark 集群时，此路径已知，并且不需要进行额外配置。
- en: 'Finally, to uninstall a specific version of Spark you can run `spark_uninstall()`
    by specifying the Spark and Hadoop versions, like so:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要卸载特定版本的 Spark，你可以运行 `spark_uninstall()`，并指定 Spark 和 Hadoop 的版本，如下所示：
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The default installation paths are *~/spark* for macOS and Linux, and *%LOCALAPPDATA%/spark*
    for Windows. To customize the installation path, you can run `options(spark.install.dir
    = "installation-path")` before `spark_install()` and `spark_connect()`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 macOS 和 Linux 下，默认安装路径是 *~/spark*，在 Windows 下是 *%LOCALAPPDATA%/spark*。要自定义安装路径，你可以在运行
    `spark_install()` 和 `spark_connect()` 之前运行 `options(spark.install.dir = "installation-path")`。
- en: Connecting
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接
- en: It’s important to mention that, so far, we’ve installed only a local Spark cluster.
    A local cluster is really helpful to get started, test code, and troubleshoot
    with ease. Later chapters explain where to find, install, and connect to real
    Spark clusters with many machines, but for the first few chapters, we focus on
    using local clusters.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，到目前为止，我们只安装了一个本地 Spark 集群。本地集群非常有助于开始、测试代码以及轻松排除故障。稍后的章节将解释如何找到、安装和连接具有多台机器的真实
    Spark 集群，但在前几章中，我们将专注于使用本地集群。
- en: 'To connect to this local cluster, simply run the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到这个本地集群，只需运行以下命令：
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you are using your own or online Spark cluster, make sure that you connect
    as specified by your cluster administrator or the online documentation. If you
    need some pointers, you can take a quick look at [Chapter 7](ch07.html#connections),
    which explains in detail how to connect to any Spark cluster.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用自己的或在线的 Spark 集群，请确保按照集群管理员或在线文档指定的方式连接。如果需要一些指引，你可以快速查看[第7章](ch07.html#connections)，其中详细解释了如何连接到任何
    Spark 集群。
- en: The `master` parameter identifies which is the “main” machine from the Spark
    cluster; this machine is often called the *driver node*. While working with real
    clusters using many machines, you’ll find that most machines will be worker machines
    and one will be the master. Since we have only a local cluster with just one machine,
    we will default to using `"local"` for now.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`master` 参数标识了 Spark 集群中的“主”机器；这台机器通常称为*驱动节点*。在实际使用多台机器的真实集群时，你会发现大多数机器是工作机器，而其中一台是主节点。由于我们只有一个本地集群，并且只有一台机器，所以我们暂时默认使用
    `"local"`。'
- en: After a connection is established, `spark_connect()` retrieves an active Spark
    connection, which most code usually names `sc`; you will then make use of `sc`
    to execute Spark commands.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 连接建立后，`spark_connect()` 将获取一个活动的 Spark 连接，大多数代码通常会将其命名为 `sc`；然后你将使用 `sc` 执行
    Spark 命令。
- en: If the connection fails, [Chapter 7](ch07.html#connections) contains a [troubleshooting](ch07.html#connections-troubleshooting)
    section that can help you to resolve your connection issue.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果连接失败，[第7章](ch07.html#connections)包含一个可以帮助你解决连接问题的[故障排除](ch07.html#connections-troubleshooting)部分。
- en: Using Spark
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark
- en: 'Now that you are connected, we can run a few simple commands. For instance,
    let’s start by copying the `mtcars` dataset into Apache Spark by using `copy_to()`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已连接，我们可以运行一些简单的命令。例如，让我们通过使用`copy_to()`将`mtcars`数据集复制到Apache Spark中：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The data was copied into Spark, but we can access it from R using the `cars`
    reference. To print its contents, we can simply type `*cars*`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 数据已复制到Spark中，但我们可以通过`cars`引用从R中访问它。要打印其内容，我们只需输入`*cars*`：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Congrats! You have successfully connected and loaded your first dataset into
    Spark.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已成功连接并将第一个数据集加载到Spark中。
- en: Let’s explain what’s going on in `copy_to()`. The first parameter, `sc`, gives
    the function a reference to the active Spark connection that was created earlier
    with `spark_connect()`. The second parameter specifies a dataset to load into
    Spark. Now, `copy_to()` returns a reference to the dataset in Spark, which R automatically
    prints. Whenever a Spark dataset is printed, Spark *collects* some of the records
    and displays them for you. In this particular case, that dataset contains only
    a few rows describing automobile models and some of their specifications like
    horsepower and expected miles per gallon.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释`copy_to()`中发生的情况。第一个参数`sc`给函数提供了一个对之前使用`spark_connect()`创建的活动Spark连接的引用。第二个参数指定要加载到Spark中的数据集。现在，`copy_to()`返回Spark中数据集的引用，R会自动打印出来。每当Spark数据集被打印时，Spark都会*收集*一些记录并显示给你。在这种特定情况下，该数据集仅包含描述汽车型号及其规格（如马力和预期每加仑英里数）的几行。
- en: Web Interface
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络界面
- en: 'Most of the Spark commands are executed from the R console; however, monitoring
    and analyzing execution is done through Spark’s web interface, shown in [Figure 2-1](#starting-spark-web).
    This interface is a web application provided by Spark that you can access by running:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数Spark命令都是从R控制台执行的；然而，监视和分析执行是通过Spark的网络界面完成的，如[图 2-1](#starting-spark-web)所示。这个界面是由Spark提供的一个Web应用程序，你可以通过运行以下命令访问：
- en: '[PRE17]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![The Apache Spark web interface](assets/mswr_0201.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark网络界面](assets/mswr_0201.png)'
- en: Figure 2-1\. The Apache Spark web interface
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. Apache Spark网络界面
- en: Printing the `cars` dataset collected a few records to be displayed in the R
    console. You can see in the Spark web interface that a job was started to collect
    this information back from Spark. You can also select the Storage tab to see the
    `mtcars` dataset cached in memory in Spark, as shown in [Figure 2-2](#starting-spark-web-storage).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 打印`cars`数据集收集了一些记录，以在R控制台中显示。你可以在Spark网络界面中看到启动了一个作业来从Spark中收集这些信息。你还可以选择存储标签，查看在Spark中缓存的`mtcars`数据集，如[图 2-2](#starting-spark-web-storage)所示。
- en: Notice that this dataset is fully loaded into memory, as indicated by the Fraction
    Cached column, which shows 100%; thus, you can see exactly how much memory this
    dataset is using through the Size in Memory column.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个数据集完全加载到内存中，如“内存占用”列所示，显示为100%；因此，通过“内存大小”列，你可以准确查看此数据集使用了多少内存。
- en: '![The Storage tab on the Apache Spark web interface](assets/mswr_0202.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark网络界面上的存储标签](assets/mswr_0202.png)'
- en: Figure 2-2\. The Storage tab on the Apache Spark web interface
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. Apache Spark网络界面上的存储标签
- en: The Executors tab, shown in [Figure 2-3](#starting-spark-executors), provides
    a view of your cluster resources. For local connections, you will find only one
    executor active with only 2 GB of memory allocated to Spark, and 384 MB available
    for computation. In [Chapter 9](ch09.html#tuning) you will learn how to request
    more compute instances and resources, and how memory is allocated.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-3](#starting-spark-executors)显示的执行者标签提供了集群资源的视图。对于本地连接，你将只找到一个活动执行者，为Spark分配了2
    GB内存，并为计算可用了384 MB。在[第9章](ch09.html#tuning)中，你将学习如何请求更多计算实例和资源，以及如何分配内存。'
- en: '![The Executors tab on the Apache Spark web interface](assets/mswr_0203.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark网络界面上的执行者标签](assets/mswr_0203.png)'
- en: Figure 2-3\. The Executors tab on the Apache Spark web interface
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. Apache Spark网络界面上的执行者标签
- en: The last tab to explore is the Environment tab, shown in [Figure 2-4](#starting-spark-environment);
    this tab lists all of the settings for this Spark application, which we look at
    in [Chapter 9](ch09.html#tuning). As you will learn, most settings don’t need
    to be configured explicitly, but to properly run them at scale, you need to become
    familiar with some of them, eventually.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 探索的最后一个选项卡是环境选项卡，如[Figure 2-4](#starting-spark-environment)所示；此选项卡列出了此Spark应用程序的所有设置，我们将在[Chapter 9](ch09.html#tuning)中研究。正如您将了解的那样，大多数设置不需要显式配置，但是要正确地按规模运行它们，您需要熟悉其中一些设置。
- en: '![The Environment tab on the Apache Spark web interface](assets/mswr_0204.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark Web界面上的环境选项卡](assets/mswr_0204.png)'
- en: Figure 2-4\. The Environment tab on the Apache Spark web interface
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. Apache Spark Web界面上的环境选项卡
- en: Next, you will make use of a small subset of the practices that we cover in
    depth in [Chapter 3](ch03.html#analysis).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将使用我们在[Chapter 3](ch03.html#analysis)中深入介绍的实践的一个小子集。
- en: Analysis
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析
- en: 'When using Spark from R to analyze data, you can use SQL (Structured Query
    Language) or `dplyr` (a grammar of data manipulation). You can use SQL through
    the `DBI` package; for instance, to count how many records are available in our
    `cars` dataset, we can run the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当从R使用Spark分析数据时，您可以使用SQL（结构化查询语言）或`dplyr`（数据操作语法）。您可以通过`DBI`包使用SQL；例如，要计算我们的`cars`数据集中有多少条记录，我们可以运行以下命令：
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When using `dplyr`, you write less code, and it’s often much easier to write
    than SQL. This is precisely why we won’t make use of SQL in this book; however,
    if you are proficient in SQL, this is a viable option for you. For instance, counting
    records in `dplyr` is more compact and easier to understand:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`dplyr`时，你会写更少的代码，通常比SQL更容易编写。这也正是为什么在本书中我们不会使用SQL的原因；然而，如果你精通SQL，这对你来说是一个可行的选择。例如，在`dplyr`中计算记录更加紧凑且易于理解：
- en: '[PRE20]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In general, we usually start by analyzing data in Spark with `dplyr`, followed
    by sampling rows and selecting a subset of the available columns. The last step
    is to collect data from Spark to perform further data processing in R, like data
    visualization. Let’s perform a very simple data analysis example by selecting,
    sampling, and plotting the `cars` dataset in Spark:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们通常从Spark中使用`dplyr`开始分析数据，然后对行进行抽样并选择可用列的子集。最后一步是从Spark中收集数据，在R中进行进一步的数据处理，如数据可视化。让我们通过在Spark中选择、抽样和绘制`cars`数据集来执行一个非常简单的数据分析示例：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The plot in [Figure 2-5](#starting-cars-hp-vs-mpg) shows that as we increase
    the horsepower in a vehicle, its fuel efficiency measured in miles per gallon
    decreases. Although this is insightful, it’s difficult to predict numerically
    how increased horsepower would affect fuel efficiency. Modeling can help us overcome
    this.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Figure 2-5](#starting-cars-hp-vs-mpg)的图中显示，随着车辆马力的增加，其每加仑英里数的燃油效率会降低。尽管这很有见地，但要数值化地预测增加马力如何影响燃油效率是困难的。建模可以帮助我们克服这一问题。
- en: '![Horsepower versus miles per gallon](assets/mswr_0205.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![马力与每加仑英里数](assets/mswr_0205.png)'
- en: Figure 2-5\. Horsepower versus miles per gallon
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. 马力与每加仑英里数
- en: Modeling
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模
- en: Although data analysis can take you quite far toward understanding data, building
    a mathematical model that describes and generalizes the dataset is quite powerful.
    In [Chapter 1](ch01.html#intro) you learned that the fields of machine learning
    and data science make use of mathematical models to perform predictions and find
    additional insights.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据分析可以帮助您深入了解数据，但构建描述和概括数据集的数学模型是非常强大的。在[Chapter 1](ch01.html#intro)中，您了解到机器学习和数据科学领域利用数学模型进行预测和发现额外的见解。
- en: 'For instance, we can use a linear model to approximate the relationship between
    fuel efficiency and horsepower:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用线性模型来近似燃油效率和马力之间的关系：
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now we can use this model to predict values that are not in the original dataset.
    For instance, we can add entries for cars with horsepower beyond 250 and also
    visualize the predicted values, as shown in [Figure 2-6](#starting-cars-hp-vs-mpg-prediction).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用这个模型来预测原始数据集中没有的值。例如，我们可以为马力超过250的汽车添加条目，并可视化预测值，如[Figure 2-6](#starting-cars-hp-vs-mpg-prediction)所示。
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Horsepower versus miles per gallon with predictions](assets/mswr_0206.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![具有预测的马力与每加仑英里数](assets/mswr_0206.png)'
- en: Figure 2-6\. Horsepower versus miles per gallon with predictions
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6\. 具有预测的马力与每加仑英里数
- en: Even though the previous example lacks many of the appropriate techniques that
    you should use while modeling, it’s also a simple example to briefly introduce
    the modeling capabilities of Spark. We introduce all of the Spark models, techniques,
    and best practices in [Chapter 4](ch04.html#modeling).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 即使前面的示例缺乏你在建模时应该使用的许多适当技术，它也是一个简单的示例，可以简要介绍 Spark 的建模能力。我们在[第四章](ch04.html#modeling)中介绍了所有
    Spark 模型、技术和最佳实践。
- en: Data
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: 'For simplicity, we copied the `mtcars` dataset into Spark; however, data is
    usually not copied into Spark. Instead, data is read from existing data sources
    in a variety of formats, like plain text, CSV, JSON, Java Database Connectivity
    (JDBC), and many more, which we examine in detail in [Chapter 8](ch08.html#data).
    For instance, we can export our `cars` dataset as a CSV file:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，我们将 `mtcars` 数据集复制到了 Spark 中；然而，通常情况下数据并不会复制到 Spark 中。相反，数据是从现有的各种格式的数据源中读取的，如纯文本、CSV、JSON、Java
    数据库连接（JDBC）等，我们会在[第八章](ch08.html#data)中详细讨论这些内容。例如，我们可以将我们的 `cars` 数据集导出为 CSV
    文件：
- en: '[PRE26]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In practice, we would read an existing dataset from a distributed storage system
    like HDFS, but we can also read back from the local file system:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们将从分布式存储系统（如 HDFS）读取现有数据集，但我们也可以从本地文件系统读取：
- en: '[PRE27]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Extensions
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展
- en: In the same way that R is known for its vibrant community of package authors,
    at a smaller scale, many extensions for Spark and R have been written and are
    available to you. [Chapter 10](ch10.html#extensions) introduces many interesting
    ones to perform advanced modeling, graph analysis, preprocessing of datasets for
    deep learning, and more.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 R 以其充满活力的包作者社区而闻名一样，在较小的尺度上，许多 Spark 和 R 的扩展已经被编写并对您可用。[第十章](ch10.html#extensions)介绍了许多有趣的扩展，用于执行高级建模、图分析、深度学习数据集预处理等操作。
- en: 'For instance, the `sparkly.nested` extension is an R package that extends `sparklyr`
    to help you manage values that contain nested information. A common use case involves
    JSON files that contain nested lists that require preprocessing before you can
    do meaningful data analysis. To use this extension, we first need to install it
    as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`sparkly.nested` 扩展是一个 R 包，它扩展了 `sparklyr` 以帮助您管理包含嵌套信息的值。一个常见的用例涉及包含需要预处理才能进行有意义数据分析的嵌套列表的
    JSON 文件。要使用这个扩展，我们首先需要按如下方式安装它：
- en: '[PRE28]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, we can use the `sparklyr.nested` extension to group all of the horsepower
    data points over the number of cylinders:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 `sparklyr.nested` 扩展将所有汽缸数上的马力数据点进行分组：
- en: '[PRE29]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Even though nesting data makes it more difficult to read, it is a requirement
    when you are dealing with nested data formats like JSON using the `spark_read_json()`
    and `spark_write_json()` functions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 即使嵌套数据使阅读变得更加困难，但在处理像 JSON 这样的嵌套数据格式时，使用 `spark_read_json()` 和 `spark_write_json()`
    函数是必需的。
- en: Distributed R
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式 R
- en: For those few cases when a particular functionality is not available in Spark
    and no extension has been developed, you can consider distributing your own R
    code across the Spark cluster. This is a powerful tool, but it comes with additional
    complexity, so you should only use it as a last resort.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于少数情况下 Spark 中没有特定功能并且也没有开发扩展的情况，你可以考虑将你自己的 R 代码分布到 Spark 集群中。这是一个强大的工具，但它带来了额外的复杂性，因此你应该将其作为最后的选择。
- en: 'Suppose that we need to round all of the values across all the columns in our
    dataset. One approach would be running custom R code, making use of R’s `round()`
    function:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要在数据集的所有列上四舍五入所有值。一种方法是运行自定义的 R 代码，利用 R 的 `round()` 函数：
- en: '[PRE31]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: If you are a proficient R user, it can be quite tempting to use `spark_apply()`
    for everything, but please, don’t! `spark_apply()` was designed for advanced use
    cases where Spark falls short. You will learn how to do proper data analysis and
    modeling without having to distribute custom R code across your cluster.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一个熟练的 R 用户，可能会很容易地为所有事情使用 `spark_apply()`，但请不要这样做！`spark_apply()` 是为 Spark
    功能不足的高级用例设计的。您将学习如何在不必将自定义 R 代码分布到集群中的情况下进行正确的数据分析和建模。
- en: Streaming
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流处理
- en: While processing large static datasets is the most typical use case for Spark,
    processing dynamic datasets in real time is also possible and, for some applications,
    a requirement. You can think of a streaming dataset as a static data source with
    new data arriving continuously, like stock market quotes. Streaming data is usually
    read from Kafka (an open source stream-processing software platform) or from distributed
    storage that receives new data continuously.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然处理大型静态数据集是 Spark 的典型用例，实时处理动态数据集也是可能的，并且对某些应用程序而言是必需的。您可以将流式数据集视为一个连续接收新数据的静态数据源，例如股票市场行情。通常从
    Kafka（一个开源流处理软件平台）或连续接收新数据的分布式存储中读取流式数据。
- en: 'To try out streaming, let’s first create an *input/* folder with some data
    that we will use as the input for this stream:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试流式处理，让我们首先创建一个包含一些数据的 *input/* 文件夹，作为该流的输入使用：
- en: '[PRE33]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then, we define a stream that processes incoming data from the *input/* folder,
    performs a custom transformation in R, and pushes the output into an *output/*
    folder:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义一个流，处理来自 *input/* 文件夹的输入数据，在 R 中执行自定义转换，并将输出推送到 *output/* 文件夹：
- en: '[PRE34]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As soon as the stream of real-time data starts, the *input/* folder is processed
    and turned into a set of new files under the *output/* folder containing the new
    transformed files. Since the input contained only one file, the output folder
    will also contain a single file resulting from applying the custom `spark_apply()`
    transformation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦实时数据流开始，*input/* 文件夹将被处理，并转换为*output/* 文件夹下的一组新文件，其中包含新转换的文件。由于输入只包含一个文件，输出文件夹也将包含一个单独的文件，这是应用自定义
    `spark_apply()` 转换的结果。
- en: '[PRE35]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Up to this point, this resembles static data processing; however, we can keep
    adding files to the *input/* location, and Spark will parallelize and process
    data automatically. Let’s add one more file and validate that it’s automatically
    processed:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这类似于静态数据处理；然而，我们可以继续向 *input/* 位置添加文件，Spark 将自动并行处理数据。让我们再添加一个文件并验证它是否被自动处理：
- en: '[PRE37]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Wait a few seconds and validate that the data is processed by the Spark stream:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 等待几秒钟，并验证数据是否被 Spark 流处理：
- en: '[PRE38]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You should then stop the stream:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您应该停止流：
- en: '[PRE40]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: You can use `dplyr`, SQL, Spark models, or distributed R to analyze streams
    in real time. In [Chapter 12](ch12.html#streaming) we properly introduce you to
    all the interesting transformations you can perform to analyze real-time data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `dplyr`、SQL、Spark 模型或分布式 R 来实时分析数据流。在[第 12 章](ch12.html#streaming)中，我们将详细介绍您可以执行的所有有趣的转换来分析实时数据。
- en: Logs
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志
- en: 'Logging is definitely less interesting than real-time data processing; however,
    it’s a tool you should be or become familiar with. A *log* is just a text file
    to which Spark appends information relevant to the execution of tasks in the cluster.
    For local clusters, we can retrieve all the recent logs by running the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录绝对比实时数据处理无聊得多；然而，这是您应该熟悉或了解的工具。*日志*只是 Spark 附加到集群中任务执行相关信息的文本文件。对于本地集群，我们可以通过运行以下命令检索所有最近的日志：
- en: '[PRE41]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Or, we can retrieve specific log entries containing, say, `sparklyr`, by using
    the `filter` parameter, as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用 `filter` 参数检索包含 `sparklyr` 的特定日志条目，如下所示：
- en: '[PRE43]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Most of the time, you won’t need to worry about Spark logs, except in cases
    for which you need to troubleshoot a failed computation; in those cases, logs
    are an invaluable resource to be aware of. Now you know.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，您不需要担心 Spark 日志，除非需要排除故障的失败计算；在这些情况下，日志是一个宝贵的资源需要注意。现在您知道了。
- en: Disconnecting
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 断开连接
- en: 'For local clusters (really, any cluster), after you are done processing data,
    you should disconnect by running the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本地集群（实际上，任何集群），在处理数据完成后，应通过运行以下命令断开连接：
- en: '[PRE45]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This terminates the connection to the cluster as well as the cluster tasks.
    If multiple Spark connections are active, or if the connection instance `sc` is
    no longer available, you can also disconnect all your Spark connections by running
    this command:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这会终止与集群的连接以及集群任务。如果有多个 Spark 连接处于活动状态，或者连接实例 `sc` 不再可用，您也可以通过运行以下命令断开所有 Spark
    连接：
- en: '[PRE46]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Notice that exiting R, or RStudio, or restarting your R session, also causes
    the Spark connection to terminate, which in turn terminates the Spark cluster
    and cached data that is not explicitly saved.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，退出 R 或 RStudio，或重新启动您的 R 会话，也会导致 Spark 连接终止，从而终止未显式保存的 Spark 集群和缓存数据。
- en: Using RStudio
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RStudio
- en: Since it’s very common to use RStudio with R, `sparklyr` provides RStudio extensions
    to help simplify your workflows and increase your productivity while using Spark
    in RStudio. If you are not familiar with RStudio, take a quick look at [“Using
    RStudio”](app01.html#appendix-using-rstudio). Otherwise, there are a couple extensions
    worth highlighting.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在RStudio中使用R非常普遍，`sparklyr`提供了RStudio扩展，帮助简化你在使用Spark时的工作流程并提高生产力。如果你对RStudio不熟悉，请快速查看[“使用RStudio”](app01.html#appendix-using-rstudio)。否则，还有几个值得强调的扩展功能。
- en: First, instead of starting a new connection using `spark_connect()` from RStudio’s
    R console, you can use the New Connection action from the Connections tab and
    then select the Spark connection, which opens the dialog shown in [Figure 2-7](#starting-rstudio-new-connection).
    You can then customize the versions and connect to Spark, which will simply generate
    the right `spark_connect()` command and execute this in the R console for you.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，不要从RStudio的R控制台使用`spark_connect()`开始新的连接，而是使用Connections选项卡中的New Connection操作，然后选择Spark连接，这将打开[图2-7](#starting-rstudio-new-connection)中显示的对话框。然后，你可以自定义版本并连接到Spark，这将为你简单生成正确的`spark_connect()`命令，并在R控制台中执行。
- en: '![RStudio New Spark Connection dialog](assets/mswr_0207.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![RStudio新的Spark连接对话框](assets/mswr_0207.png)'
- en: Figure 2-7\. RStudio New Spark Connection dialog
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7\. RStudio新的Spark连接对话框
- en: After you’re connected to Spark, RStudio displays your available datasets in
    the Connections tab, as shown in [Figure 2-8](#starting-rstudio-connections-pane).
    This is a useful way to track your existing datasets and provides an easy way
    to explore each of them.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到Spark后，RStudio会在Connections选项卡中显示你可用的数据集，如[图2-8](#starting-rstudio-connections-pane)所示。这是跟踪你现有数据集并轻松探索每个数据集的有用方式。
- en: '![The RStudio Connections tab](assets/mswr_0208.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![RStudio Connections选项卡](assets/mswr_0208.png)'
- en: Figure 2-8\. The RStudio Connections tab
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8\. RStudio Connections选项卡
- en: 'Additionally, an active connection provides the following custom actions:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，活动连接提供以下自定义操作：
- en: Spark UI
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Spark UI
- en: Opens the Spark web interface; a shortcut to `spark_web(sc)`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 打开Spark的Web界面；一个`spark_web(sc)`的快捷方式。
- en: Log
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 日志
- en: Opens the Spark web logs; a shortcut to `spark_log(sc)`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 打开Spark的Web日志；一个`spark_log(sc)`的快捷方式。
- en: SQL
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: SQL
- en: Opens a new SQL query. For more information about `DBI` and SQL support, see
    [Chapter 3](ch03.html#analysis).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个新的SQL查询。有关`DBI`和SQL支持的更多信息，请参阅[第3章](ch03.html#analysis)。
- en: Help
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助
- en: Opens the reference documentation in a new web browser window.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的Web浏览器窗口中打开参考文档。
- en: Disconnect
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 断开连接
- en: Disconnects from Spark; a shortcut to `spark_disconnect(sc)`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 断开与Spark的连接；一个`spark_disconnect(sc)`的快捷方式。
- en: The rest of this book will use plain R code. It is up to you whether to execute
    this code in the R console, RStudio, Jupyter Notebooks, or any other tool that
    supports executing R code, since the examples provided in this book execute in
    any R environment.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的其余部分将使用纯粹的R代码。你可以选择在R控制台、RStudio、Jupyter Notebooks或任何支持执行R代码的工具中执行此代码，因为本书提供的示例在任何R环境中均可执行。
- en: Resources
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: 'While we’ve put significant effort into simplifying the onboarding process,
    there are many additional resources that can help you to troubleshoot particular
    issues while getting started and, in general, introduce you to the broader Spark
    and R communities to help you get specific answers, discuss topics, and connect
    with many users actively using Spark with R:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经大力简化了入门流程，但还有许多额外的资源可以帮助你解决特定问题，帮助你了解更广泛的Spark和R社区，以便获取具体答案、讨论主题并与许多正在使用Spark的用户联系：
- en: Documentation
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 文档
- en: The documentation site hosted in [RStudio’s Spark website](https://spark.rstudio.com)
    should be your first stop to learn more about Spark when using R. The documentation
    is kept up to date with examples, reference functions, and many more relevant
    resources.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 托管在[RStudio的Spark网站](https://spark.rstudio.com)上的文档站点应该是你学习使用R时了解更多关于Spark的首选站点。该文档随时更新，包括示例、参考函数以及许多其他相关资源。
- en: Blog
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 博客
- en: To keep up to date with major `sparklyr` announcements, you can follow the [RStudio
    blog](http://bit.ly/2KQBYVK).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 要及时了解主要的`sparklyr`公告，请关注[RStudio博客](http://bit.ly/2KQBYVK)。
- en: Community
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 社区
- en: For general `sparklyr` questions, you can post in the [RStudio Community](http://bit.ly/2PfNqzN)
    tagged as `sparklyr`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般的`sparklyr`问题，你可以在[RStudio社区](http://bit.ly/2PfNqzN)中发布标记为`sparklyr`的帖子。
- en: Stack Overflow
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Stack Overflow
- en: For general Spark questions, [Stack Overflow](http://bit.ly/2TEfU4L) is a great
    resource; there are also [many topics specifically about `sparklyr`](http://bit.ly/307X5cB).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般的Spark问题，[Stack Overflow](http://bit.ly/2TEfU4L)是一个很好的资源；此外，还有[许多关于`sparklyr`的专题](http://bit.ly/307X5cB)。
- en: GitHub
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub
- en: If you believe something needs to be fixed, open a [GitHub](http://bit.ly/30b5NGT)
    issue or send us a pull request.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为有什么需要修正的地方，请打开一个[GitHub](http://bit.ly/30b5NGT)问题或发送一个拉取请求。
- en: Gitter
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Gitter
- en: For urgent issues or to keep in touch, you can chat with us in [Gitter](http://bit.ly/33ESccY).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于紧急问题或保持联系，你可以在[Gitter](http://bit.ly/33ESccY)上与我们交流。
- en: Recap
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Recap
- en: In this chapter you learned about the prerequisites required to work with Spark.
    You saw how to connect to Spark using `spark_connect()`; install a local cluster
    using `spark_install()`; load a simple dataset, launch the web interface, and
    display logs using `spark_web(sc)` and `spark_log(sc)`, respectively; and disconnect
    from RStudio using `spark_disconnect()`. We close by presenting the RStudio extensions
    that `sparklyr` provides.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了与Spark一起工作所需的先决条件。你了解了如何使用`spark_connect()`连接到Spark；使用`spark_install()`安装本地集群；加载简单数据集；启动Web界面，并分别使用`spark_web(sc)`和`spark_log(sc)`显示日志；最后使用`spark_disconnect()`断开与RStudio的连接。我们通过介绍`sparklyr`提供的RStudio扩展来结束。
- en: At this point, we hope that you feel ready to tackle actual data analysis and
    modeling problems in Spark and R, which will be introduced over the next two chapters.
    [Chapter 3](ch03.html#analysis) will present data analysis as the process of inspecting,
    cleaning, and transforming data with the goal of discovering useful information.
    Modeling, the subject of [Chapter 4](ch04.html#modeling), can be considered part
    of data analysis; however, it deserves its own chapter to truly describe and take
    advantage of the modeling functionality available in Spark.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们希望你已经准备好在接下来的两章中处理实际的Spark和R数据分析与建模问题。[第三章](ch03.html#analysis)将介绍数据分析，这是通过检查、清洗和转换数据来发现有用信息的过程。建模则是[第四章](ch04.html#modeling)的主题，虽然它是数据分析的一部分，但它需要一个独立的章节来全面描述和利用Spark中可用的建模功能。
