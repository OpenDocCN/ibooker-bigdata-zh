- en: Chapter 11\. Structured Streaming Sinks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 11 章。结构化流处理 Sinks
- en: In the previous chapter, you learned about sources, the abstraction that allows
    Structured Streaming to acquire data for processing. After that data has been
    processed, we would want to do something with it. We might want to write it to
    a database for later querying, to a file for further (batch) processing, or to
    another streaming backend to keep the data in motion.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，您学习了 sources，这是结构化流处理用于获取处理数据的抽象。在处理完这些数据后，我们可能希望对其进行某些操作。我们可能希望将其写入数据库以供后续查询，写入文件以进行进一步的（批处理）处理，或者将其写入另一个流式后端以保持数据的流动。
- en: In Structured Streaming, *sinks* are the abstraction that represents how to
    produce data to an external system. Structured Streaming comes with several built-in
    sources and defines an API that lets us create custom sinks to other systems that
    are not natively supported.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化流处理中，*sinks* 是表示如何将数据传输到外部系统的抽象概念。结构化流处理内置了几个数据源，并定义了一个 API，使我们能够创建自定义的
    sinks 来传输数据到不原生支持的其他系统。
- en: In this chapter, we look at how a sink works, review the details of the sinks
    provided by Structured Streaming, and explore how to create custom sinks to write
    data to systems not supported by the default implementations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将研究 sinks 的工作原理，审查结构化流处理提供的 sinks 的详细信息，并探讨如何创建自定义 sinks 来将数据写入不受默认实现支持的系统。
- en: Understanding Sinks
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Sinks
- en: '*Sinks* serve as output adaptors between the internal data representation in
    Structured Streaming and external systems. They provide a write path for the data
    resulting from the stream processing. Additionally, they must also close the loop
    of reliable data delivery.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*Sinks* 作为结构化流处理中内部数据表示与外部系统之间的输出适配器。它们为流处理生成的数据提供写入路径。此外，它们还必须关闭可靠数据传递的循环。'
- en: To participate in the end-to-end reliable data delivery, sinks must provide
    an *idempotent* write operation. Idempotent means that the result of executing
    the operation two or more times is equal to executing the operation once. When
    recovering from a failure, Spark might reprocess some data that was partially
    processed at the time the failure occurred. At the side of the source, this is
    done by using the replay functionality. Recall from [“Understanding Sources”](ch10.xhtml#understanding_sources),
    reliable sources must provide a means of replaying uncommitted data, based on
    a given offset. Likewise, sinks must provide the means of removing duplicated
    records before those are written to the external source.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要参与端到端可靠的数据传递，sinks 必须提供幂等写入操作。幂等意味着多次执行操作的结果等同于仅执行一次操作。在从故障中恢复时，Spark 可能会重新处理在故障发生时部分处理的数据。在源的一侧，这是通过使用重放功能来完成的。回想一下[“理解
    Sources”](ch10.xhtml#understanding_sources)，可靠的 sources 必须提供重新播放未提交数据的方法，基于给定的偏移量。同样，sinks
    必须提供在将记录写入外部源之前删除重复记录的方法。
- en: The combination of a replayable source and an idempotent sink is what grants
    Structured Streaming its *effectively exactly once* data delivery semantics. Sinks
    that are not able to implement the idempotent requirement will result in end-to-end
    delivery guarantees of at most “at least once” semantics. Sinks that are not able
    to recover from the failure of the streaming process are deemed “unreliable” because
    they might lose data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 重播源和幂等 sink 的组合赋予了结构化流处理其 *有效的仅一次* 数据传递语义。无法实现幂等性要求的 sinks 将导致至多“至少一次”语义的端到端传递保证。无法从流处理过程的故障中恢复的
    sinks 被视为“不可靠”，因为它们可能会丢失数据。
- en: In the next section, we go over the available sinks in Structured Streaming
    in detail.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将详细介绍结构化流处理中可用的 sinks。
- en: Note
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Third-party vendors might provide custom Structured Streaming sinks for their
    products. When integrating one of these external sinks in your project, consult
    their documentation to determine the data delivery warranties they support.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第三方供应商可能会为其产品提供定制的结构化流处理 sinks。在将其中一个外部 sinks 集成到您的项目中时，请参考其文档以确定它们支持的数据传递保证。
- en: Available Sinks
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可用的 Sinks
- en: Structured Streaming comes with several sinks that match the supported sources
    as well as sinks that let us output data to temporary storage or to the console.
    In rough terms, we can divide the provided sinks into reliable and learning/experimentation
    support. In addition, it also offers a programmable interface that allows us to
    work with arbitrary external systems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理提供了几种输出，与支持的源相匹配，以及允许我们将数据输出到临时存储或控制台的输出。大致来说，我们可以将提供的输出分为可靠输出和学习/实验支持输出两类。此外，它还提供了一个可编程接口，允许我们与任意外部系统进行交互。
- en: Reliable Sinks
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠输出
- en: The sinks considered reliable or production ready provide well-defined data
    delivery semantics and are resilient to total failure of the streaming process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 被视为可靠或适合生产的输出提供了明确定义的数据传输语义，并且对流处理过程的完全故障具有弹性。
- en: 'The following are the provided reliable sinks:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的可靠输出如下：
- en: The File sink
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 文件输出
- en: 'This writes data to files in a directory in the filesystem. It supports the
    same file formats as the File source: JSON, Parquet, comma-separated values (CSV),
    and Text.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这将数据写入文件系统中的目录中的文件。它支持与文件源相同的文件格式：JSON、Parquet、逗号分隔值（CSV）和文本。
- en: The Kafka sink
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka输出
- en: This writes data to Kafka, effectively keeping the data “on the move.” This
    is an interesting option to integrate the results of our process with other streaming
    frameworks that rely on Kafka as the data backbone.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这将数据写入Kafka，有效地保持数据“在移动”中。这是一个有趣的选择，可以将我们的处理结果与依赖Kafka作为数据主干的其他流处理框架集成。
- en: Sinks for Experimentation
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于实验的输出
- en: The following sinks are provided to support interaction and experimentation
    with Structured Streaming. They do not provide failure recovery and therefore
    their use in production is discouraged because it can result in data loss.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 提供以下输出以支持与结构化流处理的交互和实验。它们不提供故障恢复，因此在生产环境中使用这些输出是不鼓励的，因为可能导致数据丢失。
- en: 'The following are nonreliable sinks:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是非可靠输出：
- en: The Memory sink
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 内存输出
- en: This creates a temporary table with the results of the streaming query. The
    resulting table can be queried within the same Java virtual machine (JVM) process,
    which allows in-cluster queries to access the results of the streaming process.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个临时表，其中包含流查询的结果。生成的表可以在同一个Java虚拟机（JVM）进程内进行查询，从而允许集群内的查询访问流处理过程的结果。
- en: The Console sink
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台输出
- en: This prints the results of the query to the console. This is useful at development
    time to visually inspect the results of the stream process.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这将查询的结果打印到控制台。在开发阶段，这对于直观地检查流处理结果非常有用。
- en: The Sink API
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出API
- en: Next to the built-in sinks, we also have the option to create a sink programmatically.
    This is achieved with the `foreach` operation that, as its name implies, offers
    access to each individual resulting record of the output stream. Finally, it is
    possible to develop our own custom sinks using the `sink` API directly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 除了内置的输出，我们还可以选择以编程方式创建输出。这可以通过`foreach`操作实现，如其名称所示，它可以访问输出流的每个单独的结果记录。最后，可以直接使用`sink`
    API开发自定义输出。
- en: Exploring Sinks in Detail
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 详细探讨输出
- en: In the rest of this chapter, we explore the configuration and options available
    for each sink. We present in-depth coverage of the reliable sinks that should
    provide a thorough view of their applicability and can serve as a reference when
    you begin developing your own applications.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们探讨了每个输出的配置和可用选项。我们深入介绍了可靠输出，这应该提供了一个全面的应用视图，并且在你开始开发自己的应用程序时可以作为参考。
- en: The experimental sinks are limited in scope, and that is also reflected in the
    level of coverage that follows.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 实验性输出在范围上有所限制，这也反映在后续覆盖的程度上。
- en: Toward the end of this chapter, we look at the custom `sink` API options and
    review the considerations we need to take when developing our own sinks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾，我们将查看自定义`sink` API选项，并审查开发自己输出时需要考虑的事项。
- en: Tip
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you are in your initial exploration phase of Structured Streaming, you might
    want to skip this section and come back to it later, when you are busy developing
    your own Structured Streaming jobs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您处于结构化流处理的初探阶段，您可能希望跳过本节，并在忙于开发自己的结构化流处理作业时稍后再回来查看。
- en: The File Sink
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件输出
- en: Files are a common intersystem boundary. When used as a sink for a streaming
    process, they allow the data to become *at rest* after the stream-oriented processing.
    Those files can become part of a *data lake* or can be consumed by other (batch)
    processes as part of a larger processing pipeline that combines streaming and
    batch modes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 文件是常见的系统边界。当作为流处理的接收端使用时，它们允许数据在流导向处理后变得*静止*。这些文件可以成为*数据湖*的一部分，也可以被其他（批处理）过程消耗，作为结合了流式和批处理模式的更大处理流水线的一部分。
- en: Scalable, reliable, and distributed filesystems—such as HDFS or object stores
    like Amazon Simple Storage Service (Amazon S3)—make it possible to store large
    datasets as files in arbitrary formats. When running in local mode, at exploration
    or development time, it’s possible to use the local filesystem for this sink.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展、可靠和分布式的文件系统——如HDFS或像亚马逊简单存储服务（Amazon S3）这样的对象存储——使得可以将大型数据集以任意格式存储为文件。在本地模式运行时，可以在探索或开发时使用本地文件系统作为这个接收端。
- en: 'The File sink supports the same formats as the File source:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 文件接收端支持与文件源相同的格式：
- en: CSV
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV
- en: JSON
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON
- en: Parquet
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet
- en: ORC
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ORC
- en: Text
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Text
- en: Note
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Structured Streaming shares the same File *Data Source* implementation used
    in batch mode. The write options offered by the `DataFrameWriter` for each File
    format are also available in streaming mode. In this section, we highlight the
    most commonly used options. For the most up-to-date list, always consult the online
    documentation for your specific Spark version.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流共享与批处理模式中使用的相同文件*数据源*实现。`DataFrameWriter`为每种文件格式提供的写入选项在流处理模式下也同样适用。在本节中，我们重点介绍最常用的选项。要获取最新的列表，请始终参考特定Spark版本的在线文档。
- en: Before going into the details of each format, let’s explore a general File sink
    example that’s presented in [Example 11-1](#file_sink_example).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨每种格式的细节之前，让我们先看一个通用的文件接收端示例，即[示例 11-1](#file_sink_example)中的示例。
- en: Example 11-1\. File sink example
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-1\. 文件接收端示例
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, we are using the `csv` format to write the stream results to
    the `<dest/path>` destination directory using `TAB` as the custom separator. We
    also specify a `checkpointLocation`, where the checkpoint metadata is stored at
    regular intervals.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用`csv`格式将流结果写入到`<dest/path>`目标目录，使用`TAB`作为自定义分隔符。我们还指定了一个`checkpointLocation`，用于定期存储检查点元数据。
- en: 'The File sink supports only `append` as `outputMode`, and it can be safely
    omitted in the `writeStream` declaration. Attempting to use another mode will
    result in the following exception when the query starts: `org.apache.spark.sql.AnalysisException:
    Data source ${format} does not support ${output_mode} output mode;`.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '文件接收端仅支持`append`作为`outputMode`，并且在`writeStream`声明中可以安全地省略。尝试使用其他模式将导致查询启动时出现以下异常：`org.apache.spark.sql.AnalysisException:
    Data source ${format} does not support ${output_mode} output mode;`。'
- en: Using Triggers with the File Sink
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用文件接收端的触发器
- en: One additional parameter that we see in [Example 11-1](#file_sink_example) is
    the use of a `trigger`. When no trigger is specified, Structured Streaming starts
    the processing of a new batch as soon as the previous one is finished. In the
    case of the File sink, and depending on the throughput of the input stream, this
    might result in the generation of many small files. This might be detrimental
    for the filesystem storage capacity and performance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[示例 11-1](#file_sink_example)中看到的另一个参数是使用`trigger`。当没有指定触发器时，结构化流会在上一个批次完成后立即启动新批次的处理。对于文件接收端，根据输入流的吞吐量，这可能会导致生成许多小文件。这可能对文件系统的存储能力和性能有害。
- en: Consider [Example 11-2](#small-files-example).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑[示例 11-2](#small-files-example)。
- en: Example 11-2\. Rate source with File sink
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-2\. 使用文件接收端的速率源
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If we let this query run for a little while and then we check the target directory,
    we should observe a large number of small files:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果让此查询运行一段时间，然后检查目标目录，应该会观察到大量小文件：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we learned in [Chapter 10](ch10.xhtml#ss-sources), the Rate source generates
    one record per second by default. When we see the data contained in one file,
    we can indeed see that single record. The query is, in fact, generating one file
    each time new data is available. Although the contents of that file is not large,
    filesystems incur some overhead in keeping track of the number of files in the
    filesystem. Even more, in the Hadoop Distributed File System (HDFS) each file
    occupies a block and replicates `n` times regardless of the contents. Given that
    the typical HDFS block is 128 MB, we can see how our naive query that uses a File
    sink can quickly deplete our storage.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第10章](ch10.xhtml#ss-sources)中学到的，速率源默认每秒生成一条记录。当我们查看一个文件中包含的数据时，确实可以看到单个记录。事实上，查询每次新数据可用时生成一个文件。尽管该文件的内容不多，文件系统在跟踪文件数方面会有一些开销。在Hadoop分布式文件系统（HDFS）中，每个文件无论内容如何都会占用一个块，并复制`n`次。考虑到典型的HDFS块大小为128
    MB，我们可以看到我们使用文件汇集器的简单查询可能会迅速耗尽存储空间。
- en: The `trigger` configuration is there to help us avoid this situation. By providing
    time triggers for the production of files, we can ensure that we have a reasonably
    sufficient amount of data in each file.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`trigger`配置是为了帮助我们避免这种情况。通过为文件生成提供时间触发器，我们可以确保每个文件中有足够的数据量。'
- en: 'We can observe the effect of a time `trigger` by modifying our previous example
    as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过修改前面的示例来观察时间`trigger`的效果如下：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s issue the query and wait for a couple of minutes. When we inspect the
    target directory, we should see considerably fewer files than before, and each
    file should contain more records. The number of records per file depends on the
    `DataFrame` partitioning:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们发出查询，并等待几分钟。当我们检查目标目录时，应该比以前少很多文件，并且每个文件应该包含更多的记录。每个文件中的记录数取决于`DataFrame`的分区：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you are trying this example on a personal computer, the number of partitions
    defaults to the number of cores present. In our case, we have eight cores, and
    we observe seven or eight records per partition. That’s still very few records,
    but it shows the principle that can be extrapolated to real scenarios.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在个人计算机上尝试此示例，则分区数默认为当前核心数。在我们的情况下，我们有八个核心，并且我们观察到每个分区有七到八条记录。虽然这仍然是非常少的记录，但它显示了可以推广到实际场景的原理。
- en: Even though a `trigger` based on the number of records or the size of the data
    would arguably be more interesting in this scenario, currently, only time-based
    triggers are supported. This might change in the future, as Structured Streaming
    evolves.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这种情况下基于记录数或数据大小的`trigger`可能更有趣，但目前仅支持基于时间的触发器。随着结构化流的发展，这可能会发生变化。
- en: Common Configuration Options Across All Supported File Formats
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所有支持的文件格式的常见配置选项
- en: We have already seen in previous examples the use of the method `option` that
    takes a key and a value to set configuration options in a sink.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们已经看到了使用方法`option`来设置汇合器中的配置选项的用法。
- en: 'All supported file formats share the following configuration options:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所有支持的文件格式共享以下配置选项：
- en: '`path`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`path`'
- en: A directory in a target filesystem where the streaming query writes the data
    files.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 流查询将数据文件写入目标文件系统中的目录。
- en: '`checkpointLocation`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`checkpointLocation`'
- en: A directory in a resilient filesystem where the checkpointing metadata is stored.
    New checkpoint information is written with each query execution interval.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠文件系统中存储检查点元数据的目录。每次查询执行间隔都会写入新的检查点信息。
- en: '`compression` (default: None)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`compression`（默认值：无）'
- en: All supported file formats share the capability to compress the data, although
    the available compression `codecs` might differ from format to format. The specific
    compression algorithms are shown for each format in their corresponding section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所有支持的文件格式都可以压缩数据，尽管可用的压缩`codecs`可能因格式而异。每种格式的具体压缩算法在其相应部分中显示。
- en: Note
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When configuring options of the File sink, it’s often useful to remember that
    any file written by the File sink can be read back using the corresponding File
    source. For example, when we discussed [“JSON File Source Format”](ch10.xhtml#json_file_source_format),
    we saw that it normally expects each line of the file to be a valid JSON document.
    Likewise, the JSON sink format will produce files containing one record per line.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置文件汇总选项时，通常有必要记住，文件汇总写入的任何文件都可以使用相应的文件源读取。例如，当我们讨论 [“JSON 文件源格式”](ch10.xhtml#json_file_source_format)
    时，我们看到它通常期望文件的每一行是一个有效的 JSON 文档。同样，JSON 汇总格式将生成每行一个记录的文件。
- en: Common Time and Date Formatting (CSV, JSON)
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见的时间和日期格式化（CSV、JSON）
- en: 'Text-based files formats such as CSV and JSON accept custom formatting for
    date and timestamp data types:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 文本文件格式，如 CSV 和 JSON，接受日期和时间戳数据类型的自定义格式化：
- en: '`dateFormat` (default: `yyyy-MM-dd`)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`dateFormat`（默认：`yyyy-MM-dd`）'
- en: Configures the pattern used to format `date` fields. Custom patterns follow
    the formats defined at [java.text.SimpleDateFormat](http://bit.ly/2VwrLku).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 配置用于格式化`date`字段的模式。自定义模式遵循 [java.text.SimpleDateFormat](http://bit.ly/2VwrLku)
    中定义的格式。
- en: '`timestampFormat` (default “yyyy-MM-dd’T’HH:mm:ss.SSSXXX”)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`timestampFormat`（默认：“yyyy-MM-dd’T’HH:mm:ss.SSSXXX”）'
- en: Configures the pattern used to format `timestamp` fields. Custom patterns follow
    the formats defined at [java.text.SimpleDateFormat](http://bit.ly/2VwrLku).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 配置用于格式化`timestamp`字段的模式。自定义模式遵循 [java.text.SimpleDateFormat](http://bit.ly/2VwrLku)
    中定义的格式。
- en: '`timeZone` (default: local timezone)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`timeZone`（默认：本地时区）'
- en: Configures the time zone to use to format timestamps.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 配置用于格式化时间戳的时区。
- en: The CSV Format of the File Sink
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件汇总的 CSV 格式
- en: Using the CSV File format, we can write our data in a ubiquitous tabular format
    that can be read by many programs, from spreadsheet applications to a wide range
    of enterprise software.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CSV 文件格式，我们可以以广泛支持的表格格式编写数据，可以被许多程序读取，从电子表格应用程序到各种企业软件。
- en: Options
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选项
- en: The CSV support in Spark supports many options to control the field separator,
    quoting behavior, and the inclusion of a header. In addition to that, the common
    File sink options and the date formatting options apply to the CSV sink.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，CSV 支持多种选项来控制字段分隔符、引用行为和包含头部信息。此外，通用的文件汇总选项和日期格式化选项也适用于 CSV 汇总。
- en: Note
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In this section, we list the most commonly used options. For a comprehensive
    list, check the [online documentation](http://bit.ly/2EdjVqe).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们列出了最常用的选项。详细列表，请查看 [在线文档](http://bit.ly/2EdjVqe)。
- en: 'Following are the commonly used options for the CSV sink:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 CSV 汇总的常用选项：
- en: '`header` (default: `false`)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`header`（默认：`false`）'
- en: A flag to indicate whether we should include a header to the resulting file.
    The header consists of the name of the fields in this streaming `DataFrame`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一个标志，用于指示是否应将头部包含在生成的文件中。头部包含此流式`DataFrame`中字段的名称。
- en: '`quote` (default: `"` [double quote])'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`quote`（默认：`"` [双引号]）'
- en: Sets the character used to quote records. Quoting is necessary when records
    might contain the separator character, which, without quoting, would result in
    corrupt records.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 设置用于引用记录的字符。引用是必要的，当记录可能包含分隔符字符时，如果没有引用，将导致记录损坏。
- en: '`quoteAll` (default: `false`)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`quoteAll`（默认：`false`）'
- en: A flag used to indicate whether all values should be quoted or only those that
    contain a separator character. Some external systems require all values to be
    quoted. When using the CSV format to import the resulting files in an external
    system, check that system’s import requirements to correctly configure this option.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 用于指示是否应引用所有值或仅包含分隔符字符的标志。一些外部系统要求引用所有值。当使用 CSV 格式将生成的文件导入外部系统时，请检查该系统的导入要求以正确配置此选项。
- en: '`sep` (default: `,` [comma])'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`sep`（默认：`,` [逗号]）'
- en: Configures the separator character used between fields. The separator must be
    a single character. Otherwise, the query will throw an `IllegalArgumentException`
    at runtime when it starts.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 配置用于字段之间的分隔符字符。分隔符必须是单个字符。否则，查询在运行时启动时会抛出`IllegalArgumentException`。
- en: The JSON File Sink Format
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JSON 文件汇总格式
- en: The JSON File sink lets us write output data to files using the JSON Lines format.
    This format transforms each record in the output dataset into a valid JSON document
    written in a line of text. The JSON File sink is symmetrical to the JSON source.
    As we would expect, files written with this format can be read again using the
    JSON source.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: JSON文件汇允许我们使用JSON Lines格式将输出数据写入文件。该格式将输出数据集中的每个记录转换为一个有效的JSON文档，并写入一行文本中。JSON文件汇对称地与JSON源相对应。正如我们预期的那样，使用此格式编写的文件可以通过JSON源再次读取。
- en: Note
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When using third-party JSON libraries to read the resulting files, we should
    take care of first reading the file(s) as lines of text and then parse each line
    as a JSON document representing one record.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用第三方JSON库读取生成的文件时，我们应该先将文件读取为文本行，然后将每行解析为表示一个记录的JSON文档。
- en: Options
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选项
- en: 'Next to the common file and text format options, the JSON sink supports these
    specific configurations:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通用文件和文本格式选项之外，JSON汇还支持这些特定的配置：
- en: '`encoding` (default: `UTF-8`)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`encoding`（默认：`UTF-8`）'
- en: Configures the charset encoding used to write the JSON files.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 配置用于编写JSON文件的字符集编码。
- en: '`lineSep` (default: `\n`)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`lineSep`（默认：`\n`）'
- en: Sets the line separator to be used between JSON records.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 设置要在JSON记录之间使用的行分隔符。
- en: 'Supported `compression` options (default: `none`): `none`, `bzip2`, `deflate`,
    `gzip`, `lz4`, and `snappy`.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的`compression`选项（默认：`none`）：`none`、`bzip2`、`deflate`、`gzip`、`lz4`和`snappy`。
- en: The Parquet File Sink Format
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquet文件汇格式
- en: The Parquet File sink supports the common File sink configuration and does not
    have format-specific options.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet文件汇支持常见的文件汇配置，不具有特定于格式的选项。
- en: 'Supported `compression` options (default: `snappy`): `none`, `gzip`, `lzo`,
    and `snappy`.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的`compression`选项（默认：`snappy`）：`none`、`gzip`、`lzo`和`snappy`。
- en: The Text File Sink Format
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本文件汇格式
- en: The text file sink writes plain-text files. Although the other file formats
    would perform a conversion from the streaming `DataFrame` or `Dataset` schema
    to the particular file format structure, the text sink expects either a flattened
    streaming `Dataset[String]` or a streaming `DataFrame` with a schema containing
    a single `value` field of `StringType`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 文本文件汇写入纯文本文件。尽管其他文件格式会将流`DataFrame`或`Dataset`模式转换为特定的文件格式结构，文本汇期望的是一个展平的流`Dataset[String]`或带有单个`value`字段的流`DataFrame`，其类型为`StringType`。
- en: The typical use of the text file format is to write custom text-based formats
    not natively supported in Structured Streaming. To achieve this goal, we first
    transform programmatically the data into the desired text representation. After
    that, we use the text format to write the data to files. Attempting to write any
    complex schema to a text sink will result in an error.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 文本文件格式的典型用法是编写在Structured Streaming中原生不支持的自定义基于文本的格式。为了实现这一目标，我们首先通过编程方式将数据转换为所需的文本表示形式。然后，我们使用文本格式将数据写入文件。试图将任何复杂模式写入文本汇将导致错误。
- en: Options
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选项
- en: 'Beside the common options for sinks and text-based formats, the text sink supports
    the following configuration option:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 除了汇和基于文本的格式的通用选项之外，文本汇支持以下配置选项：
- en: '`lineSep` (default: `\n`)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`lineSep`（默认：`\n`）'
- en: Configures the line separator used for terminating each text line.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 配置用于终止每个文本行的行分隔符。
- en: The Kafka Sink
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka汇
- en: As we discussed in [“The Kafka Source”](ch10.xhtml#kafka_source), Kafka is a
    Publish/Subscribe (pub/sub) system. Although the Kafka source functions as a subscriber,
    the Kafka sink is the publisher counterpart. The Kafka sink allows us to write
    data (back) to Kafka, which then can be consumed by other subscribers to continue
    a chain of streaming processors.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[“Kafka Source”](ch10.xhtml#kafka_source)中讨论的那样，Kafka是一个发布/订阅（pub/sub）系统。虽然Kafka源充当订阅者，但Kafka汇是发布者的对应物。Kafka汇允许我们将数据写入Kafka，然后其他订阅者可以消费这些数据，从而继续一系列流处理器的链条。
- en: Downstream consumers might be other streaming processors, implemented using
    Structured Streaming or any of the other streaming frameworks available, or (micro)
    services that consume the streaming data to fuel applications in the enterprise
    ecosystem.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 下游消费者可能是其他流处理器，使用Structured Streaming或任何其他可用的流处理框架实现，或者是（微）服务，用于消费企业生态系统中的流数据来支持应用程序。
- en: Understanding the Kafka Publish Model
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Kafka发布模型
- en: In Kafka, data is represented as key–value records exchanged over a topic. Topics
    are composed of distributed partitions. Each partition maintains the messages
    in the order in which they were received. This ordering is indexed by an offset,
    which, in turn, is used by the consumer to indicate the record(s) to read. When
    a record is published to a topic, it’s placed in a partition of a topic. The choice
    of the partition depends on the key. The ruling principle is that a record with
    the same key will land in the same partition. As a result, the ordering in Kafka
    is partial. The sequence of records from a single partition will be ordered sequentially
    by arrival time but there are no ordering guarantees among partitions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kafka中，数据表示为通过主题交换的键-值记录。主题由分布式分区组成。每个分区按接收顺序维护消息。此顺序由偏移量索引，消费者根据偏移量指示要读取的记录。当将记录发布到主题时，它被放置在主题的一个分区中。分区的选择取决于键。支配原则是具有相同键的记录将落在同一个分区中。因此，Kafka中的排序是部分的。来自单个分区的记录序列将按到达时间顺序排序，但在分区之间没有排序保证。
- en: This model is highly scalable and Kafka implementation ensures low-latency reads
    and writes, making it an excellent carrier for streaming data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型具有高度的可伸缩性，Kafka的实现确保低延迟的读写，使其成为流数据的优秀载体。
- en: Using the Kafka Sink
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Kafka Sink
- en: Now that you’ve learned about the Kafka publishing model, we can look at the
    practical side of producing data to Kafka. We just saw that the Kafka records
    are structured as key–value pairs. We need to structure our data in the same shape.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了Kafka发布模型，我们可以看看如何实际地将数据生产到Kafka。我们刚刚看到Kafka记录结构化为键-值对。我们需要以相同的形式结构化我们的数据。
- en: In a minimal implementation, we must ensure that our streaming `DataFrame` or
    `Dataset` has a `value` field of `BinaryType` or `StringType`. The implication
    of this requirement is that we usually need to encode our data into a transport
    representation before sending it to Kafka.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在最小实现中，我们必须确保我们的流`DataFrame`或`Dataset`具有`BinaryType`或`StringType`的`value`字段。这个要求的含义是，通常我们需要将数据编码成传输表示形式，然后再发送到Kafka。
- en: When the key is not specified, Structured Streaming will replace the `key` with
    `null`. This makes the Kafka sink use a round-robin assignment of the partition
    for the corresponding topic.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当未指定键时，结构化流将用`null`替换`key`。这使得Kafka sink使用循环分配来分配对应主题的分区。
- en: If we want to preserve control over the key assignment, we must have a `key`
    field, also of `BinaryType` or `StringType`. This `key` is used for the partition
    assignment, resulting in a guaranteed ordering between records with equal keys.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要保留对键分配的控制权，我们必须有一个`key`字段，也是`BinaryType`或`StringType`。这个`key`用于分区分配，从而确保具有相等键的记录之间的有序性。
- en: Optionally, we can control the destination topic at the record level by adding
    a `topic` field. If present, the `topic` value must correspond to a Kafka topic.
    Setting the `topic` on the `writeStream` option overrides the value in the `topic`
    field.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，我们可以通过添加一个`topic`字段来控制记录级别的目标主题。如果存在，`topic`的值必须对应于Kafka主题。在`writeStream`选项上设置`topic`会覆盖`topic`字段中的值。
- en: The related record will be published to that topic. This option is useful when
    implementing a fan-out pattern in which incoming records are sorted in different
    dedicated topics for further processing. Think for example about classifying incoming
    support tickets into dedicated sales, technical, and troubleshooting topics that
    are consumed downstream by their corresponding (micro) service.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 相关记录将被发布到该主题。这个选项在实现分流模式时非常有用，其中传入的记录被分类到不同的专用主题，以供后续处理消费。例如，将传入的支持票据分类到专门的销售、技术和故障排除主题中，这些主题由相应的（微）服务在下游消费。
- en: After we have the data in the right shape, we also need the address of the target
    bootstrap servers in order to connect to the brokers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们把数据整理成正确的形状之后，我们还需要目标引导服务器的地址，以便连接到代理。
- en: 'In practical terms, this generally involves two steps:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，这通常涉及两个步骤：
- en: Transform each record as a single field called `value` and optionally assign
    a key and a topic to each record.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个记录转换为名为`value`的单个字段，并可选择为每个记录分配一个键和一个主题。
- en: Declare our stream sink using the `writeStream` builder.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`writeStream`构建器声明我们的流目标。
- en: '[Example 11-3](#kafka_sink_example) shows these steps in use.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 11-3](#kafka_sink_example)展示了这些步骤的使用。'
- en: Example 11-3\. Kafka sink example
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-3\. Kafka sink示例
- en: '[PRE5]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When we add `topic` information at the record level, we must omit the `topic`
    configuration option.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在记录级别添加`topic`信息时，必须省略`topic`配置选项。
- en: In [Example 11-4](#kafka_sink_topic_example), we modify the previous code to
    write each record to a dedicated topic matching the `sensorType`. That is, all
    `humidity` records go to the humidity topic, all `radiation` records go to the
    radiation topic, and so on.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Example 11-4](#kafka_sink_topic_example)中，我们修改了先前的代码，将每个记录写入与`sensorType`匹配的专用主题。即所有`humidity`记录进入humidity主题，所有`radiation`记录进入radiation主题，依此类推。
- en: Example 11-4\. Kafka sink to different topics
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 11-4\. 将Kafka接收端写入不同主题
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that we have removed the setting `option("topic", targetTopic)` and added
    a `topic` field to each record. This results in each record being routed to the
    topic corresponding to its `sensorType`. If we leave the setting `option("topic",
    targetTopic)` in place, the value of the `topic` field would have no effect. The
    `option("topic", targetTopic)` setting takes precedence.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经移除了设置`option("topic", targetTopic)`，并且为每个记录添加了一个`topic`字段。这导致每个记录被路由到与其`sensorType`对应的主题。如果我们保留设置`option("topic",
    targetTopic)`，那么`topic`字段的值将不会起作用。`option("topic", targetTopic)`设置优先级更高。
- en: Choosing an encoding
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择编码方式
- en: When we look closely at the code in [Example 11-3](#kafka_sink_example), we
    see that we create a single `value` field by converting the existing data into
    its JSON representation. In Kafka, each record consists of a key and a value.
    The `value` field contains the payload of the record. To send or receive a record
    of arbitrary complexity to and from Kafka, we need to convert said record into
    a single-field representation that we can fit into this `value` field. In Structured
    Streaming, the conversion to and from this transport value representation to the
    actual record must be done through user code. Ideally, the encoding we choose
    can be easily transformed into a structured record to take advantage of the Spark
    capabilities to manipulate data.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们仔细查看[Example 11-3](#kafka_sink_example)中的代码时，我们会看到我们通过将现有数据转换为其JSON表示来创建一个单一的`value`字段。在Kafka中，每个记录包含一个键和一个值。`value`字段包含记录的有效负载。为了向Kafka发送或接收任意复杂的记录，我们需要将该记录转换为一个单字段表示，以便将其放入`value`字段中。在结构化流中，必须通过用户代码完成从此传输值表示到实际记录的转换。理想情况下，我们选择的编码可以轻松转换为结构化记录，以利用Spark处理数据的能力。
- en: 'A common encoding format is JSON. JSON has native support in the structured
    APIs of Spark, and that extends to Structured Streaming. As we saw in [Example 11-4](#kafka_sink_topic_example),
    we write JSON by using the SQL function `to_json`: `to_json(struct($"id", $"timestamp",
    $"value")) as "value")`.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的编码格式是JSON。JSON在Spark的结构化API中有原生支持，这一支持也扩展到了结构化流。正如我们在[Example 11-4](#kafka_sink_topic_example)中看到的，我们通过使用SQL函数`to_json`来编写JSON：`to_json(struct($"id",
    $"timestamp", $"value")) as "value")`。
- en: Binary representations such as AVRO and ProtoBuffers are also possible. In such
    cases, we treat the `value` field as a `BinaryType` and use third-party libraries
    to do the encoding/decoding.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制表示例如AVRO和ProtoBuffers也是可能的。在这种情况下，我们将`value`字段视为`BinaryType`，并使用第三方库进行编码/解码。
- en: As of this writing, there is no built-in support for binary encodings, but AVRO
    support has been announced for an upcoming version.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们撰写本文时，尚未内置对二进制编码的支持，但已宣布将在即将推出的版本中支持AVRO。
- en: Warning
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: An important factor to consider when choosing an encoding format is schema support.
    In a multiservice model that uses Kafka as the communications backbone, it’s typical
    to find services producing data that use a different programming model, language,
    and/or framework than a streaming processor or other service that consumes it.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择编码格式时要考虑的一个重要因素是模式支持。在使用Kafka作为通信骨干的多服务模型中，通常会发现产生数据的服务使用与流处理器或其他消费者不同的编程模型、语言和/或框架。
- en: To ensure interoperability, schema-oriented encodings are the preferred choice.
    Having a schema definition allows for the creation of artifacts in different languages
    and ensures that produced data can be consumed later on.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保互操作性，面向模式的编码是首选。具有模式定义允许在不同语言中创建工件，并确保生产的数据可以在后续被消费。
- en: The Memory Sink
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储器接收端
- en: The Memory sink is a nonreliable sink that saves the results of the stream processing
    in an in-memory temporary table. It is considered nonreliable because all data
    will be lost in the case the streaming process ends, but it’s certainly useful
    in scenarios for which low-latency access to the streaming results are is required.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Memory sink 是一个不可靠的输出端，将流处理的结果保存在内存中的临时表中。之所以被认为是不可靠的，是因为在流处理结束时将丢失所有数据，但在需要对流处理结果进行低延迟访问的场景中，它肯定是非常有用的。
- en: The temporary table created by this sink is named after the query name. This
    table is backed up by the streaming query and will be updated at each trigger
    following the semantics of the chosen `outputMode`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由此输出端创建的临时表以查询名称命名。该表由流式查询支持，并将根据所选`outputMode`语义在每个触发器后更新。
- en: The resulting table contains an up-to-date view of the query results and can
    be queried using classical Spark SQL operations. The query must be executed in
    the same process (JVM) where the Structured Streaming query is started.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表包含查询结果的最新视图，并可以使用经典的 Spark SQL 操作进行查询。查询必须在启动结构化流查询的同一进程（JVM）中执行。
- en: The table maintained by the `Memory Sink` can be accessed interactively. That
    property makes it an ideal interface with interactive data exploration tools like
    the Spark REPL or a notebook.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Memory Sink 维护的表可以通过交互方式访问。这使得它成为与 Spark REPL 或笔记本等交互式数据探索工具理想的接口。
- en: Another common use is to provide a query service on top of the streaming data.
    This is done by combining a server module, like an HTTP server, with the Spark
    driver. Calls to specific HTTP endpoints can then be served with data from this
    in-memory table.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见用途是在流数据的顶部提供查询服务。这是通过将服务器模块（如 HTTP 服务器）与 Spark 驱动程序组合来完成的。然后，可以通过特定的 HTTP
    端点调用来提供来自此内存表的数据。
- en: '[Example 11-5](#memory_sink_example) assumes a `sensorData` streaming dataset.
    The result of the stream processing is materialized in this in-memory table, which
    is available in the SQL context as `sample_memory_query`.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 11-5](#memory_sink_example) 假设一个 `sensorData` 流数据集。流处理的结果被实例化到这个内存表中，该表在
    SQL 上下文中可用作 `sample_memory_query`。'
- en: Example 11-5\. Memory sink example
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-5\. Memory sink example
- en: '[PRE7]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Output Modes
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出模式
- en: 'The Memory sink supports all output modes: `Append`, `Update`, and `Complete`.
    Hence, we can use it with all queries, including aggregations. The combination
    of the Memory sink with the `Complete` mode is particularly interesting because
    it provides a fast, in-memory queryable store for the up-to-date computed complete
    state. Note that for a query to support `Complete` state, it must aggregate over
    a bounded-cardinality key. This is to ensure that the memory requirements to handle
    the state are likewise bounded within the system resources.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Memory sink 支持所有输出模式：`Append`、`Update` 和 `Complete`。因此，我们可以将其与所有查询一起使用，包括聚合查询。Memory
    sink 与 `Complete` 模式的结合特别有趣，因为它提供了一个快速的、内存中可查询的存储，用于最新计算的完整状态。请注意，要支持 `Complete`
    状态的查询，必须对有界基数键进行聚合，以确保处理状态的内存需求也在系统资源范围内受限。
- en: The Console Sink
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制台输出端（Console Sink）
- en: For all of us who love to print “Hello, world!” to the screen output, we have
    the Console sink. Indeed, the Console sink lets us print a small sample of the
    results of the query to the standard output.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有喜欢在屏幕上输出“Hello, world!”的人，我们有控制台输出端。确实，控制台输出端允许我们将查询结果的一个小样本打印到标准输出。
- en: Its use is limited to debugging and data exploration in an interactive shell-based
    environment, such as the `spark-shell`. As we would expect, this sink is not reliable
    given that it does not commit any data to another system.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 它的使用仅限于交互式基于 shell 的环境中的调试和数据探索，比如`spark-shell`。正如我们预期的那样，这个输出端在没有将任何数据提交到另一个系统的情况下是不可靠的。
- en: You should avoid using the Console sink in production environments, much like
    `println`s are frowned upon from an operational code base.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中应避免使用控制台输出（Console sink），就像`println`在操作代码库中不被看好一样。
- en: Options
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选项
- en: 'Here are the configurable options for the Console sink:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是控制台输出端（Console sink）的可配置选项：
- en: '`numRows` (default: `20`)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`numRows`（默认值：`20`）'
- en: The maximum number of rows to show at each query trigger.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 每个查询触发器显示的最大行数。
- en: '`truncate` (default: `true`)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`truncate`（默认值：`true`）'
- en: A flag that indicates whether the output of each cell in a row should be truncated.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行单元格的输出是否应该被截断，有一个标志用来指示。
- en: Output Modes
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出模式
- en: 'As of Spark 2.3, the Console sink supports all output modes: `Append`, `Update`,
    and `Complete`.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Spark 2.3 开始，控制台输出端支持所有输出模式：`Append`、`Update` 和 `Complete`。
- en: The Foreach Sink
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Foreach Sink
- en: There are times when we need to integrate our stream-processing applications
    with legacy systems in the enterprise. Also, as a young project, the range of
    available sinks in Structured Streaming is rather limited.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要将流处理应用程序与企业中的遗留系统集成。此外，作为一个年轻的项目，结构化流中可用接收器的范围相当有限。
- en: The `foreach` sink consists of an API and sink definition that provides access
    to the results of the query execution. It extends the writing capabilities of
    Structured Streaming to any external system that provides a Java virtual machine
    (JVM) client library.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`foreach`接收器包括一个API和接收器定义，提供对查询执行结果的访问。它将结构化流的写入能力扩展到任何提供Java虚拟机（JVM）客户端库的外部系统。'
- en: The ForeachWriter Interface
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ForeachWriter 接口
- en: To use the Foreach sink we must provide an implementation of the `ForeachWriter`
    interface. The `ForeachWriter` controls the life cycle of the writer operation.
    Its execution takes place distributed on the executors, and the methods are called
    for each partition of the streaming `DataFrame` or `Dataset`, as demonstrated
    in [Example 11-6](#foreachwriter_api).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`Foreach`接收器，我们必须提供`ForeachWriter`接口的实现。`ForeachWriter`控制写入操作的生命周期。其执行在执行器上分布，并且方法将针对流`DataFrame`或`Dataset`的每个分区调用，如[示例
    11-6](#foreachwriter_api)所示。
- en: Example 11-6\. The API definition of the ForeachWriter
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-6\. ForeachWriter 的 API 定义
- en: '[PRE8]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As we can see in [Example 11-6](#foreachwriter_api), the `ForeachWriter` is
    bound to a type `[T]` that corresponds to the type of the streaming `Dataset`
    or to `spark.sql.Row` in case of a streaming `DataFrame`. Its API consists of
    three methods: `open`, `process`, and `close`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[示例 11-6](#foreachwriter_api)中所见，`ForeachWriter`与流`Dataset`的类型`[T]`或在流`DataFrame`的情况下对应于`spark.sql.Row`绑定。其API包括三个方法：`open`，`process`和`close`：
- en: '`open`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`open`'
- en: This is called at every trigger interval with the `partitionId` and a unique
    `version` number. Using these two parameters, the `ForeachWriter` must decide
    whether to process the partition being offered. Returning `true` will lead to
    the processing of each element using the logic in the `process` method. If the
    method returns `false`, the partition will be skipped for processing.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 每个触发间隔都会调用此方法，带有`partitionId`和唯一的`version`号。使用这两个参数，`ForeachWriter`必须决定是否处理所提供的分区。返回`true`将导致使用`process`方法中的逻辑处理每个元素。如果方法返回`false`，则跳过该分区的处理。
- en: '`process`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`process`'
- en: This provides access to the data, one element at a time. The function applied
    to the data must produce a side effect, such as inserting the record in a database,
    calling a REST API, or using a networking library to communicate the data to another
    system.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供对数据的访问，每次一个元素。应用于数据的函数必须产生副作用，比如将记录插入数据库，调用REST API，或者使用网络库将数据通信到另一个系统。
- en: '`close`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`close`'
- en: This is called to notify the end of writing a partition. The `error` object
    will be null when the output operation terminated successfully for this partition
    or will contain a `Throwable` otherwise. `close` is called at the end of every
    partition writing operation, even when `open` returned `false` (to indicate that
    the partition should not be processed).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法用于通知写入分区的结束。当此分区的输出操作成功终止时，`error`对象将为null；否则，将包含一个`Throwable`。即使`open`返回`false`（表示不应处理该分区），也会在每个分区写入操作结束时调用`close`。
- en: This contract is part of the data delivery semantics because it allows us to
    remove duplicated partitions that might already have been sent to the sink but
    are reprocessed by Structured Streaming as part of a recovery scenario. For that
    mechanism to properly work, the sink must implement some persistent way to remember
    the `partition/version` combinations that it has already seen.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此合同是数据传递语义的一部分，因为它允许我们移除可能已经被发送到接收器但由于结构化流的恢复场景重新处理的重复分区。为了使该机制正常工作，接收器必须实现某种持久化方式来记住已经看到的`partition/version`组合。
- en: After we have our `ForeachWriter` implementation, we use the customary `writeStream`
    method of declaring a sink and we call the dedicated `foreach` method with the
    `ForeachWriter` instance.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现了我们的`ForeachWriter`之后，我们使用惯用的`writeStream`方法声明一个接收器，并调用带有`ForeachWriter`实例的专用`foreach`方法。
- en: The `ForeachWriter` implementation must be `Serializable`. This is mandatory
    because the `ForeachWriter` is executed distributedly on each node of the cluster
    that contains a partition of the streaming `Dataset` or `DataFrame` being processed.
    At runtime, a new deserialized copy of the provided `ForeachWriter` instance will
    be created for each partition of the `Dataset` or `DataFrame`. As a consequence,
    we might not pass any state in the initial constructor of the `ForeachWriter`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`ForeachWriter`的实现必须是`Serializable`。这是强制性的，因为`ForeachWriter`在处理流式`Dataset`或`DataFrame`的每个分区的每个节点上分布执行。在运行时，将为`Dataset`或`DataFrame`的每个分区创建一个新的反序列化的`ForeachWriter`实例的副本。因此，我们可能不会在`ForeachWriter`的初始构造函数中传递任何状态。'
- en: Let’s put this all together in a small example that shows how the Foreach sink
    works and illustrates the subtle intricacies of dealing with the state handling
    and serialization requirements.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有这些放在一个小示例中，展示Foreach接收器的工作方式，并说明处理状态处理和序列化要求的微妙复杂性。
- en: 'TCP Writer Sink: A Practical ForeachWriter Example'
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'TCP Writer Sink: A Practical ForeachWriter Example'
- en: For this example, we are going to develop a text-based TCP sink that transmits
    the results of the query to an external TCP socket receiving server. In this example,
    we will be using the `spark-shell` utility that comes with the Spark installation.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将开发一个基于文本的TCP接收器，将查询结果传输到外部TCP套接字接收服务器。在这个示例中，我们将使用与Spark安装一起提供的`spark-shell`实用工具。
- en: In [Example 11-7](#tpc_socket_client), we create a simple TCP client that can
    connect and write text to a server socket, provided its `host` and `port`. Note
    that this class is not `Serializable`. `Socket`s are inherently nonserializable
    because they are dependent on the underlying system I/O ports.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 11-7](#tpc_socket_client)中，我们创建了一个简单的TCP客户端，可以连接并向服务器套接字写入文本，只要提供其`host`和`port`。请注意，这个类不是`Serializable`。`Socket`本质上是不可序列化的，因为它们依赖于底层系统的I/O端口。
- en: Example 11-7\. TCP socket client
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-7\. TCP套接字客户端
- en: '[PRE9]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Next, in [Example 11-8](#foreach_tcp_writer), we use this `TCPWriter` in a `ForeachWriter`
    implementation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在[示例 11-8](#foreach_tcp_writer)中，我们将在`ForeachWriter`实现中使用这个`TCPWriter`。
- en: Example 11-8\. TCPForeachWriter implementation
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-8\. TCPForeachWriter 实现
- en: '[PRE10]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Pay close attention to how we have declared the `TCPWriter` variable: `@transient
    var writer:TCPWriter = _`. `@transient` means that this reference should not be
    serialized. The initial value is `null` (using the empty variable initialization
    syntax `_`). It’s only in the call to `open` that we create an instance of `TCPWriter`
    and assign it to our variable for later use.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何声明`TCPWriter`变量：`@transient var writer:TCPWriter = _`。`@transient`表示这个引用不应该被序列化。初始值为`null`（使用空变量初始化语法`_`）。只有在调用`open`时，我们才创建`TCPWriter`的实例，并将其分配给我们的变量以供稍后使用。
- en: Also, note how the `process` method takes an object of type `RateTick`. Implementing
    a `ForeachWriter` is easier when we have a typed `Dataset` to start with as we
    deal with a specific object structure instead of `spark.sql.Row`s, which are the
    generic data container for *streaming DataFrames*. In this case, we transformed
    the initial streaming `DataFrame` to a typed `Dataset[RateTick]` before proceeding
    to the sink phase.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意`process`方法如何接受`RateTick`类型的对象。当我们有一个类型化的`Dataset`时，实现`ForeachWriter`会更容易，因为我们处理特定的对象结构，而不是*流式DataFrame*的通用数据容器`spark.sql.Row`。在这种情况下，我们将初始流式`DataFrame`转换为类型化的`Dataset[RateTick]`，然后继续到接收器阶段。
- en: 'Now, to complete our example, we create a simple `Rate` source and write the
    produced stream directly to our newly developed `TCPForeachWriter`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了完成我们的示例，我们创建一个简单的`Rate`数据源，并将产生的流直接写入我们新开发的`TCPForeachWriter`：
- en: '[PRE11]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Before starting our query, we run a simple TCP server to observe the results.
    For this purpose, we use an `nc`, a useful *nix command to create TCP/UDP clients
    and servers in the command line. In this case, we use a TCP server listening to
    port `9876`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始我们的查询之前，我们运行一个简单的TCP服务器来观察结果。为此，我们使用`nc`，这是一个在命令行中创建TCP/UDP客户端和服务器的有用*nix命令。在这种情况下，我们使用监听端口`9876`的TCP服务器：
- en: '[PRE12]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we start our query:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们开始我们的查询：
- en: '[PRE13]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the shell running the `nc` command, we should see output like the following:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行`nc`命令的shell中，我们应该看到类似以下的输出：
- en: '[PRE14]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the output, the first column is the `partition`, and the second is the `version`,
    followed by the data produced by the `Rate` source. It’s interesting to note that
    the data is ordered within a partition, like `partition 0` in our example, but
    there are no ordering guarantees among different partitions. Partitions are processed
    in parallel in different machines of the cluster. There’s no guarantee which one
    comes first.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，第一列是`partition`，第二列是`version`，后面是`Rate`源产生的数据。有趣的是，数据在分区内是有序的，比如我们的示例中的`partition
    0`，但在不同分区之间没有排序保证。分区在集群的不同机器上并行处理。没有保证哪一个先到达。
- en: 'Finally, to end the query execution, we call the `stop` method:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了结束查询执行，我们调用`stop`方法：
- en: '[PRE15]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The Moral of this Example
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这个例子的教训
- en: 'In this example, you have seen how to correctly use a minimalistic `socket`
    client to output the data of a streaming query with the Foreach sink. Socket communication
    is the underlying interaction mechanism of most database drivers and many other
    application clients in the wild. The method that we have illustrated here is a
    common pattern that you can effectively apply to write to a variety of external
    systems that offer a JVM-based client library. In a nutshell, we can summarize
    this pattern as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，您已经看到了如何正确使用一个简约的`socket`客户端来输出流查询的数据，使用Foreach sink。Socket通信是大多数数据库驱动程序和许多其他应用程序客户端的底层交互机制。我们在这里展示的方法是一个常见的模式，您可以有效地应用它来写入各种提供基于JVM的客户端库的外部系统。简而言之，我们可以总结这个模式如下：
- en: Create a `@transient` mutable reference to our driver class in the body of the
    `ForeachWriter`.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`ForeachWriter`的主体中，创建一个`@transient`可变引用到我们的驱动类。
- en: In the `open` method, initialize a connection to the external system. Assign
    this connection to the mutable reference. It’s guaranteed that this reference
    will be used by a single thread.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`open`方法中，初始化到外部系统的连接。将此连接分配给可变引用。保证此引用将被单个线程使用。
- en: In `process`, publish the provided data element to the external system.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`process`中，将提供的数据元素发布到外部系统。
- en: Finally, in `close`, we terminate all connections and clean up any state.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在`close`中，我们终止所有连接并清理任何状态。
- en: Troubleshooting ForeachWriter Serialization Issues
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障排除`ForeachWriter`序列化问题
- en: 'In [Example 11-8](#foreach_tcp_writer), we saw how we needed an uninitialized
    mutable reference to the `TCPWriter`: `@transient var writer:TCPWriter = _`. This
    seemingly elaborate construct is required to ensure that we instantiate the nonserializable
    class only when the `ForeachWriter` is already deserialized and running remotely,
    in an executor.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 11-8](#foreach_tcp_writer)中，我们看到我们需要一个未初始化的可变引用到`TCPWriter`：`@transient
    var writer:TCPWriter = _`。这种看似复杂的结构是必要的，以确保我们只在`ForeachWriter`已经反序列化并在远程执行器上运行时实例化非可序列化类。
- en: 'If we want to explore what happens when we attempt to include a nonserializable
    reference in a `ForeachWriter` implementation, we could declare our `TCPWriter`
    instance like this, instead:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想探索在`ForeachWriter`实现中尝试包含非可序列化引用时发生的情况，我们可以像这样声明我们的`TCPWriter`实例：
- en: '[PRE16]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Although this looks simpler and more familiar, when we attempt to run our query
    with this `ForeachWriter` implementation, we get a `org.apache.spark.SparkException:
    Task not serializable`. This produces a very long *stack trace* that contains
    a best-effort attempt at pointing out the offending class. We must follow the
    stack trace until we find the `Caused by` statement, such as that shown in the
    following trace:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管这看起来更简单、更熟悉，但当我们尝试使用此`ForeachWriter`实现运行查询时，我们会得到`org.apache.spark.SparkException:
    Task not serializable`。这会产生一个非常长的*堆栈跟踪*，其中包含对冒犯类的最佳努力指出。我们必须跟随堆栈跟踪，直到找到`Caused
    by`语句，如以下跟踪中所示：'
- en: '[PRE17]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As this example was running in the `spark-shell`, we find some weird `$$`-notation,
    but when we remove that noise, we can see that the nonserializable object is `object
    not serializable (class: TCPWriter)`, and the reference to it is the field `field
    name: nonSerializableWriter, type: class TCPWriter`.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '正如本例在`spark-shell`中运行，我们发现一些奇怪的`$$`表示法，但当我们移除这些噪音时，我们可以看到非可序列化对象是`object not
    serializable (class: TCPWriter)`，其引用是字段`field name: nonSerializableWriter, type:
    class TCPWriter`。'
- en: Serialization issues are common in `ForeachWriter` implementations. Hopefully,
    with the tips in this section, you will be able to avoid any trouble in your own
    implementation. But for cases when this happens, Spark makes a best-effort attempt
    at determining the source of the problem. This information, provided in the stack
    trace, is very valuable to debug and solve these serialization issues.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`ForeachWriter` 实现中常见的序列化问题。希望通过本节的技巧，您能够在自己的实现中避免任何麻烦。但是，如果出现这种情况，Spark 将尽最大努力确定问题的源头。提供在堆栈跟踪中的这些信息对于调试和解决这些序列化问题非常有价值。'
