- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: We welcome you to the second edition of *Learning Spark.* It’s been five years
    since the first edition was published in 2015, originally authored by Holden Karau,
    Andy Konwinski, Patrick Wendell, and Matei Zaharia. This new edition has been
    updated to reflect Apache Spark’s evolution through Spark 2.x and Spark 3.0, including
    its expanded ecosystem of built-in and external data sources, machine learning,
    and streaming technologies with which Spark is tightly integrated.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎您阅读第二版的*Learning Spark*。自2015年首版由Holden Karau、Andy Konwinski、Patrick Wendell和Matei
    Zaharia共同撰写以来已经过去五年。这个新版已经更新，以反映Apache Spark经过Spark 2.x和Spark 3.0的演变，包括其扩展的内置和外部数据源生态系统，机器学习和流处理技术，与Spark紧密集成。
- en: Over the years since its first 1.x release, Spark has become the de facto big
    data unified processing engine. Along the way, it has extended its scope to include
    support for various analytic workloads. Our intent is to capture and curate this
    evolution for readers, showing not only how you can use Spark but how it fits
    into the new era of big data and machine learning. Hence, we have designed each
    chapter to build progressively on the foundations laid by the previous chapters,
    ensuring that the content is suited for our intended audience.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 自其首个1.x版本发布以来的多年间，Spark已成为事实上的大数据统一处理引擎。在此过程中，它已扩展其范围以支持各种分析工作负载。我们的目的是为读者捕捉和整理这一演变过程，展示不仅可以如何使用Spark，而且它如何适应大数据和机器学习的新时代。因此，我们设计每一章节都在前一章节奠定的基础上逐步建设，确保内容适合我们的目标读者群体。
- en: Who This Book Is For
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适合对象
- en: Most developers who grapple with big data are data engineers, data scientists,
    or machine learning engineers. This book is aimed at those professionals who are
    looking to use Spark to scale their applications to handle massive amounts of
    data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数处理大数据的开发人员是数据工程师、数据科学家或机器学习工程师。本书旨在为那些希望使用Spark来扩展其应用程序以处理大量数据的专业人士提供帮助。
- en: In particular, data engineers will learn how to use Spark’s Structured APIs
    to perform complex data exploration and analysis on both batch and streaming data;
    use Spark SQL for interactive queries; use Spark’s built-in and external data
    sources to read, refine, and write data in different file formats as part of their
    extract, transform, and load (ETL) tasks; and build reliable data lakes with Spark
    and the open source Delta Lake table format.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是数据工程师将学习如何使用Spark的结构化API执行复杂的数据探索和分析，无论是批量还是流式数据；使用Spark SQL进行交互式查询；使用Spark的内置和外部数据源在不同文件格式中读取、精炼和写入数据，作为其提取、转换和加载（ETL）任务的一部分；以及使用Spark和开源Delta
    Lake表格格式构建可靠的数据湖。
- en: For data scientists and machine learning engineers, Spark’s MLlib library offers
    many common algorithms to build distributed machine learning models. We will cover
    how to build pipelines with MLlib, best practices for distributed machine learning,
    how to use Spark to scale single-node models, and how to manage and deploy these
    models using the open source library MLflow.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据科学家和机器学习工程师，Spark的MLlib库提供了许多常见算法来构建分布式机器学习模型。我们将介绍如何使用MLlib构建管道，分布式机器学习的最佳实践，如何使用Spark扩展单节点模型，以及如何使用开源库MLflow管理和部署这些模型。
- en: While the book is focused on learning Spark as an analytical engine for diverse
    workloads, we will not cover all of the languages that Spark supports. Most of
    the examples in the chapters are written in Scala, Python, and SQL. Where necessary,
    we have infused a bit of Java. For those interested in learning Spark with R,
    we recommend Javier Luraschi, Kevin Kuo, and Edgar Ruiz’s [*Mastering Spark with
    R*](http://shop.oreilly.com/product/0636920223764.do) (O’Reilly).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本书着重于将Spark作为多样工作负载的分析引擎进行学习，但我们不会涵盖Spark支持的所有语言。大多数章节中的示例都是用Scala、Python和SQL编写的。在必要时，我们也会添加一些Java。对于有意通过R学习Spark的人士，我们推荐Javier
    Luraschi、Kevin Kuo和Edgar Ruiz的[*Mastering Spark with R*](http://shop.oreilly.com/product/0636920223764.do)（O’Reilly）。
- en: Finally, because Spark is a distributed engine, building an understanding of
    Spark application concepts is critical. We will guide you through how your Spark
    application interacts with Spark’s distributed components and how execution is
    decomposed into parallel tasks on a cluster. We will also cover which deployment
    modes are supported and in what environments.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于Spark是一个分布式引擎，建立对Spark应用概念的理解至关重要。我们将指导您了解您的Spark应用程序如何与Spark的分布式组件进行交互，以及如何将执行分解为集群上的并行任务。我们还将涵盖支持的部署模式及其适用环境。
- en: While there are many topics we have chosen to cover, there are a few that we
    have opted to not focus on. These include the older low-level Resilient Distributed
    Dataset (RDD) APIs and GraphX, Spark’s API for graphs and graph-parallel computation.
    Nor have we covered advanced topics such as how to extend Spark’s Catalyst optimizer
    to implement your own operations, how to implement your own catalog, or how to
    write your own DataSource V2 data sinks and sources. Though part of Spark, these
    are beyond the scope of your first book on learning Spark.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多我们选择涵盖的主题，但我们选择不专注于的主题也有几个。这些包括旧版的低级别Resilient Distributed Dataset (RDD)
    API和GraphX，即用于图形和图形并行计算的Spark API。我们也没有涵盖高级主题，比如如何扩展Spark的Catalyst优化器以实现自己的操作，如何实现自己的目录，或者如何编写自己的DataSource
    V2数据接收器和数据源。虽然这些都是Spark的一部分，但它们超出了您第一本关于学习Spark的书籍的范围。
- en: Instead, we have focused and organized the book around Spark’s Structured APIs,
    across all its components, and how you can use Spark to process structured data
    at scale to perform your data engineering or data science tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们已经将书籍围绕Spark的结构化API进行了重点和组织，跨其所有组件，并展示您如何使用Spark在规模上处理结构化数据以执行数据工程或数据科学任务。
- en: How the Book Is Organized
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 书籍的组织方式
- en: We organized the book in a way that leads you from chapter to chapter by introducing
    concepts, demonstrating these concepts via example code snippets, and providing
    full code examples or notebooks in the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照一种引导您从章节到章节的方式组织了这本书，通过介绍概念，通过示例代码片段演示这些概念，并在书的[GitHub存储库](https://github.com/databricks/LearningSparkV2)中提供完整的代码示例或笔记本。
- en: '[Chapter 1, *Introduction to Apache Spark: A Unified Analytics Engine*](ch01.html#introduction_to_apache_spark_a_unified_a)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[第一章，*Apache Spark简介：统一分析引擎*](ch01.html#introduction_to_apache_spark_a_unified_a)'
- en: Introduces you to the evolution of big data and provides a high-level overview
    of Apache Spark and its application to big data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍大数据的演变，并提供Apache Spark的高级概述及其在大数据中的应用。
- en: '[Chapter 2, *Downloading Apache Spark and Getting Started*](ch02.html#downloading_apache_spark_and_getting_sta)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[第二章，*下载Apache Spark并入门*](ch02.html#downloading_apache_spark_and_getting_sta)'
- en: Walks you through downloading and setting up Apache Spark on your local machine.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 引导您下载并在本地计算机上设置Apache Spark。
- en: '[Chapter 3, *Apache Spark’s Structured APIs*](ch03.html#apache_sparkapostrophes_structured_apis)
    through [Chapter 6, *Spark SQL and Datasets*](ch06.html#spark_sql_and_datasets)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[第三章，*Apache Spark的结构化APIs*](ch03.html#apache_sparkapostrophes_structured_apis)
    到 [第六章，*Spark SQL和数据集*](ch06.html#spark_sql_and_datasets)'
- en: These chapters focus on using the DataFrame and Dataset Structured APIs to ingest
    data from built-in and external data sources, apply built-in and custom functions,
    and utilize Spark SQL. These chapters comprise the foundation for later chapters,
    incorporating all the latest Spark 3.0 changes where appropriate.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些章节专注于使用DataFrame和Dataset结构化API从内置和外部数据源中摄取数据，应用内置和自定义函数，并利用Spark SQL。这些章节构成后续章节的基础，包括所有最新的Spark
    3.0变更。
- en: '[Chapter 7, *Optimizing and Tuning Spark Applications*](ch07.html#optimizing_and_tuning_spark_applications)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[第七章，*优化和调优Spark应用程序*](ch07.html#optimizing_and_tuning_spark_applications)'
- en: Provides you with best practices for tuning, optimizing, debugging, and inspecting
    your Spark applications through the Spark UI, as well as details on the configurations
    you can tune to increase performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Spark UI提供了调优、优化、调试和检查Spark应用程序的最佳实践，以及可以调整以提高性能的配置细节。
- en: '[Chapter 8, *Structured Streaming*](ch08.html#structured_streaming)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[第八章，*结构化流处理*](ch08.html#structured_streaming)'
- en: Guides you through the evolution of the Spark Streaming engine and the Structured
    Streaming programming model. It examines the anatomy of a typical streaming query
    and discusses the different ways to transform streaming data—stateful aggregations,
    stream joins, and arbitrary stateful aggregation—while providing guidance on how
    to design performant streaming queries.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 指导您通过Spark流处理引擎的演变和结构化流处理编程模型。它检查典型流查询的解剖，并讨论转换流数据的不同方法——有状态聚合、流连接和任意有状态聚合——同时指导您如何设计高性能的流查询。
- en: '[Chapter 9, *Building Reliable Data Lakes with Apache Spark*](ch09.html#building_reliable_data_lakes_with_apache)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[第九章，*使用Apache Spark构建可靠的数据湖*](ch09.html#building_reliable_data_lakes_with_apache)'
- en: Surveys three open source table format storage solutions, as part of the Spark
    ecosystem, that employ Apache Spark to build reliable data lakes with transactional
    guarantees. Due to Delta Lake’s tight integration with Spark for both batch and
    streaming workloads, we focus on that solution and explore how it facilitates
    a new paradigm in data management, the lakehouse.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 调查了三种开源表格式存储解决方案，作为Spark生态系统的一部分，它们利用Apache Spark构建具有事务保证的可靠数据湖。由于Delta Lake与Spark在批处理和流处理工作负载中的紧密集成，我们重点关注该解决方案，并探讨其如何促进数据管理新范式，即“lakehouse”。
- en: '[Chapter 10, *Machine Learning with MLlib*](ch10.html#machine_learning_with_mllib)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[第10章，*MLlib机器学习*](ch10.html#machine_learning_with_mllib)'
- en: Introduces MLlib, the distributed machine learning library for Spark, and walks
    you through an end-to-end example of how to build a machine learning pipeline,
    including topics such as feature engineering, hyperparameter tuning, evaluation
    metrics, and saving and loading models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍了MLlib，Spark的分布式机器学习库，并通过端到端示例演示了如何构建机器学习流水线，包括特征工程、超参数调优、评估指标以及模型的保存和加载。
- en: '[Chapter 11, *Managing, Deploying, and Scaling Machine Learning Pipelines with
    Apache Spark*](ch11.html#managingcomma_deployingcomma_and_scaling)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[第11章，*管理、部署和扩展机器学习流水线与Apache Spark*](ch11.html#managingcomma_deployingcomma_and_scaling)'
- en: Covers how to track and manage your MLlib models with MLflow, compares and contrasts
    different model deployment options, and explores how to leverage Spark for non-MLlib
    models for distributed model inference, feature engineering, and/or hyperparameter
    tuning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 讲解如何使用MLflow跟踪和管理MLlib模型，比较和对比不同的模型部署选项，探讨如何利用Spark进行非MLlib模型的分布式模型推断、特征工程和/或超参数调优。
- en: '[Chapter 12, *Epilogue: Apache Spark 3.0*](ch12.html#epilogue_apache_spark_3dot0)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[第12章，*尾声：Apache Spark 3.0*](ch12.html#epilogue_apache_spark_3dot0)'
- en: The epilogue highlights notable features and changes in Spark 3.0\. While the
    full range of enhancements and features is too extensive to fit in a single chapter,
    we highlight the major changes you should be aware of and recommend you check
    the release notes when Spark 3.0 is officially released.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尾声突出了Spark 3.0的显著特性和变化。虽然增强和特性的全面范围太广泛而无法适应单一章节，但我们突出了您应该了解的主要变化，并建议您在Spark
    3.0正式发布时查看发布说明。
- en: Throughout these chapters, we have incorporated or noted Spark 3.0 features
    where needed and tested all the code examples and notebooks against Spark 3.0.0-preview2.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些章节中，我们根据需要引入或记录了Spark 3.0的功能，并针对Spark 3.0.0-preview2测试了所有的代码示例和notebooks。
- en: How to Use the Code Examples
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用代码示例
- en: The code examples in the book range from brief snippets to complete Spark applications
    and end-to-end notebooks, in Scala, Python, SQL, and, where necessary, Java.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码示例从简短的代码片段到完整的Spark应用程序和端到端notebooks，涵盖Scala、Python、SQL以及必要时的Java。
- en: While some short code snippets in a chapter are self-contained and can be copied
    and pasted to run in a Spark shell (`pyspark` or `spark-shell`), others are fragments
    from standalone Spark applications or end-to-end notebooks. To run standalone
    Spark applications in Scala, Python, or Java, read the instructions in the respective
    chapter’s README files in this book’s [GitHub repo](https://github.com/databricks/LearningSparkV2).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然某些章节中的短代码片段是自包含的，可以复制并粘贴到Spark shell（`pyspark`或`spark-shell`）中运行，其他则是来自独立的Scala、Python或Java
    Spark应用程序或端到端notebooks的片段。要在Scala、Python或Java中运行独立的Spark应用程序，请阅读本书GitHub存储库的各自章节的README文件中的说明。
- en: As for the notebooks, to run these you will need to register for a free [Databricks
    Community Edition](https://community.cloud.databricks.com/) account. We detail
    how to import the notebooks and create a cluster using Spark 3.0 in the [README](https://github.com/databricks/LearningSparkV2/tree/master/notebooks).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 至于notebooks，要运行这些内容，您需要注册一个免费的[Databricks社区版](https://community.cloud.databricks.com/)帐户。我们详细介绍了如何导入这些notebooks，并在Spark
    3.0中创建一个集群，详见[README](https://github.com/databricks/LearningSparkV2/tree/master/notebooks)。
- en: Software and Configuration Used
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的软件和配置
- en: Most of the code in this book and the accompanying notebooks were written in
    and tested against Apache Spark 3.0.0-preview2, which was available to us at the
    time we were writing the final chapters.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中大部分代码及其相关notebooks均针对Apache Spark 3.0.0-preview2编写和测试，这是我们在编写最终章节时可用的版本。
- en: 'By the time this book is published, Apache Spark 3.0 will have been released
    and be available to the community for general use. We recommend that you [download](https://oreil.ly/WFX48)
    and use the official release with the following configurations for your operating
    system:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当本书出版时，Apache Spark 3.0 将已发布并可供社区普遍使用。我们建议您根据您操作系统的以下配置 [下载](https://oreil.ly/WFX48)
    并使用官方版本：
- en: Apache Spark 3.0 (prebuilt for Apache Hadoop 2.7)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 3.0（为 Apache Hadoop 2.7 预构建）
- en: Java Development Kit (JDK) 1.8.0
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java 开发工具包（JDK）1.8.0
- en: If you intend to use only Python, then you can simply run `pip install pyspark`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只打算使用 Python，则可以简单地运行 `pip install pyspark`。
- en: Conventions Used in This Book
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书中使用的约定
- en: 'The following typographical conventions are used in this book:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用以下排版约定：
- en: '*Italic*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*斜体*'
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表示新术语、网址、电子邮件地址、文件名和文件扩展名。
- en: '`Constant width`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`固定宽度`'
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 用于程序清单，以及段落内引用程序元素，如变量或函数名称、数据库、数据类型、环境变量、语句和关键字。
- en: '`**Constant width bold**`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`**粗体固定宽度**`'
- en: Shows commands or other text that should be typed literally by the user.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 显示用户应直接输入的命令或其他文本。
- en: '`*Constant width italic*`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`*斜体固定宽度*`'
- en: Shows text that should be replaced with user-supplied values or by values determined
    by context.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 显示应由用户提供的值或由上下文确定的值。
- en: Note
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This element signifies a general note.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示一般注释。
- en: Using Code Examples
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用代码示例
- en: If you have a technical question or a problem using the code examples, please
    send an email to [*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有技术问题或使用代码示例时遇到问题，请发送电子邮件至 [*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com)。
- en: This book is here to help you get your job done. In general, if example code
    is offered with this book, you may use it in your programs and documentation.
    You do not need to contact us for permission unless you’re reproducing a significant
    portion of the code. For example, writing a program that uses several chunks of
    code from this book does not require permission. Selling or distributing examples
    from O’Reilly books does require permission. Answering a question by citing this
    book and quoting example code does not require permission. Incorporating a significant
    amount of example code from this book into your product’s documentation does require
    permission.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在帮助您完成工作。一般情况下，如果本书提供示例代码，则您可以在您的程序和文档中使用它。除非您要复制本书的大部分代码，否则您无需联系我们以获得许可。例如，编写一个使用本书多个代码片段的程序不需要许可。销售或分发
    O'Reilly 书籍中的示例代码需要许可。引用本书并引用示例代码回答问题不需要许可。将本书中大量示例代码整合到您产品的文档中需要许可。
- en: 'We appreciate, but generally do not require, attribution. An attribution usually
    includes the title, author, publisher, and ISBN. For example: “*Learning Spark*,
    2nd Edition, by Jules S. Damji, Brooke Wenig, Tathagata Das, and Denny Lee. Copyright
    2020 Databricks, Inc., 978-1-492-05004-9.”'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们赞赏，但通常不要求署名。署名通常包括标题、作者、出版商和ISBN。例如：“*学习 Spark*，第二版，作者 Jules S. Damji、Brooke
    Wenig、Tathagata Das 和 Denny Lee。版权所有 2020 Databricks, Inc.，978-1-492-05004-9。”
- en: If you feel your use of code examples falls outside fair use or the permission
    given above, feel free to contact us at [*permissions@oreilly.com*](mailto:permissions@oreilly.com).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您认为您对代码示例的使用超出了合理使用范围或上述许可，请随时与我们联系，邮箱为 [*permissions@oreilly.com*](mailto:permissions@oreilly.com)。
- en: O’Reilly Online Learning
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: O'Reilly 在线学习
- en: Note
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For more than 40 years, [*O’Reilly Media*](http://oreilly.com) has provided
    technology and business training, knowledge, and insight to help companies succeed.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 40 多年来，[*O'Reilly Media*](http://oreilly.com) 提供技术和商业培训、知识和见解，帮助公司取得成功。
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, and our online learning platform. O’Reilly’s online learning
    platform gives you on-demand access to live training courses, in-depth learning
    paths, interactive coding environments, and a vast collection of text and video
    from O’Reilly and 200+ other publishers. For more information, visit [*http://oreilly.com*](http://oreilly.com).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专长。O’Reilly的在线学习平台为您提供按需访问的实时培训课程、深入的学习路径、交互式编码环境以及来自O’Reilly和其他200多家出版商的大量文本和视频。有关更多信息，请访问
    [*http://oreilly.com*](http://oreilly.com)。
- en: How to Contact Us
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何联系我们
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请将有关本书的评论和问题发送给出版商：
- en: O’Reilly Media, Inc.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Reilly Media, Inc.
- en: 1005 Gravenstein Highway North
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1005 Gravenstein Highway North
- en: Sebastopol, CA 95472
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sebastopol, CA 95472
- en: 800-998-9938 (in the United States or Canada)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 800-998-9938（美国或加拿大）
- en: 707-829-0515 (international or local)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0515（国际或本地）
- en: 707-829-0104 (fax)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0104 (传真)
- en: Visit our web page for this book, where we list errata, examples, and any additional
    information, at [*https://oreil.ly/LearningSpark2*](https://oreil.ly/LearningSpark2).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 访问我们的书页，我们在那里列出勘误、示例和任何额外信息，网址为 [*https://oreil.ly/LearningSpark2*](https://oreil.ly/LearningSpark2)。
- en: Email [*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com) to comment
    or ask technical questions about this book.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件 [*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com)，您可以评论或询问有关本书的技术问题。
- en: For news and information about our books and courses, visit [*http://oreilly.com*](http://oreilly.com).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有关我们的书籍和课程的新闻和信息，请访问 [*http://oreilly.com*](http://oreilly.com)。
- en: 'Find us on Facebook: [*http://facebook.com/oreilly*](http://facebook.com/oreilly)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在Facebook上找到我们：[*http://facebook.com/oreilly*](http://facebook.com/oreilly)
- en: 'Follow us on Twitter: [*http://twitter.com/oreillymedia*](http://twitter.com/oreillymedia)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在Twitter上关注我们：[*http://twitter.com/oreillymedia*](http://twitter.com/oreillymedia)
- en: 'Watch us on YouTube: [*http://www.youtube.com/oreillymedia*](http://www.youtube.com/oreillymedia)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在YouTube上关注我们：[*http://www.youtube.com/oreillymedia*](http://www.youtube.com/oreillymedia)
- en: Acknowledgments
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: This project was truly a team effort involving many people, and without their
    support and feedback we would not have been able to finish this book, especially
    in today’s unprecedented COVID-19 times.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目真正是众多人的团队努力，没有他们的支持和反馈，我们将无法完成这本书，尤其是在当今前所未有的COVID-19时代。
- en: First and foremost, we want to thank our employer, Databricks, for supporting
    us and allocating us dedicated time as part of our jobs to finish this book. In
    particular, we want to thank Matei Zaharia, Reynold Xin, Ali Ghodsi, Ryan Boyd,
    and Rick Schultz for encouraging us to write the second edition.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们要感谢我们的雇主，Databricks，支持我们并在工作中分配了专门的时间来完成这本书。特别感谢Matei Zaharia、Reynold Xin、Ali
    Ghodsi、Ryan Boyd和Rick Schultz，他们鼓励我们写第二版。
- en: 'Second, we would like to thank our technical reviewers: Adam Breindel, Amir
    Issaei, Jacek Laskowski, Sean Owen, and Vishwanath Subramanian. Their diligent
    and constructive feedback, informed by their technical expertise in the community
    and industry point of view, made this book what it is: a valuable resource to
    learn Spark.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们要感谢我们的技术审阅者：Adam Breindel、Amir Issaei、Jacek Laskowski、Sean Owen和Vishwanath
    Subramanian。他们通过他们在社区和行业的技术专业知识，提供了勤奋和建设性的反馈，使本书成为了宝贵的资源，用来学习Spark。
- en: 'Besides the formal book reviewers, we received invaluable feedback from others
    knowledgeable about specific topics and sections of the chapters, and we want
    to acknowledge their contributions. Many thanks to: Conor Murphy, Hyukjin Kwon,
    Maryann Xue, Niall Turbitt, Wenchen Fan, Xiao Li, and Yuanjian Li.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了正式的书评者外，我们还从其他具有特定主题和章节知识的人士那里获得了宝贵的反馈，我们要感谢他们的贡献。特别感谢：Conor Murphy、Hyukjin
    Kwon、Maryann Xue、Niall Turbitt、Wenchen Fan、Xiao Li和Yuanjian Li。
- en: Finally, we would like to thank our colleagues at Databricks (for their tolerance
    of us missing or neglecting project deadlines), our families and loved ones (for
    their patience and empathy as we wrote in the early light of day or late into
    the night on weekdays and weekends), and the entire open source Spark community.
    Without their continued contributions, Spark would not be where it is today—and
    we authors would not have had much to write about.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们要感谢我们在Databricks的同事（他们对我们错过或忽视项目截止日期的包容）、我们的家人和爱人（他们在我们白天或周末深夜写作时的耐心与同情）、以及整个开源Spark社区。没有他们的持续贡献，Spark就不会取得今天的成就——我们这些作者也没有什么可写的。
- en: Thank you all!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢大家！
