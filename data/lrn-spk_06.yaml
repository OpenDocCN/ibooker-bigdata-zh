- en: 'Chapter 5\. Spark SQL and DataFrames: Interacting with External Data Sources'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章：Spark SQL 和 DataFrames：与外部数据源交互
- en: 'In the previous chapter, we explored interacting with the built-in data sources
    in Spark. We also took a closer look at the DataFrame API and its interoperability
    with Spark SQL. In this chapter, we will focus on how Spark SQL interfaces with
    external components. Specifically, we discuss how Spark SQL allows you to:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了与 Spark 内置数据源的交互。我们还深入研究了 DataFrame API 及其与 Spark SQL 的互操作性。在本章中，我们将重点介绍
    Spark SQL 如何与外部组件接口。具体来说，我们将讨论 Spark SQL 如何让您实现以下功能：
- en: Use user-defined functions for both Apache Hive and Apache Spark.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用用户定义函数同时适用于 Apache Hive 和 Apache Spark。
- en: Connect with external data sources such as JDBC and SQL databases, PostgreSQL,
    MySQL, Tableau, Azure Cosmos DB, and MS SQL Server.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接外部数据源，如 JDBC 和 SQL 数据库，PostgreSQL、MySQL、Tableau、Azure Cosmos DB 和 MS SQL Server。
- en: Work with simple and complex types, higher-order functions, and common relational
    operators.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理简单和复杂类型，高阶函数以及常见的关系操作符。
- en: We’ll also look at some different options for querying Spark using Spark SQL,
    such as the Spark SQL shell, Beeline, and Tableau.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨使用 Spark SQL 查询 Spark 的一些不同选项，例如 Spark SQL shell、Beeline 和 Tableau。
- en: Spark SQL and Apache Hive
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL 和 Apache Hive
- en: Spark SQL is a foundational component of Apache Spark that integrates relational
    processing with Spark’s functional programming API. Its genesis was in [previous
    work on Shark](https://oreil.ly/QEixA). Shark was originally built on the Hive
    codebase on top of Apache Spark^([1](ch05.html#ch05fn1)) and became one of the
    first interactive SQL query engines on Hadoop systems. It demonstrated that it
    was possible to have the [best of both worlds](https://oreil.ly/FrPY6); as fast
    as an enterprise data warehouse, and scaling as well as Hive/MapReduce.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 是 Apache Spark 的基础组件，将关系处理与 Spark 的函数式编程 API 结合。它的起源可以追溯到[之前 Shark
    的工作](https://oreil.ly/QEixA)。Shark 最初是建立在 Apache Spark 之上的 Hive 代码库，并成为 Hadoop
    系统上第一个交互式 SQL 查询引擎之一。它证明了可以拥有[两者兼得的最佳选择](https://oreil.ly/FrPY6)：像企业数据仓库一样快速，并像
    Hive/MapReduce 一样可扩展。
- en: Spark SQL lets Spark programmers leverage the benefits of faster performance
    and relational programming (e.g., declarative queries and optimized storage),
    as well as call complex analytics libraries (e.g., machine learning). As discussed
    in the previous chapter, as of Apache Spark 2.x, the `SparkSession` provides a
    single unified entry point to manipulate data in Spark.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 让 Spark 程序员利用更快的性能和关系式编程的优势（例如声明式查询和优化存储），以及调用复杂的分析库（例如机器学习）。正如前一章讨论的那样，从
    Apache Spark 2.x 开始，`SparkSession` 提供了一个统一的入口点来操作 Spark 中的数据。
- en: User-Defined Functions
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户定义函数
- en: While Apache Spark has a plethora of built-in functions, the flexibility of
    Spark allows for data engineers and data scientists to define their own functions
    too. These are known as *user-defined functions* (UDFs).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Apache Spark 拥有大量内置函数，但 Spark 的灵活性允许数据工程师和数据科学家定义自己的函数。这些称为 *用户定义函数*（UDFs）。
- en: Spark SQL UDFs
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark SQL UDFs
- en: The benefit of creating your own PySpark or Scala UDFs is that you (and others)
    will be able to make use of them within Spark SQL itself. For example, a data
    scientist can wrap an ML model within a UDF so that a data analyst can query its
    predictions in Spark SQL without necessarily understanding the internals of the
    model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自己的 PySpark 或 Scala UDF 的好处在于，您（以及其他人）将能够在 Spark SQL 中使用它们。例如，数据科学家可以将 ML
    模型封装在 UDF 中，以便数据分析师可以在 Spark SQL 中查询其预测结果，而不必深入了解模型的内部。
- en: 'Here’s a simplified example of creating a Spark SQL UDF. Note that UDFs operate
    per session and they will not be persisted in the underlying metastore:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是创建 Spark SQL UDF 的简化示例。请注意，UDF 操作是会话级别的，并不会持久化在底层的元数据存储中：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can now use Spark SQL to execute either of these `cubed()` functions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以使用 Spark SQL 执行以下任意一个 `cubed()` 函数：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Evaluation order and null checking in Spark SQL
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Spark SQL 中进行评估顺序和空值检查
- en: 'Spark SQL (this includes SQL, the DataFrame API, and the Dataset API) does
    not guarantee the order of evaluation of subexpressions. For example, the following
    query does not guarantee that the `s is NOT NULL` clause is executed prior to
    the `strlen(s) > 1` clause:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL（包括 SQL、DataFrame API 和 Dataset API）不保证子表达式的评估顺序。例如，以下查询不保证 `s is NOT
    NULL` 子句在 `strlen(s) > 1` 子句之前执行：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Therefore, to perform proper `null` checking, it is recommended that you do
    the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了执行适当的 `null` 检查，建议您执行以下操作：
- en: Make the UDF itself `null`-aware and do `null` checking inside the UDF.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使UDF本身具有`null`感知性，并在UDF内进行`null`检查。
- en: Use `IF` or `CASE WHEN` expressions to do the `null` check and invoke the UDF
    in a conditional branch.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`IF`或`CASE WHEN`表达式对`null`进行检查，并在条件分支中调用UDF。
- en: Speeding up and distributing PySpark UDFs with Pandas UDFs
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Pandas UDF加速和分发PySpark UDF
- en: One of the previous prevailing issues with using PySpark UDFs was that they
    had slower performance than Scala UDFs. This was because the PySpark UDFs required
    data movement between the JVM and Python, which was quite expensive. To resolve
    this problem, [Pandas UDFs](https://oreil.ly/jo7kl) (also known as vectorized
    UDFs) were introduced as part of Apache Spark 2.3\. A Pandas UDF uses Apache Arrow
    to transfer data and Pandas to work with the data. You define a Pandas UDF using
    the keyword `pandas_udf` as the decorator, or to wrap the function itself. Once
    the data is in [Apache Arrow format](https://oreil.ly/TCsur), there is no longer
    the need to serialize/pickle the data as it is already in a format consumable
    by the Python process. Instead of operating on individual inputs row by row, you
    are operating on a Pandas Series or DataFrame (i.e., vectorized execution).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以前使用PySpark UDF的一个主要问题是，它们的性能比Scala UDF慢。这是因为PySpark UDF需要在JVM和Python之间传输数据，这是非常昂贵的。为了解决这个问题，作为Apache
    Spark 2.3的一部分引入了[Pandas UDFs](https://oreil.ly/jo7kl)（也称为向量化UDFs）。Pandas UDF使用Apache
    Arrow传输数据并使用Pandas处理数据。您可以使用关键字`pandas_udf`作为装饰器来定义Pandas UDF，或者直接包装函数本身。一旦数据处于[Apache
    Arrow格式](https://oreil.ly/TCsur)中，就不再需要序列化/pickle数据，因为数据已经以Python进程可消耗的格式存在。与逐行操作单个输入不同，您正在操作Pandas
    Series或DataFrame（即向量化执行）。
- en: 'From Apache Spark 3.0 with Python 3.6 and above, [Pandas UDFs were split into
    two API categories](https://oreil.ly/rXX-L): Pandas UDFs and Pandas Function APIs.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从Apache Spark 3.0开始，使用Python 3.6及以上版本，[将Pandas UDFs分为两个API类别](https://oreil.ly/rXX-L)：Pandas
    UDFs和Pandas函数API。
- en: Pandas UDFs
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas UDFs
- en: With Apache Spark 3.0, Pandas UDFs infer the Pandas UDF type from Python type
    hints in Pandas UDFs such as `pandas.Series`, `pandas.DataFrame`, `Tuple`, and
    `Iterator`. Previously you needed to manually define and specify each Pandas UDF
    type. Currently, the supported cases of Python type hints in Pandas UDFs are Series
    to Series, Iterator of Series to Iterator of Series, Iterator of Multiple Series
    to Iterator of Series, and Series to Scalar (a single value).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在Apache Spark 3.0中，Pandas UDF根据Pandas UDF中的Python类型提示推断Pandas UDF类型，例如`pandas.Series`、`pandas.DataFrame`、`Tuple`和`Iterator`。以前需要手动定义和指定每个Pandas
    UDF类型。当前在Pandas UDF中支持的Python类型提示的情况包括Series到Series、Series迭代器到Series迭代器、多个Series迭代器到Series迭代器以及Series到标量（单个值）。
- en: Pandas Function APIs
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas函数API
- en: Pandas Function APIs allow you to directly apply a local Python function to
    a PySpark DataFrame where both the input and output are Pandas instances. For
    Spark 3.0, the supported Pandas Function APIs are grouped map, map, co-grouped
    map.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas函数API允许您直接将本地Python函数应用于PySpark DataFrame，其中输入和输出均为Pandas实例。对于Spark 3.0，支持的Pandas函数API包括grouped
    map、map和co-grouped map。
- en: For more information, refer to [“Redesigned Pandas UDFs with Python Type Hints”](ch12.html#redesigned_pandas_udfs_with_python_type)
    in [Chapter 12](ch12.html#epilogue_apache_spark_3dot0).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参阅[“带有Python类型提示的重新设计的Pandas UDFs”](ch12.html#redesigned_pandas_udfs_with_python_type)在[第12章](ch12.html#epilogue_apache_spark_3dot0)。
- en: The following is an example of a scalar Pandas UDF for Spark 3.0:^([2](ch05.html#ch01fn5))
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是Spark 3.0标量Pandas UDF的示例:^([2](ch05.html#ch01fn5))
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding code snippet declares a function called `cubed()` that performs
    a *cubed* operation. This is a regular Pandas function with the additional `cubed_udf
    = pandas_udf()` call to create our Pandas UDF.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段声明了一个名为`cubed()`的函数，执行一个*cubed*操作。这是一个常规的Pandas函数，额外增加了`cubed_udf = pandas_udf()`调用来创建我们的Pandas
    UDF。
- en: 'Let’s start with a simple Pandas Series (as defined for `x`) and then apply
    the local function `cubed()` for the cubed calculation:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的Pandas Series（如定义为`x`）开始，然后应用本地函数`cubed()`进行立方计算：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now let’s switch to a Spark DataFrame. We can execute this function as a Spark
    vectorized UDF as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们切换到Spark DataFrame。我们可以执行此函数作为Spark向量化UDF，如下所示：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here’s the output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As opposed to a local function, using a vectorized UDF will result in the execution
    of Spark jobs; the previous local function is a Pandas function executed only
    on the Spark driver. This becomes more apparent when viewing the Spark UI for
    one of the stages of this `pandas_udf` function ([Figure 5-1](#spark_ui_stages_for_executing_a_pandas_u)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与本地函数相反，使用矢量化UDF将导致执行Spark作业；前面的本地函数仅在Spark驱动程序上执行Pandas函数。当查看此`pandas_udf`函数的一个阶段的Spark
    UI时，这一点变得更加明显（[图5-1](#spark_ui_stages_for_executing_a_pandas_u)）。
- en: Note
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For a deeper dive into Pandas UDFs, refer to [pandas user-defined functions
    documentation](https://oreil.ly/Qi-pb).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解Pandas UDF，请参阅[pandas用户定义函数文档](https://oreil.ly/Qi-pb)。
- en: '![Spark UI stages for executing a Pandas UDF on a Spark DataFrame](assets/lesp_0501.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![Spark UI执行Pandas UDF的阶段](assets/lesp_0501.png)'
- en: Figure 5-1\. Spark UI stages for executing a Pandas UDF on a Spark DataFrame
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. Spark UI执行Pandas UDF的阶段
- en: Like many Spark jobs, the job starts with `parallelize()` to send local data
    (Arrow binary batches) to executors and calls `mapPartitions()` to convert the
    Arrow binary batches to Spark’s internal data format, which can be distributed
    to the Spark workers. There are a number of `WholeStageCodegen` steps, which represent
    a fundamental step up in performance (thanks to Project Tungsten’s [whole-stage
    code generation](https://oreil.ly/5Khvp), which significantly improves CPU efficiency
    and performance). But it is the `ArrowEvalPython` step that identifies that (in
    this case) a Pandas UDF is being executed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多Spark作业一样，该作业从`parallelize()`开始，将本地数据（Arrow二进制批次）发送到执行器，并调用`mapPartitions()`将Arrow二进制批次转换为Spark的内部数据格式，然后可以分发给Spark工作节点。有许多`WholeStageCodegen`步骤，这些步骤代表了性能的重大提升（感谢项目Tungsten的[整阶段代码生成](https://oreil.ly/5Khvp)，显著提高了CPU效率和性能）。但是，`ArrowEvalPython`步骤指出（在本例中）正在执行Pandas
    UDF。
- en: Querying with the Spark SQL Shell, Beeline, and Tableau
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL Shell、Beeline和Tableau进行查询
- en: There are various mechanisms to query Apache Spark, including the Spark SQL
    shell, the Beeline CLI utility, and reporting tools like Tableau and Power BI.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种机制可以查询Apache Spark，包括Spark SQL shell、Beeline CLI实用程序以及Tableau和Power BI等报表工具。
- en: In this section, we include instructions for Tableau; for Power BI, please refer
    to the [documentation](https://oreil.ly/n_KRU).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了有关Tableau的说明；关于Power BI，请参阅[文档](https://oreil.ly/n_KRU)。
- en: Using the Spark SQL Shell
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark SQL Shell
- en: A convenient tool for executing Spark SQL queries is the `spark-sql` CLI. While
    this utility communicates with the Hive metastore service in local mode, it does
    not talk to the [Thrift JDBC/ODBC server](https://oreil.ly/kdfko) (a.k.a. *Spark
    Thrift Server* or *STS*). The STS allows JDBC/ODBC clients to execute SQL queries
    over JDBC and ODBC protocols on Apache Spark.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 执行Spark SQL查询的方便工具是`spark-sql`命令行界面。虽然此实用程序在本地模式下与Hive元数据存储服务通信，但它不会与[Thrift
    JDBC/ODBC服务器](https://oreil.ly/kdfko)（也称为*Spark Thrift Server*或*STS*）通信。STS允许JDBC/ODBC客户端通过JDBC和ODBC协议在Apache
    Spark上执行SQL查询。
- en: 'To start the Spark SQL CLI, execute the following command in the `$SPARK_HOME`
    folder:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动Spark SQL CLI，请在`$SPARK_HOME`文件夹中执行以下命令：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once you’ve started the shell, you can use it to interactively perform Spark
    SQL queries. Let’s take a look at a few examples.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动了shell，您可以使用它进行交互式执行Spark SQL查询。让我们看几个例子。
- en: Create a table
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建表
- en: 'To create a new permanent Spark SQL table, execute the following statement:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个新的永久性Spark SQL表，请执行以下语句：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Your output should be similar to this, noting the creation of the Spark SQL
    table `people` as well as its file location (`/user/hive/warehouse/people`):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 您的输出应该类似于这样，注意创建Spark SQL表`people`以及其文件位置（`/user/hive/warehouse/people`）：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Insert data into the table
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向表中插入数据
- en: 'You can insert data into a Spark SQL table by executing a statement similar
    to:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过执行类似于以下语句将数据插入到Spark SQL表中：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As you’re not dependent on loading data from a preexisting table or file, you
    can insert data into the table using `INSERT...VALUES` statements. These three
    statements insert three individuals (their names and ages, if known) into the
    `people` table:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您不依赖于从预先存在的表或文件中加载数据，因此可以使用`INSERT...VALUES`语句向表中插入数据。以下三个语句将三个个体（如果已知，包括其姓名和年龄）插入到`people`表中：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Running a Spark SQL query
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行Spark SQL查询
- en: 'Now that you have data in your table, you can run Spark SQL queries against
    it. Let’s start by viewing what tables exist in our metastore:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的表中有了数据，可以对其运行Spark SQL查询。让我们从查看我们的元数据存储中存在哪些表开始：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, let’s find out how many people in our table are younger than 20 years
    of age:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们找出我们的表中有多少人年龄小于 20 岁：
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As well, let’s see who the individuals are who did not specify their age:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，让我们看看那些未指定年龄的个体是谁：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Working with Beeline
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Beeline
- en: If you’ve worked with Apache Hive you may be familiar with the command-line
    tool [Beeline](https://oreil.ly/Lcrs-), a common utility for running HiveQL queries
    against HiveServer2\. Beeline is a JDBC client based on the [SQLLine CLI](http://sqlline.sourceforge.net).
    You can use this same utility to execute Spark SQL queries against the Spark Thrift
    server. Note that the currently implemented Thrift JDBC/ODBC server corresponds
    to HiveServer2 in Hive 1.2.1\. You can test the JDBC server with the following
    Beeline script that comes with either Spark or Hive 1.2.1.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用过 Apache Hive，可能对命令行工具 [Beeline](https://oreil.ly/Lcrs-) 比较熟悉，这是一个常用工具，用于针对
    HiveServer2 运行 HiveQL 查询。Beeline 是基于 [SQLLine CLI](http://sqlline.sourceforge.net)
    的 JDBC 客户端。您可以使用同样的工具执行 Spark SQL 查询来连接 Spark Thrift 服务器。请注意，当前实现的 Thrift JDBC/ODBC
    服务器对应于 Hive 1.2.1 中的 HiveServer2。您可以使用随 Spark 或 Hive 1.2.1 提供的以下 Beeline 脚本来测试
    JDBC 服务器。
- en: Start the Thrift server
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动 Thrift 服务器
- en: 'To start the Spark Thrift JDBC/ODBC server, execute the following command from
    the `$SPARK_HOME` folder:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动 Spark Thrift JDBC/ODBC 服务器，请在 `$SPARK_HOME` 文件夹中执行以下命令：
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you have not already started your Spark driver and worker, execute the following
    command prior to `start-thriftserver.sh`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未启动 Spark 驱动程序和工作进程，请在执行 `start-thriftserver.sh` 命令之前执行以下命令：
- en: '[PRE18]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Connect to the Thrift server via Beeline
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过 Beeline 连接到 Thrift 服务器
- en: 'To test the Thrift JDBC/ODBC server using Beeline, execute the following command:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Beeline 测试 Thrift JDBC/ODBC 服务器，请执行以下命令：
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then configure Beeline to connect to the local Thrift server:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后配置 Beeline 连接到本地 Thrift 服务器：
- en: '[PRE20]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: By default, Beeline is in *non-secure mode*. Thus, the username is your login
    (e.g., `user@learningspark.org`) and the password is blank.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Beeline 处于 *非安全模式*。因此，用户名为您的登录名（例如，`user@learningspark.org`），密码为空。
- en: Execute a Spark SQL query with Beeline
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Beeline 执行 Spark SQL 查询
- en: 'From here, you can run a Spark SQL query similar to how you would run a Hive
    query with Beeline. Here are a few sample queries and their output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，您可以运行类似于通过 Beeline 运行 Hive 查询的 Spark SQL 查询。以下是几个示例查询及其输出：
- en: '[PRE21]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Stop the Thrift server
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 停止 Thrift 服务器
- en: 'Once you’re done, you can stop the Thrift server with the following command:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，您可以使用以下命令停止 Thrift 服务器：
- en: '[PRE22]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Working with Tableau
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Tableau
- en: Similar to running queries through Beeline or the Spark SQL CLI, you can connect
    your favorite BI tool to Spark SQL via the Thrift JDBC/ODBC server. In this section,
    we will show you how to connect Tableau Desktop (version 2019.2) to your local
    Apache Spark instance.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与通过 Beeline 或 Spark SQL CLI 运行查询类似，您可以通过 Thrift JDBC/ODBC 服务器将喜爱的 BI 工具连接到 Spark
    SQL。在本节中，我们将向您展示如何将 Tableau Desktop（版本 2019.2）连接到您的本地 Apache Spark 实例。
- en: Note
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You will need to have the [Tableau’s Spark ODBC](https://oreil.ly/wIGnw) driver
    version 1.2.0 or above already installed. If you have installed (or upgraded to)
    Tableau 2018.1 or greater, this driver should already be preinstalled.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装 [Tableau 的 Spark ODBC](https://oreil.ly/wIGnw) 驱动程序版本 1.2.0 或更高版本。如果您已安装（或升级至）Tableau
    2018.1 或更高版本，则应预先安装此驱动程序。
- en: Start the Thrift server
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动 Thrift 服务器
- en: 'To start the Spark Thrift JDBC/ODBC server, execute the following command from
    the `$SPARK_HOME` folder:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 `$SPARK_HOME` 文件夹启动 Spark Thrift JDBC/ODBC 服务器，请执行以下命令：
- en: '[PRE23]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you have not already started your Spark driver and worker, execute the following
    command prior to `start-thriftserver.sh`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未启动 Spark 驱动程序和工作进程，请在执行 `start-thriftserver.sh` 命令之前执行以下命令：
- en: '[PRE24]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Start Tableau
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动 Tableau
- en: If you are starting Tableau for the first time, you will be greeted with a Connect
    dialog that allows you to connect to a plethora of data sources. By default, the
    Spark SQL option will not be included in the “To a Server” menu on the left (see
    [Figure 5-2](#tableau_connect_dialog_box)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您第一次启动 Tableau，将会看到一个连接对话框，允许您连接到多种数据源。默认情况下，左侧的“到服务器”菜单中不包括 Spark SQL 选项（参见[图 5-2](#tableau_connect_dialog_box)）。
- en: '![Tableau Connect dialog box](assets/lesp_0502.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![Tableau 连接对话框](assets/lesp_0502.png)'
- en: Figure 5-2\. Tableau Connect dialog box
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. Tableau 连接对话框
- en: To access the Spark SQL option, click More… at the bottom of that list and then
    choose Spark SQL from the list that appears in the main panel, as shown in [Figure 5-3](#choose_moresemicolonhash174semicolonspar).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问Spark SQL选项，请在列表底部单击“更多…”，然后从主面板出现的列表中选择Spark SQL，如[图 5-3](#choose_moresemicolonhash174semicolonspar)所示。
- en: '![Choose More…Spark SQL to connect to Spark SQL](assets/lesp_0503.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![选择更多…Spark SQL连接到Spark SQL](assets/lesp_0503.png)'
- en: Figure 5-3\. Choose More… > Spark SQL to connect to Spark SQL
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 选择更多… > Spark SQL连接到Spark SQL
- en: 'This will pop up the Spark SQL dialog ([Figure 5-4](#the_spark_sql_dialog_box)).
    As you’re connecting to a local Apache Spark instance, you can use the non-secure
    username authentication mode with the following parameters:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这将弹出Spark SQL对话框（参见[图 5-4](#the_spark_sql_dialog_box)）。由于您正在连接到本地Apache Spark实例，您可以使用以下参数进行非安全用户名身份验证模式：
- en: 'Server: localhost'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器：localhost
- en: 'Port: 10000 (default)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端口：10000（默认）
- en: 'Type: SparkThriftServer (default)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类型：SparkThriftServer（默认）
- en: 'Authentication: Username'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 身份验证：用户名
- en: 'Username: Your login, e.g., user@learningspark.org'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户名：您的登录，例如，user@learningspark.org
- en: 'Require SSL: Not checked'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要SSL：未选中
- en: '![The Spark SQL dialog box](assets/lesp_0504.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![Spark SQL对话框](assets/lesp_0504.png)'
- en: Figure 5-4\. The Spark SQL dialog box
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. Spark SQL对话框
- en: Once you have successfully connected to the Spark SQL data source, you will
    see a Data Source Connections view similar to [Figure 5-5](#tableau_data_source_connections_viewcomm).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到Spark SQL数据源成功后，您将看到类似[图 5-5](#tableau_data_source_connections_viewcomm)的数据源连接视图。
- en: '![Tableau Data Source Connections view, connected to a local Spark instance](assets/lesp_0505.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![Tableau数据源连接视图，连接到本地Spark实例](assets/lesp_0505.png)'
- en: Figure 5-5\. Tableau Data Source Connections view, connected to a local Spark
    instance
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. Tableau数据源连接视图，连接到本地Spark实例
- en: From the Select Schema drop-down menu on the left, choose “default.” Then enter
    the name of the table you want to query (see [Figure 5-6](#select_a_schema_and_a_table_to_query)).
    Note that you can click the magnifying glass icon to get a full list of the tables
    that are available.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧的选择模式下拉菜单中选择“default”。然后输入要查询的表的名称（见[图 5-6](#select_a_schema_and_a_table_to_query)）。请注意，您可以单击放大镜图标以获取可用表的完整列表。
- en: '![Select a schema and a table to query](assets/lesp_0506.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![选择模式和要查询的表](assets/lesp_0506.png)'
- en: Figure 5-6\. Select a schema and a table to query
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-6\. 选择模式和要查询的表
- en: Note
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on using Tableau to connect to a Spark SQL database, refer
    to Tableau’s [Spark SQL documentation](https://oreil.ly/2A6L7) and the Databricks
    [Tableau documentation](https://oreil.ly/--OXu).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解如何使用Tableau连接到Spark SQL数据库的更多信息，请参阅Tableau的[Spark SQL文档](https://oreil.ly/2A6L7)和Databricks的[Tableau文档](https://oreil.ly/--OXu)。
- en: Enter `people` as the table name, then drag and drop the table from the left
    side into the main dialog (in the space marked “Drag tables here”). You should
    see something like [Figure 5-7](#connecting_to_the_people_table_in_your_l).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输入`people`作为表名，然后从左侧拖放表格到主对话框（在“将表拖到此处”标记的空间内）。您应该看到类似[图 5-7](#connecting_to_the_people_table_in_your_l)的内容。
- en: '![Connecting to the people table in your local Spark instance](assets/lesp_0507.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![连接到本地Spark实例中的people表](assets/lesp_0507.png)'
- en: Figure 5-7\. Connecting to the people table in your local Spark instance
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-7\. 连接到本地Spark实例中的people表
- en: Click Update Now, and under the covers Tableau will query your Spark SQL data
    source ([Figure 5-8](#tableau_worksheet_table_view_querying_a)).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 单击“立即更新”，在幕后Tableau将查询您的Spark SQL数据源（参见[图 5-8](#tableau_worksheet_table_view_querying_a)）。
- en: You can now execute queries against your Spark data source, join tables, and
    more, just like with any other Tableau data source.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以执行针对Spark数据源的查询、表连接等操作，就像处理任何其他Tableau数据源一样。
- en: '![Tableau worksheet table view querying a local Spark data source](assets/lesp_0508.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![Tableau工作表表视图查询本地Spark数据源](assets/lesp_0508.png)'
- en: Figure 5-8\. Tableau worksheet table view querying a local Spark data source
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-8\. Tableau工作表表视图查询本地Spark数据源
- en: Stop the Thrift server
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 停止Thrift服务器
- en: 'Once you’re done, you can stop the Thrift server with the following command:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，您可以使用以下命令停止Thrift服务器：
- en: '[PRE25]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: External Data Sources
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部数据源
- en: In this section, we will focus on how to use Spark SQL to connect to external
    data sources, starting with JDBC and SQL databases.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点介绍如何使用Spark SQL连接外部数据源，从JDBC和SQL数据库开始。
- en: JDBC and SQL Databases
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JDBC和SQL数据库
- en: Spark SQL includes a data source API that can read data from other databases
    using [JDBC](https://oreil.ly/PHi6y). It simplifies querying these data sources
    as it returns the results as a DataFrame, thus providing all of the benefits of
    Spark SQL (including performance and the ability to join with other data sources).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 包括一个数据源 API，可以使用 [JDBC](https://oreil.ly/PHi6y) 从其他数据库读取数据。它简化了对这些数据源的查询，因为它将结果作为
    DataFrame 返回，从而提供了 Spark SQL 的所有优势（包括性能和与其他数据源的连接能力）。
- en: 'To get started, you will need to specify the JDBC driver for your JDBC data
    source and it will need to be on the Spark classpath. From the `$SPARK_HOME` folder,
    you’ll issue a command like the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用，您需要为 JDBC 数据源指定 JDBC 驱动程序，并将其放置在 Spark 类路径中。从 `$SPARK_HOME` 文件夹中，您可以执行如下命令：
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Using the data source API, the tables from the remote database can be loaded
    as a DataFrame or Spark SQL temporary view. Users can specify the JDBC connection
    properties in the data source options. [Table 5-1](#common_connection_properties)
    contains some of the more common connection properties (case-insensitive) that
    Spark supports.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据源 API，可以将远程数据库的表加载为 DataFrame 或 Spark SQL 临时视图。用户可以在数据源选项中指定 JDBC 连接属性。[Table 5-1](#common_connection_properties)
    包含 Spark 支持的一些常见连接属性（不区分大小写）。
- en: Table 5-1\. Common connection properties
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Table 5-1\. 常见连接属性
- en: '| Property name | Description |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 属性名称 | 描述 |'
- en: '| --- | --- |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `user`, `password` | These are normally provided as connection properties
    for logging into the data sources. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| `user`, `password` | 这些通常作为连接属性提供，用于登录数据源。 |'
- en: '| `url` | JDBC connection URL, e.g., `jdbc:postgresql://localhost/test?user=fred&password=secret`.
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| `url` | JDBC 连接 URL，例如，`jdbc:postgresql://localhost/test?user=fred&password=secret`。
    |'
- en: '| `dbtable` | JDBC table to read from or write to. You can’t specify the `dbtable`
    and `query` options at the same time. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| `dbtable` | 要读取或写入的 JDBC 表。不能同时指定 `dbtable` 和 `query` 选项。 |'
- en: '| `query` | Query to be used to read data from Apache Spark, e.g., `SELECT
    column1, column2, ..., columnN FROM [table&#124;subquery]`. You can’t specify
    the `query` and `dbtable` options at the same time. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| `query` | 用于从 Apache Spark 读取数据的查询，例如，`SELECT column1, column2, ..., columnN
    FROM [table|subquery]`。不能同时指定 `query` 和 `dbtable` 选项。 |'
- en: '| `driver` | Class name of the JDBC driver to use to connect to the specified
    URL. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `driver` | 用于连接到指定 URL 的 JDBC 驱动程序的类名。 |'
- en: For the full list of connection properties, see the [Spark SQL documentation](https://oreil.ly/OUG9A).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 查看完整的连接属性列表，请参阅[Spark SQL 文档](https://oreil.ly/OUG9A)。
- en: The importance of partitioning
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分区的重要性
- en: When transferring large amounts of data between Spark SQL and a JDBC external
    source, it is important to partition your data source. All of your data is going
    through one driver connection, which can saturate and significantly slow down
    the performance of your extraction, as well as potentially saturate the resources
    of your source system. While these JDBC properties are optional, for any large-scale
    operations it is highly recommended to use the properties shown in [Table 5-2](#partitioning_connection_properties).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark SQL 和 JDBC 外部数据源之间传输大量数据时，重要的是对数据源进行分区。所有数据都通过一个驱动程序连接，这可能会使数据提取的性能显著下降，并可能使源系统的资源饱和。虽然这些
    JDBC 属性是可选的，但对于任何大规模操作，强烈建议使用 [Table 5-2](#partitioning_connection_properties)
    中显示的属性。
- en: Table 5-2\. Partitioning connection properties
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Table 5-2\. 分区连接属性
- en: '| Property name | Description |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 属性名称 | 描述 |'
- en: '| --- | --- |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `numPartitions` | The maximum number of partitions that can be used for parallelism
    in table reading and writing. This also determines the maximum number of concurrent
    JDBC connections. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `numPartitions` | 用于表读写并行性的最大分区数。这也决定了最大并发 JDBC 连接数。 |'
- en: '| `partitionColumn` | When reading an external source, `partitionColumn` is
    the column that is used to determine the partitions; note, `partitionColumn` must
    be a numeric, date, or timestamp column. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `partitionColumn` | 在读取外部数据源时，`partitionColumn` 是用于确定分区的列；请注意，`partitionColumn`
    必须是数值、日期或时间戳列。 |'
- en: '| `lowerBound` | Sets the minimum value of `partitionColumn` for the partition
    stride. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| `lowerBound` | 设置分区步长中`partitionColumn`的最小值。 |'
- en: '| `upperBound` | Sets the maximum value of `partitionColumn` for the partition
    stride. |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| `upperBound` | 设置分区步长中`partitionColumn`的最大值。 |'
- en: 'Let’s take a look at an [example](https://oreil.ly/g7Cjc) to help you understand
    how these properties work. Suppose we use the following settings:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个 [示例](https://oreil.ly/g7Cjc)，帮助您理解这些属性如何工作。假设我们使用以下设置：
- en: '`numPartitions`: `10`'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numPartitions`：`10`'
- en: '`lowerBound`: `1000`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lowerBound`：`1000`'
- en: '`upperBound`: `10000`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`upperBound`：`10000`'
- en: 'Then the stride is equal to 1,000, and 10 partitions will be created. This
    is the equivalent of executing these 10 queries (one for each partition):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然后步长为 1,000，将创建 10 个分区。这相当于执行这 10 个查询（每个分区一个）：
- en: '`SELECT * FROM table WHERE partitionColumn BETWEEN 1000 and 2000`'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SELECT * FROM table WHERE partitionColumn BETWEEN 1000 and 2000`'
- en: '`SELECT * FROM table WHERE partitionColumn BETWEEN 2000 and 3000`'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SELECT * FROM table WHERE partitionColumn BETWEEN 2000 and 3000`'
- en: '`...`'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`...`'
- en: '`SELECT * FROM table WHERE partitionColumn BETWEEN 9000 and 10000`'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SELECT * FROM table WHERE partitionColumn BETWEEN 9000 and 10000`'
- en: 'While not all-encompassing, the following are some hints to keep in mind when
    using these properties:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不是全面的，但以下是在使用这些属性时要记住的一些提示：
- en: A good starting point for `numPartitions` is to use a multiple of the number
    of Spark workers. For example, if you have four Spark worker nodes, then perhaps
    start with 4 or 8 partitions. But it is also important to note how well your source
    system can handle the read requests. For systems that have processing windows,
    you can maximize the number of concurrent requests to the source system; for systems
    lacking processing windows (e.g., an OLTP system continuously processing data),
    you should reduce the number of concurrent requests to prevent saturation of the
    source system.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numPartitions` 的一个良好起点是使用 Spark 工作节点数量的倍数。例如，如果您有四个 Spark 工作节点，可以从 4 或 8 个分区开始。但重要的是注意源系统如何处理读取请求。对于有处理窗口的系统，可以最大化对源系统的并发请求；对于没有处理窗口的系统（例如连续处理数据的
    OLTP 系统），应减少并发请求以避免源系统饱和。'
- en: 'Initially, calculate the `lowerBound` and `upperBound` based on the minimum
    and maximum `partitionColumn` *actual* values. For example, if you choose `{numPartitions:10,
    lowerBound: 1000, upperBound: 10000}`, but all of the values are between `2000`
    and `4000`, then only 2 of the 10 queries (one for each partition) will be doing
    all of the work. In this scenario, a better configuration would be `{numPartitions:10,
    lowerBound: 2000, upperBound: 4000}`.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '最初，基于 `partitionColumn` 的最小和最大 *实际* 值来计算 `lowerBound` 和 `upperBound`。例如，如果您选择
    `{numPartitions:10, lowerBound: 1000, upperBound: 10000}`，但所有值都在 `2000` 和 `4000`
    之间，那么这 10 个查询（每个分区一个）中只有 2 个将承担所有工作。在这种情况下，更好的配置是 `{numPartitions:10, lowerBound:
    2000, upperBound: 4000}`。'
- en: 'Choose a `partitionColumn` that can be uniformly distributed to avoid data
    skew. For example, if the majority of your `partitionColumn` has the value `2500`,
    with `{numPartitions:10, lowerBound: 1000, upperBound: 10000}` most of the work
    will be performed by the task requesting the values between `2000` and `3000`.
    Instead, choose a different `partitionColumn`, or if possible generate a new one
    (perhaps a hash of multiple columns) to more evenly distribute your partitions.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '选择一个可以均匀分布的 `partitionColumn`，以避免数据倾斜。例如，如果大多数 `partitionColumn` 的值为 `2500`，那么在
    `{numPartitions:10, lowerBound: 1000, upperBound: 10000}` 的情况下，大部分工作将由请求值在 `2000`
    和 `3000` 之间的任务执行。相反，选择一个不同的 `partitionColumn`，或者如果可能的话生成一个新的（可能是多列的哈希）以更均匀地分布分区。'
- en: PostgreSQL
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PostgreSQL
- en: 'To connect to a PostgreSQL database, build or download the JDBC jar from [Maven](https://oreil.ly/Tg5Z3)
    and add it to your classpath. Then start a Spark shell (`spark-shell` or `pyspark`),
    specifying that jar:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到 PostgreSQL 数据库，请构建或从 [Maven](https://oreil.ly/Tg5Z3) 下载 JDBC jar 并将其添加到类路径中。然后启动
    Spark shell (`spark-shell` 或 `pyspark`)，指定该 jar 文件：
- en: '[PRE27]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following examples show how to load from and save to a PostgreSQL database
    using the Spark SQL data source API and JDBC in Scala:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用 Spark SQL 数据源 API 和 Scala 中的 JDBC 从 PostgreSQL 数据库加载数据和保存数据：
- en: '[PRE28]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'And here’s how to do it in PySpark:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何在 PySpark 中执行此操作的示例：
- en: '[PRE29]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: MySQL
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MySQL
- en: 'To connect to a MySQL database, build or download the JDBC jar from [Maven](https://oreil.ly/c1sAC)
    or [MySQL](https://oreil.ly/bH5zb) (the latter is easier!) and add it to your
    classpath. Then start a Spark shell (`spark-shell` or `pyspark`), specifying that
    jar:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到 MySQL 数据库，请从 [Maven](https://oreil.ly/c1sAC) 或 [MySQL](https://oreil.ly/bH5zb)
    （后者更简单！）构建或下载 JDBC jar 并将其添加到类路径中。然后启动 Spark shell (`spark-shell` 或 `pyspark`)，指定该
    jar 文件：
- en: '[PRE30]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following examples show how to load data from and save it to a MySQL database
    using the Spark SQL data source API and JDBC in Scala:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用 Spark SQL 数据源 API 和 JDBC 在 Scala 中从 MySQL 数据库加载数据并保存数据：
- en: '[PRE31]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'And here’s how to do it in Python:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 而这是如何在 Python 中执行该操作的：
- en: '[PRE32]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Azure Cosmos DB
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure Cosmos DB
- en: 'To connect to an Azure Cosmos DB database, build or download the JDBC jar from
    [Maven](https://oreil.ly/vDVQ6) or [GitHub](https://oreil.ly/dJMx1) and add it
    to your classpath. Then start a Scala or PySpark shell, specifying this jar (note
    that this example is using Spark 2.4):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到 Azure Cosmos DB 数据库，构建或下载来自 [Maven](https://oreil.ly/vDVQ6) 或 [GitHub](https://oreil.ly/dJMx1)
    的 JDBC jar，并将其添加到您的类路径中。然后启动 Scala 或 PySpark shell，指定这个 jar（请注意，此示例使用的是 Spark
    2.4）：
- en: '[PRE33]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You also have the option of using `--packages` to pull the connector from [Spark
    Packages](https://spark-packages.org) using its Maven coordinates:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以选择使用 `--packages` 从 [Spark Packages](https://spark-packages.org) 使用其 Maven
    坐标来获取连接器：
- en: '[PRE34]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following examples show how to load data from and save it to an Azure Cosmos
    DB database using the Spark SQL data source API and JDBC in Scala and PySpark.
    Note that it is common to use the `query_custom` configuration to make use of
    the various indexes within Cosmos DB:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用 Spark SQL 数据源 API 和 JDBC 在 Scala 和 PySpark 中从 Azure Cosmos DB 数据库加载数据并保存数据。请注意，通常使用
    `query_custom` 配置来利用 Cosmos DB 中的各种索引：
- en: '[PRE35]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: For more information, please refer to the [Azure Cosmos DB documentation](https://oreil.ly/OMXBH).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参阅 [Azure Cosmos DB 文档](https://oreil.ly/OMXBH)。
- en: MS SQL Server
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MS SQL Server
- en: 'To connect to an MS SQL Server database, [download the JDBC jar](https://oreil.ly/xHkDl)
    and add it to your classpath. Then start a Scala or PySpark shell, specifying
    this jar:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到 MS SQL Server 数据库，[下载 JDBC jar](https://oreil.ly/xHkDl) 并将其添加到您的类路径中。然后启动
    Scala 或 PySpark shell，指定这个 jar：
- en: '[PRE37]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following examples show how to load data from and save it to an MS SQL
    Server database using the Spark SQL data source API and JDBC in Scala and PySpark:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用 Spark SQL 数据源 API 和 JDBC 在 Scala 和 PySpark 中从 MS SQL Server 数据库加载数据并保存数据：
- en: '[PRE38]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Other External Sources
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他外部来源
- en: 'There are just some of the many external data sources Apache Spark can connect
    to; other popular data sources include:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这些仅是 Apache Spark 可以连接的许多外部数据源之一；其他流行的数据源包括：
- en: '[Apache Cassandra](https://oreil.ly/j8XSa)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Apache Cassandra](https://oreil.ly/j8XSa)'
- en: '[Snowflake](https://oreil.ly/NJOii)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Snowflake](https://oreil.ly/NJOii)'
- en: '[MongoDB](https://oreil.ly/MK64A)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MongoDB](https://oreil.ly/MK64A)'
- en: Higher-Order Functions in DataFrames and Spark SQL
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据框架和 Spark SQL 中的高阶函数
- en: 'Because complex data types are amalgamations of simple data types, it is tempting
    to manipulate them directly. There are two [typical solutions](https://oreil.ly/JL1UJ)
    for manipulating complex data types:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于复杂数据类型是简单数据类型的综合，直接操作它们是很诱人的。有两种 [典型解决方案](https://oreil.ly/JL1UJ) 用于操作复杂数据类型：
- en: Exploding the nested structure into individual rows, applying some function,
    and then re-creating the nested structure
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将嵌套结构扩展为单独的行，应用某些函数，然后重新创建嵌套结构
- en: Building a user-defined function
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建用户定义函数
- en: These approaches have the benefit of allowing you to think of the problem in
    tabular format. They typically involve (but are not limited to) using [utility
    functions](https://oreil.ly/gF-0D) such as `get_json_object()`, `from_json()`,
    `to_json()`, `explode()`, and `selectExpr()`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的好处是可以以表格格式思考问题。它们通常涉及（但不限于）使用 [utility functions](https://oreil.ly/gF-0D)
    如 `get_json_object()`、`from_json()`、`to_json()`、`explode()` 和 `selectExpr()`。
- en: Let’s take a closer look at these two options.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这两个选项。
- en: 'Option 1: Explode and Collect'
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选项 1：展开和收集
- en: 'In this nested SQL statement, we first `explode(values)`, which creates a new
    row (with the `id`) for each element (`value`) within `values`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个嵌套的 SQL 语句中，我们首先执行 `explode(values)`，这会为 `values` 中的每个元素 `value` 创建一个带有 `id`
    的新行：
- en: '[PRE40]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: While `collect_list()` returns a list of objects with duplicates, the `GROUP
    BY` statement requires shuffle operations, meaning the order of the re-collected
    array isn’t necessarily the same as that of the original array. As `values` could
    be any number of dimensions (a really wide and/or really long array) and we’re
    doing a `GROUP BY`, this approach could be very expensive.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `collect_list()` 返回带有重复对象的列表时，`GROUP BY` 语句需要执行洗牌操作，这意味着重新收集的数组的顺序不一定与原始数组相同。由于
    `values` 可能具有任意数量的维度（一个非常宽或非常长的数组），并且我们正在进行 `GROUP BY`，这种方法可能非常昂贵。
- en: 'Option 2: User-Defined Function'
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选项 2：用户定义函数
- en: 'To perform the same task (adding `1` to each element in `values`), we can also
    create a UDF that uses `map()` to iterate through each element (`value`) and perform
    the addition operation:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行相同的任务（将 `values` 中的每个元素加 `1`），我们还可以创建一个使用 `map()` 迭代每个元素 `value` 并执行加法操作的
    UDF：
- en: '[PRE41]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We could then use this UDF in Spark SQL as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 Spark SQL 中如下使用此 UDF：
- en: '[PRE42]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: While this is better than using `explode()` and `collect_list()` as there won’t
    be any ordering issues, the serialization and deserialization process itself may
    be expensive. It’s also important to note, however, that `collect_list()` may
    cause executors to experience out-of-memory issues for large data sets, whereas
    using UDFs would alleviate these issues.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这比使用 `explode()` 和 `collect_list()` 更好，因为不会有任何排序问题，但序列化和反序列化过程本身可能很昂贵。然而，值得注意的是，对于大数据集，`collect_list()`
    可能会导致执行器遇到内存不足的问题，而使用 UDF 则可以缓解这些问题。
- en: Built-in Functions for Complex Data Types
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复杂数据类型的内置函数
- en: Instead of using these potentially expensive techniques, you may be able to
    use some of the built-in functions for complex data types included as part of
    Apache Spark 2.4 and later. Some of the more common ones are listed in [Table 5-3](#array_type_functions)
    (array types) and [Table 5-4](#map_functions) (map types); for the full list refer
    to [this notebook](https://oreil.ly/GOJ5z) in the Databricks documentation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试使用 Apache Spark 2.4 及更高版本提供的一些复杂数据类型的内置函数，其中一些常见函数列在[表 5-3](#array_type_functions)（数组类型）和[表 5-4](#map_functions)（映射类型）中；完整列表请参阅[Databricks
    文档中的笔记本](https://oreil.ly/GOJ5z)。
- en: Table 5-3\. Array type functions
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-3\. 数组类型函数
- en: '| Function/Description | Query | Output |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 函数/描述 | 查询 | 输出 |'
- en: '| --- | --- | --- |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `array_distinct(array<T>): array<T>` Removes duplicates within an array |
    `SELECT array_distinct(array(1, 2, 3, null, 3));` | `[1,2,3,null]` |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| `array_distinct(array<T>): array<T>` 从数组中移除重复项 | `SELECT array_distinct(array(1,
    2, 3, null, 3));` | `[1,2,3,null]` |'
- en: '| `array_intersect(array<T>, array<T>): array<T>` Returns the intersection
    of two arrays without duplicates | `SELECT array_intersect(array(1, 2, 3), array(1,
    3, 5));` | `[1,3]` |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| `array_intersect(array<T>, array<T>): array<T>` 返回两个数组的交集（去重） | `SELECT array_intersect(array(1,
    2, 3), array(1, 3, 5));` | `[1,3]` |'
- en: '| `array_union(array<T>, array<T>): array<T>` Returns the union of two arrays
    without duplicates | `SELECT array_union(array(1, 2, 3), array(1, 3, 5));` | `[1,2,3,5]`
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| `array_union(array<T>, array<T>): array<T>` 返回两个数组的并集，无重复 | `SELECT array_union(array(1,
    2, 3), array(1, 3, 5));` | `[1,2,3,5]` |'
- en: '| `array_except(array<T>, array<T>): array<T>` Returns elements in `array1`
    but not in `array2`, without duplicates | `SELECT array_except(array(1, 2, 3),
    array(1, 3, 5));` | `[2]` |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| `array_except(array<T>, array<T>): array<T>` 返回存在于 `array1` 中但不存在于 `array2`
    中的元素，无重复 | `SELECT array_except(array(1, 2, 3), array(1, 3, 5));` | `[2]` |'
- en: '| `array_join(array<String>, String[, String]): String` Concatenates the elements
    of an array using a delimiter | `SELECT array_join(array(''hello'', ''world''),
    '' '');` | `hello world` |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `array_join(array<String>, String[, String]): String` 使用分隔符连接数组的元素 | `SELECT
    array_join(array(''hello'', ''world''), '' '');` | `hello world` |'
- en: '| `array_max(array<T>): T` Returns the maximum value within the array; `null`
    elements are skipped | `SELECT array_max(array(1, 20, null, 3));` | `20` |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `array_max(array<T>): T` 返回数组中的最大值；跳过 `null` 元素 | `SELECT array_max(array(1,
    20, null, 3));` | `20` |'
- en: '| `array_min(array<T>): T` Returns the minimum value within the array; `null`
    elements are skipped | `SELECT array_min(array(1, 20, null, 3));` | `1` |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `array_min(array<T>): T` 返回数组中的最小值；跳过 `null` 元素 | `SELECT array_min(array(1,
    20, null, 3));` | `1` |'
- en: '| `array_position(array<T>, T): Long` Returns the (1-based) index of the first
    element of the given array as a `Long` | `SELECT array_position(array(3, 2, 1),
    1);` | `3` |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| `array_position(array<T>, T): Long` 返回给定数组的第一个元素的（从 1 开始的）索引作为 `Long` | `SELECT
    array_position(array(3, 2, 1), 1);` | `3` |'
- en: '| `array_remove(array<T>, T): array<T>` Removes all elements that are equal
    to the given element from the given array | `SELECT array_remove(array(1, 2, 3,
    null, 3), 3);` | `[1,2,null]` |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `array_remove(array<T>, T): array<T>` 从给定的数组中移除所有等于给定元素的元素 | `SELECT array_remove(array(1,
    2, 3, null, 3), 3);` | `[1,2,null]` |'
- en: '| `arrays_overlap(array<T>, array<T>): array<T>` Returns `true` if `array1`
    contains at least one non-`null` element also present in `array2` | `SELECT arrays_overlap(array(1,
    2, 3), array(3, 4, 5));` | `true` |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| `arrays_overlap(array<T>, array<T>): array<T>` 如果 `array1` 包含至少一个非 `null`
    元素，同时也存在于 `array2` 中，则返回 `true` | `SELECT arrays_overlap(array(1, 2, 3), array(3,
    4, 5));` | `true` |'
- en: '| `array_sort(array<T>): array<T>` Sorts the input array in ascending order,
    with null elements placed at the end of the array | `SELECT array_sort(array(''b'',
    ''d'', null, ''c'', ''a''));` | `["a","b","c","d",null]` |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `array_sort(array<T>): array<T>` 将输入数组按升序排序，将 null 元素放置在数组末尾 | `SELECT array_sort(array(''b'',
    ''d'', null, ''c'', ''a''));` | `["a","b","c","d",null]` |'
- en: '| `concat(array<T>, ...): array<T>` Concatenates strings, binaries, arrays,
    etc. | `SELECT concat(array(1, 2, 3), array(4, 5), array(6));` | `[1,2,3,4,5,6]`
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `concat(array<T>, ...): array<T>` 连接字符串、二进制、数组等 | `SELECT concat(array(1,
    2, 3), array(4, 5), array(6));` | `[1,2,3,4,5,6]` |'
- en: '| `flatten(array<array<T>>): array<T>` Flattens an array of arrays into a single
    array | `SELECT flatten(array(array(1, 2), array(3, 4)));` | `[1,2,3,4]` |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `flatten(array<array<T>>): array<T>` 将数组的数组展平为单个数组 | `SELECT flatten(array(array(1,
    2), array(3, 4)));` | `[1,2,3,4]` |'
- en: '| `array_repeat(T, Int): array<T>` Returns an array containing the specified
    element the specified number of times | `SELECT array_repeat(''123'', 3);` | `["123","123","123"]`
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `array_repeat(T, Int): array<T>` 返回包含指定元素的数组，元素重复指定次数 | `SELECT array_repeat(''123'',
    3);` | `["123","123","123"]` |'
- en: '| `reverse(array<T>): array<T>` Returns a reversed string or an array with
    the reverse order of elements | `SELECT reverse(array(2, 1, 4, 3));` | `[3,4,1,2]`
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| `reverse(array<T>): array<T>` 返回反转顺序的字符串或数组 | `SELECT reverse(array(2, 1,
    4, 3));` | `[3,4,1,2]` |'
- en: '| `sequence(T, T[, T]): array<T>` Generates an array of elements from start
    to stop (inclusive) by incremental step | `SELECT sequence(1, 5);` `SELECT sequence(5,
    1);`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '| `sequence(T, T[, T]): array<T>` 通过递增步长生成从起始到结束（包括）的元素数组 | `SELECT sequence(1,
    5);` `SELECT sequence(5, 1);`'
- en: '`SELECT sequence(to_date(''2018-01-01''), to_date(''2018-03-01''), interval
    1 month);` | `[1,2,3,4,5]` `[5,4,3,2,1]`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`SELECT sequence(to_date(''2018-01-01''), to_date(''2018-03-01''), interval
    1 month);` | `[1,2,3,4,5]` `[5,4,3,2,1]` |'
- en: '`["2018-01-01", "2018-02-01", "2018-03-01"]` |'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`["2018-01-01", "2018-02-01", "2018-03-01"]` |'
- en: '| `shuffle(array<T>): array<T>` Returns a random permutation of the given array
    | `SELECT shuffle(array(1, 20, null, 3));` | `[null,3,20,1]` |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| `shuffle(array<T>): array<T>` 返回给定数组的随机排列 | `SELECT shuffle(array(1, 20,
    null, 3));` | `[null,3,20,1]` |'
- en: '| `slice(array<T>, Int, Int): array<T>` Returns a subset of the given array
    starting from the given index (counting from the end if the index is negative),
    of the specified length | `SELECT slice(array(1, 2, 3, 4), -2, 2);` | `[3,4]`
    |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| `slice(array<T>, Int, Int): array<T>` 返回从给定索引开始的给定长度的子数组（如果索引为负数则从末尾计数） |
    `SELECT slice(array(1, 2, 3, 4), -2, 2);` | `[3,4]` |'
- en: '| `array_zip(array<T>, array<U>, ...): array<struct<T, U, ...>>` Returns a
    merged array of structs | `SELECT arrays_zip(array(1, 2), array(2, 3), array(3,
    4));` | `[{"0":1,"1":2,"2":3},{"0":2,"1":3,"2":4}]` |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| `array_zip(array<T>, array<U>, ...): array<struct<T, U, ...>>` 返回合并的结构数组
    | `SELECT arrays_zip(array(1, 2), array(2, 3), array(3, 4));` | `[{"0":1,"1":2,"2":3},{"0":2,"1":3,"2":4}]`
    |'
- en: '| `element_at(array<T>, Int): T /` Returns the element of the given array at
    the given (1-based) index | `SELECT element_at(array(1, 2, 3), 2);` | `2` |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| `element_at(array<T>, Int): T /` 返回给定数组在指定（基于 1 的）索引处的元素 | `SELECT element_at(array(1,
    2, 3), 2);` | `2` |'
- en: '| `cardinality(array<T>): Int` An alias of `size`; returns the size of the
    given array or a map | `SELECT cardinality(array(''b'', ''d'', ''c'', ''a''));`
    | `4` |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| `cardinality(array<T>): Int` `size` 的别名；返回给定数组或映射的大小 | `SELECT cardinality(array(''b'',
    ''d'', ''c'', ''a''));` | `4` |'
- en: Table 5-4\. Map functions
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-4\. 映射函数
- en: '| Function/Description | Query | Output |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 函数/描述 | 查询 | 输出 |'
- en: '| --- | --- | --- |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `map_form_arrays(array<K>, array<V>): map<K, V>` Creates a map from the given
    pair of key/value arrays; elements in keys should not be `null` | `SELECT map_from_arrays(array(1.0,
    3.0), array(''2'', ''4''));` | `{"1.0":"2", "3.0":"4"}` |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| `map_form_arrays(array<K>, array<V>): map<K, V>` 从给定的键/值数组对创建映射；键中的元素不应为
    `null` | `SELECT map_from_arrays(array(1.0, 3.0), array(''2'', ''4''));` | `{"1.0":"2",
    "3.0":"4"}` |'
- en: '| `map_from_entries(array<struct<K, V>>): map<K, V>` Returns a map created
    from the given array | `SELECT map_from_entries(array(struct(1, ''a''), struct(2,
    ''b'')));` | `{"1":"a", "2":"b"}` |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| `map_from_entries(array<struct<K, V>>): map<K, V>` 返回从给定数组创建的映射 | `SELECT
    map_from_entries(array(struct(1, ''a''), struct(2, ''b'')));` | `{"1":"a", "2":"b"}`
    |'
- en: '| `map_concat(map<K, V>, ...): map<K, V>` Returns the union of the input maps
    | `SELECT map_concat(map(1, ''a'', 2, ''b''), map(2, ''c'', 3, ''d''));` | `{"1":"a",
    "2":"c","3":"d"}` |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| `map_concat(map<K, V>, ...): map<K, V>` 返回输入映射的并集 | `SELECT map_concat(map(1,
    ''a'', 2, ''b''), map(2, ''c'', 3, ''d''));` | `{"1":"a", "2":"c","3":"d"}` |'
- en: '| `element_at(map<K, V>, K): V` Returns the value of the given key, or `null`
    if the key is not contained in the map | `SELECT element_at(map(1, ''a'', 2, ''b''),
    2);` | `b` |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| `element_at(map<K, V>, K): V` 返回给定键的值，如果映射中不包含该键则返回 `null` | `SELECT element_at(map(1,
    ''a'', 2, ''b''), 2);` | `b` |'
- en: '| `cardinality(array<T>): Int` An alias of `size`; returns the size of the
    given array or a map | `SELECT cardinality(map(1, ''a'', 2, ''b''));` | `2` |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| `cardinality(array<T>): Int` `size` 的别名；返回给定数组或映射的大小 | `SELECT cardinality(map(1,
    ''a'', 2, ''b''));` | `2` |'
- en: Higher-Order Functions
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Higher-Order Functions
- en: 'In addition to the previously noted built-in functions, there are higher-order
    functions that take anonymous lambda functions as arguments. An example of a higher-order
    function is the following:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前述的内置函数外，还有接受匿名lambda函数作为参数的高阶函数。一个高阶函数的例子如下：
- en: '[PRE43]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The `transform()` function takes an array (`values`) and anonymous function
    (`lambda` expression) as input. The function transparently creates a new array
    by applying the anonymous function to each element, and then assigning the result
    to the output array (similar to the UDF approach, but more efficiently).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`transform()`函数接受一个数组（`values`）和匿名函数（`lambda`表达式）作为输入。该函数通过将匿名函数应用于每个元素来透明地创建一个新数组，并将结果分配给输出数组（类似于UDF方法，但更高效）。'
- en: 'Let’s create a sample data set so we can run some examples:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个样本数据集，以便我们可以运行一些示例：
- en: '[PRE44]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here’s the output:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '[PRE46]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: With the preceding DataFrame you can run the following higher-order function
    queries.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述DataFrame，您可以运行以下高阶函数查询。
- en: transform()
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`transform()`'
- en: '[PRE47]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The `transform()` function produces an array by applying a function to each
    element of the input array (similar to a `map()` function):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`transform()`函数通过将函数应用于输入数组的每个元素来生成一个数组（类似于`map()`函数）：'
- en: '[PRE48]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: filter()
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`filter()`'
- en: '[PRE49]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The `filter()` function produces an array consisting of only the elements of
    the input array for which the Boolean function is `true`:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter()`函数生成一个数组，其中只包含布尔函数为`true`的输入数组的元素：'
- en: '[PRE50]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: exists()
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`exists()`'
- en: '[PRE51]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The `exists()` function returns `true` if the Boolean function holds for any
    element in the input array:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '`exists()`函数如果布尔函数对输入数组中的任何元素成立，则返回`true`：'
- en: '[PRE52]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: reduce()
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`reduce()`'
- en: '[PRE53]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The `reduce()` function reduces the elements of the array to a single value
    by merging the elements into a buffer `B` using `function<B, T, B>` and applying
    a finishing `function<B, R>` on the final buffer:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce()`函数通过将元素合并到缓冲区`B`中，使用`function<B, T, B>`，并在最终缓冲区上应用结束`function<B,
    R>`，将输入数组的元素减少为单个值：'
- en: '[PRE54]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Common DataFrames and Spark SQL Operations
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见的DataFrame和Spark SQL操作
- en: 'Part of the power of Spark SQL comes from the wide range of DataFrame operations
    (also known as untyped Dataset operations) it supports. The list of operations
    is quite extensive and includes:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL的强大之处在于支持的广泛DataFrame操作（也称为无类型Dataset操作）。支持的操作列表非常广泛，包括：
- en: Aggregate functions
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合函数
- en: Collection functions
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集合函数
- en: Datetime functions
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期时间函数
- en: Math functions
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数学函数
- en: Miscellaneous functions
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杂项函数
- en: Non-aggregate functions
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非聚合函数
- en: Sorting functions
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排序功能
- en: String functions
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串函数
- en: UDF functions
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UDF函数
- en: Window functions
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口函数
- en: For the full list, see the [Spark SQL documentation](https://oreil.ly/e1AYA).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 完整列表请参阅[Spark SQL文档](https://oreil.ly/e1AYA)。
- en: 'Within this chapter, we will focus on the following common relational operations:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于以下常见的关系操作：
- en: Unions and joins
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 联合和连接
- en: Windowing
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口函数
- en: Modifications
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改
- en: 'To perform these DataFrame operations, we’ll first prepare some data. In the
    following code snippet, we:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这些DataFrame操作，我们首先要准备一些数据。在以下代码片段中，我们：
- en: Import two files and create two DataFrames, one for airport (`airportsna`) information
    and one for US flight delays (`departureDelays`).
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入两个文件并创建两个DataFrame，一个用于机场（`airportsna`）信息，另一个用于美国航班延误（`departureDelays`）。
- en: Using `expr()`, convert the `delay` and `distance` columns from `STRING` to
    `INT`.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`expr()`，将`delay`和`distance`列从`STRING`转换为`INT`。
- en: Create a smaller table, `foo`, that we can focus on for our demo examples; it
    contains only information on three flights originating from Seattle (SEA) to the
    destination of San Francisco (SFO) for a small time range.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个较小的表`foo`，我们可以专注于我们的演示示例；它只包含从西雅图（SEA）出发到旧金山（SFO）目的地的三个航班的信息，时间范围较小。
- en: 'Let’s get started:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: '[PRE55]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The `departureDelays` DataFrame contains data on >1.3M flights while the `foo`
    DataFrame contains just three rows with information on flights from SEA to SFO
    for a specific time range, as noted in the following output:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '`departureDelays` DataFrame包含超过1.3M个航班的数据，而`foo` DataFrame只包含三行关于从SEA到SFO的航班信息，具体时间范围如下输出所示：'
- en: '[PRE57]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: In the following sections, we will execute union, join, and windowing examples
    with this data.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将使用这些数据执行union、join和windowing示例。
- en: Unions
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联合
- en: 'A common pattern within Apache Spark is to union two different DataFrames with
    the same schema together. This can be achieved using the `union()` method:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark中的常见模式之一是将两个具有相同模式的不同DataFrame联合在一起。这可以通过`union()`方法实现：
- en: '[PRE58]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The `bar` DataFrame is the union of `foo` with `delays`. Using the same filtering
    criteria results in the `bar` DataFrame, we see a duplication of the `foo` data,
    as expected:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '`bar` DataFrame是`foo`与`delays`的并集。使用相同的过滤条件得到`bar` DataFrame，我们看到`foo`数据的重复，这是预期的：'
- en: '[PRE60]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Joins
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接
- en: A common DataFrame operation is to join two DataFrames (or tables) together.
    By default, a Spark SQL join is an `inner join`, with the options being `inner`,
    `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`,
    `left_semi`, and `left_anti`. More information is available in the [documentation](https://oreil.ly/CFEhb)
    (this is applicable to Scala as well as Python).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的DataFrame操作是将两个DataFrame（或表）连接在一起。默认情况下，Spark SQL连接是一个`inner join`，选项包括`inner`、`cross`、`outer`、`full`、`full_outer`、`left`、`left_outer`、`right`、`right_outer`、`left_semi`和`left_anti`。更多信息请参阅[文档](https://oreil.ly/CFEhb)（适用于Scala和Python）。
- en: 'The following code sample performs the default of an `inner` join between the
    `airportsna` and `foo` DataFrames:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例执行了`airportsna`和`foo` DataFrames之间的默认`inner`连接：
- en: '[PRE61]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The preceding code allows you to view the date, delay, distance, and destination
    information from the `foo` DataFrame joined to the city and state information
    from the `airports` DataFrame:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码允许您查看从`foo` DataFrame连接到`airports` DataFrame的城市和州信息的日期、延误、距离和目的地信息：
- en: '[PRE64]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Windowing
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 窗口化
- en: A [window function](https://oreil.ly/PV7si) uses values from the rows in a window
    (a range of input rows) to return a set of values, typically in the form of another
    row. With window functions, it is possible to operate on a group of rows while
    still returning a single value for every input row. In this section, we will show
    how to use the `dense_rank()` window function; there are many other functions,
    as noted in [Table 5-5](#window_functions).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[窗口函数](https://oreil.ly/PV7si)使用窗口中行的值（一系列输入行）来返回一组值，通常以另一行的形式。使用窗口函数，可以在一组行上操作，同时为每个输入行返回单个值。在本节中，我们将展示如何使用`dense_rank()`窗口函数；如表5-5所示，还有许多其他函数。'
- en: Table 5-5\. Window functions
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 表5-5\. 窗口函数
- en: '|   | SQL | DataFrame API |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|   | SQL | DataFrame API |'
- en: '| --- | --- | --- |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Ranking functions** | `rank()` | `rank()` |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| **排名函数** | `rank()` | `rank()` |'
- en: '|   | `dense_rank()` | `denseRank()` |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|   | `dense_rank()` | `denseRank()` |'
- en: '|   | `percent_rank()` | `percentRank()` |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|   | `percent_rank()` | `percentRank()` |'
- en: '|   | `ntile()` | `ntile()` |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|   | `ntile()` | `ntile()` |'
- en: '|   | `row_number()` | `rowNumber()` |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|   | `row_number()` | `rowNumber()` |'
- en: '| **Analytic functions** | `cume_dist()` | `cumeDist()` |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| **分析函数** | `cume_dist()` | `cumeDist()` |'
- en: '|   | `first_value()` | `firstValue()` |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|   | `first_value()` | `firstValue()` |'
- en: '|   | `last_value()` | `lastValue()` |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|   | `last_value()` | `lastValue()` |'
- en: '|   | `lag()` | `lag()` |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|   | `lag()` | `lag()` |'
- en: '|   | `lead()` | `lead()` |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|   | `lead()` | `lead()` |'
- en: 'Let’s start with a review of the `TotalDelays` (calculated by `sum(Delay)`)
    experienced by flights originating from Seattle (SEA), San Francisco (SFO), and
    New York City (JFK) and going to a specific set of destination locations, as noted
    in the following query:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从查看源自西雅图（SEA）、旧金山（SFO）和纽约市（JFK）并且前往特定目的地的航班所经历的`TotalDelays`（通过`sum(Delay)`计算）开始。
- en: '[PRE65]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'What if for each of these origin airports you wanted to find the three destinations
    that experienced the most delays? You could achieve this by running three different
    queries for each origin and then unioning the results together, like this:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对于每个起点机场，您想要找到经历最多延误的三个目的地怎么办？您可以为每个起点运行三个不同的查询，然后像这样合并结果：
- en: '[PRE66]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: where `[ORIGIN]` is the three different origin values of `JFK`, `SEA`, and `SFO`.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`[ORIGIN]`是`JFK`、`SEA`和`SFO`的三个不同起点值。
- en: 'But a better approach would be to use a window function like `dense_rank()`
    to perform the following calculation:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 但更好的方法是使用类似`dense_rank()`的窗口函数来执行以下计算：
- en: '[PRE67]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'By using the `dense_rank()` window function, we can quickly ascertain that
    the destinations with the worst delays for the three origin cities were:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`dense_rank()`窗口函数，我们可以快速确定对于这三个起点城市而言，延误最严重的目的地是：
- en: 'Seattle (SEA): San Francisco (SFO), Denver (DEN), and Chicago (ORD)'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 西雅图（SEA）：旧金山（SFO）、丹佛（DEN）和芝加哥（ORD）
- en: 'San Francisco (SFO): Los Angeles (LAX), Chicago (ORD), and New York (JFK)'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旧金山（SFO）：洛杉矶（LAX）、芝加哥（ORD）和纽约（JFK）
- en: 'New York (JFK): Los Angeles (LAX), San Francisco (SFO), and Atlanta (ATL)'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纽约（JFK）：洛杉矶（LAX）、旧金山（SFO）和亚特兰大（ATL）
- en: It’s important to note that each window grouping needs to fit in a single executor
    and will get composed into a single partition during execution. Therefore, you
    need to ensure that your queries are not unbounded (i.e., limit the size of your
    window).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，每个窗口分组需要适合单个执行器，并在执行期间组成单个分区。因此，需要确保查询不是无界的（即限制窗口的大小）。
- en: Modifications
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改
- en: 'Another common operation is to perform *modifications* to the DataFrame. While
    DataFrames themselves are immutable, you can modify them through operations that
    create new, different DataFrames, with different columns, for example. (Recall
    from earlier chapters that the underlying RDDs are immutable—i.e., they cannot
    be changed—to ensure there is data lineage for Spark operations.) Let’s start
    with our previous small DataFrame example:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见操作是对DataFrame执行*修改*。虽然DataFrame本身是不可变的，但可以通过创建新的、不同的DataFrame进行修改，例如添加不同的列。
    （回想一下前几章中提到的底层RDD是不可变的——即不能更改的——以确保Spark操作有数据血统。）让我们从之前的小DataFrame示例开始：
- en: '[PRE68]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Adding new columns
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加新列
- en: 'To add a new column to the `foo` DataFrame, use the `withColumn()` method:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 要向`foo` DataFrame 添加新列，请使用`withColumn()`方法：
- en: '[PRE69]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The newly created `foo2` DataFrame has the contents of the original `foo` DataFrame
    plus the additional `status` column defined by the `CASE` statement:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 新创建的`foo2` DataFrame 包含了原始`foo` DataFrame 的内容，还增加了由`CASE`语句定义的`status`列：
- en: '[PRE71]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Dropping columns
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理数据时，有时候需要交换列和行，即[*些使用高阶函数的示例。最后，我们讨论了一些常见的关系运算符，并展示了如何执行一些 DataFrame 操作。
- en: 'To drop a column, use the `drop()` method. For example, let’s remove the `delay`
    column as we now have a `status` column, added in the previous section:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除列，请使用`drop()`方法。例如，让我们移除`delay`列，因为在上一节中已经添加了`status`列：
- en: '[PRE72]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Renaming columns
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重命名列
- en: 'You can rename a column using the `rename()` method:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`rename()`方法重命名列：
- en: '[PRE74]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Pivoting
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 透视
- en: 'When working with your data, sometimes you will need to swap the columns for
    the rows—i.e., [*pivot* your data](https://oreil.ly/XXmqM). Let’s grab some data
    to demonstrate this concept:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在   当处理数据时，有时需要交换列和行，即[*将数据透视*](https://oreil.ly/XXmqM)。让我们获取一些数据来演示这个概念：
- en: '[PRE76]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Pivoting allows you to place names in the `month` column (instead of `1` and
    `2` you can show `Jan` and `Feb`, respectively) as well as perform aggregate calculations
    (in this case average and max) on the delays by destination and month:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 透视允许你将名称放置在`month`列中（而不是`1`和`2`，你可以显示`Jan`和`Feb`），并对目的地和月份的延迟进行聚合计算（本例中为平均值和最大值）：
- en: '[PRE77]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Summary
  id: totrans-379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter explored how Spark SQL interfaces with external components. We
    discussed creating user-defined functions, including Pandas UDFs, and presented
    some options for executing Spark SQL queries (including the Spark SQL shell, Beeline,
    and Tableau). We then provided examples of how to use Spark SQL to connect with
    a variety of external data sources, such as SQL databases, PostgreSQL, MySQL,
    Tableau, Azure Cosmos DB, MS SQL Server, and others.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了Spark SQL如何与外部组件进行交互。我们讨论了创建用户定义函数（包括Pandas UDFs），并介绍了执行Spark SQL查询的一些选项（包括Spark
    SQL shell、Beeline和Tableau）。然后，我们提供了如何使用Spark SQL与各种外部数据源连接的示例，如SQL数据库、PostgreSQL、MySQL、Tableau、Azure
    Cosmos DB、MS SQL Server等。
- en: We explored Spark’s built-in functions for complex data types, and gave some
    examples of working with higher-order functions. Finally, we discussed some common
    relational operators and showed how to perform a selection of DataFrame operations.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了Spark用于复杂数据类型的内置函数，并给出了使用高阶函数的一些示例。最后，我们讨论了一些常见的关系操作符，并展示了如何执行一系列DataFrame操作。
- en: In the next chapter, we explore how to work with Datasets, the benefits of strongly
    typed operations, and when and why to use them.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何处理Datasets，强类型操作的好处，以及何时以及为何使用它们。
- en: ^([1](ch05.html#ch05fn1-marker)) The current Spark SQL engine no longer uses
    the Hive code in its implementation.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.html#ch05fn1-marker)) 当前的Spark SQL引擎在实现中不再使用Hive代码。
- en: ^([2](ch05.html#ch01fn5-marker)) Note there are slight differences when working
    with Pandas UDFs between Spark [2.3](https://oreil.ly/pIZk-), [2.4](https://oreil.ly/0NYG-),
    and [3.0](https://oreil.ly/9wA4s).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.html#ch01fn5-marker)) 注意，在处理Pandas UDF时，Spark [2.3](https://oreil.ly/pIZk-)、[2.4](https://oreil.ly/0NYG-)和[3.0](https://oreil.ly/9wA4s)之间有些许差异。
