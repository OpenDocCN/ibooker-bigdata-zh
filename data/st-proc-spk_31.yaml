- en: Chapter 25\. Monitoring Spark Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第25章 监控 Spark Streaming
- en: Monitoring in streaming applications is required to gain operational confidence
    of the deployed applications and should include a holistic view of the resources
    used by the application, such as CPU, memory, and secondary storage. As a distributed
    application, the number of factors to monitor is multiplied by the number of nodes
    that are part of a clustered deployment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 流应用程序中的监控需要获得部署应用程序的操作信心，应包括应用程序使用的资源的整体视图，例如CPU、内存和二级存储。作为分布式应用程序，要监控的因素数量将与集群部署中节点数量相乘。
- en: To manage this complexity, we need a comprehensive and smart monitoring system.
    It needs to collect metrics from all the key *moving parts* that participate in
    the streaming application runtime and, at the same time, it needs to provide them
    in an understandable and consumable form.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了管理这种复杂性，我们需要一个全面而智能的监控系统。它需要从参与流应用运行时的所有关键**组件**收集指标，并同时以易于理解和可消费的形式提供它们。
- en: In the case of Spark Streaming, next to the general indicators just discussed,
    we are mainly concerned with the relationship between the amount of data received,
    the batch interval chosen for our application, and the actual execution time of
    every microbatch. The relation between these three parameters is key for a stable
    Spark Streaming job in the long run. To ensure that our job performs within stable
    boundaries, we need to make performance monitoring an integral part of the development
    and production process.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Spark Streaming而言，除了刚讨论的通用指标外，我们主要关注接收到的数据量、应用程序选择的批处理间隔以及每个微批处理的实际执行时间之间的关系。这三个参数之间的关系对于长期稳定的Spark
    Streaming作业至关重要。为确保我们的作业在稳定的边界内执行，我们需要将性能监控作为开发和生产过程的一个整体组成部分。
- en: 'Spark offers several monitoring interfaces that cater to the different stages
    of that process:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了几种监控接口，以满足该过程不同阶段的需求：
- en: The Streaming UI
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 流式 UI
- en: A web interface that provides charts of key indicators about the running job
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 提供有关运行作业的关键指标的图表的Web界面
- en: The Monitoring REST API
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[监控 REST API](https://wiki.example.org/monitoring_rest_api)'
- en: A set of APIs that can be consumed by an external monitoring system to obtain
    metrics through an HTTP interface
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一组API，可通过HTTP接口被外部监控系统消费，以获取指标
- en: The Metrics Subsystem
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 指标子系统
- en: A pluggable Service Provider Interface (SPI) that allows for tight integration
    of external monitoring tools into Spark
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 允许将外部监控工具紧密集成到Spark中的可插拔服务提供者接口（SPI）
- en: The Internal Event Bus
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 内部事件总线
- en: A pub/sub subsystem in Spark in which programmatic subscribers can receive events
    about different aspects of the application execution on the cluster
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的发布/订阅子系统，其中编程订阅者可以接收有关集群上应用程序执行不同方面的事件
- en: In this chapter, we explore these monitoring interfaces and how they apply to
    the different phases of the life cycle of a streaming application. We begin with
    the Streaming UI, in which we survey the functionality provided by this interface
    and its links to the different aspects of a running Spark Streaming job. The Streaming
    UI is a powerful visual tool that we can use in the initial development and deployment
    phase to better understand how our application behaves from a pragmatic perspective.
    We dedicate a section to detailing the use of the Streaming UI with this particular
    focus on performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨这些监控接口及其如何应用于流应用程序的生命周期的不同阶段。我们从流式 UI 开始，其中我们调查此接口提供的功能及其与运行中Spark
    Streaming作业不同方面的联系。流式 UI 是一个强大的视觉工具，我们可以在初始开发和部署阶段使用它，以更好地了解我们的应用程序从实用的角度如何运行。我们专门详述了使用流式
    UI 的部分，重点关注性能方面。
- en: In the rest of the chapter, we cover the different monitoring integration capabilities
    of Spark Streaming. We explore the APIs offered by the REST API and the Metrics
    Subsystem SPI to expose internal metrics to external monitoring clients. To finalize,
    we describe the data model and interactions of the *Internal Event Bus*, which
    we can use to programmatically access all the metrics offered by Spark Streaming
    for cases in which maximal flexibility is required to integrate a custom monitoring
    solution.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们涵盖了Spark Streaming的不同监控集成能力。我们探索REST API和Metrics Subsystem SPI提供的API，以向外部监控客户端公开内部指标。最后，我们描述了内部事件总线的数据模型和交互，这可以用于以编程方式访问Spark
    Streaming提供的所有指标，适用于需要最大灵活性以集成自定义监控解决方案的情况。
- en: The Streaming UI
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式 UI
- en: The SparkUI is a web application available on the Spark driver node, usually
    running on port 4040, unless there are other Spark processes running on the same
    node, in which case, the port used will increase (4041, 4042, and so on) until
    one free port is found. We can also configure this port by using the configuration
    key `spark.ui.port`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: SparkUI 是一个网络应用程序，位于 Spark 驱动节点上，通常运行在端口 4040 上，除非同一节点上有其他 Spark 进程运行，否则将使用增加的端口（4041、4042
    等），直到找到一个空闲端口。我们也可以通过配置键 `spark.ui.port` 来配置此端口。
- en: What we refer to as the *Streaming UI* is a tab in the Spark UI that becomes
    active whenever a `StreamingContext` is started, as illustrated in [Figure 25-1](#streaming-ui-screenshot).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的 *流式 UI* 是 Spark UI 中的一个选项卡，仅当启动 `StreamingContext` 时才会激活，如 [图 25-1](#streaming-ui-screenshot)
    所示。
- en: '![spas 2501](Images/spas_2501.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2501](Images/spas_2501.png)'
- en: Figure 25-1\. Streaming UI
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 25-1\. 流式 UI
- en: 'The Streaming UI comprises several visual elements that provide an at-a-glance
    view of the performance of a Spark Streaming job. Following the numerical clues
    in the image, these are the elements that make up the UI:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 流式 UI 包括几个视觉元素，提供 Spark Streaming 作业性能的一览无余视图。根据图像中的数字线索，以下是组成 UI 的元素：
- en: (1) Streaming UI tab
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 流式 UI 选项卡
- en: This is the Streaming tab on the Spark UI. Clicking it opens that Streaming
    UI.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Spark UI 上的流式选项卡。单击它会打开该流式 UI。
- en: (2) Time-based stats
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 基于时间的统计数据
- en: The overall statistics line includes that batch interval, the time this application
    has been up, and the start timestamp.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 总体统计行包括批处理间隔、此应用程序已运行的时间和启动时间戳。
- en: (3) Summary of batches and records
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 批次和记录摘要
- en: Next to the timing information, we find the total number of batches completed
    and the grand total of records processed.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间信息旁边，我们找到已完成的批次总数以及处理的记录总数。
- en: (4) Performance charts
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 性能图表
- en: The title in the table of charts reports what data is used in the charts displayed.
    The data represented in the charts is preserved in circular buffers. We see only
    the thousand (1,000) most recent data points received.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图表标题在图表报告的表中指出了使用的数据。图表中表示的数据保存在循环缓冲区中。我们只看到最近的一千（1,000）个接收到的数据点。
- en: (5) The Input Rate chart
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 输入速率图表
- en: A time-series representation of the number of records received at each batch
    interval with a distribution histogram next to it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 每个批次间隔接收的记录数量的时间序列表示，并且旁边还有一个分布直方图。
- en: (6) The Scheduling Delay chart
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (6) 调度延迟图表
- en: This chart reports the difference between the moment when the batch is scheduled
    and when it’s processed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此图表报告了批次调度和处理之间的差异。
- en: (7) The Processing Time chart
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (7) 处理时间图表
- en: A time-series of the amount of time (duration) needed to process each batch.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 处理每个批次所需时间（持续时间）的时间序列。
- en: (8) Total Delay chart
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (8) 总延迟图表
- en: A time-series of the sum of the scheduling delay and the processing time. It
    provides a view of the joint execution of Spark Streaming and the Spark core engine.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是调度延迟和处理时间的总和的时间序列。它提供了对 Spark Streaming 和 Spark 核心引擎的联合执行的视图。
- en: (9) Active batches
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (9) 活动批次
- en: Provides a list of the batches currently in the Spark Streaming queue. It shows
    the batch or batches currently executing as well as the batches of any potential
    backlog in case of an overload. Ideally, only batches in status processing are
    in this list. This list might appear empty if at the time of loading the Streaming
    UI, the current batch had finished processing and the next one is not yet due
    for scheduling.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 提供当前在 Spark Streaming 队列中的批次列表。它显示当前正在执行的批次或批次，以及在过载情况下可能存在的任何潜在积压批次。理想情况下，此列表中只有处理中的批次。如果在加载流式
    UI 时当前批次已完成处理且下一批次尚未计划，则此列表可能为空。
- en: (10) Completed batches
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (10) 已完成批次
- en: A list of the most recent batches processed with a link to the details of that
    batch.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最近处理的批次列表，以及指向该批次详细信息的链接。
- en: Understanding Job Performance Using the Streaming UI
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用流式 UI 理解作业性能
- en: As discussed in [“The Streaming UI”](#streaming_ui), the four charts that comprise
    the main screen of the Streaming UI provide a snapshot of the current and most
    recent performance of our streaming job. By default, the last 1,000 processing
    intervals are presented in the UI, meaning that the period of time we are able
    to view is intervals x batch interval, so for a job with a two-second batch interval,
    we see the metrics of roughly the last half hour (2 x 1,000 = 2,000 seconds, or
    33,33 minutes). We can configure the number of remembered intervals by using the
    `spark.ui.retainedJobs` configuration parameter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如[“流式处理 UI”](#streaming_ui)中讨论的，流式处理作业的主屏幕包含四个图表，展示了当前和最近的性能快照。默认情况下，UI 显示最后
    1,000 个处理间隔，这意味着我们能够查看的时间段是间隔 × 批处理间隔，因此对于批处理间隔为两秒的作业，我们可以看到大约最近半小时的指标（2 × 1,000
    = 2,000 秒，或 33.33 分钟）。我们可以使用 `spark.ui.retainedJobs` 配置参数来配置记忆间隔的数量。
- en: Input Rate Chart
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入速率图表
- en: The Input Rate chart, on top, gives a view of the input load that the application
    is sustaining. All charts share a common timeline. We can imagine a vertical line
    through all of them that would serve as a reference to correlate the different
    metrics to the input load, as illustrated in [Figure 25-2](#streaming-ui-correl).
    The data points of chart lines are clickable and will link to the corresponding
    job detail line that appears underneath the charts. As we will explore later on,
    this navigation feature is very helpful to track back the origins of certain behaviors
    that we can observe on the charts.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部的输入速率图表显示了应用程序正在维持的输入负载。所有图表共享一个公共时间轴。我们可以想象一条竖直线通过所有图表，这将作为一个参考，用于将不同的指标与输入负载相关联，正如在[图
    25-2](#streaming-ui-correl)中所示。图表线的数据点可点击，并将链接到图表下方出现的相应作业详细信息行。正如我们稍后将探讨的，这种导航功能对于追踪我们可以在图表上观察到的某些行为的起源非常有帮助。
- en: '![spas 2502](Images/spas_2502.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2502](Images/spas_2502.png)'
- en: 'Figure 25-2\. Streaming UI: correlation of metrics'
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 25-2\. 流式处理 UI：度量的相关性
- en: Scheduling Delay Chart
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度延迟图表
- en: 'The next chart in the UI is the Scheduling Delay chart. This is a key health
    indicator: for an application that is running well within its time and resource
    constraints, this metric will be constantly flat on zero. Small periodic disturbances
    might point to regular supporting processes such as snapshots.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: UI 中的下一个图表是**调度延迟**图表。这是一个关键的健康指标：对于一个在其时间和资源约束内运行良好的应用程序，这个度量指标将始终保持在零上。小的周期性干扰可能指向定期支持过程，例如快照。
- en: Window operations that create exceptional load will potentially also affect
    this metric. It’s important to note that the delay will show on the batches that
    immediately follow a batch that took longer than the batch interval to complete.
    These delays will not show a correlation with the input rate. Delays introduced
    by peaks in data input will correlate with a high peak in the Input Rate with
    an offset. [Figure 25-3](#streaming-ui-delay) depicts how the peak in the Input
    Rate chart happens earlier than the corresponding increase in the Scheduling Delay.
    Because this chart represents a delay, we see the effects of the peak in input
    rate after the system has begun to “digest” the data overload.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 创建异常负载的窗口操作也可能影响这个度量。重要的是要注意，延迟将显示在紧随需要超过批处理间隔完成的批次后面的批次上。这些延迟不会与输入速率显示相关。由数据输入高峰引起的延迟将与输入速率的高峰相关联，并带有偏移。[图
    25-3](#streaming-ui-delay)显示了输入速率图中的高峰比调度延迟的相应增加早。因为这个图表代表了一个延迟，我们看到系统在开始“消化”数据过载后才会显示这些效果。
- en: '![spas 2503](Images/spas_2503.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2503](Images/spas_2503.png)'
- en: 'Figure 25-3\. Streaming UI: Scheduling Delay'
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 25-3\. 流式处理 UI：调度延迟
- en: Processing Time Chart
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理时间图表
- en: This chart, shown in [Figure 25-4](#processing_time_chart), represents the time
    that the data-processing part of the streaming job takes to execute. This execution
    takes place on the Spark cluster, so this is the main indicator of how the actual
    data crunching is performing on the (potentially) distributed environment. An
    important aspect of this chart is the high watermark line at the level corresponding
    to the batch interval time. Let’s quickly recall that the batch interval is the
    time of each microbatch and also the time we have to process the data that arrived
    at the previous interval. A processing time below this watermark is considered
    stable. An occasional peak above this line might be acceptable if the job has
    enough room to recover from it. A job that is constantly above this line will
    build a backlog using storage resources in memory and/or disk. If the available
    storage is exhausted, the job will eventually crash. This chart usually has a
    high correlation with the Input Rate chart because it’s common that the execution
    time of the job is related to the volume of data received on each batch interval.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此图表显示了[图 25-4](#processing_time_chart)中数据处理部分的执行时间。此执行发生在 Spark 集群上，因此这是实际数据处理在（可能的）分布式环境中表现的主要指标。图表的一个重要方面是水位线，其水平对应批处理间隔时间。让我们迅速回顾一下，批处理间隔是每个微批处理的时间，也是我们处理前一间隔到达的数据的时间。低于此水位线的处理时间被视为稳定的。偶尔超过此线的峰值可能是可以接受的，如果作业有足够的空间从中恢复。如果作业始终高于此线，则会利用内存和/或磁盘中的存储资源构建积压。如果可用存储资源耗尽，则作业最终会崩溃。此图表通常与输入速率图表高度相关，因为作业的执行时间通常与每个批处理间隔接收的数据量有关。
- en: '![spas 2504](Images/spas_2504.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2504](Images/spas_2504.png)'
- en: 'Figure 25-4\. Streaming UI: Processing Time'
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 25-4\. 流式 UI：处理时间
- en: Total Delay Chart
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总延迟图表
- en: The Total Delay chart is a graphical representation of the end-to-end latency
    of the system. Total delay comprises the length of time that Spark Streaming takes
    to collect, schedule, and submit each microbatch for processing and the amount
    of time it takes for Spark to apply the job’s logic and produce a result. This
    chart provides a holistic view of the system performance and reveals any delays
    that might happen during the execution of the job. As for the Scheduling Delay
    chart, any sustained increase in the total delay metric is a reason of concern
    and might indicate high load or other conditions such as increasing storage latency
    that negatively affect the job’s performance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 总延迟图表是系统端到端延迟的图形化表示。总延迟包括 Spark Streaming 收集、调度和提交每个微批处理以进行处理的时间，以及 Spark 应用作业逻辑并生成结果的时间。此图表提供了系统性能的整体视图，并揭示了作业执行过程中可能发生的任何延迟。至于调度延迟图表，总延迟指标的持续增加是一个值得关注的原因，可能表明高负载或其他条件（如增加的存储延迟）对作业性能产生负面影响。
- en: Batch Details
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批次详情
- en: 'When we scroll below the charts that constitute the main screen of the Streaming
    UI, we find two tables: Active Batches and Completed Batches. These tables have
    columns that correspond to the charts that we just studied: Input Size, Scheduling
    Delay, Processing Time, and an Output Operations counter. In addition to these
    fields, Completed Batches also shows the Total Delay of the corresponding microbatch.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向下滚动到构成流式 UI 主屏幕的图表下方时，我们会发现两个表格：活跃批次和已完成批次。这些表格的列对应我们刚刚学习的图表：输入大小、调度延迟、处理时间以及输出操作计数器。除了这些字段，已完成批次还显示了相应微批次的总延迟。
- en: Note
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '*Output Ops* relates to the number of output operations registered to execute
    on the microbatch. This refers to the job’s code structure and should not be confused
    with a parallelism indicator. As we recall, output operations, such as `print()`
    or `foreachRDD`, are the operations that trigger the lazy execution of a DStream.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出操作* 指的是注册要在微批次上执行的输出操作数量。这涉及作业的代码结构，不应与并行性指标混淆。正如我们所记得的，输出操作（如 `print()`
    或 `foreachRDD`）是触发 DStream 惰性执行的操作。'
- en: Active Batches
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 活跃批次
- en: 'Contains information about the microbatches in the Spark Streaming scheduler
    queue. For a healthy job, this table contains at most one row: the batch currently
    executing. This entry indicates the number of records contained in the microbatch
    and any delay before the start of execution. Processing Time is unknown until
    the microbatch has been completed, so this metric is never present on the Active
    Batches table.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 包含关于 Spark Streaming 调度队列中微批次的信息。对于一个健康的作业，该表最多包含一行：当前正在执行的批次。此条目指示微批次中包含的记录数及执行开始前的任何延迟。处理时间在微批次完成之前是未知的，因此在活动批次表上从未显示该指标。
- en: When more than one row is present in this table, it indicates that the job has
    a delay beyond the batch interval and new microbatches queue awaiting execution,
    forming an execution backlog.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当此表中存在多行时，表示作业超出了批次间隔并且新的微批次排队等待执行，形成执行积压。
- en: Completed batches
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 已完成批次
- en: Right after a batch execution completes, its corresponding entry in the Active
    Batches table transitions to the Completed Batches table. In this transition,
    the Processing Time field is filled with its execution time and the Total Delay
    is finally known and also included.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 批次执行完成后，其在活动批次表中对应的条目会转移到已完成批次表中。在此转换中，处理时间字段填充其执行时间，并且总延迟也会被确定并包括在内。
- en: Each entry is identified by the timestamp, labeled as batch time. This label
    also provides a link to the details of the corresponding Spark job that provided
    the execution for this batch.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个条目由时间戳标识，标记为批次时间。此标签还提供了链接，指向提供此批次执行的 Spark 作业的详细信息。
- en: The link to the batch details deserves further exploration. As explained in
    earlier chapters, the Spark Streaming model is based on microbatches. The Batch
    Detail page provides insights in the execution of each batch, broken down into
    the different jobs that constitute the batch. [Figure 25-5](#streaming-ui-batch-det)
    presents the structure. A batch is defined as a sequence of output operations
    that are executed in the same order as they were defined in the application code.
    Each output operation contains one or more jobs. This page summarizes this relationship,
    displays the duration per job, and the parallelism level in the task overview.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 批次详情的链接值得进一步探索。正如前几章所解释的，Spark Streaming 模型基于微批次。批次详情页面提供了每个批次执行的洞察，分解为构成批次的不同作业。[Figure 25-5](#streaming-ui-batch-det)
    展示了结构。批次定义为按照应用程序代码中定义的顺序执行的输出操作序列。每个输出操作包含一个或多个作业。本页面总结了这种关系，显示了每个作业的持续时间以及任务概述中的并行级别。
- en: '![spas 2505](Images/spas_2505.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2505](Images/spas_2505.png)'
- en: 'Figure 25-5\. Streaming UI: batch details'
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 25-5\. 流式 UI：批次详情
- en: The jobs are listed by job ID and provide a link to the job page in the Spark
    UI. These are *normal* Spark jobs executed by the core engine. By clicking through,
    we can explore the execution, in terms of stages, assigned executor, and execution
    time statistics.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作业按作业 ID 列出，并提供指向 Spark UI 中作业页面的链接。这些是核心引擎执行的*普通* Spark 作业。通过点击，我们可以探索执行情况，包括阶段、分配的执行器和执行时间统计信息。
- en: Note
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'To exercise the use of the Streaming UI locally, there are two notebooks provided
    in the online resources of this book:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要在本地使用流式 UI，本书的在线资源中提供了两个笔记本：
- en: kafka-data-generator.snb
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: kafka-data-generator.snb
- en: This notebook is used to produce a configurable number of records per second
    that are sent to a local Kafka topic.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此笔记本用于按秒产生可配置数量的记录，这些记录发送到本地 Kafka 主题。
- en: kafka-streaming-data.snb
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: kafka-streaming-data.snb
- en: This notebook consumes from that same topic, joins the data with a reference
    dataset, and writes the result to a local file.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本从相同主题消费数据，将数据与参考数据集进行连接，并将结果写入本地文件。
- en: By experimenting with the producer rate, we can observe the behavior of the
    Streaming UI and experience what stable and overload situations look like. This
    is a good exercise to put into practice when moving a streaming application to
    production because it helps in understanding its performance characteristics and
    in determining the load thresholds where the application performs correctly.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过尝试生产者速率，我们可以观察流式 UI 的行为，并体验稳定和过载情况。这是将流式应用程序推向生产时的良好练习，因为它有助于理解其性能特征并确定应用程序在何时执行正确的负载阈值。
- en: The Monitoring REST API
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控 REST API
- en: The Monitoring REST API exposes the job’s streaming metrics as a set of predefined
    HTTP endpoints that deliver the data as JSON-formatted objects. These objects
    can be consumed by external monitoring and alerting applications to integrate
    the Spark Streaming jobs with some external monitoring system.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 监控 REST API 将作业的流式指标公开为一组预定义的 HTTP 终端，这些终端以 JSON 格式的对象传送数据。这些对象可以被外部监控和警报应用程序消耗，以将
    Spark Streaming 作业与某些外部监控系统集成。
- en: Using the Monitoring REST API
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用监控 REST API
- en: 'The Monitoring REST API is served by the Spark driver node, on the same port
    as the Spark UI, and mounted at the `/api/v1` endpoint: `http://<driver-host>:<ui-port>/api/v1`.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 监控 REST API 由 Spark 驱动节点提供，端口与 Spark UI 相同，并挂载在 `/api/v1` 终点：`http://<driver-host>:<ui-port>/api/v1`。
- en: The `/api/v1/applications/:app-id` resource offers information about the application
    ID `app-id` provided. This `id` first must be queried by calling `/api/v1/applications`
    to construct the application-specific URL. In the following URLs, we refer to
    this variable application ID as `app-id`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`/api/v1/applications/:app-id` 资源提供有关提供的应用程序 ID `app-id` 的信息。首先必须通过调用 `/api/v1/applications`
    查询此 `id`，以构建特定应用程序的 URL。在以下 URL 中，我们将这个变量应用程序 ID 称为 `app-id`。'
- en: Note that for a running Spark Streaming context, there will be only one current
    application ID.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于运行中的 Spark Streaming 上下文，只会有一个当前应用程序 ID。
- en: Warning
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The Monitoring REST API introduced support for Spark Streaming only from Spark
    version 2.2 onward. For earlier versions of Spark, consider the metrics servlet,
    further explained in the next section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 监控 REST API 从 Spark 版本 2.2 开始仅支持 Spark Streaming。对于较早版本的 Spark，请考虑在下一节中进一步解释的度量
    servlet。
- en: Information Exposed by the Monitoring REST API
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控 REST API 提供的信息
- en: The resources corresponding to the running Spark Streaming context are available
    at `/api/v1/applications/:app-id/streaming`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与运行中的 Spark Streaming 上下文对应的资源位于 `/api/v1/applications/:app-id/streaming`。
- en: '[Table 25-1](#streaming-sub-resources) summarizes the subresources provided
    by this endpoint.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 25-1](#streaming-sub-resources) 总结了此终端提供的子资源。'
- en: Table 25-1\. Subresources of the streaming resource
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 25-1\. 流处理资源的子资源
- en: '| Resource | Meaning | Corresponding UI element |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | 含义 | 对应的 UI 元素 |'
- en: '| --- | --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `/statistics` | A set of summarized metrics (see the sections that follow
    for details) | Streaming UI charts |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `/statistics` | 一组汇总的指标（有关详细信息，请参阅后续章节） | 流处理 UI 图表 |'
- en: '| `/receivers` | A list of all receivers instantiated in this streaming job
    | Receivers summary on the Input Rate chart |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `/receivers` | 在这个流处理作业中实例化的所有接收器列表 | 输入速率图表上的接收器摘要 |'
- en: '| `/receivers/:stream-id` | Details of the receiver indexed by the provided
    `stream-id` | Click-open receivers on the Input Rate chart |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `/receivers/:stream-id` | 根据提供的 `stream-id` 索引的接收器的详细信息 | 点击打开输入速率图表上的接收器
    |'
- en: '| `/batches` | A list of all the currently kept batches | List of batches underneath
    the charts |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `/batches` | 当前保存的所有批次列表 | 图表下的批次列表 |'
- en: '| `/batches/:batch-id/operations` | The output operations | Click-through a
    batch in the batch list |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `/batches/:batch-id/operations` | 输出操作 | 点击批处理列表中的一个批处理 |'
- en: '| `/batches/:batch-id/operations/:output-op-id` | Detailed information of the
    corresponding output operation in the given batch | Click-through in the operations
    details under the batch detail page |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `/batches/:batch-id/operations/:output-op-id` | 给定批次中相应输出操作的详细信息 | 在批处理详细页面下的操作详情中点击
    |'
- en: 'From a monitoring perspective, we should pay some additional attention to the
    `statistics` object. It contains the key performance indicators that we need to
    monitor in order to ensure a healthy streaming job: `/api/v1/applications/:app-id/
    streaming/statistics`.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 从监控的角度来看，我们应该额外关注 `statistics` 对象。它包含了我们需要监视的关键性能指标，以确保流处理作业的健康运行：`/api/v1/applications/:app-id/
    streaming/statistics`。
- en: '[Table 25-2](#stats-object-streaming-resource) presents a listing of the different
    data delivered by the `statistics` endpoint, their type, and a brief description
    of the metric involved.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 25-2](#stats-object-streaming-resource) 展示了`statistics`端点提供的不同数据列表、它们的类型以及涉及的指标简要描述。'
- en: Table 25-2\. Statistics object of the streaming resource
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 25-2\. 流处理资源的统计对象
- en: '| Key | Type | Description |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 键 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `startTime` | String | Timestamp encoded in ISO 8601 format |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| `startTime` | 字符串 | 以 ISO 8601 格式编码的时间戳 |'
- en: '| `batchDuration` | Number (Long) | Batch interval duration in milliseconds
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| `batchDuration` | 数字 (长整型) | 批处理间隔的持续时间（以毫秒为单位） |'
- en: '| `numReceivers` | Number (Long) | Count of registered receivers |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| `numReceivers` | Number (Long) | 注册接收器的计数 |'
- en: '| `numActiveReceivers` | Number (Long) | Count of the currently active receivers
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `numActiveReceivers` | Number (Long) | 当前活动接收器的计数 |'
- en: '| `numInactiveReceivers` | Number (Long) | Count of the currently inactive
    receivers |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| `numInactiveReceivers` | Number (Long) | 当前不活动接收器的计数 |'
- en: '| `numTotalCompletedBatches` | Number (Long) | Count of the completed batches
    since the start of this streaming job |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `numTotalCompletedBatches` | Number (Long) | 从流处理作业启动以来已完成的批次计数 |'
- en: '| `numRetainedCompletedBatches` | Number (Long) | Count of the currently kept
    batches from which we still store information |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| `numRetainedCompletedBatches` | Number (Long) | 目前保留的批次计数，我们仍然存储其信息 |'
- en: '| `numActiveBatches` | Number (Long) | Count of the batches in the execution
    queue of the streaming context |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| `numActiveBatches` | Number (Long) | 流处理上下文执行队列中批次的计数 |'
- en: '| `numProcessedRecords` | Number (Long) | Totalized sum of the records processed
    by the currently running job |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| `numProcessedRecords` | Number (Long) | 当前运行作业处理的记录总和 |'
- en: '| `numReceivedRecords` | Number (Long) | Totalized sum of the records received
    by the currently running job |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `numReceivedRecords` | Number (Long) | 当前运行作业接收的记录总和 |'
- en: '| `avgInputRate` | Number (Double) | Arithmetic mean of the input rate over
    the last kept batches |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `avgInputRate` | Number (Double) | 最近保留的批次的输入速率的算术平均值 |'
- en: '| `avgSchedulingDelay` | Number (Double) | Arithmetic mean of the scheduling
    delay over the last kept batches |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| `avgSchedulingDelay` | Number (Double) | 最近保留的批次的调度延迟的算术平均值 |'
- en: '| `avgProcessingTime` | Number (Double) | Arithmetic mean of the processing
    time over the last kept batches |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| `avgProcessingTime` | Number (Double) | 最近保留的批次的处理时间的算术平均值 |'
- en: '| `avgTotalDelay` | Number (Double) | Arithmetic mean of the total delay over
    the last kept batches |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| `avgTotalDelay` | Number (Double) | 最近保留的批次的总延迟的算术平均值 |'
- en: When integrating a monitoring tool with Spark Streaming, we are particularly
    interested in `avgSchedulingDelay` and making sure that it does not grow over
    time. Many monitoring applications use a *grow rate* (or similar) gauge to measure
    change over time. Given that the value provided by the API is an average over
    the last kept batches (1,000 by default), setting up alarms on this metric should
    take into consideration small increasing changes.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在将监控工具集成到 Spark Streaming 中时，我们特别关注 `avgSchedulingDelay`，并确保它不会随时间增长。许多监控应用程序使用
    *增长率*（或类似）测量指标随时间的变化。由于 API 提供的值是最近保留的批次（默认为 1,000）的平均值，设置关于此指标的警报应考虑小幅增长的变化。
- en: The Metrics Subsystem
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指标子系统
- en: In Spark, the Metrics Subsystem is an SPI that allows the implementation of
    metric sinks to integrate Spark with external management and monitoring solutions.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，指标子系统是一个 SPI，允许实现指标接收器以将 Spark 与外部管理和监控解决方案集成。
- en: 'It offers a number of built-in implementations such as the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 提供一些内置实现，例如以下内容：
- en: Console
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台
- en: Logs metric information to the standard output of the job
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 将指标信息记录到作业的标准输出
- en: HTTP Servlet
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP Servlet
- en: Delivers metrics using HTTP/JSON
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 HTTP/JSON 传递指标
- en: CSV
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: CSV
- en: Delivers the metrics to files in the configured directory using a comma-separated
    values (CSV) format
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以逗号分隔值 (CSV) 格式将指标传递到配置目录中的文件
- en: JMX
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: JMX
- en: Enables reporting of metrics through Java Management Extensions (JMX)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Java 管理扩展 (JMX) 启用指标报告
- en: Log
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 日志
- en: Delivers metric information to the logs of the application
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 将指标信息传递到应用程序日志中
- en: Graphite, StatsD
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Graphite、StatsD
- en: Forwards metrics to a Graphite/StatsD service
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 将指标转发至 Graphite/StatsD 服务
- en: Ganglia
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Ganglia
- en: Delivers metrics to an existing Ganglia deployment (note that due to license
    restrictions, using this option requires recompilation of the Spark binaries)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将指标传递到现有的 Ganglia 部署（请注意，由于许可限制，使用此选项需要重新编译 Spark 二进制文件）
- en: The Metrics Subsystem reports the most recent values of Spark and Spark Streaming
    processes. This gives us raw access to performance metrics that can feed remote
    performance monitoring applications and also enable accurate and timely alerting
    on abnormalities in a streaming process.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 指标子系统报告 Spark 和 Spark Streaming 进程的最新值。这为我们提供了对性能指标的原始访问，可以向远程性能监控应用程序提供数据，并在流处理过程中的异常发生时进行准确及时的报警。
- en: The specific metrics for the streaming job are available under the key `<app-id>.driver.<application
    name>.StreamingMetrics.streaming`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理作业的特定指标可以在键 `<app-id>.driver.<application name>.StreamingMetrics.streaming`
    下找到。
- en: 'These are as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示：
- en: '`lastCompletedBatch_processingDelay`'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lastCompletedBatch_processingDelay`'
- en: '`lastCompletedBatch_processingEndTime`'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lastCompletedBatch_processingEndTime`'
- en: '`lastCompletedBatch_processingStartTime`'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lastCompletedBatch_processingStartTime`'
- en: '`lastCompletedBatch_schedulingDelay`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lastCompletedBatch_schedulingDelay`'
- en: '`lastCompletedBatch_submissionTime`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lastCompletedBatch_submissionTime`'
- en: '`lastCompletedBatch_totalDelay`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lastCompletedBatch_totalDelay`'
- en: '`lastReceivedBatch_processingEndTime`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lastReceivedBatch_processingEndTime`'
- en: '`lastReceivedBatch_processingStartTime`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lastReceivedBatch_processingStartTime`'
- en: '`lastReceivedBatch_records`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lastReceivedBatch_records`'
- en: '`lastReceivedBatch_submissionTime`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lastReceivedBatch_submissionTime`'
- en: '`receivers`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`receivers`'
- en: '`retainedCompletedBatches`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retainedCompletedBatches`'
- en: '`runningBatches`'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`runningBatches`'
- en: '`totalCompletedBatches`'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`totalCompletedBatches`'
- en: '`totalProcessedRecords`'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`totalProcessedRecords`'
- en: '`totalReceivedRecords`'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`totalReceivedRecords`'
- en: '`unprocessedBatches`'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unprocessedBatches`'
- en: '`waitingBatches`'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`waitingBatches`'
- en: 'Although not readily offered, you can obtain the `lastCompletedBatch_processingTime`
    by simple arithmetic: `lastCompletedBatch_processingEndTime - lastCompletedBatch_processingStartTime`.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不直接提供，您可以通过简单的算术运算获取 `lastCompletedBatch_processingTime`：`lastCompletedBatch_processingEndTime
    - lastCompletedBatch_processingStartTime`。
- en: In this API, the key indicator to track for job stability is `lastCompletedBatch_processingDelay`,
    which we expect to be zero or close to zero and stable over time. A moving average
    of the last 5 to 10 values should remove the noise that small delays sometimes
    introduce and offer a metric that we can rely on to trigger alarms or pager calls.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 API 中，跟踪作业稳定性的关键指标是 `lastCompletedBatch_processingDelay`，我们期望它接近零并且随时间稳定。对最近
    5 到 10 个值的移动平均值应该消除由小延迟引入的噪声，并提供一个可靠的度量标准，以触发警报或呼叫。
- en: The Internal Event Bus
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内部事件总线
- en: 'All the metrics interfaces discussed in this chapter have a single source of
    truth in common: they are all consuming data from an internal Spark event bus
    though dedicated `StreamingListener` implementations.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的所有度量接口都有一个共同的真实数据来源：它们都通过专用的 `StreamingListener` 实现从内部 Spark 事件总线消耗数据。
- en: Spark uses several internal event buses to deliver life cycle events and metadata
    about the executing Spark jobs to subscribed clients. This interface is mostly
    used by internal Spark consumers that offer the data in some processed form. The
    Spark UI is the most significant example of such interaction.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 使用多个内部事件总线向订阅客户端传递有关正在执行的 Spark 作业的生命周期事件和元数据。这种接口主要由内部 Spark 消费者使用，以某种处理过的形式提供数据。Spark
    UI 就是这种交互的最显著例子。
- en: For those cases in which the existing high-level interfaces are not sufficient
    to fulfill our requirements, it’s possible to develop custom listeners and register
    them to receive events. To create a custom Spark Streaming listener, we extend
    the `org.apache.spark.streaming.scheduler.StreamingListener` trait. This trait
    has a no-op default implementation for all of its callback methods, so that extending
    it requires only to override the desired callbacks with our custom metric processing
    logic.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些现有的高级接口无法满足我们需求的情况，可以开发自定义监听器并注册它们来接收事件。要创建自定义的 Spark Streaming 监听器，我们扩展
    `org.apache.spark.streaming.scheduler.StreamingListener` trait。该 trait 对所有回调方法都有一个无操作的默认实现，因此扩展它只需要覆盖我们自定义的指标处理逻辑所需的回调方法。
- en: Note that this internal Spark API is marked as `DeveloperApi`. Hence, its definitions,
    such as classes and interfaces, are subject to change without public notice.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此内部 Spark API 标记为 `DeveloperApi`。因此，其定义（如类和接口）可能会在未公开通知的情况下更改。
- en: Note
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can explore a custom Spark Streaming listener implementation in the online
    resources. The notebook *kafka-streaming-with-listener* extends the Kafka notebook
    that we used earlier with a custom notebook listener that delivers all events
    to a `TableWidget` that can be directly displayed in the notebook.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在在线资源中探索自定义的 Spark Streaming 监听器实现。笔记本 *kafka-streaming-with-listener* 扩展了之前使用的
    Kafka 笔记本，使用自定义笔记本监听器将所有事件传递到可直接在笔记本中显示的 `TableWidget` 中。
- en: Interacting with the Event Bus
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与事件总线交互
- en: The `StreamingListener` interface consists of a trait with several callback
    methods. Each method is called by the notification process with an instance of
    a subclass of `StreamingListenerEvent` that contains the relevant information
    for the callback method.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamingListener` 接口由一个带有多个回调方法的 trait 组成。每个方法都由通知过程调用，使用 `StreamingListenerEvent`
    的子类实例来传递相关信息给回调方法。'
- en: The StreamingListener interface
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: StreamingListener 接口
- en: 'In the following overview, we highlight the most interesting parts of those
    data events:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下概述中，我们突出显示这些数据事件中最有趣的部分：
- en: '`onStreamingStarted`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`onStreamingStarted`'
- en: '[PRE0]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This method is called when the streaming job starts. The `StreamingListenerStreamingStarted`
    instance has a single field, `time`, that contains the timestamp in milliseconds
    when the streaming job started.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当流处理作业启动时调用此方法。`StreamingListenerStreamingStarted` 实例具有一个字段 `time`，其中包含流处理作业启动时的时间戳（以毫秒为单位）。
- en: Receiver events
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接收器事件
- en: 'All callback methods that relate to the life cycle of receivers share a common
    `ReceiverInfo` class that describes the receiver that is being reported. Each
    event-reporting class will have a single `receiverInfo: ReceiverInfo` member.
    The information contained in the attributes of the `ReceiverInfo` class will depend
    on the relevant receiver information for the reported event.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '所有与接收器生命周期相关的回调方法共享一个名为 `ReceiverInfo` 的通用类，描述正在报告的接收器。每个事件报告类都将有一个单独的 `receiverInfo:
    ReceiverInfo` 成员。`ReceiverInfo` 类属性中包含的信息将取决于报告事件的相关接收器信息。'
- en: '`onReceiverStarted`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`onReceiverStarted`'
- en: '[PRE1]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This method is called when a receiver has been started. The `StreamingListenerReceiverStarted`
    instance contains a `ReceiverInfo` instance that describes the receiver started
    for this streaming job. Note that this method is only relevant for the receiver-based
    streaming model.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当接收器已启动时调用此方法。`StreamingListenerReceiverStarted` 实例包含描述为此流处理作业启动的接收器的 `ReceiverInfo`
    实例。请注意，此方法仅适用于基于接收器的流模型。
- en: '`onReceiverError`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`onReceiverError`'
- en: '[PRE2]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This method is called when an existing receiver reports an error. Like for the
    `onReceiverStarted` call, the provided `StreamingListenerReceiverError` instance
    contains a `ReceiverInfo` object. Within this `Receiver Info` instance, we will
    find detailed information about the error, such as the error message and the timestamp
    of the occurrence.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当现有接收器报告错误时调用此方法。与 `onReceiverStarted` 调用类似，提供的 `StreamingListenerReceiverError`
    实例包含一个 `ReceiverInfo` 对象。在此 `Receiver Info` 实例中，我们将找到关于错误的详细信息，例如错误消息和发生时间戳。
- en: '`onReceiverStopped`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`onReceiverStopped`'
- en: '[PRE3]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is the counterpart of the `onReceiverStarted` event. It fires when a receiver
    has been stopped.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `onReceiverStarted` 事件的对应项。当接收器已停止时触发此事件。
- en: Batch events
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批处理事件
- en: These events relate to the life cycle of batch processing, from submission to
    completion. Recall that each output operation registered on a DStream will lead
    to an independent job execution. Those jobs are grouped in batches that are submitted
    together and in sequence to the Spark core engine for execution. This section
    of the listener interface fires events following the life cycle of submission
    and execution of batches.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这些事件与批处理生命周期相关，从提交到完成。请注意，在 DStream 上注册的每个输出操作都会导致独立的作业执行。这些作业被分组成批次，一起顺序提交到
    Spark 核心引擎执行。此监听器接口的这一部分在批次提交和执行生命周期后触发事件。
- en: As these events will fire following the processing of each microbatch, their
    reporting rate is at least as frequent as the batch interval of the related `StreamingContext`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些事件将在每个微批处理的处理后触发，因此它们的报告频率至少与相关 `StreamingContext` 的批次间隔一样频繁。
- en: Following the same implementation pattern as the `receiver` callback interface,
    all batch-related events report a container class with a single `BatchInfo` member.
    Each `BatchInfo` instance reported contains the relevant information corresponding
    to the reporting callback.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `receiver` 回调接口的相同实现模式一致，所有与批处理相关的事件都报告一个带有单个 `BatchInfo` 成员的容器类。每个报告的 `BatchInfo`
    实例包含对应于报告回调的相关信息。
- en: '`BatchInfo` also contains a `Map` of the output operations registered in this
    batch, represented by the `OutputOperationInfo` class. This class contains detailed
    information about the time, duration, and eventual errors for each individual
    output operation. We could use this data point to split the total execution time
    of a batch into the time taken by the different operations that lead to individual
    job execution on the Spark core engine:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`BatchInfo` 还包含此批次中注册的输出操作的 `Map`，由 `OutputOperationInfo` 类表示。此类包含每个单独输出操作的时间、持续时间和最终错误的详细信息。我们可以利用这些数据点将批处理的总执行时间拆分为导致在
    Spark 核心引擎上执行单个作业的不同操作所需的时间：'
- en: '`onBatchSubmitted`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`onBatchSubmitted`'
- en: '[PRE4]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This method is called when a batch of jobs is submitted to Spark for processing.
    The corresponding `BatchInfo` object, reported by the `StreamingListenerBatchSubmitted`
    contains the timestamp of the batch submission time. At this point, the optional
    values `processingStartTime` and `processingEndTime` are set to `None` because
    those values are unknown at this stage of the batch-processing cycle.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当一批作业提交给Spark进行处理时，将调用此方法。由`StreamingListenerBatchSubmitted`报告的`BatchInfo`对象包含了批处理提交时间的时间戳。此时，可选值`processingStartTime`和`processingEndTime`都被设置为`None`，因为这些值在批处理周期的此阶段是未知的。
- en: '`onBatchStarted`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`onBatchStarted`'
- en: '[PRE5]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This method is called when the processing of an individual job has just started.
    The `BatchInfo` object, embedded in the `StreamingListenerBatchStarted` instance
    provided, contains a populated `processingStartTime`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个单独作业的处理刚刚开始时，将调用此方法。由提供的`StreamingListenerBatchStarted`实例嵌入的`BatchInfo`对象包含了填充的`processingStartTime`。
- en: '`onBatchCompleted`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`onBatchCompleted`'
- en: '[PRE6]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This method is called when the processing of a batch has been completed. The
    provided `BatchInfo` instance will be fully populated with the overall timing
    of the batch. The map of `OutputOperationInfo` will also contain the detailed
    timing information for the execution of each output operation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当批处理完成时，将调用此方法。提供的`BatchInfo`实例将完全填充有批处理的总体时间信息。`OutputOperationInfo`的映射还将包含每个输出操作执行的详细时间信息。
- en: Output operation events
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出操作事件
- en: This section of the `StreamingListener` callback interface provides information
    at the level of each job execution triggered by the submission of a batch. Because
    there might be many output operations registered into the same batch, this interface
    might fire at a much higher rate than the batch interval of the `StreamingContext`.
    Any receiving client of the data provided by this interface should be dimensioned
    to receive such a load of events.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamingListener`回调接口的此部分提供了由批次提交触发的每个作业执行级别的信息。由于可能有多个输出操作注册到同一批次中，此接口可能以比`StreamingContext`的批处理间隔更高的频率触发。任何接收此接口提供的数据的客户端都应该调整为接收这样大量的事件。'
- en: The events reported by these methods contain an `OutputOperationInfo` instance
    that provides further timing details about the output operation being reported.
    This `OutputOperationInfo` is the same data structure contained by the `outputOperationInfo`
    of the `BatchInfo` object we just saw in the batch-related events. For cases in
    which we are interested only in the timing information of each job, but we don’t
    need to be informed in real time about the execution life cycle, you should consult
    the event provided by `onBatchCompleted` (as just described).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法报告的事件包含了一个`OutputOperationInfo`实例，该实例提供了关于正在报告的输出操作的进一步时间详细信息。此`OutputOperationInfo`是刚刚在与批处理相关的事件中看到的`BatchInfo`对象的`outputOperationInfo`中包含的相同数据结构。对于仅对每个作业的时间信息感兴趣，但不需要实时了解执行生命周期的情况，您应该参考由`onBatchCompleted`提供的事件（正如刚刚所描述的）。
- en: Here are the callbacks you can use to inject your own processing during a job’s
    execution.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是您可以在作业执行期间注入自己处理的回调。
- en: '`onOutputOperationStarted`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`onOutputOperationStarted`'
- en: '[PRE7]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This method is called when the processing of a job, corresponding to an output
    operation, starts. Individual jobs can be related back into their corresponding
    batch, by the `batchTime` attribute.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个作业的处理（对应于一个输出操作）开始时，将调用此方法。可以通过`batchTime`属性将单个作业关联回其对应的批次。
- en: '`onOutputOperationCompleted`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`onOutputOperationCompleted`'
- en: '[PRE8]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This method is called when the processing of an individual job has completed.
    Note that there are no callbacks to notify individual job failures. The `OutputOperationInfo`
    instance contains an attribute, `failureReason`, which is an `Option[String]`.
    In case of a job failure, this option will be populated with `Some(error message)`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当单个作业的处理完成时，将调用此方法。请注意，没有回调来通知单个作业的失败。`OutputOperationInfo`实例包含一个名为`failureReason`的属性，它是一个`Option[String]`。在作业失败的情况下，此选项将被填充为`Some(error
    message)`。
- en: StreamingListener registration
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注册`StreamingListener`
- en: After we have developed our custom `StreamingListener`, we need to register
    it to consume events. The streaming listener bus is hosted by the `StreamingContext`,
    which exposes a registration call to add custom implementations of the `StreamingListener`
    trait.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开发了自定义的`StreamingListener`之后，我们需要将其注册以消费事件。流监听总线由`StreamingContext`托管，后者提供了一个注册调用来添加`StreamingListener`特性的自定义实现。
- en: Suppose that we implemented a `LogReportingStreamingListener` that forwards
    all events to a logging framework. [Example 25-1](#custom-listener-registration)
    shows how to register our custom listener in a `StreamingContext`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们实现了一个`LogReportingStreamingListener`，将所有事件转发到一个日志框架。[示例 25-1](#custom-listener-registration)展示了如何在`StreamingContext`中注册我们的自定义监听器。
- en: Example 25-1\. Custom listener registration
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 25-1\. 自定义监听器注册
- en: '[PRE9]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about the different methods to observe and continuously
    monitor your running Spark Streaming applications. Given that the performance
    characteristics of a job are a critical aspect to guarantee its stable deployment
    to a production environment, performance monitoring is an activity that you should
    perform from the early phases of the development cycle.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了观察和持续监控运行中的Spark Streaming应用程序的不同方法。考虑到作业的性能特征是保证其稳定部署到生产环境的关键因素，性能监控是您在开发周期的早期阶段应执行的活动。
- en: The Streaming UI, a tab in the Spark UI, is ideal for the interactive monitoring
    of your streaming application. It provides high-level charts that track the key
    performance indicators of a streaming job and provides links to detailed views
    where you can investigate down to the level of the individual jobs that make up
    a batch. This is an active process that leads to actionable insights into the
    performance characteristics of the streaming job.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Streaming UI作为Spark UI中的一个选项，非常适合对流式应用程序进行交互式监控。它提供高级图表，跟踪流作业的关键性能指标，并提供详细视图的链接，您可以深入调查组成批处理的各个作业的级别。这是一个能够产生可操作洞见的积极过程，了解流作业的性能特征。
- en: After the job is put to a continuous execution, it’s clear that we cannot keep
    human monitoring on a 24/7 basis. You can achieve integration with existing monitoring
    and alerting tools by consuming the REST-based interface that reports data equivalent
    to that available in the Streaming UI or through the Metrics Subsystem, which
    offers standard pluggable external interfaces that monitoring tools can consume.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦作业被持续执行，显然我们无法24/7进行人工监控。通过消费基于REST的接口，您可以与现有的监控和警报工具集成，该接口报告的数据与Streaming
    UI或Metrics Subsystem中可用的数据相当，后者提供了标准的可插拔外部接口，供监控工具使用。
- en: For the particular use cases that require a fully customizable solution that
    can get the finest-grained resolution of execution information in real time, you
    can implement a custom `StreamingListener` and register it to the `StreamingContext`
    of your application.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要完全可定制解决方案的特定用例，可以实现自定义`StreamingListener`，并将其注册到应用程序的`StreamingContext`中，以实时获取最精细粒度的执行信息。
- en: This broad spectrum of monitoring alternatives ensures that your Spark Streaming
    application deployments can be deployed and coexist in a production infrastructure
    with a common monitoring and alerting subsystem, following the internal quality
    processes of the enterprise.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这种广泛的监控方案确保您的Spark Streaming应用程序部署可以与生产基础设施中的通用监控和警报子系统共存，并遵循企业内部的质量流程。
