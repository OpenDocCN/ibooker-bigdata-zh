- en: Chapter 5\. Spark’s Distributed Processing Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。Spark的分布式处理模型
- en: As a distributed processing system, Spark relies on the availability and addressability
    of computing resources to execute any arbitrary workload.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个分布式处理系统，Spark依赖于计算资源的可用性和可寻址性来执行任意的工作负载。
- en: Although it’s possible to deploy Spark as a standalone distributed system to
    solve a punctual problem, organizations evolving in their data maturity level
    are often required to deploy a complete data architecture, as we discussed in
    [Chapter 3](ch03.xhtml#streaming-architectures).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以将Spark作为一个独立的分布式系统来解决一个特定的问题，但随着组织在其数据成熟度水平上的发展，通常需要部署一个完整的数据架构，正如我们在[第3章](ch03.xhtml#streaming-architectures)中讨论的那样。
- en: In this chapter, we want to discuss the interaction of Spark with its computational
    environment and how, in turn, it needs to adapt to the features and constraints
    of the environment of choice.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们希望讨论Spark与其计算环境的交互以及如何适应所选择环境的特性和约束。
- en: 'First, we survey the current choices for a cluster manager: YARN, Mesos, and
    Kubernetes. The scope of a cluster manager goes beyond running data analytics,
    and therefore, there are plenty of resources available to get in-depth knowledge
    on any of them. For our purposes, we are going to provide additional details on
    the cluster manager provider by Spark as a reference.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们调查集群管理器的当前选择：YARN、Mesos和Kubernetes。集群管理器的范围超出了运行数据分析，因此有大量资源可以获取关于它们任何一个的深入知识。对于我们的目的，我们将提供Spark作为参考的集群管理器提供商的额外细节。
- en: After you have an understanding of the role of the cluster manager and the way
    Spark interacts with it, we look into the aspects of fault tolerance in a distributed
    environment and how the execution model of Spark functions in that context.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您了解了集群管理器的角色及Spark如何与其交互，我们将探讨在分布式环境中容错性的各个方面以及Spark的执行模型如何在该上下文中运作。
- en: With this background, you will be prepared to understand the data reliability
    guarantees that Spark offers and how they apply to the streaming execution model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些背景，您将能够理解Spark提供的数据可靠性保证及其如何适用于流式执行模型。
- en: Running Apache Spark with a Cluster Manager
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行Apache Spark与集群管理器
- en: We are first going to look at the discipline of distributing stream processing
    on a set of machines that collectively form a *cluster*. This set of machines
    has a general purpose and needs to receive the streaming application’s runtime
    binaries and launching scripts—something known as *provisioning*. Indeed, modern
    clusters are managed automatically and include a large number of machines in a
    situation of *multitenancy*, which means that many stakeholders want to access
    and use the same cluster at various times in the day of a business. The clusters
    are therefore managed by *cluster managers*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要讨论在一个*集群*上分布流处理的学科。这组机器具有一般目的，并且需要接收流应用程序的运行时二进制文件和启动脚本——这被称为*配置*。事实上，现代集群是自动管理的，并且包括大量的机器在*多租户*的情况下运行，这意味着许多利益相关者希望在一天中的不同时间访问和使用同一个集群。因此，这些集群由*集群管理器*管理。
- en: Cluster managers are pieces of software that receive utilization requests from
    a number of users, match them to some resources, reserve the resources on behalf
    of the users for a given duration, and place user applications onto a number of
    resources for them to use. The challenges of the cluster manager’s role include
    nontrivial tasks such as figuring out the best placements of user requests among
    a pool of available machines or securely isolating the user applications if several
    share the same physical infrastructure. Some considerations where these managers
    can shine or break include fragmentation of tasks, optimal placement, availability,
    preemption, and prioritization. Cluster management is, therefore, a discipline
    in and of itself, beyond the scope of Apache Spark. Instead, Apache Spark takes
    advantage of existing cluster managers to distribute its workload over a cluster.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理器是一种软件，接收来自多个用户的利用请求，将其匹配到一些资源上，并代表用户预留这些资源一定的时间，并将用户应用程序放置在一些资源上供其使用。集群管理器角色的挑战包括非平凡的任务，如在可用机器池中为用户请求找到最佳位置或者在多个用户应用程序共享同一物理基础设施时安全地隔离用户应用程序。这些管理器能够发挥作用或者失效的一些考虑因素包括任务的碎片化、最佳位置、可用性、抢占和优先级。因此，集群管理本身就是一门学科，超出了Apache
    Spark的范围。相反，Apache Spark利用现有的集群管理器将其工作负载分布到集群中。
- en: Examples of Cluster Managers
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群管理器示例
- en: 'Some examples of popular cluster managers include the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一些流行的集群管理器示例包括以下内容：
- en: Apache YARN, which is a relatively mature cluster manager born out of the Apache
    Hadoop project
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache YARN，这是一个相对成熟的集群管理器，起源于Apache Hadoop项目。
- en: Apache Mesos, which is a cluster manager based on Linux’s container technology,
    and which was originally the reason for the existence of Apache Spark
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Mesos，这是一个基于Linux容器技术的集群管理器，最初是Apache Spark存在的原因。
- en: Kubernetes, which is a modern cluster manager born out of service-oriented deployment
    APIs, originated in practice at Google and developed in its modern form under
    the flag of the Cloud Native Computing Foundation
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes，这是一个现代的集群管理器，诞生于面向服务的部署API，实践中起源于Google，并在Cloud Native Computing
    Foundation的旗下以其现代形式发展。
- en: Where Spark can sometimes confuse people is that Apache Spark, as a distribution,
    includes a cluster manager of its own, meaning Apache Spark has the ability to
    serve as its own particular deployment orchestrator.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有时会让人感到困惑的地方在于，作为一个发行版，Apache Spark包含了自己的集群管理器，这意味着Apache Spark有能力作为其特定的部署协调器。
- en: 'In the rest of this chapter we look at the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将看以下内容：
- en: Spark’s own cluster managers and how their *special purpose* means that they
    take on less responsibility in the domain of fault tolerance or multitenancy than
    production cluster managers like Mesos, YARN, or Kubernetes.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark自己的集群管理器及其*特殊用途*意味着它在容错或多租户领域承担的责任比生产集群管理器如Mesos、YARN或Kubernetes少。
- en: How there is a standard level of *delivery guarantees* expected out of a distributed
    streaming application, how they differ from one another, and how Spark meets those
    guarantees.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式流应用程序期望的*交付保证*的标准级别如何不同，以及Spark如何满足这些保证。
- en: How microbatching, a distinctive factor of Spark’s approach to stream processing,
    comes from the decade-old model of *bulk-synchronous processing* (BSP), and paves
    the evolution path from Spark Streaming to Structured Streaming.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微批处理（microbatching），作为Spark处理流处理的独特因素，来自于*大同步处理*（BSP）十年前的模型，并为从Spark Streaming到Structured
    Streaming的演进路径铺平道路。
- en: Spark’s Own Cluster Manager
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark自己的集群管理器
- en: 'Spark has two internal cluster managers:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有两个内部集群管理器：
- en: The *local* cluster manager
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*本地*集群管理器'
- en: This emulates the function of a cluster manager (or resource manager) for testing
    purposes. It reproduces the presence of a cluster of distributed machines using
    a threading model that relies on your local machine having only a few available
    cores. This mode is usually not very confusing because it executes only on the
    user’s laptop.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 用于测试目的的集群管理器（或资源管理器）的功能。它通过依赖于本地机器仅有少量可用核心的线程模型来复制分布式机器群的存在。这种模式通常不会引起很多困惑，因为它仅在用户的笔记本电脑上执行。
- en: The *standalone* cluster manager
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*独立*集群管理器'
- en: A relatively simple, Spark-only cluster manager that is rather limited in its
    availability to slice and dice resource allocation. The standalone cluster manager
    holds and makes available the entire worker node on which a Spark executor is
    deployed and started. It also expects the executor to have been predeployed there,
    and the actual shipping of that *.jar* to a new machine is not within its scope.
    It has the ability to take over a specific number of executors, which are part
    of its deployment of worker nodes, and execute a task on it. This cluster manager
    is extremely useful for the Spark developers to provide a bare-bones resource
    management solution that allows you to focus on improving Spark in an environment
    without any bells and whistles. The standalone cluster manager is not recommended
    for production deployments.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相对简单的、仅限于Spark的集群管理器，在资源分配的切片和切割能力方面相当有限。独立集群管理器持有并使得Spark执行器部署和启动的整个工作节点可用。它还期望执行器已经预先部署在那里，并且将*.jar*实际传送到新机器不在其范围之内。它有能力接管一定数量的执行器，这些执行器是其工作节点部署的一部分，并在其上执行任务。这个集群管理器对于Spark开发人员来说非常有用，提供了一个简单的资源管理解决方案，让您可以专注于在没有任何花哨功能的环境中改进Spark。不建议将独立集群管理器用于生产部署。
- en: 'As a summary, Apache Spark is a *task scheduler* in that what it schedules
    are *tasks*, units of distribution of computation that have been extracted from
    the user program. Spark also communicates and is deployed through cluster managers
    including Apache Mesos, YARN, and Kubernetes, or allowing for some cases its own
    standalone cluster manager. The purpose of that communication is to reserve a
    number of *executors*, which are the units to which Spark understands equal-sized
    amounts of computation resources, a virtual “node” of sorts. The reserved resources
    in question could be provided by the cluster manager as the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Apache Spark是一个*任务调度器*，它调度的是*任务*，这些任务是从用户程序中提取的计算分布单元。Spark还通过包括Apache
    Mesos、YARN和Kubernetes在内的集群管理器进行通信和部署，或者在某些情况下允许使用其自己的独立集群管理器。这种通信的目的是预留一定数量的*执行器*，这些执行器是Spark理解的等大小的计算资源单位，一种虚拟的“节点”。所讨论的预留资源可以由集群管理器提供，如下：
- en: Limited processes (e.g., in some basic use cases of YARN), in which processes
    have their resource consumption metered but are not prevented from accessing each
    other’s resource by default.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有限的进程（例如，在某些YARN的基本用例中），其中进程的资源消耗受到计量，但默认情况下不会阻止它们访问彼此的资源。
- en: '*Containers* (e.g., in the case of Mesos or Kubernetes), in which containers
    are a relatively lightweight resource reservation technology that is born out
    of the cgroups and namespaces of the Linux kernel and have known their most popular
    iteration with the Docker project.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*容器*（例如，在Mesos或Kubernetes的情况下），其中容器是一种相对轻量级的资源预留技术，源自Linux内核的cgroups和命名空间，并通过Docker项目实现了它们的最流行版本。'
- en: They also could be one of the above deployed on *virtual machines* (VMs), themselves
    coming with specific cores and memory reservation.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们也可以是上述任一部署在*虚拟机*（VMs）上，这些虚拟机本身带有特定的核心和内存预留。
- en: Cluster Operations
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群操作
- en: Detailing the different levels of isolations entailed by these three techniques
    is beyond the scope of this book but well worth exploring for production setups.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 详细描述这三种技术所涉及的不同隔离级别超出了本书的范围，但对于生产环境的设置非常值得探索。
- en: Note that in an enterprise-level production cluster management domain, we also
    encounter notions such as job queues, priorities, multitenancy options, and preemptions
    that are properly the domain of that cluster manager and therefore not something
    that is very frequently talked about in material that is focused on Spark.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在企业级生产集群管理领域，我们还会遇到作业队列、优先级、多租户选项和抢占等概念，这些都是集群管理器的领域，因此在专注于Spark的材料中很少讨论。
- en: However, it will be essential for you to have a firm grasp of the specifics
    of your cluster manager setup to understand how to be a “good citizen” on a cluster
    of machines, which are often shared by several teams. There are many good practices
    on how to run a proper cluster manager while many teams compete for its resources.
    And for those recommendations, you should consult both the references listed at
    the end of this chapter and your local DevOps team.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于您来说，理解您的集群管理器设置的具体细节是至关重要的，以了解如何在多个团队共享的机器集群上成为一个“好公民”。有许多关于如何运行适当的集群管理器的良好实践，而许多团队竞争其资源。关于这些建议，您应该咨询本章末尾列出的参考资料以及您的本地DevOps团队。
- en: Understanding Resilience and Fault Tolerance in a Distributed System
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分布式系统中的弹性和容错能力
- en: 'Resilience and fault tolerance are absolutely essential for a distributed application:
    they are the condition by which we will be able to perform the user’s computation
    to completion. Nowadays, clusters are made of commodity machines that are ideally
    operated near peak capacity over their lifetime.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分布式应用程序来说，弹性和容错是绝对必要的：它们是我们能够完成用户计算的条件。如今，集群由理想情况下在其生命周期内接近峰值容量运行的廉价机器组成。
- en: To put it mildly, hardware breaks quite often. A *resilient* application can
    make progress with its process despite latencies and noncritical faults in its
    distributed environment. A *fault-tolerant* application is able to succeed and
    complete its process despite the unplanned termination of one or several of its
    nodes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，硬件经常出现故障。一个*弹性*的应用程序可以在其分布式环境中处理延迟和非关键故障，并取得进展。一个*容错*的应用程序能够在其一个或多个节点意外终止的情况下成功完成其进程。
- en: This sort of resiliency is especially relevant in stream processing given that
    the applications we’re scheduling are supposed to live for an undetermined amount
    of time. That undetermined amount of time is often correlated with the life cycle
    of the data source. For example, if we are running a retail website and we are
    analyzing transactions and website interactions as they come into the system against
    the actions and clicks and navigation of users visiting the site, we potentially
    have a data source that will be available for the entire duration of the lifetime
    of our business, which we hope to be very long, if our business is going to be
    successful.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这种弹性尤其在流处理中尤为重要，因为我们安排的应用程序应该能够持续运行一段未确定的时间。这段未确定的时间通常与数据源的生命周期相关联。例如，如果我们正在运行一个零售网站，并分析用户访问网站时的交易和网站交互，我们可能有一个数据源将在我们业务的整个生命周期内可用，而我们希望这个时间非常长，如果我们的业务要成功的话。
- en: As a consequence, a system that will process our data in a streaming fashion
    should run uninterrupted for long periods of time.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个以流式方式处理我们数据的系统应该能够长时间不间断地运行。
- en: This “show must go on” approach of streaming computation makes the resiliency
    and fault-tolerance characteristics of our applications more important. For a
    batch job, we could launch it, hope it would succeed, and relaunch if we needed
    to change it or in case of failure. For an online streaming Spark pipeline, this
    is not a reasonable assumption.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 流式计算中的“秀必须继续”的方法使得我们应用的容错和故障容忍特性变得更加重要。对于批处理作业，我们可以启动它，希望它能成功，如果需要更改或在失败的情况下重新启动。但对于在线流式
    Spark 流水线，这不是一个合理的假设。
- en: Fault Recovery
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障恢复
- en: 'In the context of fault tolerance, we are also interested in understanding
    how long it takes to recover from failure of one particular node. Indeed, stream
    processing has a particular aspect: data continues being generated by the data
    source in real time. To deal with a batch computing failure, we always have the
    opportunity to restart from scratch and accept that obtaining the results of computation
    will take longer. Thus, a very primitive form of fault tolerance is detecting
    the failure of a particular node of our deployment, stopping the computation,
    and restarting from scratch. That process can take more than twice the original
    duration that we had budgeted for that computation, but if we are not in a hurry,
    this still acceptable.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在容错的背景下，我们还希望了解从一个特定节点故障到恢复需要多长时间。实际上，流处理有一个特定的方面：数据源持续实时生成数据。要处理批处理计算的故障，我们总是有机会从头重新启动，并接受获取计算结果需要更长时间的事实。因此，容错的一个非常原始的形式是检测到部署的特定节点失败，停止计算，并从头重新启动。这个过程可能需要比我们为该计算预算的原始持续时间长两倍以上，但如果我们不赶时间，这仍然可以接受。
- en: 'For stream processing, *we need to keep receiving data* and thus potentially
    storing it, if the recovering cluster is not ready to assume any processing yet.
    This can pose a problem at a high throughput: if we try restarting from scratch,
    we will need not only to reprocess all of the data that we have observed since
    the beginning of the application—which in and of itself can be a challenge—but
    during that reprocessing of historical data, we will need it to continue receiving
    and thus potentially storing new data that was generated while we were trying
    to catch up. This pattern of restarting from scratch is something so intractable
    for streaming that we will pay special attention to Spark’s ability to restart
    only *minimal* amounts of computation in the case that a node becomes unavailable
    or nonfunctional.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于流处理，*我们需要继续接收数据*，因此在恢复的集群尚未准备好进行任何处理时，可能需要存储数据。这可能在高吞吐量时成为问题：如果我们尝试从头开始重新启动，我们不仅需要重新处理自应用程序开始以来观察到的所有数据——这本身就可能是一个挑战——而且在重新处理历史数据期间，我们需要继续接收并可能存储在我们试图赶上时生成的新数据。这种从头开始重新启动的模式对于流式处理来说是如此棘手，以至于我们将特别关注
    Spark 在节点不可用或无功能情况下仅重新启动 *最小* 计算量的能力。
- en: Cluster Manager Support for Fault Tolerance
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群管理器对容错的支持
- en: We want to highlight why it is still important to understand Spark’s fault tolerance
    guarantees, even if there are similar features present in the cluster managers
    of YARN, Mesos, or Kubernetes. To understand this, we can consider that cluster
    managers help with fault tolerance when they work hand in hand with a framework
    that is able to report failures and request new resources to cope with those exceptions.
    Spark possesses such capabilities.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想强调为什么仍然重要理解Spark的容错保证，即使YARN、Mesos或Kubernetes的集群管理器中也有类似的功能。要理解这一点，我们可以考虑集群管理器在与能够报告故障并请求新资源以应对这些异常的框架紧密合作时帮助容错。Spark具有这样的能力。
- en: For example, *production* cluster managers such as YARN, Mesos, or Kubernetes
    have the ability to detect a node’s failure by inspecting endpoints on the node
    and asking the node to report on its own readiness and liveness state. If these
    cluster managers detect a failure and they have spare capacity, they will replace
    that node with another, made available to Spark. That particular action implies
    that the Spark executor code will start anew in another node, and then attempt
    to join the existing Spark cluster.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*生产*集群管理器（如YARN、Mesos或Kubernetes）具有通过检查节点上的端点并要求节点报告其自身的就绪状态和活跃状态来检测节点故障的能力。如果这些集群管理器检测到故障并且有备用容量，它们将用另一个节点替换该节点，以供Spark使用。这一特定操作意味着Spark执行器代码将在另一个节点上重新启动，然后尝试加入现有的Spark集群。
- en: The cluster manager, by definition, does not have introspection capabilities
    into the applications being run on the nodes that it reserves. Its responsibility
    is limited to the container that runs the user’s code.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理器本质上不具有对其保留的节点上运行的应用程序进行内省的能力。它的责任仅限于运行用户代码的容器。
- en: 'That responsibility boundary is where the Spark resilience features start.
    To recover from a failed node, Spark needs to do the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 那个责任边界是Spark弹性特性开始的地方。为了从失败节点中恢复，Spark需要执行以下操作：
- en: Determine whether that node contains some state that should be reproduced in
    the form of checkpointed files
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定该节点是否包含应以检查点文件形式再现的某些状态。
- en: Understand at which stage of the job a node should rejoin the computation
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解在作业的哪个阶段节点应重新加入计算。
- en: The goal here is for us to explore that if a node is being replaced by the cluster
    manager, Spark has capabilities that allow it to take advantage of this new node
    and to distribute computation onto it.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是我们探索一下，如果一个节点被集群管理器替换，Spark是否具有能力利用这个新节点，并将计算分布到它上面。
- en: 'Within this context, our focus is on Spark’s responsibilities as an application
    and underline the capabilities of a cluster manager only when necessary: for instance,
    a node could be replaced because of a hardware failure or because its work was
    simply preempted by a higher-priority job. Apache Spark is blissfully unaware
    of the *why*, and focuses on the *how*.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种背景下，我们关注Spark作为应用程序的责任，并在必要时强调集群管理器的能力：例如，节点可能由于硬件故障或其工作被更高优先级的作业简单地抢占而被替换。Apache
    Spark对*为什么*毫不知情，而专注于*如何*。
- en: Data Delivery Semantics
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据传递语义
- en: As you have seen in the streaming model, the fact that streaming jobs act on
    the basis of data that is generated in real time means that intermediate results
    need to be provided to the *consumer* of that streaming pipeline on a regular
    basis.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在流式模型中看到的，流式作业基于实时生成的数据操作，这意味着中间结果需要定期提供给流水线的*消费者*。
- en: Those results are being produced by some part of our cluster. Ideally, we would
    like those observable results to be coherent, in line, and in real time with respect
    to the arrival of data. This means that we want results that are exact, and we
    want them as soon as possible. However, distributed computation has its own challenges
    in that it sometimes includes not only individual nodes failing, as we have mentioned,
    but it also encounters situations like *network partitions*, in which some parts
    of our cluster are not able to communicate with other parts of that cluster, as
    illustrated in [Figure 5-1](#network_partition_image).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果由我们集群的某些部分产生。理想情况下，我们希望这些可观察结果与数据到达的实时性相一致。这意味着我们希望得到精确的结果，并希望尽快获得它们。然而，分布式计算也有自己的挑战，有时不仅包括个别节点的故障，如我们所提到的，还包括像*网络分区*这样的情况，其中集群的某些部分无法与该集群的其他部分进行通信，如[图5-1](#network_partition_image)所示。
- en: '![spas 0501](Images/spas_0501.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0501](Images/spas_0501.png)'
- en: Figure 5-1\. A network partition
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 网络分区
- en: Spark has been designed using a *driver/executor* architecture. A specific machine,
    the *driver*, is tasked with keeping track of the *job progression* along with
    the job submissions of a user, and the computation of that program occurs as the
    data arrives. However, if the network partitions separate some part of the cluster,
    the *driver* might be able to keep track of only the part of the executors that
    form the initial cluster. In the other section of our partition, we will find
    nodes that are entirely able to function, but will simply be unable to account
    for the proceedings of their computation to the *driver*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是使用*driver/executor*架构设计的。一个特定的机器，*driver*，负责跟踪*作业进度*以及用户的作业提交，并且该程序的计算是随着数据的到达而发生的。然而，如果网络分区分隔了集群的某些部分，*driver*可能只能跟踪形成初始集群的执行器的一部分。在我们的分区的另一部分中，我们将找到完全能够运行但无法向*driver*报告其计算进程的节点。
- en: This creates an interesting case in which those “zombie” nodes do not receive
    new tasks, but might well be in the process of completing some fragment of computation
    that they were previously given. Being unaware of the partition, they will report
    their results as any executor would. And because this reporting of results sometimes
    does not go through the *driver* (for fear of making the *driver* a bottleneck),
    the reporting of these zombie results could succeed.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了一个有趣的情况，即那些“僵尸”节点不会接收新任务，但可能正在完成它们之前获得的某些计算片段。由于不知道分区的存在，它们将像任何执行器一样报告它们的结果。由于这些“僵尸”结果的报告有时不通过*driver*（为了避免*driver*成为瓶颈），这些结果的报告可能会成功。
- en: 'Because the *driver*, a single point of bookkeeping, does not know that those
    zombie executors are still functioning and reporting results, it will reschedule
    the same tasks that the lost executors had to accomplish on new nodes. This creates
    a *double-answering* problem in which the zombie machines lost through partitioning
    and the machines bearing the rescheduled tasks both report the same results. This
    bears real consequences: one example of stream computation that we previously
    mentioned is routing tasks for financial transactions. A double withdrawal, in
    that context, or double stock purchase orders, could have tremendous consequences.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*driver*，一个单一的记账点，并不知道那些僵尸执行器仍在运行并报告结果，它将重新安排丢失的执行器需要在新节点上完成的相同任务。这造成了一个“双答复”问题，即通过分区丢失的僵尸机器和承载重新安排任务的机器都报告了相同的结果。这带来了真实的后果：我们之前提到的流计算的一个例子是路由金融交易的任务。在这种情况下，双重提款或双重股票购买订单可能会产生巨大的后果。
- en: It is not only the aforementioned problem that causes different processing semantics.
    Another important reason is that when output from a stream-processing application
    and state checkpointing cannot be completed in one atomic operation, it will cause
    data corruption if failure happens between checkpointing and outputting.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 引起不同处理语义的问题不仅仅是上述问题。另一个重要的原因是，当流处理应用的输出和状态检查点无法在一个原子操作中完成时，在检查点和输出之间发生故障将导致数据损坏。
- en: 'These challenges have therefore led to a distinction between *at least once*
    processing and *at most once* processing:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些挑战导致了“至少一次”处理和“至多一次”处理之间的区别：
- en: At least once
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 至少一次
- en: This processing ensures that every element of a stream has been processed once
    or more.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个处理确保了流的每个元素至少被处理一次或更多次。
- en: At most once
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 至多一次
- en: This processing ensures that every element of the stream is processed once or
    less.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个处理确保了流的每个元素最多被处理一次。
- en: Exactly once
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 确切一次
- en: This is the combination of “at least once” and “at most once.”
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是“至少一次”和“至多一次”的组合。
- en: At-least-once processing is the notion that we want to make sure that every
    chunk of initial data has been dealt with—it deals with the node failure we were
    talking about earlier. As we’ve mentioned, when a streaming process suffers a
    partial failure in which some nodes need to be replaced or some data needs to
    be recomputed, we need to reprocess the lost units of computation while keeping
    the ingestion of data going. That requirement means that if you do not respect
    at-least-once processing, there is a chance for you, under certain conditions,
    to lose data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 至少一次处理是我们想确保每个初始数据块都已处理的概念——它处理了我们之前谈到的节点故障。正如我们提到的，当流处理过程遭受部分失败时，需要替换一些节点或重新计算一些数据，我们需要重新处理丢失的计算单元，同时保持数据的摄入。如果不遵守至少一次处理，有可能在特定条件下丢失数据。
- en: The antisymmetric notion is called at-most-once processing. At-most-once processing
    systems guarantee that the zombie nodes repeating the same results as a rescheduled
    node are treated in a coherent manner, in which we keep track of only one set
    of results. By keeping track of *what data* their results *were about*, we’re
    able to make sure we can discard repeated results, yielding at-most-once processing
    guarantees. The way in which we achieve this relies on the notion of *idempotence*
    applied to the “last mile” of result reception. Idempotence qualifies a function
    such that if we apply it twice (or more) to any data, we will get the same result
    as the first time. This can be achieved by keeping track of the data that we are
    reporting a result for, and having a bookkeeping system at the output of our streaming
    process.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 反对称概念称为最多一次处理。最多一次处理系统保证，将重复结果的僵尸节点与重新安排的节点一样以一致的方式处理，我们只保留一组结果。通过跟踪*其结果所涉及的数据*，我们能够确保丢弃重复的结果，从而得到最多一次处理的保证。我们实现这一点的方式依赖于作用于结果接收的“最后一英里”的幂等性概念。幂等性使得函数的应用两次（或更多次）于任何数据时，结果与第一次相同。这可以通过跟踪我们报告结果的数据，并在流处理输出处有一个记账系统来实现。
- en: Microbatching and One-Element-at-a-Time
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微批处理和逐个元素处理
- en: 'In this section, we want to address two important approaches to stream processing:
    *bulk-synchronous processing*, and *one-at-a-time record processing*.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们要讨论流处理的两种重要方法：*批同步处理*和*逐条记录处理*。
- en: 'The objective of this is to connect those two ideas to the two APIs that Spark
    possesses for stream processing: Spark Streaming and Structured Streaming.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这一目标是将这两个概念连接到Spark用于流处理的两个API：Spark Streaming和Structured Streaming。
- en: 'Microbatching: An Application of Bulk-Synchronous Processing'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微批处理：批量同步处理的一种应用
- en: Spark Streaming, the more mature model of stream processing in Spark, is roughly
    approximated by what’s called a *Bulk Synchronous Parallelism* (BSP) system.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming，在Spark中流处理的更成熟模型，大致近似于所谓的*批量同步并行*（BSP）系统。
- en: 'The gist of BSP is that it includes two things:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: BSP的要点在于它包括两个方面：
- en: A split distribution of asynchronous work
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步工作的分割分布
- en: A synchronous barrier, coming in at fixed intervals
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同步屏障，按固定间隔到达
- en: The split is the idea that each of the successive steps of work to be done in
    streaming is separated in a number of parallel chunks that are roughly proportional
    to the number of executors available to perform this task. Each executor receives
    its own chunk (or chunks) of work and works separately until the second element
    comes in. A particular resource is tasked with keeping track of the progress of
    computation. With Spark Streaming, this is a synchronization point at the “driver”
    that allows the work to progress to the next step. Between those scheduled steps,
    all of the executors on the cluster are doing the same thing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 分割是每个连续的流处理步骤要完成的工作分割成数量大致与可用于执行此任务的执行器数量成比例的并行块的概念。每个执行器接收其自己的工作块（或块），并单独工作，直到第二个元素到来。特定的资源负责跟踪计算的进度。在Spark
    Streaming中，这是一个“驱动程序”上的同步点，允许工作进入下一步。在这些预定的步骤之间，集群上的所有执行器都在做相同的事情。
- en: Note that what is being passed around in this scheduling process are the functions
    that describe the processing that the user wants to execute on the data. The data
    is already on the various executors, most often being delivered directly to these
    resources over the lifetime of the cluster.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在此调度过程中传递的是描述用户希望对数据执行的处理的函数。数据已经位于各个执行器上，通常在集群的生命周期内直接传递到这些资源。
- en: 'This was coined “function-passing style” by Heather Miller in 2016 (and formalized
    in [[Miller2016]](app01.xhtml#Miller2016)): asynchronously pass safe functions
    to distributed, stationary, immutable data in a stateless container, and use lazy
    combinators to eliminate intermediate data structures.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这在2016年由Heather Miller称为“函数传递风格”（并在[[Miller2016]](app01.xhtml#Miller2016)中正式化）：异步将安全函数传递给分布式、静态、不可变数据，在无状态容器中使用惰性组合子来消除中间数据结构。
- en: The frequency at which further rounds of data processing are scheduled is dictated
    by a time interval. This time interval is an arbitrary duration that is measured
    in batch processing time; that is, what you would expect to see as a “wall clock”
    time observation in your cluster.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 进行进一步数据处理轮次的频率由时间间隔决定。这个时间间隔是一个任意的持续时间，以批处理时间来衡量；也就是说，在您的集群中作为“挂钟”时间观察所期望看到的内容。
- en: For stream processing, we choose to implement barriers at small, fixed intervals
    that better approximate the real-time notion of data processing.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于流处理，我们选择在小的固定间隔内实现屏障，以更好地近似数据处理的实时概念。
- en: One-Record-at-a-Time Processing
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逐条记录处理
- en: 'By contrast, one-record-at-a-time processing functions by *pipelining*: it
    analyzes the whole computation as described by user-specified functions and deploys
    it as pipelines using the resources of the cluster. Then, the only remaining matter
    is to flow data through the various resources, following the prescribed pipeline.
    Note that in this latter case, each step of the computation is materialized at
    some place in the cluster at any given point.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，逐条记录处理通过*流水线处理*：它分析用户指定函数描述的整个计算，并将其部署为使用集群资源的流水线。然后，唯一剩下的问题就是通过各种资源流动数据，按照规定的流水线进行操作。请注意，在后一种情况下，计算的每个步骤在集群中的某个地方都有具体体现。
- en: Systems that function mostly according to this paradigm include Apache Flink,
    Naiad, Storm, and IBM Streams. (You can read more on these in [Chapter 29](ch29.xhtml#other-stream-processing).)
    This does not necessarily mean that those systems are incapable of microbatching,
    but rather characterizes their major or most native mode of operation and makes
    a statement on their dependency on the process of pipelining, often at the heart
    of their processing.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数按照这一范式运行的系统包括Apache Flink，Naiad，Storm和IBM Streams。（您可以在[第29章](ch29.xhtml#other-stream-processing)中进一步了解这些内容。）这并不一定意味着这些系统无法进行微批处理，而是表明它们的主要或最原生的操作模式，并说明它们对流水线处理过程的依赖通常是其核心。
- en: 'The minimum latency, or time needed for the system to react to the arrival
    of one particular event, is very different between those two: minimum latency
    of the microbatching system is therefore the time needed to complete the reception
    of the current microbatch (the batch interval) plus the time needed to start a
    task at the executor where this data falls (also called scheduling time). On the
    other hand, a system processing records one by one can react as soon as it meets
    the event of interest.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者之间特定事件到达系统反应所需的最小延迟时间非常不同：微批处理系统的最小延迟时间因此是完成当前微批（批处理间隔）的接收所需的时间加上在数据落到的执行器上启动任务所需的时间（也称为调度时间）。另一方面，逐条记录处理系统可以在遇到感兴趣事件时立即作出反应。
- en: 'Microbatching Versus One-at-a-Time: The Trade-Offs'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微批处理与逐条处理之间的权衡
- en: 'Despite their higher latency, microbatching systems offer significant advantages:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其延迟较高，微批处理系统提供了显著的优势：
- en: They are able to *adapt* at the synchronization barrier boundaries. That adaptation
    might represent the task of recovering from failure, if a number of executors
    have been shown to become deficient or lose data. The periodic synchronization
    can also give us an opportunity to add or remove executor nodes, giving us the
    possibility to grow or shrink our resources depending on what we’re seeing as
    the cluster load, observed through the throughput on the data source.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们能够在同步障碍边界上*适应*。如果一些执行器已经显示出不足或丢失数据，这种适应可能代表了从故障中恢复的任务。周期性的同步还可以为我们提供添加或移除执行器节点的机会，使我们能够根据我们观察到的集群负载通过数据源的吞吐量来增加或减少资源。
- en: Our BSP systems can sometimes have an easier time providing *strong consistency*
    because their batch determinations—that indicate the beginning and the end of
    a particular batch of data—are deterministic and recorded. Thus, any kind of computation
    can be redone and produce the same results the second time.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的BSP系统有时可以更容易地提供*强一致性*，因为它们的批处理决策—指示特定数据批次的开始和结束—是确定性的并被记录下来。因此，任何计算都可以重新进行，并且第二次会产生相同的结果。
- en: Having data available *as a set* that we can probe or inspect at the beginning
    of the microbatch allows us to perform efficient optimizations that can provide
    ideas on the way to compute on the data. Exploiting that on *each* microbatch,
    we can consider the specific case rather than the general processing, which is
    used for all possible input. For example, we could take a sample or compute a
    statistical measure before deciding to process or drop each microbatch.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在微批的开始时，我们可以将数据作为*集合*提供，这样我们可以执行有效的优化，为计算数据提供思路。利用每个微批次，我们可以考虑具体的情况，而不是一般处理，这适用于所有可能的输入。例如，我们可以在决定处理或丢弃每个微批之前进行采样或计算统计量。
- en: More importantly, the simple presence of the microbatch as a well-identified
    element also allows an efficient way of specifying programming for both batch
    processing (where the data is at rest and has been saved somewhere) and streaming
    (where the data is in flight). The microbatch, even for mere instants, *looks*
    like data at rest.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，即使是瞬时的微批也可以作为明确定义的元素来有效地指定批处理和流处理的编程方式。即使只有瞬时，微批看起来也*像是*静态数据。
- en: Bringing Microbatch and One-Record-at-a-Time Closer Together
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将微批处理和逐条处理结合在一起
- en: The marriage between microbatching and one-record-at-a-time processing as is
    implemented in systems like Apache Flink or Naiad is still a subject of research.^([1](ch05.xhtml#idm46385832844312)).]
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在像Apache Flink或Naiad这样的系统中实现的微批处理和逐条处理的结合仍然是研究的课题。^([1](ch05.xhtml#idm46385832844312)).]
- en: Although it does not solve every issue, Structured Streaming, which is backed
    by a main implementation that relies on microbatching, does not expose that choice
    at the API level, allowing for an evolution that is independent of a fixed-batch
    interval. In fact, the default internal execution model of Structured Streaming
    is that of microbatching with a dynamic batch interval. Structured Streaming is
    also implementing continuous processing for some operators, which is something
    we touch upon in [Chapter 15](ch15.xhtml#ss-experimental).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它不能解决每一个问题，但由微批处理支持的结构化流处理并不会在API级别上暴露这种选择，允许独立于固定批处理间隔的演进。事实上，结构化流处理的默认内部执行模型是微批处理，具有动态批处理间隔。对于某些操作符，结构化流处理还实现了连续处理，这是我们在[第15章](ch15.xhtml#ss-experimental)中涉及的内容。
- en: Dynamic Batch Interval
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态批处理间隔
- en: What is this notion of *dynamic batch interval*? The dynamic batch interval
    is the notion that the recomputation of data in a streaming `DataFrame` or `Dataset`
    consists of an update of existing data with the new elements seen over the wire.
    This update is occurring based on a trigger and the usual basis of this would
    be time duration. That time duration is still determined based on a fixed world
    clock signal that we expect to be synchronized within our entire cluster and that
    represents a single synchronous source of time that is shared among every executor.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是*动态批处理间隔*？动态批处理间隔是指在流式处理的`DataFrame`或`Dataset`中，数据的重新计算包括对新接收到的数据进行更新。这种更新基于触发器，并且通常基于时间段进行。该时间段仍然是基于我们预期在整个集群内同步的固定世界时钟信号，代表了每个执行器共享的单一同步时间源。
- en: However, this trigger can also be the statement of “as often as possible.” That
    statement is simply the idea that a new batch should be started as soon as the
    previous one has been processed, given a reasonable initial duration for the first
    batch. This means that the system will launch batches as often as possible. In
    this situation, the latency that can be observed is closer to that of one-element-at-a-time
    processing. The idea here is that the microbatches produced by this system will
    converge to the smallest manageable size, making our stream flow faster through
    the executor computations that are necessary to produce a result. As soon as that
    result is produced, a new query will be started and scheduled by the Spark driver.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个触发器也可以是“尽可能频繁”这一声明。这个声明只是一个新批次应该在前一个批次处理完之后立即启动的想法，给第一个批次一个合理的初始持续时间。这意味着系统将尽可能频繁地启动批次。在这种情况下，可以观察到的延迟接近于逐个处理的情况。这里的思想是，由此系统产生的微批次将收敛到最小可管理的大小，使得我们的流通过执行器计算更快地产生结果。一旦产生了那个结果，Spark驱动程序将启动并安排一个新的查询。
- en: Structured Streaming Processing Model
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流处理模型
- en: 'The main steps in Structured Streaming processing are as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理的主要步骤如下：
- en: When the Spark driver triggers a new batch, processing starts with updating
    the account of data read from a data source, in particular, getting data offsets
    for the beginning and the end of the latest batch.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当Spark驱动程序触发新的批次时，处理从更新从数据源读取的数据账户开始，特别是获取最新批次的起始和结束的数据偏移量。
- en: This is followed by logical planning, the construction of successive steps to
    be executed on data, followed by query planning (intrastep optimization).
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这之后是逻辑规划，构建要在数据上执行的连续步骤，然后是查询规划（步内优化）。
- en: And then the launch and scheduling of the actual computation by adding a new
    batch of data to update the continuous query that we’re trying to refresh.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过添加新的数据批次来更新我们试图刷新的连续查询的实际计算的启动和调度。
- en: Hence, from the point of view of the computation model, we will see that the
    API is significantly different from Spark Streaming.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从计算模型的角度来看，我们将看到API与Spark Streaming有显著的不同。
- en: The Disappearance of the Batch Interval
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理间隔的消失
- en: We now briefly explain what Structured Streaming batches mean and their impact
    with respect to operations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们简要解释一下结构化流批处理的含义及其对操作的影响。
- en: In Structured Streaming, the batch interval that we are using is no longer a
    computation budget. With Spark Streaming, the idea was that if we produce data
    every two minutes and flow data into Spark’s memory every two minutes, we should
    produce the results of computation on that batch of data in at least two minutes,
    to clear the memory from our cluster for the next microbatch. Ideally, as much
    data flows out as flows in, and the usage of the collective memory of our cluster
    remains stable.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化流处理中，我们使用的批处理间隔不再是一个计算预算。在Spark Streaming中，理念是，如果我们每两分钟产生一次数据，并且每两分钟将数据流入Spark的内存，我们应该在至少两分钟内对该批数据进行计算，并清除我们集群中的内存以便下一个微批次。理想情况下，数据流入的量与流出的量相同，并且我们集群的集体内存使用保持稳定。
- en: 'With Structured Streaming, without this fixed time synchronization, our ability
    to see performance issues in our cluster is more complex: a cluster that is unstable—that
    is, unable to “clear out” data by finishing to compute on it as fast as new data
    flows in—will see ever-growing batch processing times, with an accelerating growth.
    We can expect that keeping a hand on this batch processing time will be pivotal.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用结构化流，没有这种固定的时间同步，我们在集群中看到性能问题的能力更加复杂：一个不稳定的集群——即无法通过尽快完成计算来“清除”数据的集群——将看到不断增长的批处理时间，增长速度加快。我们可以预期，控制这个批处理时间将至关重要。
- en: However, if we have a cluster that is correctly sized with respect to the throughput
    of our data, there are a lot of advantages to have an as-often-as-possible update.
    In particular, we should expect to see very frequent results from our Structured
    Streaming cluster with a higher granularity than we used to in the time of a conservative
    batch interval.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们的集群与数据吞吐量的大小正确匹配，那么具有尽可能频繁更新的许多优势。特别是，我们应该期望在我们结构化流集群中看到比以往保守批处理间隔时间更高粒度的非常频繁的结果。
- en: ^([1](ch05.xhtml#idm46385832844312-marker)) One interesting Spark-related project
    that recently came out of the University of Berkeley is called Drizzle and uses
    “group scheduling” to form a sort of longer-lived pipeline that persists across
    several batches, for the purpose of creating near-continuous queries. See [Venkataraman2016
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.xhtml#idm46385832844312-marker)) 最近由伯克利大学推出的一个与Spark相关的有趣项目名为Drizzle，它使用“组调度”来形成一种类似于长寿命的流水线，跨多个批次持久存在，旨在创建接近连续的查询。参见
    [Venkataraman2016
