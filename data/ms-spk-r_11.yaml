- en: Chapter 10\. Extensions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 10 章\. 扩展
- en: I try to know as many people as I can. You never know which one you’ll need.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我尽量认识尽可能多的人。你永远不知道哪一个你会需要。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Tyrion Lannister
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ——泰利昂·兰尼斯特
- en: In [Chapter 9](ch09.html#tuning), you learned how Spark processes data at large
    scale by allowing users to configure the cluster resources, partition data implicitly
    or explicitly, execute commands across distributed compute nodes, shuffle data
    across them when needed, cache data to improve performance, and serialize data
    efficiently over the network. You also learned how to configure the different
    Spark settings used while connecting, submitting a job, and running an application,
    as well as particular settings applicable only to R and R extensions that we present
    in this chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 9 章](ch09.html#tuning) 中，您学习了 Spark 如何通过允许用户配置集群资源、隐式或显式分区数据、在分布式计算节点上执行命令、在需要时在它们之间进行数据洗牌、缓存数据以提高性能以及高效地在网络上序列化数据来处理大规模数据。您还学习了如何配置连接、提交作业和运行应用程序时使用的不同
    Spark 设置，以及仅适用于 R 和 R 扩展的特定设置，在本章中我们将介绍。
- en: Chapters [3](ch03.html#analysis), [4](ch04.html#modeling), and [8](ch08.html#data)
    provided a foundation to read and understand most datasets. However, the functionality
    that was presented was scoped to Spark’s built-in features and tabular datasets.
    This chapter goes beyond tabular data and explores how to analyze and model networks
    of interconnected objects through graph processing, analyze genomics datasets,
    prepare data for deep learning, analyze geographic datasets, and use advanced
    modeling libraries like H2O and XGBoost over large-scale datasets.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 第 [3](ch03.html#analysis)、[4](ch04.html#modeling) 和 [8](ch08.html#data) 章提供了阅读和理解大多数数据集的基础。然而，所呈现的功能范围仅限于
    Spark 的内置功能和表格数据集。本章将超越表格数据，探讨如何通过图处理分析和建模网络中相互连接的对象，分析基因组数据集，为深度学习准备数据，分析地理数据集，并使用像
    H2O 和 XGBoost 这样的高级建模库处理大规模数据集。
- en: The combination of all the content presented in the previous chapters should
    take care of most of your large-scale computing needs. However, for those few
    use cases for which functionality is still lacking, the following chapters provide
    tools to extend Spark yourself—through custom R transformation, custom Scala code,
    or a recent new execution mode in Spark that enables analyzing real-time datasets.
    However, before reinventing the wheel, let’s examine some of the extensions available
    in Spark.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章介绍的所有内容应该可以满足大多数您的大规模计算需求。然而，对于仍然缺乏功能的少数用例，以下章节提供了通过自定义 R 转换、自定义 Scala 代码或
    Spark 中的最新执行模式来扩展 Spark 的工具，该模式使得能够分析实时数据集。但在重新发明轮子之前，让我们先看看 Spark 中可用的一些扩展。
- en: Overview
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In [Chapter 1](ch01.html#intro), we presented the R community as a vibrant group
    of individuals collaborating with each other in many ways—for example, moving
    open science forward by creating R packages that you can install from CRAN. In
    a similar way, but at a much smaller scale, the R community has contributed extensions
    that increase the functionality initially supported in Spark and R. Spark itself
    also provides support for creating Spark extensions and, in fact, many R extensions
    make use of Spark extensions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 1 章](ch01.html#intro) 中，我们将 R 社区描述为一个充满活力的个体群体，通过多种方式相互合作，例如通过创建您可以从 CRAN
    安装的 R 包来推动开放科学。类似地，尽管规模小得多，但 R 社区也贡献了增强最初在 Spark 和 R 中支持的功能的扩展。Spark 本身还支持创建 Spark
    扩展，实际上，许多 R 扩展使用了 Spark 扩展。
- en: Extensions are constantly being created, so this section will be outdated at
    some given point in time. In addition, we might not even be aware of many Spark
    and R extensions. However, at the very least we can track the extensions that
    are available in CRAN by looking at the [“reverse imports” for `sparklyr` in CRAN](http://bit.ly/2Z568xz).
    Extensions and R packages published in CRAN tend to be the most stable since when
    a package is published in CRAN, it goes through a review process that increases
    the overall quality of a contribution.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展不断被创建，所以这部分内容在某个时刻可能会过时。此外，我们可能甚至不知道许多 Spark 和 R 的扩展。然而，至少我们可以通过查看在 [CRAN
    中 `sparklyr` 的“反向导入”](http://bit.ly/2Z568xz) 来追踪 CRAN 中可用的扩展。在 CRAN 中发布的扩展和 R
    包通常是最稳定的，因为当一个包在 CRAN 中发布时，它经过了审查流程，提高了贡献的整体质量。
- en: While we wish we could present all the extensions, we’ve instead scoped this
    chapter to the extensions that should be the most interesting to you. You can
    find additional extensions at the [github.com/r-spark](https://github.com/r-spark)
    organization or by searching repositories on GitHub with the `sparklyr` tag.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们希望能够呈现所有扩展，但我们将此章节范围限定在应该对您最有趣的扩展上。您可以在 [github.com/r-spark](https://github.com/r-spark)
    组织或通过使用 `sparklyr` 标签在 GitHub 上搜索仓库中找到其他扩展。
- en: '`rsparkling`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`rsparkling`'
- en: The `rsparkling` extensions allow you to use H2O and Spark from R. This extension
    is what we would consider advanced modeling in Spark. While Spark’s built-in modeling
    library, Spark MLlib, is quite useful in many cases, H2O’s modeling capabilities
    can compute additional statistical metrics and can provide performance and scalability
    improvements over Spark MLlib. We have neither performed detailed comparisons
    nor benchmarks between MLlib and H2O; this is something you will need to research
    on your own to create a complete picture of when to use H2O’s capabilities.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`rsparkling` 扩展允许您从 R 中使用 H2O 和 Spark。这个扩展是我们认为在 Spark 中进行高级建模的一部分。虽然 Spark
    的内置建模库 Spark MLlib 在许多情况下非常有用，但 H2O 的建模能力可以计算额外的统计指标，并且在性能和可扩展性上比 Spark MLlib
    有所提升。我们没有进行 MLlib 和 H2O 之间的详细比较或基准测试；这是您需要自行研究的内容，以形成何时使用 H2O 能力的完整图景。'
- en: '`graphframes`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`graphframes`'
- en: The `graphframes` extensions adds support to process graphs in Spark. A graph
    is a structure that describes a set of objects in which some pairs of the objects
    are in some sense related. As you learned in [Chapter 1](ch01.html#intro), ranking
    web pages was an early motivation to develop precursors to Spark powered by MapReduce;
    web pages happen to form a graph if you consider a link between pages as the relationship
    between each pair of pages. Computing operations likes PageRank over graphs can
    be quite useful in web search and social networks, for example.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`graphframes` 扩展增加了在 Spark 中处理图形的支持。图是一种描述一组对象的结构，其中某些对象对在某种意义上相关。正如您在[第一章](ch01.html#intro)中所学到的，排名网页是开发以
    MapReduce 为基础的 Spark 前身的早期动机；如果将页面之间的链接视为页面对之间的关系，网页恰好形成图形。在网页搜索和社交网络中，计算诸如 PageRank
    等操作在图形上可以非常有用。'
- en: '`sparktf`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`sparktf`'
- en: The `sparktf` extension provides support to write TensorFlow records in Spark.
    TensorFlow is one of the leading deep learning frameworks, and it is often used
    with large amounts of numerical data represented as TensorFlow records, a file
    format optimized for TensorFlow. Spark is often used to process unstructured and
    large-scale datasets into smaller numerical datasets that can easily fit into
    a GPU. You can use this extension to save datasets in the TensorFlow record file
    format.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`sparktf` 扩展提供了在 Spark 中编写 TensorFlow 记录的支持。TensorFlow 是领先的深度学习框架之一，通常与大量数值数据一起使用，这些数据表示为
    TensorFlow 记录，这是一种针对 TensorFlow 优化的文件格式。Spark 经常用于将非结构化和大规模数据集处理为可以轻松适应 GPU 的较小数值数据集。您可以使用此扩展来保存
    TensorFlow 记录文件格式的数据集。'
- en: '`xgboost`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`xgboost`'
- en: The `xgboost` extension brings the well-known XGBoost modeling library to the
    world of large-scale computing. XGBoost is a scalable, portable, and distributed
    library for gradient boosting. It became well known in the machine learning competition
    circles after its use in the winning solution of the [Higgs Boson Machine Learning
    Challenge](http://bit.ly/2YPE2qO) and has remained popular in other Kaggle competitions
    since then.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`xgboost` 扩展将著名的 XGBoost 建模库引入到大规模计算的世界中。XGBoost 是一个可扩展、便携和分布式的梯度提升库。在[希格斯玻色子机器学习挑战赛](http://bit.ly/2YPE2qO)中使用后，它在机器学习竞赛圈中广为人知，并且自那时以来在其他
    Kaggle 竞赛中仍然很受欢迎。'
- en: '`variantspark`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`variantspark`'
- en: The `variantspark` extension provides an interface to use Variant Spark, a scalable
    toolkit for genome-wide association studies (GWAS). It currently provides functionality
    to build random forest models, estimating variable importance, and reading variant
    call format (VCF) files. While there are other random forest implementations in
    Spark, most of them are not optimized to deal with GWAS datasets, which usually
    come with thousands of samples and millions of variables.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`variantspark` 扩展提供了使用 Variant Spark 的接口，这是一个用于全基因组关联研究（GWAS）的可扩展工具包。它目前提供了构建随机森林模型、估计变量重要性和读取变异调用格式（VCF）文件的功能。虽然
    Spark 中有其他随机森林实现，但大多数并未经过优化，不能处理通常带有数千个样本和数百万个变量的 GWAS 数据集。'
- en: '`geospark`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`geospark`'
- en: The `geospark` extension enables us to load and query large-scale geographic
    datasets. Usually datasets containing latitude and longitude points or complex
    areas are defined in the well-known text (WKT) format, a text markup language
    for representing vector geometry objects on a map.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`geospark`扩展使我们能够加载和查询大规模地理数据集。通常，包含纬度和经度点或复杂区域的数据集以Well-Known Text（WKT）格式定义，这是一种在地图上表示矢量几何对象的文本标记语言。'
- en: Before you learn how and when to use each extension, we should first briefly
    explain how you can use extensions with R and Spark.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在您学习如何以及何时使用每个扩展之前，我们应该首先简要解释如何在R和Spark中使用扩展。
- en: 'First, a Spark extension is just an R package that happens to be aware of Spark.
    As with any other R package, you will first need to install the extension. After
    you’ve installed it, it is important to know that you will need to reconnect to
    Spark before the extension can be used. So, in general, here’s the pattern you
    should follow:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Spark扩展只是一个意识到Spark的R包。与任何其他R包一样，您首先需要安装该扩展。安装完成后，重要的是要知道，在可以使用扩展之前，您需要重新连接到Spark。因此，一般而言，您应该遵循以下模式：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Notice that `sparklyr` is loaded after the extension to allow the extension
    to register properly. If you had to install and load a new extension, you would
    first need to disconnect using `spark_disconnect(sc)`, restart your R session,
    and repeat the preceding steps with the new extension.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在扩展注册之前加载`sparklyr`，以确保扩展能够正确注册。如果您需要安装和加载新的扩展，首先需要使用`spark_disconnect(sc)`断开连接，重新启动R会话，然后使用新的扩展重复前述步骤。
- en: It’s not difficult to install and use Spark extensions from R; however, each
    extension can be a world of its own, so most of the time you will need to spend
    time understanding what the extension is, when to use it, and how to use it properly.
    The first extension you will learn about is the `rsparkling` extension, which
    enables you to use H2O in Spark with R.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从R中安装和使用Spark扩展并不困难；然而，每个扩展都可能是一个独立的世界，因此大多数情况下，您需要花时间了解扩展是什么、何时使用以及如何正确使用。您将首先了解的第一个扩展是`rsparkling`扩展，它使您能够在Spark中使用H2O与R。
- en: H2O
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: H2O
- en: '[H2O](https://www.h2o.ai/), created by H2O.ai, is open source software for
    large-scale modeling that allows you to fit thousands of potential models as part
    of discovering patterns in data. You can consider using H2O to complement or replace
    Spark’s default modeling algorithms. It is common to use Spark’s default modeling
    algorithms and transition to H2O when Spark’s algorithms fall short or when advanced
    functionality (like additional modeling metrics or automatic model selection)
    is desired.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[H2O](https://www.h2o.ai/)由H2O.ai创建，是用于大规模建模的开源软件，允许您在发现数据模式的过程中拟合成千上万个潜在模型。您可以考虑使用H2O来补充或替换Spark的默认建模算法。通常在Spark的算法不足或需要高级功能（如额外的建模指标或自动模型选择）时，会使用H2O。'
- en: We can’t do justice to H2O’s great modeling capabilities in a single paragraph;
    explaining H2O properly would require a book in and of itself. Instead, we would
    like to recommend reading Darren Cook’s [*Practical Machine Learning with H2O*](https://oreil.ly/l5RHI)
    (O’Reilly) to explore in-depth H2O’s modeling algorithms and features. In the
    meantime, you can use this section as a brief guide to get started using H2O in
    Spark with R.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法在一段话中充分展示H2O强大的建模能力；要适当地解释H2O，需要一本专著。我们建议阅读Darren Cook的[*Practical Machine
    Learning with H2O*](https://oreil.ly/l5RHI)（O'Reilly），深入探索H2O的建模算法和功能。同时，您可以使用本节作为开始在Spark中使用H2O与R的简要指南。
- en: 'To use H2O with Spark, it is important to know that there are four components
    involved: H2O, Sparkling Water, [`rsparkling`](http://bit.ly/2MlFxqa), and Spark.
    Sparkling Water allows users to combine the fast, scalable machine learning algorithms
    of H2O with the capabilities of Spark. You can think of Sparkling Water as a component
    bridging Spark with H2O and `rsparkling` as the R frontend for Sparkling Water,
    as depicted in [Figure 10-1](#extensions-h2o-diagram).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Spark中使用H2O，了解涉及的四个组件非常重要：H2O，Sparkling Water，[`rsparkling`](http://bit.ly/2MlFxqa)，和Spark。Sparkling
    Water允许用户将H2O的快速可扩展的机器学习算法与Spark的功能结合起来。您可以将Sparkling Water视为将Spark与H2O桥接的组件，`rsparkling`是Sparkling
    Water的R前端，如[图 10-1](#extensions-h2o-diagram)所示。
- en: '![H2O components with Spark and R](assets/mswr_1001.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![H2O组件与Spark和R](assets/mswr_1001.png)'
- en: Figure 10-1\. H2O components with Spark and R
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. H2O组件与Spark和R
- en: First, install `rsparkling` and `h2o` as specified on the [`rsparkling` documentation
    site](http://bit.ly/2Z78MD0).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，按照[`rsparkling`文档网站](http://bit.ly/2Z78MD0)上的说明安装`rsparkling`和`h2o`。
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It is important to note that you need to use compatible versions of Spark,
    Sparkling Water, and H2O as specified in their documentation; we present instructions
    for Spark 2.3, but using different Spark versions will require you to install
    different versions. So let’s start by checking the version of H2O by running the
    following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，你需要按照它们的文档使用兼容版本的Spark、Sparkling Water和H2O；我们提供了适用于Spark 2.3的说明，但使用不同版本的Spark将需要安装不同版本。因此，让我们从运行以下检查H2O版本开始：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We then can connect with the supported Spark versions as follows (you will
    have to adjust the `master` parameter for your particular cluster):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以按照支持的Spark版本进行连接（你需要根据你的特定集群调整`master`参数）：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: H2O provides a web interface that can help you monitor training and access much
    of H2O’s functionality. You can access the web interface (called H2O Flow) through
    `h2o_flow(sc)`, as shown in [Figure 10-2](#extensions-h2o-flow).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: H2O提供了一个Web界面，可以帮助你监控训练并访问H2O的许多功能。你可以通过`h2o_flow(sc)`访问Web界面（称为H2O Flow），如图10-2所示。
- en: 'When using H2O, you will need to convert your Spark DataFrame into an H2O DataFrame
    through `as_h2o_frame`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用H2O时，你需要通过`as_h2o_frame`将你的Spark DataFrame转换为H2O DataFrame：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![The H2O Flow interface using Spark with R](assets/mswr_1002.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark和R的H2O Flow界面](assets/mswr_1002.png)'
- en: Figure 10-2\. The H2O Flow interface using Spark with R
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2. 使用Spark和R的H2O Flow界面
- en: 'Then, you can use many of the modeling functions available in the `h2o` package
    with ease. For instance, we can fit a generalized linear model with ease:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以轻松使用`h2o`包中提供的许多建模功能。例如，我们可以轻松拟合广义线性模型：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: H2O provides additional metrics not necessarily available in Spark’s modeling
    algorithms. The model that we just fit, `Residual Deviance`, is provided in the
    model, while this would not be a standard metric when using Spark MLlib.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: H2O提供了额外的度量标准，这些度量标准在Spark的建模算法中可能不一定可用。我们刚刚拟合的模型，`残差偏差`，在模型中提供了，而在使用Spark MLlib时，这不是标准度量标准。
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, you can run prediction over the generalized linear model (GLM). A similar
    approach would work for many other models available in H2O:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以运行广义线性模型（GLM）的预测。对于H2O中可用的许多其他模型，类似的方法也适用：
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can also use H2O to perform automatic training and tuning of many models,
    meaning that H2O can choose which model to use for you using [AutoML](https://oreil.ly/Ck9Ao):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用H2O执行许多模型的自动训练和调优，这意味着H2O可以选择为你使用哪个模型，使用[AutoML](https://oreil.ly/Ck9Ao)：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For this particular dataset, H2O determines that a deep learning model is a
    better fit than a GLM.^([1](ch10.html#idm46099142036360)) Specifically, H2O’s
    AutoML explored using XGBoost, deep learning, GLM, and a Stacked Ensemble model:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '对于这个特定的数据集，H2O确定深度学习模型比GLM更适合。^([1](ch10.html#idm46099142036360)) 具体来说，H2O的AutoML探索了使用XGBoost、深度学习、GLM和堆叠集成模型： '
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Rather than using the leaderboard, you can focus on the best model through
    `automl@leader`; for example, you can glance at the particular parameters from
    this deep learning model as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与其使用排行榜，你可以通过`automl@leader`专注于最佳模型；例如，你可以查看以下深度学习模型的特定参数：
- en: '[PRE17]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You can then predict using the leader as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以像下面这样使用领导者进行预测：
- en: '[PRE19]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Many additional examples are [available](http://bit.ly/2NdTIwX), and you can
    also request help from the official [GitHub repository](http://bit.ly/2MlFxqa)
    for the `rsparkling` package.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他示例可以[查看](http://bit.ly/2NdTIwX)，你也可以从官方[GitHub代码库](http://bit.ly/2MlFxqa)获取`rsparkling`包的帮助。
- en: The next extension, `graphframes`, allows you to process large-scale relational
    datasets. Before you start using it, make sure to disconnect with `spark_disconnect(sc)`
    and restart your R session since using a different extension requires you to reconnect
    to Spark and reload `sparklyr`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的扩展，`graphframes`，允许你处理大规模关系数据集。在开始使用之前，请确保使用`spark_disconnect(sc)`断开连接并重新启动你的R会话，因为使用不同的扩展需要重新连接到Spark并重新加载`sparklyr`。
- en: Graphs
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图表
- en: The first paper in the history of graph theory was written by Leonhard Euler
    on the Seven Bridges of Königsberg in 1736\. The problem was to devise a walk
    through the city that would cross each bridge once and only once. [Figure 10-3](#extensions-eulers-paths)
    presents the original diagram.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图论史上的第一篇论文是由莱昂哈德·欧拉于1736年在康斯堡七桥上写的。问题是设计一条穿过城市的路线，每座桥只过一次且仅一次。[图10-3](#extensions-eulers-paths)展示了原始图表。
- en: '![The Seven Bridges of Königsberg from the Euler archive](assets/mswr_1003.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![来自欧拉档案的康斯贝格七桥](assets/mswr_1003.png)'
- en: Figure 10-3\. The Seven Bridges of Königsberg from the Euler archive
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 来自欧拉档案的康斯贝格七桥
- en: Today, a graph is defined as an ordered pair <math><mrow><mi>G</mi> <mo>=</mo>
    <mo>(</mo> <mi>V</mi> <mo>,</mo> <mi>E</mi> <mo>)</mo></mrow></math> , with <math><mi>V</mi></math>
    a set of vertices (nodes or points) and <math><mrow><mi>E</mi> <mo>⊆</mo> <mo>{</mo>
    <mrow><mo>{</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>}</mo></mrow> <mo>|</mo>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>∈</mo>
    <msup><mi mathvariant="normal">V</mi> <mn>2</mn></msup> <mo>∧</mo> <mi>x</mi>
    <mo>≠</mo> <mi>y</mi> <mo>}</mo></mrow></math> a set of edges (links or lines),
    which are either an unordered pair for *undirected graphs* or an ordered pair
    for *directed graphs*. The former describes links where the direction does not
    matter, and the latter describes links where it does.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，图被定义为一个有序对 <math><mrow><mi>G</mi> <mo>=</mo> <mo>(</mo> <mi>V</mi> <mo>,</mo>
    <mi>E</mi> <mo>)</mo></mrow></math>，其中 <math><mi>V</mi></math> 是顶点（节点或点）的集合，<math><mrow><mi>E</mi>
    <mo>⊆</mo> <mo>{</mo> <mrow><mo>{</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>}</mo></mrow>
    <mo>|</mo> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow>
    <mo>∈</mo> <msup><mi mathvariant="normal">V</mi> <mn>2</mn></msup> <mo>∧</mo>
    <mi>x</mi> <mo>≠</mo> <mi>y</mi> <mo>}</mo></mrow></math> 是边（链接或线）的集合，它们可以是无序对（用于
    *无向图*）或有序对（用于 *有向图*）。前者描述的是方向无关的链接，而后者描述的是方向相关的链接。
- en: 'As a simple example, we can use the `highschool` dataset from the `ggraph`
    package, which tracks friendship among high school boys. In this dataset, the
    vertices are the students and the edges describe pairs of students who happen
    to be friends in a particular year:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个简单的例子，我们可以使用 `ggraph` 包中的 `highschool` 数据集，该数据集跟踪高中男孩之间的友谊关系。在这个数据集中，顶点是学生，边描述的是在特定年份中成为朋友的学生对：
- en: '[PRE21]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: While the high school dataset can easily be processed in R, even medium-size
    graph datasets can be difficult to process without distributing this work across
    a cluster of machines, for which Spark is well suited. Spark supports processing
    graphs through the [`graphframes`](http://bit.ly/2Z5hVYB) extension, which in
    turn uses the [GraphX](http://bit.ly/30cbKU6) Spark component. GraphX is Apache
    Spark’s API for graphs and graph-parallel computation. It’s comparable in performance
    to the fastest specialized graph-processing systems and provides a growing library
    of graph algorithms.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然高中数据集可以在 R 中轻松处理，但即使是中等规模的图数据集，如果没有将工作分布到机器群集中进行处理，也可能难以处理，而 Spark 正是为此而设计的。Spark
    支持通过 [`graphframes`](http://bit.ly/2Z5hVYB) 扩展来处理图，该扩展进而使用 [GraphX](http://bit.ly/30cbKU6)
    Spark 组件。GraphX 是 Apache Spark 的图形和图并行计算 API，其性能可与最快的专业图处理系统相媲美，并提供一个日益丰富的图算法库。
- en: 'A graph in Spark is also represented as a DataFrame of edges and vertices;
    however, our format is slightly different since we will need to construct a DataFrame
    for vertices. Let’s first install the [`graphframes`](http://bit.ly/2Z5hVYB) extension:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中的图也被表示为边和顶点的 DataFrame；然而，我们的格式稍有不同，因为我们需要构建一个顶点的 DataFrame。让我们首先安装 [`graphframes`](http://bit.ly/2Z5hVYB)
    扩展：
- en: '[PRE24]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we need to connect, copying the `highschool` dataset and transforming
    the graph to the format that this extension expects. Here, we scope this dataset
    to the friendships of the year 1957:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要连接，复制 `highschool` 数据集并转换为此扩展所期望的格式。在这里，我们将数据集范围限定为 1957 年的友谊关系：
- en: '[PRE25]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `vertices_tbl` table is expected to have a single `id` column:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`vertices_tbl` 表预期只包含一个 `id` 列：'
- en: '[PRE26]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And the `edges_tbl` is expected to have `src` and `dst` columns:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`edges_tbl` 应包含 `src` 和 `dst` 列：'
- en: '[PRE28]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You can now create a GraphFrame:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以创建一个 GraphFrame：
- en: '[PRE30]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We now can use this graph to start analyzing this dataset. For instance, we’ll
    find out how many friends on average every boy has, which is referred to as the
    *degree* or *valency* of a *vertex*:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用这个图来开始分析这个数据集。例如，我们将找出每个男孩平均有多少朋友，这被称为 *顶点* 的 *度* 或 *价值*：
- en: '[PRE31]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We then can find what the shortest path to some specific vertex (a boy for
    this dataset). Since the data is anonymized, we can just pick the boy identified
    as `33` and find how many degrees of separation exist between them:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以找到到某个特定顶点（对于此数据集为一个男孩）的最短路径。由于数据是匿名化的，我们只能选择被标识为 `33` 的男孩，并查找他们之间存在多少个分离度：
- en: '[PRE33]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Finally, we can also compute PageRank over this graph, which was presented
    in [Chapter 1](ch01.html#intro)’s discussion of Google’s page ranking algorithm:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还可以计算这个图上的 PageRank，这在 [第1章](ch01.html#intro) 中讨论了 Google 页面排名算法的部分：
- en: '[PRE35]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To give you some insights into this dataset, [Figure 10-4](#extensions-graph-pagerank)
    plots this chart using the `ggraph` and highlights the highest PageRank scores
    for the following dataset:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您对这个数据集有些见解，[图 10-4](#extensions-graph-pagerank) 使用`ggraph`绘制了此图表，并突出显示了以下数据集的最高
    PageRank 分数：
- en: '[PRE37]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![High school ggraph dataset with highest PageRank highlighted](assets/mswr_1004.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![高中 ggraph 数据集中最高 PageRank 被突出显示](assets/mswr_1004.png)'
- en: Figure 10-4\. High school ggraph dataset with highest PageRank highlighted
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 高中 ggraph 数据集中最高 PageRank 被突出显示
- en: There are many more graph algorithms provided in `graphframes`—for example,
    breadth-first search, connected components, label propagation for detecting communities,
    strongly connected components, and triangle count. For questions on this extension
    refer to the official [GitHub repository](http://bit.ly/2Z5hVYB). We now present
    a popular gradient-boosting framework—make sure to disconnect and restart before
    trying the next extension.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在`graphframes`中提供了许多图算法，例如广度优先搜索、连通组件、用于检测社区的标签传播、强连通分量和三角形计数。有关此扩展的问题，请参阅官方[GitHub
    仓库](http://bit.ly/2Z5hVYB)。现在我们介绍一个流行的梯度增强框架——确保在尝试下一个扩展之前断开并重新启动。
- en: XGBoost
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost
- en: A *decision tree* is a flowchart-like structure in which each internal node
    represents a test on an attribute, each branch represents the outcome of the test,
    and each leaf node represents a class label. For example, [Figure 10-5](#extensions-decision-diagram)
    shows a decision tree that could help classify whether an employee is likely to
    leave given a set of factors like job satisfaction and overtime. When a decision
    tree is used to predict continuous variables instead of discrete outcomes—say,
    how likely someone is to leave a company—it is referred to as a *regression tree*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策树*是一种类似流程图的结构，其中每个内部节点表示对属性的测试，每个分支表示测试的结果，每个叶节点表示一个类标签。例如，[图 10-5](#extensions-decision-diagram)
    显示了一个决策树，该决策树可以帮助分类员工是否有可能离开，给定诸如工作满意度和加班情况等因素。当决策树用于预测连续变量而不是离散结果时——比如，某人离开公司的可能性——它被称为*回归树*。'
- en: '![A decision tree to predict job attrition based on known factors](assets/mswr_1005.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![基于已知因素预测工作流失的决策树](assets/mswr_1005.png)'
- en: Figure 10-5\. A decision tree to predict job attrition based on known factors
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 基于已知因素预测工作流失的决策树
- en: 'While a decision tree representation is quite easy to understand and to interpret,
    finding out the decisions in the tree requires mathematical techniques like *gradient
    descent* to find a local minimum. Gradient descent takes steps proportional to
    the negative of the gradient of the function at the current point. The gradient
    is represented by <math><mi>∇</mi></math> , and the learning rate by <math><mi>γ</mi></math>
    . You simply start from a given state <math><msub><mi>a</mi> <mi>n</mi></msub></math>
    and compute the next iteration <math><msub><mi>a</mi> <mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math>
    by following the direction of the gradient:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树表示法虽然易于理解和解释，但要找出树中的决策则需要像*梯度下降*这样的数学技术来寻找局部最小值。梯度下降是指按当前点的负梯度方向前进的步骤。梯度由<math><mi>∇</mi></math>表示，学习率由<math><mi>γ</mi></math>表示。你从给定状态<math><msub><mi>a</mi>
    <mi>n</mi></msub></math>开始，并通过以下梯度方向计算下一个迭代<math><msub><mi>a</mi> <mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math>：
- en: <math><mrow><msub><mi>a</mi> <mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <msub><mi>a</mi> <mi>n</mi></msub> <mo>-</mo> <mi>γ</mi> <mi>∇</mi>
    <mi>F</mi> <mrow><mo>(</mo> <msub><mi>a</mi> <mi>n</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mi>a</mi> <mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <msub><mi>a</mi> <mi>n</mi></msub> <mo>-</mo> <mi>γ</mi> <mi>∇</mi>
    <mi>F</mi> <mrow><mo>(</mo> <msub><mi>a</mi> <mi>n</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: XGBoost is an open source software library that provides a gradient-boosting
    framework. It aims to provide scalable, portable, and distributed gradient boosting
    for training gradient-boosted decision trees (GBDT) and gradient-boosted regression
    trees (GBRT). Gradient-boosted means XGBoost uses gradient descent and boosting,
    which is a technique that chooses each predictor sequentially.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 是一个开源软件库，提供梯度增强框架。其目标是为训练梯度增强决策树（GBDT）和梯度增强回归树（GBRT）提供可伸缩、可移植和分布式支持。梯度增强意味着
    XGBoost 使用梯度下降和增强技术，这是一种依次选择每个预测器的技术。
- en: '`sparkxgb` is an extension that you can use to train XGBoost models in Spark;
    however, be aware that currently Windows is unsupported. To use this extension,
    first install it from CRAN:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`sparkxgb`是一个扩展，您可以使用它在Spark中训练XGBoost模型；然而，请注意，目前不支持Windows。要使用此扩展，请首先从CRAN安装它：'
- en: '[PRE38]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, you need to import the `sparkxgb` extension followed by your usual Spark
    connection code, adjusting `master` as needed:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您需要导入`sparkxgb`扩展，然后是您通常的Spark连接代码，根据需要调整`master`：
- en: '[PRE39]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'For this example, we use the `attrition` dataset from the `rsample` package,
    which you would need to install by using `install.packages("rsample")`. This is
    a fictional dataset created by IBM data scientists to uncover the factors that
    lead to employee attrition:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此示例，我们使用`rsample`包中的`attrition`数据集，您需要通过`install.packages("rsample")`安装该包。这是IBM数据科学家创建的一个虚构数据集，旨在揭示导致员工离职的因素：
- en: '[PRE40]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'To build an XGBoost model in Spark, use `xgboost_classifier()`. We will compute
    attrition against all other features by using the `Attrition ~ .` formula and
    specify `2` for the number of classes since the attrition attribute tracks only
    whether an employee leaves or stays. Then, you can use `ml_predict()` to predict
    over large-scale datasets:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Spark中构建XGBoost模型，请使用`xgboost_classifier()`。我们将使用`Attrition ~ .`公式计算所有其他特征对离职的影响，并指定`2`作为类别数量，因为离职属性仅跟踪员工是否离职。然后，您可以使用`ml_predict()`来预测大规模数据集：
- en: '[PRE42]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: XGBoost became well known in the competition circles after its use in the winning
    solution of the Higgs Machine Learning Challenge, which uses the ATLAS experiment
    to identify the Higgs boson. Since then, it has become a popular model and used
    for a large number of Kaggle competitions. However, decision trees could prove
    limiting especially in datasets with nontabular data like images, audio, and text,
    which you can better tackle with deep learning models—should we remind you to
    disconnect and restart?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在Higgs机器学习挑战的获胜解决方案中使用后，XGBoost在竞赛界变得广为人知，该挑战使用ATLAS实验来识别希格斯玻色子。从那时起，它已成为一种流行的模型，并用于大量的Kaggle竞赛。然而，决策树可能会在非表格数据（如图像、音频和文本）的数据集中受限，您可以通过深度学习模型更好地处理这些数据—我们需要提醒您断开并重新启动吗？
- en: Deep Learning
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: 'A *perceptron* is a mathematical model introduced by Frank Rosenblatt,^([2](ch10.html#idm46099141036088))
    who developed it as a theory for a hypothetical nervous system. The perceptron
    maps stimuli to numeric inputs that are weighted into a threshold function that
    activates only when enough stimuli is present, mathematically:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*感知器*是由Frank Rosenblatt引入的数学模型，^([2](ch10.html#idm46099141036088))他将其发展为一个假设的神经系统理论。感知器将刺激映射到数字输入，这些输入被加权到一个阈值函数中，只有在足够的刺激存在时才会激活，数学上表达为：
- en: <math><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfenced close="" open="{" separators=""><mtable><mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <msub><mi>w</mi> <mi>i</mi></msub> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>+</mo> <mi>b</mi> <mo>></mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mtext>otherwise</mtext></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfenced close="" open="{" separators=""><mtable><mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <msub><mi>w</mi> <mi>i</mi></msub> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>+</mo> <mi>b</mi> <mo>></mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mtext>否则</mtext></mtd></mtr></mtable></mfenced></mrow></math>
- en: Minsky and Papert found out that a single perceptron can classify only datasets
    that are linearly separable; however, they also revealed in their book *Perceptrons*
    that layering perceptrons would bring additional classification capabilities.^([3](ch10.html#idm46099141014776))
    [Figure 10-6](#extensions-minsky-layered) presents the original diagram showcasing
    a multilayered perceptron.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Minsky和Papert发现单个感知器只能分类线性可分的数据集；然而，在他们的书《感知器》中，他们还揭示了层叠感知器会带来额外的分类能力。^([3](ch10.html#idm46099141014776))
    [图10-6](#extensions-minsky-layered)展示了原始图表，展示了多层感知器。
- en: '![Layered perceptrons, as illustrated in the perceptrons book](assets/mswr_1006.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![分层感知器，如《感知器》书中所示](assets/mswr_1006.png)'
- en: Figure 10-6\. Layered perceptrons, as illustrated in the book Perceptrons
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6\. 分层感知器，如《感知器》书中所示
- en: 'Before we start, let’s first install all the packages that we are about to
    use:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们先安装所有即将使用的包：
- en: '[PRE44]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Using Spark we can create a multilayer perceptron classifier with `ml_multilayer_perceptron_classifier()`
    and gradient descent to classify and predict over large datasets. Gradient descent
    was introduced to layered perceptrons by Geoff Hinton.^([4](ch10.html#idm46099141006552))
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark，我们可以创建一个多层感知器分类器，使用`ml_multilayer_perceptron_classifier()`和梯度下降来对大型数据集进行分类和预测。梯度下降是由Geoff
    Hinton在分层感知器中引入的。^([4](ch10.html#idm46099141006552))
- en: '[PRE45]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Notice that the columns must be numeric, so you will need to manually convert
    them with the feature transforming techniques presented in [Chapter 4](ch04.html#modeling).
    It is natural to try to add more layers to classify more complex datasets; however,
    adding too many layers will cause the gradient to vanish, and other techniques
    will need to use these deep layered networks, also known as *deep learning models*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，列必须是数值的，因此你需要使用[第4章](ch04.html#modeling)中介绍的特征转换技术手动转换它们。尝试添加更多层来分类更复杂的数据集是很自然的；然而，添加太多层会导致梯度消失，需要使用这些深层网络的其他技术，也称为*深度学习模型*。
- en: Deep learning models solve the vanishing gradient problem by making use of special
    activation functions, dropout, data augmentation and GPUs. You can use Spark to
    retrieve and preprocess large datasets into numerical-only datasets that can fit
    in a GPU for deep learning training. TensorFlow is one of the most popular deep
    learning frameworks. As mentioned previously, it supports a binary format known
    as TensorFlow records.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型通过使用特殊的激活函数、dropout、数据增强和GPU来解决梯度消失问题。你可以使用Spark从大型数据集中检索和预处理成只含数值的数据集，这些数据集可以适应GPU进行深度学习训练。TensorFlow是最流行的深度学习框架之一。正如之前提到的，它支持一种称为TensorFlow记录的二进制格式。
- en: You can write TensorFlow records using the `sparktf` in Spark, which you can
    prepare to process in GPU instances with libraries like Keras or TensorFlow.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`sparktf`在Spark中编写TensorFlow记录，然后可以准备在GPU实例中处理，使用像Keras或TensorFlow这样的库。
- en: 'You can then preprocess large datasets in Spark and write them as TensorFlow
    records using `spark_write_tf()`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Spark中预处理大型数据集，然后使用`spark_write_tf()`将它们写为TensorFlow记录：
- en: '[PRE47]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: After you have trained the dataset with Keras or TensorFlow, you can use the
    `tfdatasets` package to load it. You will also need to install the TensorFlow
    runtime by using `install_tensorflow()` and install Python on your own. To learn
    more about training deep learning models with Keras, we recommend reading *Deep
    Learning with R*.^([5](ch10.html#idm46099140741320))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Keras或TensorFlow训练数据集后，你可以使用`tfdatasets`包来加载它。你还需要使用`install_tensorflow()`安装TensorFlow运行时，并自行安装Python。要了解更多关于使用Keras训练深度学习模型的信息，我们建议阅读《深度学习与R》。^([5](ch10.html#idm46099140741320))
- en: '[PRE48]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Training deep learning models in a single local node with one or more GPUs is
    often enough for most applications; however, recent state-of-the-art deep learning
    models train using distributed computing frameworks like Apache Spark. Distributed
    computing frameworks are used to achieve higher petaflops each day the systems
    spends training these models. [OpenAI](http://bit.ly/2HawofQ) analyzed trends
    in the field of *artificial intelligence* (AI) and cluster computing (illustrated
    in [Figure 10-7](#extensions-distributed-training)). It should be obvious from
    the figure that there is a trend in recent years to use distributed computing
    frameworks.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数应用程序来说，在单个本地节点上使用一个或多个GPU训练深度学习模型通常已经足够了；然而，最新的深度学习模型通常使用像Apache Spark这样的分布式计算框架进行训练。分布式计算框架用于实现每天训练这些模型所需的更高的petaflops。[OpenAI](http://bit.ly/2HawofQ)分析了人工智能（AI）和集群计算领域的趋势（见[图10-7](#extensions-distributed-training)）。从图中可以明显看出，近年来使用分布式计算框架的趋势日益明显。
- en: '![Training using distributed systems based on OpenAI analysis](assets/mswr_1007.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![使用OpenAI分析基于分布式系统进行训练](assets/mswr_1007.png)'
- en: Figure 10-7\. Training using distributed systems based on OpenAI analysis
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-7。使用OpenAI分析基于分布式系统进行训练
- en: Training large-scale deep learning models is possible in Spark and TensorFlow
    through frameworks like Horovod. Today, it’s possible to use Horovod with Spark
    from R using the `reticulate` package, since Horovod requires Python and Open
    MPI, this goes beyond the scope of this book. Next, we will introduce a different
    Spark extension in the domain of genomics.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Horovod等框架可以在Spark和TensorFlow中训练大规模深度学习模型。今天，通过`reticulate`包从R使用Horovod与Spark结合使用是可能的，因为Horovod需要Python和Open
    MPI，这超出了本书的范围。接下来，我们将介绍基因组学领域中不同的Spark扩展。
- en: Genomics
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基因组学
- en: 'The [human genome](http://bit.ly/2z2gMqn) consists of two copies of about 3
    billion base pairs of DNA within the 23 chromosome pairs. [Figure 10-8](#extensions-genomics-diagram)
    shows the organization of the genome into chromosomes. DNA strands are composed
    of nucleotides, each composed of one of four nitrogen-containing nucleobases:
    cytosine (C), guanine (G), adenine (A), or thymine (T). Since the DNA of all humans
    is nearly identical, we need to store only the differences from the reference
    genome in the form of a variant call format (VCF) file.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[人类基因组](http://bit.ly/2z2gMqn) 由 23 对染色体内大约 30 亿个碱基对的两份拷贝组成。图 10-8（#extensions-genomics-diagram）显示了染色体的基因组组织。DNA链由核苷酸组成，每个核苷酸包含四种含氮碱基之一：胞嘧啶（C）、鸟嘌呤（G）、腺嘌呤（A）或胸腺嘧啶（T）。由于所有人类的DNA几乎相同，我们只需要以变异调用格式（VCF）文件的形式存储与参考基因组的差异。'
- en: '![The idealized human diploid karyotype showing the organization of the genome
    into chromosomes](assets/mswr_1008.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![理想化的人类二倍体核型显示染色体的基因组组织](assets/mswr_1008.png)'
- en: Figure 10-8\. The idealized human diploid karyotype showing the organization
    of the genome into chromosomes
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. 理想化的人类二倍体核型显示染色体的基因组组织
- en: '`variantspark` is a framework based on Scala and Spark to analyze genome datasets.
    It is being developed by CSIRO Bioinformatics team in Australia. `variantspark`
    was tested on datasets with 3,000 samples, each one containing 80 million features
    in either unsupervised clustering approaches or supervised applications, like
    classification and regression. `variantspark` can read VCF files and run analyses
    while using familiar Spark DataFrames.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`variantspark` 是基于 Scala 和 Spark 的框架，用于分析基因组数据集。它由澳大利亚 CSIRO 生物信息团队开发。`variantspark`
    在包含 80 百万特征的 3,000 个样本数据集上进行了测试，可以进行无监督聚类或监督分类和回归等应用。`variantspark` 可以读取 VCF 文件并在使用熟悉的
    Spark DataFrames 时运行分析。'
- en: 'To get started, install `variantspark` from CRAN, connect to Spark, and retrieve
    a `vsc` connection to `variantspark`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，请从 CRAN 安装`variantspark`，连接到 Spark，并获取到`variantspark`的`vsc`连接：
- en: '[PRE50]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We can start by loading a VCF file:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过加载 VCF 文件开始：
- en: '[PRE51]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '`variantspark` uses random forest to assign an importance score to each tested
    variant reflecting its association to the interest phenotype. A variant with higher
    importance score implies it is more strongly associated with the phenotype of
    interest. You can compute the importance and transform it into a Spark table,
    as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`variantspark` 使用随机森林为每个测试变体分配重要性评分，反映其与感兴趣表型的关联。具有较高重要性评分的变体意味着其与感兴趣表型的关联更强。您可以计算重要性并将其转换为
    Spark 表，如下所示：'
- en: '[PRE52]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'You then can use `dplyr` and `ggplot2` to transform the output and visualize
    it (see [Figure 10-9](#extensions-genomics-importance)):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用`dplyr`和`ggplot2`转换输出并进行可视化（参见 [Figure 10-9](#extensions-genomics-importance)）：
- en: '[PRE54]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![Genomic importance analysis using variantspark](assets/mswr_1009.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![使用 variantspark 进行基因组重要性分析](assets/mswr_1009.png)'
- en: Figure 10-9\. Genomic importance analysis using variantspark
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-9\. 使用 variantspark 进行基因组重要性分析
- en: This concludes a brief introduction to genomic analysis in Spark using the `variantspark`
    extension. Next, we move away from microscopic genes to macroscopic datasets that
    contain geographic locations across the world.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了对使用`variantspark`扩展进行基因组分析的简要介绍。接下来，我们将从微观基因转向包含世界各地地理位置的宏观数据集。
- en: Spatial
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 空间
- en: '[`geospark`](http://bit.ly/2zbTEW8) enables distributed geospatial computing
    using a grammar compatible with [`dplyr`](http://bit.ly/2KYKOAC) and [`sf`](http://bit.ly/2ZerAwb),
    which provides a set of tools for working with geospatial vectors.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[`geospark`](http://bit.ly/2zbTEW8) 支持使用与[`dplyr`](http://bit.ly/2KYKOAC) 和
    [`sf`](http://bit.ly/2ZerAwb) 兼容的语法进行分布式地理空间计算，提供了一套用于处理地理空间向量的工具。'
- en: 'You can install `geospark` from CRAN, as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照以下步骤从 CRAN 安装`geospark`：
- en: '[PRE55]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Then, initialize the `geospark` extension and connect to Spark:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，初始化`geospark`扩展并连接到 Spark：
- en: '[PRE56]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next, we load a spatial dataset containing polygons and points:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载包含多边形和点的空间数据集：
- en: '[PRE57]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: There are various spatial operations defined in `geospark`, as depicted in [Figure 10-10](#extensions-geospark-operations).
    These operations allow you to control how geospatial data should be queried based
    on overlap, intersection, disjoint sets, and so on.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在`geospark`中定义了各种空间操作，如图 10-10（#extensions-geospark-operations）所示。这些操作允许您根据重叠、交集、不相交集等来查询地理空间数据应如何处理。
- en: '![Spatial operations available in geospark](assets/mswr_1010.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![geospark 中的空间操作](assets/mswr_1010.png)'
- en: Figure 10-10\. Spatial operations available in geospark
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-10\. Geospark 中可用的空间操作
- en: 'For instance, we can use these operations to find the polygons that contain
    a given set of points using `st_contains()`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用这些操作来查找包含给定点集的多边形，使用 `st_contains()`：
- en: '[PRE58]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: You can also plot these datasets by collecting a subset of the entire dataset
    or aggregating the geometries in Spark before collecting them. One package you
    should look into is `sf`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过在收集它们之前在 Spark 中收集整个数据集的子集或聚合几何图形来绘制这些数据集。您应该查看的一个包是 `sf`。
- en: We close this chapter by presenting a couple of troubleshooting techniques applicable
    to all extensions.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过介绍适用于所有扩展的几种故障排除技术来结束本章。
- en: Troubleshooting
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除
- en: 'When you are using a new extension for the first time, we recommend increasing
    the connection timeout (given that Spark will usually need to download extension
    dependencies) and changing logging to verbose to help you troubleshoot when the
    download process does not complete:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 当您第一次使用新扩展时，我们建议增加连接超时时间（因为Spark通常需要下载扩展依赖项）并将日志级别更改为详细，以帮助您在下载过程未完成时进行故障排除：
- en: '[PRE60]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'In addition, you should know that [Apache IVY](http://ant.apache.org/ivy) is
    a popular dependency manager focusing on flexibility and simplicity, and is used
    by Apache Spark for installing extensions. When the connection fails while you
    are using an extension, consider clearing your [IVY cache](http://bit.ly/2Zcubun)
    by running the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您应该知道 [Apache IVY](http://ant.apache.org/ivy) 是一个专注于灵活性和简易性的流行依赖管理器，被 Apache
    Spark 用于安装扩展。当您使用扩展时连接失败时，考虑通过运行以下命令清除您的 [IVY cache](http://bit.ly/2Zcubun)：
- en: '[PRE61]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'In addition, you can also consider opening GitHub issues from the following
    extensions repositories to get help from the extension authors:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以考虑从以下扩展存储库中开启 GitHub 问题以获得扩展作者的帮助：
- en: '[`rsparkling`](http://bit.ly/2KUAx8M)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`rsparkling`](http://bit.ly/2KUAx8M)'
- en: '[`sparkxgb`](http://bit.ly/30hG9Ar)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`sparkxgb`](http://bit.ly/30hG9Ar)'
- en: '[`sparktf`](http://bit.ly/2z7qNCv)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`sparktf`](http://bit.ly/2z7qNCv)'
- en: '[`variantspark`](http://bit.ly/2NfjdxX)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`variantspark`](http://bit.ly/2NfjdxX)'
- en: '[`geospark`](http://bit.ly/2HcgD82)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`geospark`](http://bit.ly/2HcgD82)'
- en: Recap
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter provided a brief overview on using some of the Spark extensions
    available in R, which happens to be as easy as installing a package. You then
    learned how to use the `rsparkling` extension, which provides access to H2O in
    Spark, which in turn provides additional modeling functionality like enhanced
    metrics and the ability to automatically select models. We then jumped to `graphframes`,
    an extension to help you process relational datasets that are formally referred
    to as graphs. You also learned how to compute simple connection metrics or run
    complex algorithms like PageRank.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要介绍了在 R 中使用一些 Spark 扩展的概述，这与安装软件包一样简单。然后您学习了如何使用 `rsparkling` 扩展，该扩展提供了在
    Spark 中访问 H2O 的能力，进而提供了增强的模型功能，如增强指标和自动选择模型。接着我们转向 `graphframes`，这是一个帮助您处理形式上称为图的关系数据集的扩展。您还学习了如何计算简单的连接度指标或运行像
    PageRank 这样的复杂算法。
- en: 'The XGBoost and deep learning sections provided alternate modeling techniques
    that use gradient descent: the former over decision trees, and the latter over
    deep multilayered perceptrons where we can use Spark to preprocess datasets into
    records that later can be consumed by TensorFlow and Keras using the `sparktf`
    extension. The last two sections introduced extensions to process genomic and
    spatial datasets through the `variantspark` and `geospark` extensions.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 和深度学习部分提供了使用梯度下降的备选建模技术：前者是决策树，后者是深度多层感知器，其中我们可以使用 Spark 将数据集预处理为稍后可以通过
    `sparktf` 扩展由 TensorFlow 和 Keras 消耗的记录。最后两节介绍了通过 `variantspark` 和 `geospark` 扩展处理基因组和空间数据集。
- en: These extensions, and many more, provide a comprehensive library of advanced
    functionality that, in combination with the analysis and modeling techniques presented,
    should cover most tasks required to run in computing clusters. However, when functionality
    is lacking, you can consider writing your own extension, which is what we discuss
    in [Chapter 13](ch13.html#contributing), or you can apply custom transformations
    over each partition using R code, as we describe in [Chapter 11](ch11.html#distributed).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这些扩展及更多内容提供了一个高级功能的全面库，结合所介绍的分析和建模技术，应该可以涵盖大多数在计算集群中运行所需的任务。然而，当功能不足时，您可以考虑编写自己的扩展，这是我们在[第13章](ch13.html#contributing)中讨论的内容，或者您可以使用R代码在每个分区上应用自定义转换，正如我们在[第11章](ch11.html#distributed)中描述的那样。
- en: ^([1](ch10.html#idm46099142036360-marker)) Notice that AutoML uses cross-validation,
    which we did not use in GLM.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.html#idm46099142036360-marker)) 注意，AutoML 使用了交叉验证，而我们在 GLM 中没有使用。
- en: '^([2](ch10.html#idm46099141036088-marker)) Rosenblatt F (1958). “The perceptron:
    a probabilistic model for information storage and organization in the brain.”
    *Psychological review*.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.html#idm46099141036088-marker)) Rosenblatt F (1958). “感知器：大脑中信息存储和组织的概率模型。”
    *心理评论*。
- en: '^([3](ch10.html#idm46099141014776-marker)) Minsky M, Papert SA (2017). *Perceptrons:
    An introduction to computational geometry*. MIT press.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch10.html#idm46099141014776-marker)) Minsky M, Papert SA (2017). *感知机：计算几何导论*.
    MIT 出版社。
- en: ^([4](ch10.html#idm46099141006552-marker)) Ackley DH, Hinton GE, Sejnowski TJ
    (1985). “A learning algorithm for Boltzmann machines.” *Cognitive science*.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch10.html#idm46099141006552-marker)) Ackley DH, Hinton GE, Sejnowski TJ
    (1985). “Boltzmann 机的学习算法。” *认知科学*。
- en: ^([5](ch10.html#idm46099140741320-marker)) Chollet F, Allaire J (2018). *Deep
    Learning with R*. Manning Publications.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch10.html#idm46099140741320-marker)) Chollet F, Allaire J (2018). *Deep
    Learning with R*. Manning 出版社。
