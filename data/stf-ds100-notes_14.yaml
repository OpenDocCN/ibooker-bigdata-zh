- en: 13  Gradient Descent
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13  梯度下降
- en: 原文：[https://ds100.org/course-notes/gradient_descent/gradient_descent.html](https://ds100.org/course-notes/gradient_descent/gradient_descent.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://ds100.org/course-notes/gradient_descent/gradient_descent.html](https://ds100.org/course-notes/gradient_descent/gradient_descent.html)
- en: '*Learning Outcomes* ***   Optimizing complex models'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习成果* ***   优化复杂模型'
- en: Identifying cases where straight calculus or geometric arguments won’t help
    solve the loss function
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别直接微积分或几何论证无法帮助解决损失函数的情况
- en: Applying gradient descent for numerical optimization**  **At this point, we’ve
    grown quite familiar with the process of choosing a model and a corresponding
    loss function and optimizing parameters by choosing the values of \(\theta\) that
    minimize the loss function. So far, we’ve optimized \(\theta\) by 1\. Using calculus
    to take the derivative of the loss function with respect to \(\theta\), setting
    it equal to 0, and solving for \(\theta\). 2\. Using the geometric argument of
    orthogonality to derive the OLS solution \(\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1}\mathbb{X}^T
    \mathbb{Y}\).
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用梯度下降进行数值优化**  **到目前为止，我们已经非常熟悉选择模型和相应损失函数的过程，并通过选择最小化损失函数的\(\theta\)的值来优化参数。到目前为止，我们已经通过以下两种方法优化了\(\theta\)：1.
    使用微积分对损失函数关于\(\theta\)的导数进行求导，将其置为0，并解出\(\theta\)。2. 使用正交性的几何论证来推导OLS解\(\hat{\theta}
    = (\mathbb{X}^T \mathbb{X})^{-1}\mathbb{X}^T \mathbb{Y}\)。
- en: 'One thing to note, however, is that the techniques we used above can only be
    applied if we make some big assumptions. For the calculus approach, we assumed
    that the loss function was differentiable at all points and that the algebra was
    manageable; for the geometric approach, OLS *only* applies when using a linear
    model with MSE loss. What happens when we have more complex models with different,
    more complex loss functions? The techniques we’ve learned so far will not work,
    so we need a new optimization technique: **gradient descent**.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的一点是，我们上面使用的技术只有在我们做出一些重大假设时才能应用。对于微积分方法，我们假设损失函数在所有点上都是可微的，并且代数是可管理的；对于几何方法，OLS*仅*适用于使用MSE损失的线性模型。当我们有更复杂的模型和不同的更复杂的损失函数时会发生什么？到目前为止我们学到的技术将不起作用，所以我们需要一种新的优化技术：**梯度下降**。
- en: '**BIG IDEA**: use an algorithm instead of solving for an exact answer'
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要思想**：使用算法而不是求解精确答案'
- en: 13.1 Minimizing a 1D Function
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 最小化1D函数
- en: Let’s consider an arbitrary function. Our goal is to find the value of \(x\)
    that minimizes this function.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个任意的函数。我们的目标是找到最小化这个函数的\(x\)的值。
- en: <details><summary>Code</summary>
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE0]</details>'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE0]</details>'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![arbitrary](../Images/7888745e1eb2ea4aad4da8144efdd738.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![arbitrary](../Images/7888745e1eb2ea4aad4da8144efdd738.png)'
- en: '13.1.1 The Naive Approach: Guess and Check'
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.1 天真的方法：猜测和检查
- en: Above, we saw that the minimum is somewhere around 5.3\. Let’s see if we can
    figure out how to find the exact minimum algorithmically from scratch. One very
    slow (and terrible) way would be manual guess-and-check.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以上，我们看到最小值大约在5.3左右。让我们看看我们是否能够弄清楚如何从头开始算法地找到确切的最小值。一种非常慢（而且糟糕）的方法是手动猜测和检查。
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A somewhat better (but still slow) approach is to use brute force to try out
    a bunch of x values and return the one that yields the lowest loss.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个稍微好一点（但仍然很慢）的方法是使用蛮力来尝试一堆x值，并返回产生最低损失的值。
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This process is essentially the same as before where we made a graphical plot,
    it’s just that we’re only looking at 20 selected points.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程本质上与我们以前制作图形图表的过程相同，只是我们只看了20个选定的点。
- en: <details><summary>Code</summary>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE6]</details>'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE6]</details>'
- en: 'This basic approach suffers from three major flaws: 1\. If the minimum is outside
    our range of guesses, the answer will be completely wrong. 2\. Even if our range
    of guesses is correct, if the guesses are too coarse, our answer will be inaccurate.
    3\. It is absurdly computationally inefficient, considering potentially vast numbers
    of guesses that are useless.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基本方法存在三个主要缺陷：1. 如果最小值在我们猜测的范围之外，答案将完全错误。2. 即使我们的猜测范围是正确的，如果猜测太粗糙，我们的答案将不准确。3.
    考虑到可能庞大的无用猜测数量，这是绝对计算效率低下的。
- en: 13.1.2 Scipy.optimize.minimize
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.2 Scipy.optimize.minimize
- en: One way to minimize this mathematical function is to use the `scipy.optimize.minimize`
    function. It takes a function and a starting guess and tries to find the minimum.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化这个数学函数的一种方法是使用`scipy.optimize.minimize`函数。它接受一个函数和一个起始猜测，并尝试找到最小值。
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`scipy.optimize.minimize` is great. It may also seem a bit magical. How could
    you write a function that can find the minimum of any mathematical function? There
    are a number of ways to do this, which we’ll explore in today’s lecture, eventually
    arriving at the important idea of **gradient descent**, which is the principle
    that `scipy.optimize.minimize` uses.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`scipy.optimize.minimize`很棒。它也可能看起来有点神奇。你怎么能写一个函数来找到任何数学函数的最小值呢？有许多方法可以做到这一点，我们将在今天的讲座中探讨这些方法，最终到达`scipy.optimize.minimize`使用的**梯度下降**的重要思想。'
- en: It turns out that under the hood, the `fit` method for `LinearRegression` models
    uses gradient descent. Gradient descent is also how much of machine learning works,
    including even advanced neural network models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，在幕后，`LinearRegression`模型的`fit`方法使用了梯度下降。梯度下降也是许多机器学习模型的工作原理，甚至包括先进的神经网络模型。
- en: In Data 100, the gradient descent process will usually be invisible to us, hidden
    beneath an abstraction layer. However, to be good data scientists, it’s important
    that we know the underlying principles that optimization functions harness to
    find optimal parameters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在Data 100中，梯度下降过程通常对我们来说是不可见的，隐藏在抽象层下。然而，作为优秀的数据科学家，我们知道优化函数利用的基本原理是很重要的。
- en: 13.2 Digging into Gradient Descent
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 深入研究梯度下降
- en: Looking at the function across this domain, it is clear that the function’s
    minimum value occurs around \(\theta = 5.3\). Let’s pretend for a moment that
    we *couldn’t* see the full view of the cost function. How would we guess the value
    of \(\theta\) that minimizes the function?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个域中查看函数，很明显函数的最小值出现在\(\theta = 5.3\)附近。假设一下，我们*看不到*成本函数的完整视图。我们如何猜测最小化函数的\(\theta\)值？
- en: It turns out that the first derivative of the function can give us a clue. In
    the plots below, the line indicates the value of the derivative of each value
    of \(\theta\). The derivative is negative where it is red and positive where it
    is green.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 原来，函数的一阶导数可以给我们一些线索。在下面的图中，线表示每个\(\theta\)值的导数值。导数为负值时为红色，为正值时为绿色。
- en: Say we make a guess for the minimizing value of \(\theta\). Remember that we
    read plots from left to right, and assume that our starting \(\theta\) value is
    to the left of the optimal \(\hat{\theta}\). If the guess “undershoots” the true
    minimizing value – our guess for \(\theta\) is lower than the value of the \(\hat{\theta}\)
    that minimizes the function – the derivative will be **negative**. This means
    that if we increase \(\theta\) (move further to the right), then we **can decrease**
    our loss function further. If this guess “overshoots” the true minimizing value,
    the derivative will be positive, implying the converse.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们对最小化\(\theta\)的值进行了猜测。记住我们从左到右读取图，并假设我们的起始\(\theta\)值在最佳\(\hat{\theta}\)的左边。如果猜测“低估”了真正的最小值
    - 我们对最小化函数的\(\hat{\theta}\)的猜测低于真正的\(\hat{\theta}\)值 - 导数将是**负数**。这意味着如果我们增加\(\theta\)（向右移动），那么我们**可以进一步减少**我们的损失函数。如果这个猜测“高估”了真正的最小值，导数将是正数，暗示着相反。
- en: '| ![step](../Images/c80c6dd4aeca8215cab3766a0d9c940d.png) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| ![step](../Images/c80c6dd4aeca8215cab3766a0d9c940d.png) |'
- en: We can use this pattern to help formulate our next guess for the optimal \(\hat{\theta}\).
    Consider the case where we’ve undershot \(\theta\) by guessing too low of a value.
    We’ll want our next guess to be greater in value than our previous guess – that
    is, we want to shift our guess to the right. You can think of this as following
    the slope “downhill” to the function’s minimum value.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这种模式来帮助制定我们对最佳\(\hat{\theta}\)的下一个猜测。考虑一种情况，我们通过猜测一个太低的值而低估了\(\theta\)。我们希望我们的下一个猜测的值比上一个猜测的值更大
    - 也就是说，我们希望将我们的猜测向右移动。你可以把这看作是沿着斜坡“下坡”到函数的最小值。
- en: '| ![neg_step](../Images/c823b75285ee3c945366a62e57683d28.png) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| ![neg_step](../Images/c823b75285ee3c945366a62e57683d28.png) |'
- en: If we’ve overshot \(\hat{\theta}\) by guessing too high of a value, we’ll want
    our next guess to be lower in value – we want to shift our guess for \(\hat{\theta}\)
    to the left.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过猜测一个太高的值而高估了\(\hat{\theta}\)，我们希望我们的下一个猜测的值更低 - 我们希望将\(\hat{\theta}\)的猜测向左移动。
- en: '| ![pos_step](../Images/4a50850c8412063f06a4c913bcc9c8a7.png) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| ![pos_step](../Images/4a50850c8412063f06a4c913bcc9c8a7.png) |'
- en: In other words, the derivative of the function at each point tells us the direction
    of our next guess. * A negative slope means we want to step to the right, or move
    in the *positive* direction. * A positive slope means we want to step to the left,
    or move in the *negative* direction.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，每个点的函数的导数告诉我们我们下一个猜测的方向。*负斜率意味着我们想向右走，或者向*正*方向移动。*正斜率意味着我们想向左走，或者向*负*方向移动。
- en: 13.2.1 Algorithm Attempt 1
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 算法尝试1
- en: Armed with this knowledge, let’s try to see if we can use the derivative to
    optimize the function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个知识，让我们试着看看我们是否可以使用导数来优化函数。
- en: 'We start by making some guess for the minimizing value of \(x\). Then, we look
    at the derivative of the function at this value of \(x\), and step downhill in
    the *opposite* direction. We can express our new rule as a recurrence relation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对\(x\)的最小值进行一些猜测。然后，我们查看该\(x\)值的函数的导数，并向*相反*方向下坡。我们可以将我们的新规则表示为一个递归关系：
- en: \[x^{(t+1)} = x^{(t)} - \frac{d}{dx} f(x^{(t)})\]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \[x^{(t+1)} = x^{(t)} - \frac{d}{dx} f(x^{(t)})\]
- en: 'Translating this statement into English: we obtain **our next guess** for the
    minimizing value of \(x\) at timestep \(t+1\) (\(x^{(t+1)}\)) by taking **our
    last guess** (\(x^{(t)}\)) and subtracting the **derivative of the function**
    at that point (\(\frac{d}{dx} f(x^{(t)})\)).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个陈述翻译成英文：我们通过取上一次的猜测(\(x^{(t)}\))并减去该点的函数的导数(\(\frac{d}{dx} f(x^{(t)})\))来获得**我们下一次的猜测**，即在时间步长\(t+1\)(\(x^{(t+1)}\))的最小值\(x\)。
- en: A few steps are shown below, where the old step is shown as a transparent point,
    and the next step taken is the green-filled dot.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示了一些步骤，其中旧步骤显示为透明点，下一个步骤是填充为绿色的点。
- en: '| ![grad_descent_2](../Images/3b9f6c5dc86cd0ba64b667c803423b80.png) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ![grad_descent_2](../Images/3b9f6c5dc86cd0ba64b667c803423b80.png) |'
- en: Looking pretty good! We do have a problem though – once we arrive close to the
    minimum value of the function, our guesses “bounce” back and forth past the minimum
    without ever reaching it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错！但是我们确实有一个问题 - 一旦我们接近函数的最小值，我们的猜测就会在最小值附近“反弹”而永远无法到达它。
- en: '| ![grad_descent_2](../Images/b4c707d1ec7f7e3aed5b30938737911a.png) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| ![grad_descent_2](../Images/b4c707d1ec7f7e3aed5b30938737911a.png) |'
- en: In other words, each step we take when updating our guess moves us too far.
    We can address this by decreasing the size of each step.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，每次更新我们的猜测时，我们走得太远了。我们可以通过减小每个步骤的大小来解决这个问题。
- en: 13.2.2 Algorithm Attempt 2
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.2 算法尝试2
- en: Let’s update our algorithm to use a **learning rate** (also sometimes called
    the step size), which controls how far we move with each update. We represent
    the learning rate with \(\alpha\).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新我们的算法以使用**学习率**（有时也称为步长），它控制我们每次更新的距离。我们用\(\alpha\)表示学习率。
- en: \[x^{(t+1)} = x^{(t)} - \alpha \frac{d}{dx} f(x^{(t)})\]
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: \[x^{(t+1)} = x^{(t)} - \alpha \frac{d}{dx} f(x^{(t)})\]
- en: A small \(\alpha\) means that we will take small steps; a large \(\alpha\) means
    we will take large steps.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 小的\(\alpha\)意味着我们会采取小步；大的\(\alpha\)意味着我们会采取大步。
- en: Updating our function to use \(\alpha=0.3\), our algorithm successfully **converges**
    (settles on a solution and stops updating significantly, or at all) on the minimum
    value.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 更新我们的函数使用\(\alpha=0.3\)，我们的算法成功地**收敛**（在最小值上解决并停止显着更新，或者完全停止）。
- en: '| ![grad_descent_3](../Images/109d8667f3a5651a8bfe85ed7ac70cea.png) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| ![grad_descent_3](../Images/109d8667f3a5651a8bfe85ed7ac70cea.png) |'
- en: 13.2.3 Convexity
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.3 凸性
- en: 'In our analysis above, we focused our attention on the global minimum of the
    loss function. You may be wondering: what about the local minimum that’s just
    to the left?'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上面的分析中，我们把注意力集中在损失函数的全局最小值上。你可能会想：左边的局部最小值呢？
- en: If we had chosen a different starting guess for \(\theta\), or a different value
    for the learning rate \(\alpha\), our algorithm may have gotten “stuck” and converged
    on the local minimum, rather than on the true optimum value of loss.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择了不同的\(\theta\)起始猜测，或者不同的学习率\(\alpha\)值，我们的算法可能会“卡住”，收敛到局部最小值，而不是真正的最优损失值。
- en: '| ![local](../Images/4cb438229709ea2763d5d60b615e4243.png) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ![local](../Images/4cb438229709ea2763d5d60b615e4243.png) |'
- en: 'If the loss function is **convex**, gradient descent is guaranteed to converge
    and find the global minimum of the objective function. Formally, a function \(f\)
    is convex if: \[tf(a) + (1-t)f(b) \geq f(ta + (1-t)b)\] for all \(a, b\) in the
    domain of \(f\) and \(t \in [0, 1]\).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果损失函数是**凸的**，梯度下降保证会收敛并找到目标函数的全局最小值。形式上，如果一个函数\(f\)满足：\[tf(a) + (1-t)f(b) \geq
    f(ta + (1-t)b)\] 对于所有\(f\)的定义域中的\(a, b\)和\(t \in [0, 1]\)。
- en: 'To put this into words: if you drew a line between any two points on the curve,
    all values on the curve must be *on or below* the line. Importantly, any local
    minimum of a convex function is also its global minimum.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说：如果你在曲线上的任意两点之间画一条线，曲线上的所有值必须*在或下*于该线。重要的是，凸函数的任何局部最小值也是它的全局最小值。
- en: '| ![convex](../Images/fcc3d9877d05b713270dc22a8f0f91f2.png) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| ![convex](../Images/fcc3d9877d05b713270dc22a8f0f91f2.png) |'
- en: In summary, non-convex loss functions can cause problems with optimization.
    This means that our choice of loss function is a key factor in our modeling process.
    It turns out that MSE *is* convex, which is a major reason why it is such a popular
    choice of loss function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，非凸损失函数可能会导致优化问题。这意味着我们选择的损失函数是建模过程中的关键因素。事实证明，MSE *是*凸的，这也是它成为如此受欢迎的损失函数的主要原因。
- en: 13.3 Gradient Descent in 1 Dimension
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3 一维梯度下降
- en: '**Terminology clarification**: In past lectures, we have used “loss” to refer
    to the error incurred on a *single* datapoint. In applications, we usually care
    more about the average error across *all* datapoints. Going forward, we will take
    the “model’s loss” to mean the model’s average error across the dataset. This
    is sometimes also known as the empirical risk, cost function, or objective function.
    \[L(\theta) = R(\theta) = \frac{1}{n} \sum_{i=1}^{n} L(y, \hat{y})\]'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**术语澄清**：在过去的讲座中，我们使用“损失”来指代*单个*数据点上发生的错误。在应用中，我们通常更关心*所有*数据点的平均误差。未来，我们将把“模型的损失”理解为数据集上的模型平均误差。这有时也被称为经验风险、成本函数或目标函数。\[L(\theta)
    = R(\theta) = \frac{1}{n} \sum_{i=1}^{n} L(y, \hat{y})\]'
- en: In our discussion above, we worked with some arbitrary function \(f\). As data
    scientists, we will almost always work with gradient descent in the context of
    optimizing *models* – specifically, we want to apply gradient descent to find
    the minimum of a *loss function*. In a modeling context, our goal is to minimize
    a *loss function* by choosing the minimizing model *parameters*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的讨论中，我们使用了一些任意函数\(f\)。作为数据科学家，我们几乎总是在优化*模型*的情况下使用梯度下降 - 具体来说，我们想要应用梯度下降来找到*损失函数*的最小值。在建模的情况下，我们的目标是通过选择最小化的模型*参数*来最小化*损失函数*。
- en: 'Recall our modeling workflow from the past few lectures: * Define a model with
    some parameters \(\theta_i\) * Choose a loss function * Select the values of \(\theta_i\)
    that minimize the loss function on the data'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下我们过去几节讲座的建模工作流程：* 定义具有一些参数\(\theta_i\)的模型 * 选择一个损失函数 * 选择使数据上的损失函数最小化的\(\theta_i\)的值
- en: Gradient descent is a powerful technique for completing this last task. By applying
    the gradient descent algorithm, we can select values for our parameters \(\theta_i\)
    that will lead to the model having minimal loss on the training data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是完成最后任务的强大技术。通过应用梯度下降算法，我们可以选择参数\(\theta_i\)的值，这将导致模型在训练数据上损失最小。
- en: 'When using gradient descent in a modeling context: * We make guesses for the
    minimizing \(\theta_i\) * We compute the derivative of the loss function \(L\)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模上下文中使用梯度下降时：* 我们对最小化的\(\theta_i\)进行猜测 * 我们计算损失函数\(L\)的导数
- en: 'We can “translate” our gradient descent rule from before by replacing \(x\)
    with \(\theta\) and \(f\) with \(L\):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过用\(\theta\)替换\(x\)和用\(L\)替换\(f\)来“翻译”我们之前的梯度下降规则：
- en: \[\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta} L(\theta^{(t)})\]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: \[\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta} L(\theta^{(t)})\]
- en: 13.3.1 Gradient Descent on the `tips` Dataset
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.1 在`tips`数据集上的梯度下降
- en: To see this in action, let’s consider a case where we have a linear model with
    no offset. We want to predict the tip (y) given the price of a meal (x). To do
    this, we
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到这一点，让我们考虑一个没有偏移的线性模型的情况。我们想要预测小费（y）给定一顿饭的价格（x）。为了做到这一点，我们
- en: 'Choose a model: \(\hat{y} = \theta_1 x\),'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个模型：\(\hat{y} = \theta_1 x\)，
- en: 'Choose a loss function: \(L(\theta) = MSE(\theta) = \frac{1}{n} \sum_{i=1}^n
    (y_i - \theta_1x_i)^2\).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个损失函数：\(L(\theta) = MSE(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \theta_1x_i)^2\)。
- en: Let’s apply our `gradient_descent` function from before to optimize our model
    on the `tips` dataset. We will try to select the best parameter \(\theta_i\) to
    predict the `tip` \(y\) from the `total_bill` \(x\).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用之前的`gradient_descent`函数来优化我们在`tips`数据集上的模型。我们将尝试选择最佳参数\(\theta_i\)来预测`total_bill`\(x\)的`tip`\(y\)。
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|  | total_bill | tip | sex | smoker | day | time | size |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | total_bill | tip | sex | smoker | day | time | size |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 16.99 | 1.01 | Female | No | Sun | Dinner | 2 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 16.99 | 1.01 | 女性 | 否 | 星期日 | 晚餐 | 2 |'
- en: '| 1 | 10.34 | 1.66 | Male | No | Sun | Dinner | 3 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 10.34 | 1.66 | 男性 | 否 | 太阳 | 晚餐 | 3 |'
- en: '| 2 | 21.01 | 3.50 | Male | No | Sun | Dinner | 3 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 21.01 | 3.50 | 男性 | 否 | 太阳 | 晚餐 | 3 |'
- en: '| 3 | 23.68 | 3.31 | Male | No | Sun | Dinner | 2 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 23.68 | 3.31 | 男性 | 否 | 太阳 | 晚餐 | 2 |'
- en: '| 4 | 24.59 | 3.61 | Female | No | Sun | Dinner | 4 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 24.59 | 3.61 | 女性 | 否 | 太阳 | 晚餐 | 4 |'
- en: We can visualize the value of the MSE on our dataset for different possible
    choices of \(\theta_1\). To optimize our model, we want to select the value of
    \(\theta_1\) that leads to the lowest MSE.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以可视化我们的数据集上MSE的值，对于不同可能的\(\theta_1\)的选择。为了优化我们的模型，我们希望选择导致最低MSE的\(\theta_1\)的值。
- en: <details><summary>Code</summary>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE10]</details>'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE10]</details>'
- en: To apply gradient descent, we need to compute the derivative of the loss function
    with respect to our parameter \(\theta_1\).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用梯度下降，我们需要计算损失函数对我们的参数\(\theta_1\)的导数。
- en: Given our loss function, \[L(\theta) = MSE(\theta) = \frac{1}{n} \sum_{i=1}^n
    (y_i - \theta_1x_i)^2\]
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定我们的损失函数，\[L(\theta) = MSE(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \theta_1x_i)^2\]
- en: We take the derivative with respect to \(\theta_1\) \[\frac{\partial}{\partial
    \theta_{1}} L(\theta_1^{(t)}) = \frac{-2}{n} \sum_{i=1}^n (y_i - \theta_1^{(t)}
    x_i) x_i\]
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对\(\theta_1\)进行导数\[\frac{\partial}{\partial \theta_{1}} L(\theta_1^{(t)})
    = \frac{-2}{n} \sum_{i=1}^n (y_i - \theta_1^{(t)} x_i) x_i\]
- en: Which results in the gradient descent update rule \[\theta_1^{(t+1)} = \theta_1^{(t)}
    - \alpha \frac{d}{d\theta}L(\theta_1^{(t)})\]
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这导致梯度下降更新规则\[\theta_1^{(t+1)} = \theta_1^{(t)} - \alpha \frac{d}{d\theta}L(\theta_1^{(t)})\]
- en: for some learning rate \(\alpha\).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些学习率\(\alpha\)。
- en: Implementing this in code, we can visualize the MSE loss on the `tips` data.
    **MSE is convex**, so there is one global minimum.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中实现这一点，我们可以可视化`tips`数据上的MSE损失。**MSE是凸的**，因此有一个全局最小值。
- en: <details><summary>Code</summary>
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE11]</details>'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE11]</details>'
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/632cfc6657cf38c34f3a15198ddd608b.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/632cfc6657cf38c34f3a15198ddd608b.png)'
- en: 13.4 Gradient Descent on Multi-Dimensional Models
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4 多维模型的梯度下降
- en: 'The function we worked with above was one-dimensional – we were only minimizing
    the function with respect to a single parameter, \(\theta\). However, models usually
    have a cost function with multiple parameters that need to be optimized. For example,
    simple linear regression has 2 parameters: \(\hat{y} + \theta_0 + \theta_1x\),
    and multiple linear regression has \(p+1\) parameters: \(\mathbb{Y} = \theta_0
    + \theta_1 \Bbb{X}_{:,1} + \theta_2 \Bbb{X}_{:,2} + \cdots + \theta_p \Bbb{X}_{:,p}\)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上面使用的函数是一维的 - 我们只是在最小化函数相对于单个参数\(\theta\)。然而，模型通常具有需要优化的多个参数的成本函数。例如，简单线性回归有2个参数：\(\hat{y}
    + \theta_0 + \theta_1x\)，多元线性回归有\(p+1\)个参数：\(\mathbb{Y} = \theta_0 + \theta_1
    \Bbb{X}_{:,1} + \theta_2 \Bbb{X}_{:,2} + \cdots + \theta_p \Bbb{X}_{:,p}\)
- en: We’ll need to expand gradient descent so we can update our guesses for all model
    parameters, all in one go.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要扩展梯度下降，这样我们就可以一次性更新所有模型参数的猜测。
- en: With multiple parameters to optimize, we consider a **loss surface**, or the
    model’s loss for a particular *combination* of possible parameter values.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个参数要优化时，我们考虑**损失表面**，或者模型对一组可能的参数值的损失。
- en: <details><summary>Code</summary>
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE13]</details>'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE13]</details>'
- en: 'We can also visualize a bird’s-eye view of the loss surface from above using
    a contour plot:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用等高线图从上方可视化损失表面的鸟瞰图：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 13.4.1 The Gradient Vector
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4.1 梯度向量
- en: As before, the derivative of the loss function tells us the best way towards
    the minimum value.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，损失函数的导数告诉我们通往最小值的最佳方式。
- en: On a 2D (or higher) surface, the best way to go down (gradient) is described
    by a *vector*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在2D（或更高维度）的表面上，向下（梯度）的最佳方式由*向量*描述。
- en: '| ![loss_surface](../Images/fde7c93873040d9466b17d1c29037f06.png) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| ![loss_surface](../Images/fde7c93873040d9466b17d1c29037f06.png) |'
- en: 'Math Aside: Partial Derivatives - For an equation with multiple variables,
    we take a **partial derivative** by differentiating with respect to just one variable
    at a time. The partial derivative is denoted with a \(\partial\). Intuitively,
    we want to see how the function changes if we only vary one variable while holding
    other variables constant. - Using \(f(x, y) = 3x^2 + y\) as an example, - taking
    the partial derivative with respect to x and treating y as a constant gives us
    \(\frac{\partial f}{\partial x} = 6x\) - taking the partial derivative with respect
    to y and treating x as a constant gives us \(\frac{\partial f}{\partial y} = 1\)'
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数学旁注：偏导数 - 对于具有多个变量的方程，我们通过分别针对一个变量进行微分来进行**偏导数**。偏导数用\(\partial\)表示。直观地，我们想要看到函数在只改变一个变量的情况下如何变化，同时保持其他变量不变。-
    以\(f(x, y) = 3x^2 + y\)为例，- 对x进行偏导数并将y视为常数，得到\(\frac{\partial f}{\partial x} =
    6x\) - 对y进行偏导数并将x视为常数，得到\(\frac{\partial f}{\partial y} = 1\)
- en: 'For the *vector* of parameter values \(\vec{\theta} = \begin{bmatrix} \theta_{0}
    \\ \theta_{1} \\ \end{bmatrix}\), we take the *partial derivative* of loss with
    respect to each parameter: \(\frac{\partial L}{\partial \theta_0}\) and \(\frac{\partial
    L}{\partial \theta_1}\).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于参数值的*向量*\(\vec{\theta} = \begin{bmatrix} \theta_{0} \\ \theta_{1} \\ \end{bmatrix}\)，我们对每个参数的损失进行*偏导数*：\(\frac{\partial
    L}{\partial \theta_0}\)和\(\frac{\partial L}{\partial \theta_1}\)。
- en: The **gradient vector** is therefore \[\nabla_\theta L = \begin{bmatrix} \frac{\partial
    L}{\partial \theta_0} \\ \frac{\partial L}{\partial \theta_1} \\ \vdots \end{bmatrix}\]
    where \(\nabla_\theta L\) always points in the downhill direction of the surface.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度向量**因此为\[\nabla_\theta L = \begin{bmatrix} \frac{\partial L}{\partial \theta_0}
    \\ \frac{\partial L}{\partial \theta_1} \\ \vdots \end{bmatrix}\]其中\(\nabla_\theta
    L\)始终指向表面的下坡方向。'
- en: We can use this to update our 1D gradient rule for models with multiple parameters.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个来更新我们的具有多个参数的1D梯度规则的模型。
- en: 'Recall our 1D update rule: \[\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta}L(\theta^{(t)})\]'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回想一下我们的1D更新规则：\[\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta}L(\theta^{(t)})\]
- en: 'For models with multiple parameters, we work in terms of vectors: \[\begin{bmatrix}
    \theta_{0}^{(t+1)} \\ \theta_{1}^{(t+1)} \\ \vdots \end{bmatrix} = \begin{bmatrix}
    \theta_{0}^{(t)} \\ \theta_{1}^{(t)} \\ \vdots \end{bmatrix} - \alpha \begin{bmatrix}
    \frac{\partial L}{\partial \theta_{0}} \\ \frac{\partial L}{\partial \theta_{1}}
    \\ \vdots \\ \end{bmatrix}\]'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于具有多个参数的模型，我们使用向量来工作：\[\begin{bmatrix} \theta_{0}^{(t+1)} \\ \theta_{1}^{(t+1)}
    \\ \vdots \end{bmatrix} = \begin{bmatrix} \theta_{0}^{(t)} \\ \theta_{1}^{(t)}
    \\ \vdots \end{bmatrix} - \alpha \begin{bmatrix} \frac{\partial L}{\partial \theta_{0}}
    \\ \frac{\partial L}{\partial \theta_{1}} \\ \vdots \\ \end{bmatrix}\]
- en: Written in a more compact form, \[\vec{\theta}^{(t+1)} = \vec{\theta}^{(t)}
    - \alpha \nabla_{\vec{\theta}} L(\theta^{(t)}) \]
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更紧凑地写成，\[\vec{\theta}^{(t+1)} = \vec{\theta}^{(t)} - \alpha \nabla_{\vec{\theta}}
    L(\theta^{(t)}) \]
- en: \(\theta\) is a vector with our model weights
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\theta\) 是我们模型权重的向量
- en: \(L\) is the loss function
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(L\) 是损失函数
- en: \(\alpha\) is the learning rate (ours is constant, but other techniques use
    an \(\alpha\) that decreases over time)
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\alpha\) 是学习率（我们的是恒定的，但其他技术使用随时间减小的 \(\alpha\)）
- en: \(\vec{\theta}^{(t)}\) is the current value of \(\theta\)
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\vec{\theta}^{(t)}\) 是 \(\theta\) 的当前值
- en: \(\vec{\theta}^{(t+1)}\) is the next value of \(\theta\)
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\vec{\theta}^{(t+1)}\) 是 \(\theta\) 的下一个值
- en: \(\nabla_{\vec{\theta}} L(\theta^{(t)})\) is the gradient of the loss function
    evaluated at the current \(\vec{\theta}^{(t)}\)
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\nabla_{\vec{\theta}} L(\theta^{(t)})\) 是在当前 \(\vec{\theta}^{(t)}\) 处评估的损失函数的梯度
- en: 13.5 Batch, Mini-Batch Gradient Descent and Stochastic Gradient Descent
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.5 批量、小批量梯度下降和随机梯度下降
- en: Formally, the algorithm we derived above is called **batch gradient descent.**
    For each iteration of the algorithm, the derivative of loss is computed across
    the *entire* batch of all \(n\) datapoints. While this update rule works well
    in theory, it is not practical in most circumstances. For large datasets (with
    perhaps billions of datapoints), finding the gradient across all the data is incredibly
    computationally taxing; gradient descent will converge slowly because each individual
    update is slow.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，我们上面推导的算法称为**批量梯度下降**。对于算法的每次迭代，计算整个包含所有 \(n\) 个数据点的批次的损失的导数。虽然这个更新规则在理论上效果很好，但在大多数情况下并不实用。对于大型数据集（可能有数十亿个数据点），在所有数据上找到梯度是非常耗费计算资源的；梯度下降会收敛缓慢，因为每次单独的更新都很慢。
- en: '**Mini-batch gradient descent** tries to address this issue. In mini-batch
    descent, only a *subset* of the data is used to estimate the gradient. The **batch
    size** is the number of data points used in each subset.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**小批量梯度下降**试图解决这个问题。在小批量下降中，只使用*子集*数据来估计梯度。**批量大小**是每个子集中使用的数据点数。'
- en: Each complete “pass” through the data is known as a **training epoch**. In a
    single **training epoch** of mini-batch gradient descent, we
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 每次完整“通过”数据称为**训练周期**。在小批量梯度下降的单个**训练周期**中，我们
- en: Compute the gradient on the first x% of the data. Update the parameter guesses.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算第一个 x% 的数据的梯度。更新参数猜测。
- en: Compute the gradient on the next x% of the data. Update the parameter guesses.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算下一个 x% 的数据的梯度。更新参数猜测。
- en: \(\dots\)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （省略）
- en: Compute the gradient on the last x% of the data. Update the parameter guesses.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算最后 x% 的数据的梯度。更新参数猜测。
- en: Every data point is once in a single training epoch. We then perform several
    training epochs until we’re satisfied.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据点在单个训练周期中只出现一次。然后我们进行多个训练周期，直到满意为止。
- en: In the most extreme case, we might choose a batch size of only 1 datapoint –
    that is, a single datapoint is used to estimate the gradient of loss with each
    update step. This is known as **stochastic gradient descent**. In a single **training
    epoch** of stochastic gradient descent, we
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在最极端的情况下，我们可能选择只有 1 个数据点的批量大小——也就是说，每次更新步骤中只使用一个数据点来估计损失的梯度。这被称为**随机梯度下降**。在随机梯度下降的单个**训练周期**中，我们
- en: Compute the gradient on the first data point. Update the parameter guesses.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算第一个数据点的梯度。更新参数猜测。
- en: Compute the gradient on the next data point. Update the parameter guesses.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算下一个数据点的梯度。更新参数猜测。
- en: \(\dots\)
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （省略）
- en: Compute the gradient on the last data point. Update the parameter guesses.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算最后一个数据点的梯度。更新参数猜测。
- en: Batch gradient descent is a deterministic technique – because the entire dataset
    is used at each update iteration, the algorithm will always advance towards the
    minimum of the loss surface. In contrast, both mini-batch and stochastic gradient
    descent involve an element of randomness. Since only a subset of the full data
    is used to update the guess for \(\vec{\theta}\) at each iteration, there’s a
    chance the algorithm will not progress towards the true minimum of loss with each
    update. Over the longer term, these stochastic techniques should still converge
    towards the optimal solution.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降是一种确定性技术——因为在每次更新迭代中都使用整个数据集，算法总是朝着损失曲面的最小值前进。相比之下，小批量和随机梯度下降都涉及一定的随机性。由于每次迭代更新参数
    \(\vec{\theta}\) 时只使用完整数据的子集，算法有可能不会在每次更新中朝着真正的损失最小值前进。从长远来看，这些随机技术仍然应该收敛到最优解。
- en: The diagrams below represent a “bird’s eye view” of a loss surface from above.
    Notice that batch gradient descent takes a direct path towards the optimal \(\hat{\theta}\).
    Stochastic gradient descent, in contrast, “hops around” on its path to the minimum
    point on the loss surface. This reflects the randomness of the sampling process
    at each update step.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表代表了从上方俯视的损失曲面。注意批量梯度下降直接朝向最优的 \(\hat{\theta}\)。相比之下，随机梯度下降在损失曲面上“跳跃”。这反映了每次更新步骤中抽样过程的随机性。
- en: '| ![stochastic](../Images/963353267e3bf27ff5a97ebad0bec216.png) |**'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![随机](../Images/963353267e3bf27ff5a97ebad0bec216.png) |**'
