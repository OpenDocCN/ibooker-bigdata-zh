- en: Chapter 9\. Building Reliable Data Lakes with Apache Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。使用Apache Spark构建可靠的数据湖
- en: In the previous chapters, you learned how to easily and effectively use Apache
    Spark to build scalable and performant data processing pipelines. However, in
    practice, expressing the processing logic only solves half of the end-to-end problem
    of building a pipeline. For a data engineer, data scientist, or data analyst,
    the ultimate goal of building pipelines is to query the processed data and get
    insights from it. The choice of storage solution determines the end-to-end (i.e.,
    from raw data to insights) robustness and performance of the data pipeline.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，您学习了如何轻松有效地使用Apache Spark构建可扩展和高性能的数据处理流水线。然而，在实践中，仅仅表达处理逻辑只解决了构建流水线端到端问题的一半。对于数据工程师、数据科学家或数据分析师来说，构建流水线的最终目标是查询处理后的数据并从中获取见解。存储解决方案的选择决定了从原始数据到见解（即端到端）的数据流水线的稳健性和性能。
- en: In this chapter, we will first discuss the key features of a storage solution
    that you need to look out for. Then we will discuss two broad classes of storage
    solutions, databases and data lakes, and how to use Apache Spark with them. Finally,
    we will introduce the next wave of storage solution, called lakehouses, and explore
    some of the new open source processing engines in this space.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先讨论您需要关注的存储解决方案的关键特性。然后，我们将讨论两类广泛的存储解决方案，即数据库和数据湖，以及如何与它们一起使用Apache
    Spark。最后，我们将介绍存储解决方案的下一波发展，称为湖仓库，并探讨这个领域中一些新的开源处理引擎。
- en: The Importance of an Optimal Storage Solution
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**优化存储解决方案的重要性**'
- en: 'Here are some of the properties that are desired in a storage solution:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是存储解决方案中所需的一些属性：
- en: Scalability and performance
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性和性能
- en: The storage solution should be able to scale to the volume of data and provide
    the read/write throughput and latency that the workload requires.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 存储解决方案应能够扩展到所需的数据量，并提供工作负载所需的读/写吞吐量和延迟。
- en: Transaction support
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 事务支持
- en: Complex workloads are often reading and writing data concurrently, so support
    for [ACID transactions](https://oreil.ly/6Jn97) is essential to ensure the quality
    of the end results.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的工作负载通常同时读写数据，因此支持[ACID事务](https://oreil.ly/6Jn97)对于确保最终结果的质量至关重要。
- en: Support for diverse data formats
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 支持多种数据格式
- en: The storage solution should be able to store unstructured data (e.g., text files
    like raw logs), semi-structured data (e.g., JSON data), and structured data (e.g.,
    tabular data).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 存储解决方案应能够存储非结构化数据（例如原始日志文本文件）、半结构化数据（例如JSON数据）和结构化数据（例如表格数据）。
- en: Support for diverse workloads
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 支持多样化的工作负载
- en: 'The storage solution should be able to support a diverse range of business
    workloads, including:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 存储解决方案应能够支持多样化的业务工作负载，包括：
- en: SQL workloads like traditional BI analytics
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像传统BI分析这样的SQL工作负载
- en: Batch workloads like traditional ETL jobs processing raw unstructured data
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像传统ETL作业处理原始非结构化数据的批处理工作负载
- en: Streaming workloads like real-time monitoring and alerting
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像实时监控和报警这样的流式工作负载
- en: ML and AI workloads like recommendations and churn predictions
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像推荐和流失预测这样的ML和AI工作负载
- en: Openness
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 开放性
- en: Supporting a wide range of workloads often requires the data to be stored in
    open data formats. Standard APIs allow the data to be accessed from a variety
    of tools and engines. This allows the business to use the most optimal tools for
    each type of workload and make the best business decisions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 支持广泛的工作负载通常要求数据以开放数据格式存储。标准API允许从各种工具和引擎访问数据。这使得企业可以针对每种类型的工作负载使用最优的工具，并做出最佳的业务决策。
- en: 'Over time, different kinds of storage solutions have been proposed, each with
    its unique advantages and disadvantages with respect to these properties. In this
    chapter, we will explore how the available storage solutions evolved from *databases*
    to *data lakes*, and how to use Apache Spark with each of them. We’ll then turn
    our attention to the next generation of storage solutions, often called data *lakehouses*,
    that can provide the best of both worlds: the scalability and flexibility of data
    lakes with the transactional guarantees of databases.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，不同类型的存储解决方案被提出，每种解决方案在这些属性方面都有其独特的优缺点。在本章中，我们将探讨可用的存储解决方案是如何从*数据库*发展到*数据湖*，以及如何与每种解决方案一起使用Apache
    Spark。然后我们将转向下一代存储解决方案，通常被称为数据*湖仓库*，它们可以提供数据湖的可扩展性和灵活性，同时具备数据库的事务性保证。
- en: Databases
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据库
- en: For many decades, databases have been the most reliable solution for building
    data warehouses to store business-critical data. In this section, we will explore
    the architecture of databases and their workloads, and how to use Apache Spark
    for analytics workloads on databases. We will end this section with a discussion
    of the limitations of databases in supporting modern non-SQL workloads.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，数据库一直是构建存储业务关键数据的最可靠解决方案。在本节中，我们将探讨数据库及其工作负载的架构，以及如何在数据库上使用 Apache Spark
    进行分析工作负载。我们将结束本节讨论数据库在支持现代非 SQL 工作负载方面的限制。
- en: A Brief Introduction to Databases
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据库简介
- en: Databases are designed to store structured data as tables, which can be read
    using SQL queries. The data must adhere to a strict schema, which allows a database
    management system to heavily co-optimize the data storage and processing. That
    is, they tightly couple their internal layout of the data and indexes in on-disk
    files with their highly optimized query processing engines, thus providing very
    fast computations on the stored data along with strong transactional ACID guarantees
    on all read/write operations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库旨在以表格形式存储结构化数据，可以使用 SQL 查询进行读取。数据必须遵循严格的模式，这允许数据库管理系统在数据存储和处理方面进行高度协同优化。也就是说，它们紧密地将数据和索引的内部布局与高度优化的查询处理引擎耦合在磁盘文件中，因此能够在存储的数据上提供非常快速的计算，并在所有读写操作上提供强大的事务
    ACID 保证。
- en: 'SQL workloads on databases can be broadly classified into two categories, as
    follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库上的 SQL 工作负载可以广泛分类为两类，如下所示：
- en: '[Online transaction processing (OLTP) workloads](https://oreil.ly/n94tD)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[在线事务处理 (OLTP) 工作负载](https://oreil.ly/n94tD)'
- en: Like bank account transactions, OLTP workloads are typically high-concurrency,
    low-latency, simple queries that read or update a few records at a time.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 像银行账户交易一样，OLTP 工作负载通常是高并发、低延迟、简单查询，每次读取或更新少量记录。
- en: '[Online analytical processing (OLAP)](https://oreil.ly/NJQ2m)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[在线分析处理 (OLAP)](https://oreil.ly/NJQ2m)'
- en: OLAP workloads, like periodic reporting, are typically complex queries (involving
    aggregates and joins) that require high-throughput scans over many records.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: OLAP 工作负载，例如周期性报告，通常是涉及聚合和连接的复杂查询，需要高吞吐量的扫描许多记录。
- en: It is important to note that Apache Spark is a query engine that is primarily
    designed for OLAP workloads, not OLTP workloads. Hence, in the rest of the chapter
    we are going to focus our discussion on storage solutions for analytical workloads.
    Next, let’s see how Apache Spark can be used to read from and write to databases.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Apache Spark 是专为 OLAP 工作负载而设计的查询引擎，而不是 OLTP 工作负载。因此，在本章的其余部分，我们将专注于分析工作负载的存储解决方案讨论。接下来，让我们看看如何使用
    Apache Spark 读写数据库。
- en: Reading from and Writing to Databases Using Apache Spark
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Apache Spark 读写数据库
- en: Thanks to the ever-growing ecosystem of connectors, Apache Spark can connect
    to a wide variety of databases for reading and writing data. For databases that
    have JDBC drivers (e.g., PostgreSQL, MySQL), you can use the built-in JDBC data
    source along with the appropriate JDBC driver jars to access the data. For many
    other modern databases (e.g., Azure Cosmos DB, Snowflake), there are dedicated
    connectors that you can invoke using the appropriate format name. Several examples
    were discussed in detail in [Chapter 5](ch05.html#spark_sql_and_dataframes_interacting_wit).
    This makes it very easy to augment your data warehouses and databases with workloads
    and use cases based on Apache Spark.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于日益增长的连接器生态系统，Apache Spark 能够连接多种数据库进行数据读写。对于具有 JDBC 驱动程序的数据库（例如 PostgreSQL、MySQL），您可以使用内置的
    JDBC 数据源以及适当的 JDBC 驱动程序 jar 包访问数据。对于许多其他现代数据库（例如 Azure Cosmos DB、Snowflake），还有专用连接器，可以使用适当的格式名称调用。本书的[第
    5 章](ch05.html#spark_sql_and_dataframes_interacting_wit)详细讨论了几个示例，这使得基于 Apache
    Spark 的数据仓库和数据库的工作负载和用例扩展变得非常简单。
- en: Limitations of Databases
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据库的限制
- en: 'Since the last century, databases and SQL queries have been known as great
    building solutions for BI workloads. However, the last decade has seen two major
    new trends in analytical workloads:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 自上个世纪以来，数据库和 SQL 查询被认为是构建 BI 工作负载的重要解决方案。然而，过去十年中出现了两大新的分析工作负载趋势：
- en: Growth in data sizes
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据规模的增长
- en: With the advent of big data, there has been a global trend in the industry to
    measure and collect everything (page views, clicks, etc.) in order to understand
    trends and user behaviors. As a result, the amount of data collected by any company
    or organization has increased from gigabytes a couple of decades ago to terabytes
    and petabytes today.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大数据的出现，全球工业界出现了一种趋势，即为了理解趋势和用户行为，衡量和收集一切（页面访问量、点击等）。因此，任何公司或组织收集的数据量从几十年前的几吉字节增加到今天的几百或几千吉字节。
- en: Growth in the diversity of analytics
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 分析多样性的增长
- en: Along with the increase in data collection, there is a need for deeper insights.
    This has led to an explosive growth of complex analytics like machine learning
    and deep learning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据收集量的增加，深入洞察的需求也在增加。这导致了像机器学习和深度学习这样的复杂分析技术的爆炸性增长。
- en: 'Databases have been shown to be rather inadequate at accommodating these new
    trends, because of the following limitations:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库已被证明在适应这些新趋势方面相当不足，原因如下：
- en: Databases are extremely expensive to scale out
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库在横向扩展方面成本极高
- en: Although databases are extremely efficient at processing data on a single machine,
    the rate of growth of data volumes has far outpaced the growth in performance
    capabilities of a single machine. The only way forward for processing engines
    is to scale out—that is, use multiple machines to process data in parallel. However,
    most databases, especially the open source ones, are not designed for scaling
    out to perform distributed processing. The few industrial database solutions that
    can remotely keep up with the processing requirements tend to be proprietary solutions
    running on specialized hardware, and are therefore very expensive to acquire and
    maintain.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据库在单机上处理数据非常高效，但数据量的增长速度远远超过了单机性能的增长。处理引擎前进的唯一途径是横向扩展，即使用多台机器并行处理数据。然而，大多数数据库，尤其是开源数据库，并未设计成能够横向扩展执行分布式处理。少数能够满足处理需求的工业级数据库解决方案，往往是运行在专用硬件上的专有解决方案，因此非常昂贵，无论是获取还是维护。
- en: Databases do not support non–SQL based analytics very well
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库对非SQL基础分析的支持不够好
- en: Databases store data in complex (often proprietary) formats that are typically
    highly optimized for only that database’s SQL processing engine to read. This
    means other processing tools, like machine learning and deep learning systems,
    cannot efficiently access the data (except by inefficiently reading all the data
    from the database). Nor can databases be easily extended to perform non–SQL based
    analytics like machine learning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库以复杂（通常是专有的）格式存储数据，这些格式通常被高度优化，只能由该数据库的SQL处理引擎有效读取。这意味着其他处理工具，如机器学习和深度学习系统，无法有效地访问数据（除非通过从数据库中低效地读取所有数据）。数据库也不能轻易扩展以执行像机器学习这样的非SQL基础分析。
- en: These limitations of databases led to the development of a completely different
    approach to storing data, known as *data lakes*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正是由于这些数据库的限制，才引发了一种完全不同的存储数据方法的发展，即*数据湖*。
- en: Data Lakes
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据湖
- en: In contrast to most databases, a data lake is a distributed storage solution
    that runs on commodity hardware and easily scales out horizontally. In this section,
    we will start with a discussion of how data lakes satisfy the requirements of
    modern workloads, then see how Apache Spark integrates with data lakes to make
    workloads scale to data of any size. Finally, we will explore the impact of the
    architectural sacrifices made by data lakes to achieve scalability.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数数据库相反，数据湖是一种分布式存储解决方案，运行在通用硬件上，可以轻松实现横向扩展。在本节中，我们将从讨论数据湖如何满足现代工作负载的需求开始，然后看看Apache
    Spark如何与数据湖集成，使工作负载能够处理任意规模的数据。最后，我们将探讨数据湖为实现可扩展性而做出的架构上的牺牲所带来的影响。
- en: A Brief Introduction to Data Lakes
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据湖简介
- en: The data lake architecture, unlike that of databases, decouples the distributed
    storage system from the distributed compute system. This allows each system to
    scale out as needed by the workload. Furthermore, the data is saved as files with
    open formats, such that any processing engine can read and write them using standard
    APIs. This idea was popularized in the late 2000s by the Hadoop File System (HDFS)
    from the [Apache Hadoop project](https://hadoop.apache.org/), which itself was
    heavily inspired by the research paper [“The Google File System”](https://oreil.ly/v6py_)
    by Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖架构与数据库的架构不同，它将分布式存储系统与分布式计算系统解耦。这使得每个系统可以根据工作负载的需求进行横向扩展。此外，数据以开放格式的文件保存，因此任何处理引擎都可以使用标准API读取和写入它们。这个想法在2000年代后期由Apache
    Hadoop项目中的Hadoop文件系统（HDFS）推广开来，该项目本身深受Sanjay Ghemawat、Howard Gobioff和Shun-Tak
    Leung的研究论文《Google文件系统》的启发。
- en: 'Organizations build their data lakes by independently choosing the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 组织通过独立选择以下内容来构建他们的数据湖：
- en: Storage system
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 存储系统
- en: They choose to either run HDFS on a cluster of machines or use any cloud object
    store (e.g., AWS S3, Azure Data Lake Storage, or Google Cloud Storage).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 他们选择在机器群集上运行HDFS或使用任何云对象存储（例如AWS S3、Azure Data Lake Storage或Google Cloud Storage）。
- en: File format
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 文件格式
- en: Depending on the downstream workloads, the data is stored as files in either
    structured (e.g., Parquet, ORC), semi-structured (e.g., JSON), or sometimes even
    unstructured formats (e.g., text, images, audio, video).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 根据下游工作负载的不同，数据以文件形式存储，可以是结构化（例如Parquet、ORC）、半结构化（例如JSON）或有时甚至是非结构化格式（例如文本、图像、音频、视频）。
- en: Processing engine(s)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 处理引擎
- en: Again, depending on the kinds of analytical workloads to be performed, a processing
    engine is chosen. This can either be a batch processing engine (e.g., Spark, Presto,
    Apache Hive), a stream processing engine (e.g., Spark, Apache Flink), or a machine
    learning library (e.g., Spark MLlib, scikit-learn, R).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，根据要执行的分析工作负载的类型，选择处理引擎。这可以是批处理引擎（例如Spark、Presto、Apache Hive）、流处理引擎（例如Spark、Apache
    Flink）或机器学习库（例如Spark MLlib、scikit-learn、R）。
- en: This flexibility—the ability to choose the storage system, open data format,
    and processing engine that are best suited to the workload at hand—is the biggest
    advantage of data lakes over databases. On the whole, for the same performance
    characteristics, data lakes often provide a much cheaper solution than databases.
    This key advantage has led to the explosive growth of the big data ecosystem.
    In the next section, we will discuss how you can use Apache Spark to read and
    write common file formats on any storage system.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种灵活性——能够选择最适合当前工作负载的存储系统、开放数据格式和处理引擎——是数据湖比数据库更大的优势。总体而言，对于相同的性能特征，数据湖通常提供比数据库更便宜的解决方案。这一关键优势导致了大数据生态系统的爆炸式增长。在接下来的部分中，我们将讨论如何使用Apache
    Spark在任何存储系统上读写常见文件格式。
- en: Reading from and Writing to Data Lakes using Apache Spark
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Apache Spark读写数据湖
- en: 'Apache Spark is one of the best processing engines to use when building your
    own data lake, because it provides all the key features they require:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建自己的数据湖时，Apache Spark是其中一种最佳处理引擎，因为它提供了他们需要的所有关键功能：
- en: Support for diverse workloads
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 支持多样化的工作负载
- en: Spark provides all the necessary tools to handle a diverse range of workloads,
    including batch processing, ETL operations, SQL workloads using Spark SQL, stream
    processing using Structured Streaming (discussed in [Chapter 8](ch08.html#structured_streaming)),
    and machine learning using MLlib (discussed in [Chapter 10](ch10.html#machine_learning_with_mllib)),
    among many others.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了处理多种工作负载所需的所有必要工具，包括批处理、ETL操作、使用Spark SQL进行SQL工作负载、使用结构化流进行流处理（在[第8章](ch08.html#structured_streaming)中讨论）、以及使用MLlib进行机器学习（在[第10章](ch10.html#machine_learning_with_mllib)中讨论），等等。
- en: Support for diverse file formats
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 支持多样化的文件格式
- en: In [Chapter 4](ch04.html#spark_sql_and_dataframes_introduction_to), we explored
    in detail how Spark has built-in support for unstructured, semi-structured, and
    structured file formats.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html#spark_sql_and_dataframes_introduction_to)中，我们详细探讨了Spark对非结构化、半结构化和结构化文件格式的内置支持。
- en: Support for diverse filesystems
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 支持多样化的文件系统
- en: Spark supports accessing data from any storage system that supports Hadoop’s
    FileSystem APIs. Since this API has become the de facto standard in the big data
    ecosystem, most cloud and on-premises storage systems provide implementations
    for it—which means Spark can read from and write to most storage systems.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持从支持Hadoop FileSystem API的任何存储系统访问数据。由于这个API已成为大数据生态系统的事实标准，大多数云和本地存储系统都为其提供了实现——这意味着Spark可以读取和写入大多数存储系统。
- en: However, for many filesystems (especially those based on cloud storage, like
    AWS S3), you have to configure Spark such that it can access the filesystem in
    a secure manner. Furthermore, cloud storage systems often do not have the same
    file operation semantics expected from a standard filesystem (e.g., eventual consistency
    in S3), which can lead to inconsistent results if you do not configure Spark accordingly.
    See the [documentation on cloud integration](https://oreil.ly/YncTL) for details.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于许多文件系统（特别是基于云存储的文件系统，如AWS S3），您必须配置Spark以便以安全的方式访问文件系统。此外，云存储系统通常没有标准文件系统期望的相同文件操作语义（例如，S3的最终一致性），如果不按照Spark的配置进行配置，可能会导致不一致的结果。有关详细信息，请参阅[云集成文档](https://oreil.ly/YncTL)。
- en: Limitations of Data Lakes
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据湖的局限性
- en: 'Data lakes are not without their share of flaws, the most egregious of which
    is the lack of transactional guarantees. Specifically, data lakes fail to provide
    ACID guarantees on:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖并非没有缺陷，其中最严重的是缺乏事务性保证。具体来说，数据湖无法提供ACID保证：
- en: Atomicity and isolation
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 原子性和隔离
- en: Processing engines write data in data lakes as many files in a distributed manner.
    If the operation fails, there is no mechanism to roll back the files already written,
    thus leaving behind potentially corrupted data (the problem is exacerbated when
    concurrent workloads modify the data because it is very difficult to provide isolation
    across files without higher-level mechanisms).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 处理引擎以分布方式写入数据湖中的数据。如果操作失败，就没有机制回滚已经写入的文件，从而可能留下潜在的损坏数据（当并发工作负载修改数据时，提供跨文件的隔离是非常困难的，因此问题进一步恶化）。
- en: Consistency
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性
- en: Lack of atomicity on failed writes further causes readers to get an inconsistent
    view of the data. In fact, it is hard to ensure data quality even in successfully
    written data. For example, a very common issue with data lakes is accidentally
    writing out data files in a format and schema inconsistent with existing data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 失败写入的缺乏原子性进一步导致读者获取数据的不一致视图。事实上，即使成功写入数据，也很难确保数据质量。例如，数据湖的一个非常常见的问题是意外地以与现有数据不一致的格式和架构写出数据文件。
- en: 'To work around these limitations of data lakes, developers employ all sorts
    of tricks. Here are a few examples:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决数据湖的这些局限性，开发人员采用各种技巧。以下是几个示例：
- en: Large collections of data files in data lakes are often “partitioned” by subdirectories
    based on a column’s value (e.g., a large Parquet-formatted Hive table partitioned
    by date). To achieve atomic modifications of existing data, often entire subdirectories
    are rewritten (i.e., written to a temporary directory, then references swapped)
    just to update or delete a few records.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖中的大量数据文件通常根据列的值（例如，一个大型的Parquet格式的Hive表按日期分区）“分区”到子目录中。为了实现对现有数据的原子修改，通常整个子目录会被重写（即写入临时目录，然后交换引用），以便更新或删除少量记录。
- en: The schedules of data update jobs (e.g., daily ETL jobs) and data querying jobs
    (e.g., daily reporting jobs) are often staggered to avoid concurrent access to
    the data and any inconsistencies caused by it.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据更新作业（例如，每日ETL作业）和数据查询作业（例如，每日报告作业）的调度通常错开，以避免对数据的并发访问及由此引起的任何不一致性。
- en: Attempts to eliminate such practical issues have led to the development of new
    systems, such as lakehouses.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 试图消除这些实际问题的努力导致了新系统的开发，例如湖岸。
- en: 'Lakehouses: The Next Step in the Evolution of Storage Solutions'
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 湖岸：存储解决方案演进的下一个步骤
- en: 'The *lakehouse* is a new paradigm that combines the best elements of data lakes
    and data warehouses for OLAP workloads. Lakehouses are enabled by a new system
    design that provides data management features similar to databases directly on
    the low-cost, scalable storage used for data lakes. More specifically, they provide
    the following features:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*湖岸*是一种新范式，它结合了数据湖和数据仓库的最佳元素，用于OLAP工作负载。湖岸由一个新的系统设计驱动，提供了类似数据库直接在用于数据湖的低成本可扩展存储上的数据管理功能。更具体地说，它们提供以下功能：'
- en: Transaction support
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 事务支持
- en: Similar to databases, lakehouses provide ACID guarantees in the presence of
    concurrent workloads.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于数据库，湖仓在并发工作负载下提供ACID保证。
- en: Schema enforcement and governance
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 模式强制执行和治理
- en: Lakehouses prevent data with an incorrect schema being inserted into a table,
    and when needed, the table schema can be explicitly evolved to accommodate ever-changing
    data. The system should be able to reason about data integrity, and it should
    have robust governance and auditing mechanisms.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 湖仓防止将带有错误模式的数据插入表格，必要时，可以显式演变表格模式以适应不断变化的数据。系统应能够理解数据完整性，并具备强大的治理和审计机制。
- en: Support for diverse data types in open formats
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 支持开放格式中多样化的数据类型
- en: Unlike databases, but similar to data lakes, lakehouses can store, refine, analyze,
    and access all types of data needed for many new data applications, be it structured,
    semi-structured, or unstructured. To enable a wide variety of tools to access
    it directly and efficiently, the data must be stored in open formats with standardized
    APIs to read and write them.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据库不同，但类似于数据湖，湖仓可以存储、精炼、分析和访问所有类型的数据，以支持许多新数据应用程序所需的结构化、半结构化或非结构化数据。为了让各种工具能够直接和高效地访问它，数据必须以开放格式存储，并具备标准化的API来读取和写入。
- en: Support for diverse workloads
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 支持多样化工作负载
- en: Powered by the variety of tools reading data using open APIs, lakehouses enable
    diverse workloads to operate on data in a single repository. Breaking down isolated
    data silos (i.e., multiple repositories for different categories of data) enables
    developers to more easily build diverse and complex data solutions, from traditional
    SQL and streaming analytics to machine learning.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 借助各种工具的驱动，使用开放API读取数据，湖仓使得多样化工作负载能够在单一仓库中处理数据。打破孤立的数据孤岛（即针对不同数据类别的多个仓库），使开发人员更轻松地构建从传统SQL和流式分析到机器学习的多样化和复杂数据解决方案。
- en: Support for upserts and deletes
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 支持插入更新和删除操作
- en: Complex use cases like [change-data-capture (CDC)](https://oreil.ly/eEj_m) and
    [slowly changing dimension (SCD)](https://oreil.ly/13zll) operations require data
    in tables to be continuously updated. Lakehouses allow data to be concurrently
    deleted and updated with transactional guarantees.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 像[变更数据捕获(CDC)](https://oreil.ly/eEj_m)和[缓慢变化维度(SCD)](https://oreil.ly/13zll)这样的复杂用例需要对表格中的数据进行持续更新。湖仓允许数据在具有事务保证的情况下进行并发删除和更新。
- en: Data governance
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据治理
- en: Lakehouses provide the tools with which you can reason about data integrity
    and audit all the data changes for policy compliance.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 湖仓提供工具，帮助您理解数据完整性，并审计所有数据变更以符合政策要求。
- en: 'Currently, there are a few open source systems, such as Apache Hudi, Apache
    Iceberg, and Delta Lake, that can be used to build lakehouses with these properties.
    At a very high level, all three projects have a similar architecture inspired
    by well-known database principles. They are all open data storage formats that
    do the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有几个开源系统，如Apache Hudi、Apache Iceberg和Delta Lake，可以用来构建具备这些特性的湖仓。在非常高的层次上，这三个项目都受到了著名数据库原理的启发，具有类似的架构。它们都是开放的数据存储格式，具有以下特点：
- en: Store large volumes of data in structured file formats on scalable filesystems.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可扩展的文件系统中以结构化文件格式存储大量数据。
- en: Maintain a transaction log to record a timeline of atomic changes to the data
    (much like databases).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护事务日志记录数据的原子变更时间线（类似数据库）。
- en: Use the log to define versions of the table data and provide snapshot isolation
    guarantees between readers and writers.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用日志定义表数据的版本，并在读写者之间提供快照隔离保证。
- en: Support reading and writing to tables using Apache Spark.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持使用Apache Spark读写表格数据。
- en: Within these broad strokes, each project has unique characteristics in terms
    of APIs, performance, and the level of integration with Apache Spark’s data source
    APIs. We will explore them next. Note that all of these projects are evolving
    fast, and therefore some of the descriptions may be outdated at the time you are
    reading them. Refer to the online documentation for each project for the most
    up-to-date information.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些大致描述的框架内，每个项目在API、性能以及与Apache Spark数据源API集成程度上都具有独特特征。我们将在接下来探讨它们。请注意，所有这些项目都在快速发展，因此在阅读时可能有些描述已经过时。请参考每个项目的在线文档获取最新信息。
- en: Apache Hudi
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Hudi
- en: 'Initially built by [Uber Engineering](https://eng.uber.com/hoodie), [Apache
    Hudi](https://hudi.apache.org)—an acronym for Hadoop Update Delete and Incremental—is
    a data storage format that is designed for incremental upserts and deletes over
    key/value-style data. The data is stored as a combination of columnar formats
    (e.g., Parquet files) and row-based formats (e.g., Avro files for recording incremental
    changes over Parquet files). Besides the common features mentioned earlier, it
    supports:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最初由 [Uber Engineering](https://eng.uber.com/hoodie) 创建，[Apache Hudi](https://hudi.apache.org)——Hadoop
    更新删除和增量的首字母缩写——是一种专为 key/value 数据的增量 upserts 和删除而设计的数据存储格式。数据存储为列式格式的组合（例如 Parquet
    文件）和基于行的格式（例如用于在 Parquet 文件上记录增量变更的 Avro 文件）。除了前面提到的常见功能外，它还支持：
- en: Upserting with fast, pluggable indexing
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速、可插拔索引的 upsert
- en: Atomic publishing of data with rollback support
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原子发布数据，支持回滚
- en: Reading incremental changes to a table
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取表的增量更改
- en: Savepoints for data recovery
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据恢复的保存点
- en: File size and layout management using statistics
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件大小和布局管理使用统计信息
- en: Async compaction of row and columnar data
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步压缩行和列式数据
- en: Apache Iceberg
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Iceberg
- en: 'Originally built at [Netflix](https://github.com/Netflix/iceberg), [Apache
    Iceberg](https://iceberg.apache.org) is another open storage format for huge data
    sets. However, unlike Hudi, which focuses on upserting key/value data, Iceberg
    focuses more on general-purpose data storage that scales to petabytes in a single
    table and has schema evolution properties. Specifically, it provides the following
    additional features (besides the common ones):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最初在 [Netflix](https://github.com/Netflix/iceberg) 创建，[Apache Iceberg](https://iceberg.apache.org)
    是另一种用于大数据集的开放存储格式。但与专注于 upserting 键/值数据的 Hudi 不同，Iceberg 更专注于通用数据存储，可在单个表中扩展到
    PB 级并具有模式演化特性。具体来说，它提供以下附加功能（除了常见功能）：
- en: Schema evolution by adding, dropping, updating, renaming, and reordering of
    columns, fields, and/or nested structures
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过添加、删除、更新、重命名和重新排序列、字段和/或嵌套结构来进行模式演化
- en: Hidden partitioning, which under the covers creates the partition values for
    rows in a table
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏分区，它在表中为行创建分区值
- en: Partition evolution, where it automatically performs a metadata operation to
    update the table layout as data volume or query patterns change
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区演化，根据数据量或查询模式的变化自动执行元数据操作以更新表布局
- en: Time travel, which allows you to query a specific table snapshot by ID or by
    timestamp
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间旅行，允许您通过 ID 或时间戳查询特定表快照
- en: Rollback to previous versions to correct errors
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚到以前的版本以更正错误
- en: Serializable isolation, even between multiple concurrent writers
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可串行化隔离，即使在多个并发写入者之间也是如此
- en: Delta Lake
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake
- en: '[Delta Lake](https://delta.io/) is an open source project hosted by the Linux
    Foundation, built by the original creators of Apache Spark. Similar to the others,
    it is an open data storage format that provides transactional guarantees and enables
    schema enforcement and evolution. It also provides several other interesting features,
    some of which are unique. Delta Lake supports:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[Delta Lake](https://delta.io/) 是由 Linux Foundation 托管的开源项目，由 Apache Spark
    的原始创建者构建。与其他类似的项目一样，它是一种提供事务保证并支持模式强制和演化的开放数据存储格式。它还提供几个其他有趣的特性，其中一些是独特的。Delta
    Lake 支持：'
- en: Streaming reading from and writing to tables using Structured Streaming sources
    and sinks
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用结构化流源和接收器进行表的流式读取和写入
- en: Update, delete, and merge (for upserts) operations, even in Java, Scala, and
    Python APIs
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使在 Java、Scala 和 Python API 中也支持更新、删除和合并（用于 upsert 操作）
- en: Schema evolution either by explicitly altering the table schema or by implicitly
    merging a DataFrame’s schema to the table’s during the DataFrame’s write. (In
    fact, the merge operation in Delta Lake supports advanced syntax for conditional
    updates/inserts/deletes, updating all columns together, etc., as you’ll see later
    in the chapter.)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过显式更改表模式或在 DataFrame 写入期间将 DataFrame 的模式隐式合并到表的模式中进行模式演化。（事实上，Delta Lake 中的合并操作支持条件更新/插入/删除的高级语法，同时更新所有列等，正如您稍后在本章中将看到的。）
- en: Time travel, which allows you to query a specific table snapshot by ID or by
    timestamp
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间旅行，允许您通过 ID 或时间戳查询特定表快照
- en: Rollback to previous versions to correct errors
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚到以前的版本以更正错误
- en: Serializable isolation between multiple concurrent writers performing any SQL,
    batch, or streaming operations
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个并发写入者之间的可序列化隔离，执行任何 SQL、批处理或流操作
- en: In the rest of this chapter, we are going to explore how such a system, along
    with Apache Spark, can be used to build a lakehouse that provides the aforementioned
    properties. Of these three systems, so far Delta Lake has the tightest integration
    with Apache Spark data sources (both for batch and streaming workloads) and SQL
    operations (e.g., `MERGE`). Hence, we will use Delta Lake as the vehicle for further
    exploration.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将探讨如何使用 Apache Spark 系统来构建提供上述属性的 lakehouse。在这三个系统中，到目前为止 Delta Lake
    与 Apache Spark 数据源（用于批处理和流处理工作负载）以及 SQL 操作（例如`MERGE`）的整合最紧密。因此，我们将使用 Delta Lake
    进一步探索。
- en: Note
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This project is called Delta Lake because of its analogy to streaming. Streams
    flow into the sea to create deltas—this is where all of the sediments accumulate,
    and thus where the valuable crops are grown. Jules S. Damji (one of our coauthors)
    came up with this!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目被称为 Delta Lake，因为它类似于流。溪流流入海洋形成三角洲，这里是所有沉积物积累的地方，因此也是有价值的农作物生长的地方。朱尔斯·S·达姆吉（我们的合著者之一）提出了这个比喻！
- en: Building Lakehouses with Apache Spark and Delta Lake
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Apache Spark 和 Delta Lake 构建 Lakehouse
- en: 'In this section, we are going to take a quick look at how Delta Lake and Apache
    Spark can be used to build lakehouses. Specifically, we will explore the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将快速了解 Delta Lake 和 Apache Spark 如何用于构建 lakehouse。具体来说，我们将探索以下内容：
- en: Reading and writing Delta Lake tables using Apache Spark
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Apache Spark 读写 Delta Lake 表格
- en: How Delta Lake allows concurrent batch and streaming writes with ACID guarantees
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delta Lake 如何允许并发批处理和流式写入，并提供 ACID 保证
- en: How Delta Lake ensures better data quality by enforcing schema on all writes,
    while allowing for explicit schema evolution
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delta Lake 如何通过在所有写操作上强制执行模式并允许显式模式演变来确保更好的数据质量
- en: Building complex data pipelines using update, delete, and merge operations,
    all of which ensure ACID guarantees
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更新、删除和合并操作构建复杂的数据管道，所有操作均保证 ACID 保证
- en: Auditing the history of operations that modified a Delta Lake table and traveling
    back in time by querying earlier versions of the table
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审计修改 Delta Lake 表格的操作历史，并通过查询早期版本的表格实现时间旅行
- en: The data we will use in this section is a modified version (a subset of columns
    in Parquet format) of the public [Lending Club Loan Data](https://oreil.ly/P7AR-).^([1](ch09.html#ch01fn12))
    It includes all funded loans from 2012 to 2017\. Each loan record includes applicant
    information provided by the applicant as well as the current loan status (current,
    late, fully paid, etc.) and latest payment information.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中使用的数据是公共[Lending Club Loan Data](https://oreil.ly/P7AR-)的修改版本（Parquet 格式中的列子集）。^([1](ch09.html#ch01fn12))
    它包括了2012年至2017年间所有资助的贷款记录。每条贷款记录包括申请人提供的申请信息以及当前贷款状态（当前、逾期、已完全还清等）和最新的付款信息。
- en: Configuring Apache Spark with Delta Lake
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Apache Spark 配置 Delta Lake
- en: 'You can configure Apache Spark to link to the Delta Lake library in one of
    the following ways:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下任一方式配置 Apache Spark 以链接到 Delta Lake 库：
- en: Set up an interactive shell
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一个交互式 shell
- en: 'If you’re using Apache Spark 3.0, you can start a PySpark or Scala shell with
    Delta Lake by using the following command-line argument:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用 Apache Spark 3.0，可以通过以下命令行参数启动 PySpark 或 Scala shell，并与 Delta Lake 一起使用：
- en: '[PRE0]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For example:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE1]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you are running Spark 2.4, you have to use Delta Lake 0.6.0.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在运行 Spark 2.4，则必须使用 Delta Lake 0.6.0。
- en: Set up a standalone Scala/Java project using Maven coordinates
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Maven 坐标设置一个独立的 Scala/Java 项目
- en: 'If you want to build a project using Delta Lake binaries from the Maven Central
    repository, you can add the following Maven coordinates to the project dependencies:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要使用 Maven 中央仓库中的 Delta Lake 二进制文件构建项目，可以将以下 Maven 坐标添加到项目依赖项中：
- en: '[PRE2]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Again, if you are running Spark 2.4 you have to use Delta Lake 0.6.0.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果您正在运行 Spark 2.4，则必须使用 Delta Lake 0.6.0。
- en: Note
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: See the [Delta Lake documentation](https://oreil.ly/MmlC3) for the most up-to-date
    information.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[Delta Lake 文档](https://oreil.ly/MmlC3)获取最新信息。
- en: Loading Data into a Delta Lake Table
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据到 Delta Lake 表格中
- en: 'If you are used to building data lakes with Apache Spark and any of the structured
    data formats—say, Parquet—then it is very easy to migrate existing workloads to
    use the Delta Lake format. All you have to do is change all the DataFrame read
    and write operations to use `format("delta")` instead of `format("parquet")`.
    Let’s try this out with some of the aforementioned loan data, which is available
    [as a Parquet file](https://oreil.ly/7pP1y). First let’s read this data and save
    it as a Delta Lake table:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您习惯于使用Apache Spark和任何结构化数据格式（比如Parquet）构建数据湖，那么很容易将现有工作负载迁移到使用Delta Lake格式。您只需将所有DataFrame读写操作更改为使用`format("delta")`而不是`format("parquet")`。让我们试试将一些前述的贷款数据，作为[Parquet文件](https://oreil.ly/7pP1y)，首先读取这些数据并保存为Delta
    Lake表格：
- en: '[PRE3]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we can read and explore the data as easily as any other table:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以像处理任何其他表格一样轻松读取和探索数据：
- en: '[PRE5]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Loading Data Streams into a Delta Lake Table
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据流加载到Delta Lake表格中
- en: 'As with static DataFrames, you can easily modify your existing Structured Streaming
    jobs to write to and read from a Delta Lake table by setting the format to `"delta"`.
    Say you have a stream of new loan data as a DataFrame named `newLoanStreamDF`,
    which has the same schema as the table. You can append to the table as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与静态DataFrame一样，您可以通过将格式设置为`"delta"`轻松修改现有的结构化流作业以写入和读取Delta Lake表格。假设您有一个名为`newLoanStreamDF`的DataFrame，其模式与表格相同。您可以如下追加到表格中：
- en: '[PRE6]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With this format, just like any other, Structured Streaming offers end-to-end
    exactly-once guarantees. However, Delta Lake has a few additional advantages over
    traditional formats like JSON, Parquet, or ORC:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此格式，就像任何其他格式一样，结构化流提供端到端的幂等保证。但是，Delta Lake相比于传统格式（如JSON、Parquet或ORC）具有一些额外的优势：
- en: It allows writes from both batch and streaming jobs into the same table
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 允许批处理和流作业向同一表格写入
- en: With other formats, data written into a table from a Structured Streaming job
    will overwrite any existing data in the table. This is because the metadata maintained
    in the table to ensure exactly-once guarantees for streaming writes does not account
    for other nonstreaming writes. Delta Lake’s advanced metadata management allows
    both batch and streaming data to be written.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用其他格式，从结构化流作业写入表格的数据将覆盖表格中的任何现有数据。这是因为表格中维护的元数据用于确保流写入的幂等性，并不考虑其他非流写入。Delta
    Lake的高级元数据管理允许同时写入批处理和流数据。
- en: It allows multiple streaming jobs to append data to the same table
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 允许多个流作业向同一表格追加数据
- en: The same limitation of metadata with other formats also prevents multiple Structured
    Streaming queries from appending to the same table. Delta Lake’s metadata maintains
    transaction information for each streaming query, thus enabling any number of
    streaming queries to concurrently write into a table with exactly-once guarantees.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他格式的元数据相同限制也会阻止多个结构化流查询向同一表格追加数据。Delta Lake的元数据为每个流查询维护事务信息，从而使任意数量的流查询能够并发写入具有幂等保证的表格。
- en: It provides ACID guarantees even under concurrent writes
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在并发写入的情况下也提供ACID保证
- en: Unlike built-in formats, Delta Lake allows concurrent batch and streaming operations
    to write data with ACID guarantees.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 与内置格式不同，Delta Lake允许并发批处理和流操作写入具有ACID保证的数据。
- en: Enforcing Schema on Write to Prevent Data Corruption
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写入时强制执行模式以防止数据损坏
- en: A common problem with managing data with Spark using common formats like JSON,
    Parquet, and ORC is accidental data corruption caused by writing incorrectly formatted
    data. Since these formats define the data layout of individual files and not of
    the entire table, there is no mechanism to prevent any Spark job from writing
    files with different schemas into existing tables. This means there are no guarantees
    of consistency for the entire table of many Parquet files.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark管理数据时的常见问题是使用JSON、Parquet和ORC等常见格式时由于错误格式的数据写入而导致的意外数据损坏。由于这些格式定义了单个文件的数据布局而不是整个表格的布局，因此没有机制防止任何Spark作业将具有不同模式的文件写入到现有表中。这意味着对于由多个Parquet文件组成的整个表格，不存在一致性保证。
- en: 'The Delta Lake format records the schema as table-level metadata. Hence, all
    writes to a Delta Lake table can verify whether the data being written has a schema
    compatible with that of the table. If it is not compatible, Spark will throw an
    error before any data is written and committed to the table, thus preventing such
    accidental data corruption. Let’s test this by trying to write some data with
    an additional column, `closed`, that signifies whether the loan has been terminated.
    Note that this column does not exist in the table:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 格式将模式记录为表级元数据。因此，对 Delta Lake 表的所有写操作都可以验证正在写入的数据是否与表的模式兼容。如果不兼容，Spark
    将在写入和提交数据之前抛出错误，从而防止此类意外数据损坏。让我们通过尝试写入带有额外列`closed`的一些数据来测试这一点，该列表示贷款是否已终止。请注意，该列在表中不存在：
- en: '[PRE8]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This write will fail with the following error message:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此写入将失败，并显示以下错误消息：
- en: '[PRE10]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This illustrates how Delta Lake blocks writes that do not match the schema of
    the table. However, it also gives a hint about how to actually evolve the schema
    of the table using the option `mergeSchema`, as discussed next.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了 Delta Lake 如何阻止不匹配表模式的写入。然而，它也提供了如何使用选项 `mergeSchema` 实际演进表模式的提示，接下来将进行讨论。
- en: Evolving Schemas to Accommodate Changing Data
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适应变化数据的演进模式
- en: 'In our world of ever-changing data, it is possible that we might want to add
    this new column to the table. This new column can be explicitly added by setting
    the option `"mergeSchema"` to `"true"`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们这个变化不断的世界中，我们可能希望将这个新列添加到表中。可以通过设置选项 `"mergeSchema"` 为 `"true"` 显式地添加这个新列：
- en: '[PRE11]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With this, the column `closed` will be added to the table schema, and new data
    will be appended. When existing rows are read, the value of the new column is
    considered as `NULL`. In Spark 3.0, you can also use the SQL DDL command `ALTER
    TABLE` to add and modify columns.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个操作，列 `closed` 将被添加到表模式中，并且新数据将被追加。当读取现有行时，新列的值将被视为 `NULL`。在 Spark 3.0 中，您还可以使用
    SQL DDL 命令 `ALTER TABLE` 来添加和修改列。
- en: Transforming Existing Data
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换现有数据
- en: Delta Lake supports the DML commands `UPDATE`, `DELETE`, and `MERGE`, which
    allow you to build complex data pipelines. These commands can be invoked using
    Java, Scala, Python, and SQL, giving users the flexibility of using the commands
    with any APIs they are familiar with, using either DataFrames or tables. Furthermore,
    each of these data modification operations ensures ACID guarantees.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 支持 `UPDATE`、`DELETE` 和 `MERGE` 等 DML 命令，允许您构建复杂的数据管道。这些命令可以使用 Java、Scala、Python
    和 SQL 调用，使用户能够使用他们熟悉的任何 API，无论是使用 DataFrames 还是表。此外，每个数据修改操作都确保 ACID 保证。
- en: Let’s explore this with a few examples of real-world use cases.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过几个实际用例来探索这一点。
- en: Updating data to fix errors
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更新数据以修复错误
- en: 'A common use case when managing data is fixing errors in the data. Suppose,
    upon reviewing the data, we realized that all of the loans assigned to `addr_state
    = ''OR''` should have been assigned to `addr_state = ''WA''`. If the loan table
    were a Parquet table, then to do such an update we would need to:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在管理数据时，一个常见的用例是修复数据中的错误。假设在查看数据时，我们意识到所有分配给 `addr_state = 'OR'` 的贷款都应该分配给 `addr_state
    = 'WA'`。如果贷款表是 Parquet 表，那么要执行这样的更新，我们需要：
- en: Copy all of the rows that are not affected into a new table.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有未受影响的行复制到一个新表中。
- en: Copy all of the rows that are affected into a DataFrame, then perform the data
    modification.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有受影响的行复制到一个 DataFrame 中，然后执行数据修改。
- en: Insert the previously noted DataFrame’s rows into the new table.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将之前提到的 DataFrame 的行插入到新表中。
- en: Remove the old table and rename the new table to the old table name.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除旧表并将新表重命名为旧表名。
- en: 'In Spark 3.0, which added direct support for DML SQL operations like `UPDATE`,
    `DELETE`, and `MERGE`, instead of manually performing all these steps you can
    simply run the SQL `UPDATE` command. However, with a Delta Lake table, users can
    run this operation too, by using Delta Lake’s programmatic APIs as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 3.0 中，直接支持像 `UPDATE`、`DELETE` 和 `MERGE` 这样的 DML SQL 操作，而不是手动执行所有这些步骤，您可以简单地运行
    SQL `UPDATE` 命令。然而，对于 Delta Lake 表，用户也可以通过使用 Delta Lake 的编程 API 来运行此操作，如下所示：
- en: '[PRE13]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Deleting user-related data
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 删除与用户相关的数据
- en: 'With data protection policies like the EU’s [General Data Protection Regulation
    (GDPR)](https://oreil.ly/hOdBE) coming into force, it is more important now than
    ever to be able to delete user data from all your tables. Say it is mandated that
    you have to delete the data on all loans that have been fully paid off. With Delta
    Lake, you can do the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 随着类似欧盟[通用数据保护条例（GDPR）](https://oreil.ly/hOdBE)这样的数据保护政策的实施，现在比以往任何时候都更重要能够从所有表中删除用户数据。假设您必须删除所有已完全偿还贷款的数据。使用Delta
    Lake，您可以执行以下操作：
- en: '[PRE15]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Similar to updates, with Delta Lake and Apache Spark 3.0 you can directly run
    the `DELETE` SQL command on the table.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与更新类似，在Delta Lake和Apache Spark 3.0中，您可以直接在表上运行`DELETE` SQL命令。
- en: Upserting change data to a table using merge()
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用merge()向表中插入变更数据
- en: 'A common use case is change data capture, where you have to replicate row changes
    made in an OLTP table to another table for OLAP workloads. To continue with our
    loan data example, say we have another table of new loan information, some of
    which are new loans and others of which are updates to existing loans. In addition,
    let’s say this `changes` table has the same schema as the `loan_delta` table.
    You can upsert these changes into the table using the `DeltaTable.merge()` operation,
    which is based on the `MERGE` SQL command:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的用例是变更数据捕获，您必须将OLTP表中的行更改复制到另一个表中，以供OLAP工作负载使用。继续我们的贷款数据示例，假设我们有另一张新贷款信息表，其中一些是新贷款，另一些是对现有贷款的更新。此外，假设此`changes`表与`loan_delta`表具有相同的架构。您可以使用基于`MERGE`
    SQL命令的`DeltaTable.merge()`操作将这些变更插入表中：
- en: '[PRE17]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As a reminder, you can run this as a SQL `MERGE` command starting with Spark
    3.0\. Furthermore, if you have a stream of such captured changes, you can continuously
    apply those changes using a Structured Streaming query. The query can read the
    changes in micro-batches (see [Chapter 8](ch08.html#structured_streaming)) from
    any streaming source, and use `foreachBatch()` to apply the changes in each micro-batch
    to the Delta Lake table.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，您可以将此作为SQL `MERGE`命令在Spark 3.0中运行。此外，如果您有这类捕获变更的流，您可以使用Structured Streaming查询连续应用这些变更。查询可以从任何流源中的微批次（参见[第8章](ch08.html#structured_streaming)）读取变更，并使用`foreachBatch()`将每个微批次中的变更应用于Delta
    Lake表中。
- en: Deduplicating data while inserting using insert-only merge
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用仅插入合并去重数据
- en: 'The merge operation in Delta Lake supports an extended syntax beyond that specified
    by the ANSI standard, including advanced features like the following:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake中的合并操作支持比ANSI标准指定的更多扩展语法，包括以下高级特性：
- en: Delete actions
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 删除操作
- en: For example, `MERGE ... WHEN MATCHED THEN DELETE`.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`MERGE ... WHEN MATCHED THEN DELETE`。
- en: Clause conditions
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 子句条件
- en: For example, `MERGE ... WHEN MATCHED AND *<condition>* THEN ...`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`MERGE ... WHEN MATCHED AND *<condition>* THEN ...`。
- en: Optional actions
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 可选操作
- en: All the `MATCHED` and `NOT MATCHED` clauses are optional.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 所有`MATCHED`和`NOT MATCHED`子句都是可选的。
- en: Star syntax
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 星号语法
- en: For example, `UPDATE *` and `INSERT *` to update/insert all the columns in the
    target table with matching columns from the source data set. The equivalent Delta
    Lake APIs are `updateAll()` and `insertAll()`, which we saw in the previous section.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`UPDATE *`和`INSERT *`以将源数据集中匹配列的所有列更新/插入目标表中。等效的Delta Lake API是`updateAll()`和`insertAll()`，我们在前一节中看到了。
- en: 'This allows you to express many more complex use cases with little code. For
    example, say you want to backfill the `loan_delta` table with historical data
    on past loans. But some of the historical data may already have been inserted
    in the table, and you don’t want to update those records because they may contain
    more up-to-date information. You can deduplicate by the `loan_id` while inserting
    by running the following merge operation with only the `INSERT` action (since
    the `UPDATE` action is optional):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这使您能够用很少的代码表达更多复杂的用例。例如，假设您希望为`loan_delta`表回填历史数据。但是，一些历史数据可能已经插入了表中，您不希望更新这些记录，因为它们可能包含更为更新的信息。您可以通过`loan_id`进行插入时进行去重，运行以下仅包含`INSERT`操作的合并操作（因为`UPDATE`操作是可选的）：
- en: '[PRE19]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: There are even more complex use cases, like CDC with deletes and SCD tables,
    that are made simple with the extended merge syntax. Refer to the [documentation](https://oreil.ly/XBag7)
    for more details and examples.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更复杂的用例，例如包括删除和SCD表的CDC，使用扩展合并语法变得简单。请参阅[文档](https://oreil.ly/XBag7)获取更多详细信息和示例。
- en: Auditing Data Changes with Operation History
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过操作历史审核数据更改
- en: 'All of the changes to your Delta Lake table are recorded as commits in the
    table’s transaction log. As you write into a Delta Lake table or directory, every
    operation is automatically versioned. You can query the table’s operation history
    as noted in the following code snippet:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 所有对 Delta Lake 表的更改都记录在表的事务日志中作为提交。当你向 Delta Lake 表或目录写入时，每个操作都会自动进行版本控制。你可以像以下代码片段中所示查询表的操作历史记录：
- en: '[PRE21]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'By default this will show a huge table with many versions and a lot of columns.
    Let’s instead print some of the key columns of the last three operations:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，这将显示一个包含许多版本和大量列的巨大表格。我们可以打印最后三个操作的一些关键列：
- en: '[PRE22]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This will generate the following output:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE24]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note the `operation` and `operationParameters` that are useful for auditing
    the changes.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 注意对审核更改有用的 `operation` 和 `operationParameters`。
- en: Querying Previous Snapshots of a Table with Time Travel
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用时间旅行查询表的先前快照
- en: 'You can query previous versioned snapshots of a table by using the `DataFrameReader`
    options `"versionAsOf"` and `"timestampAsOf"`. Here are a few examples:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `DataFrameReader` 选项 `"versionAsOf"` 和 `"timestampAsOf"` 查询表的以前版本化的快照。以下是几个例子：
- en: '[PRE25]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This is useful in a variety of situations, such as:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这在各种情况下都很有用，例如：
- en: Reproducing machine learning experiments and reports by rerunning the job on
    a specific table version
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过重新运行特定表版本上的作业复现机器学习实验和报告
- en: Comparing the data changes between different versions for auditing
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较不同版本之间的数据变化以进行审核
- en: Rolling back incorrect changes by reading a previous snapshot as a DataFrame
    and overwriting the table with it
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过读取前一个快照作为 DataFrame 并用其覆盖表格来回滚不正确的更改
- en: Summary
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter examined the possibilities for building reliable data lakes using
    Apache Spark. To recap, databases have solved data problems for a long time, but
    they fail to fulfill the diverse requirements of modern use cases and workloads.
    Data lakes were built to alleviate some of the limitations of databases, and Apache
    Spark is one of the best tools to build them with. However, data lakes still lack
    some of the key features provided by databases (e.g., ACID guarantees). Lakehouses
    are the next generation of data solutions, which aim to provide the best features
    of databases and data lakes and meet all the requirements of diverse use cases
    and workloads.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了使用 Apache Spark 构建可靠数据湖的可能性。简而言之，数据库长期以来解决了数据问题，但未能满足现代用例和工作负载的多样化需求。数据湖的建立旨在减轻数据库的一些限制，而
    Apache Spark 是构建它们的最佳工具之一。然而，数据湖仍然缺乏数据库提供的一些关键特性（例如 ACID 保证）。Lakehouse 是数据解决方案的下一代，旨在提供数据库和数据湖的最佳功能，并满足多样化用例和工作负载的所有要求。
- en: 'We briefly explored a couple of open source systems (Apache Hudi and Apache
    Iceberg) that can be used to build lakehouses, then took a closer look at Delta
    Lake, a file-based open source storage format that, along with Apache Spark, is
    a great building block for lakehouses. As you saw, it provides the following:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要探讨了几个开源系统（Apache Hudi 和 Apache Iceberg），可以用来构建 Lakehouse，然后更详细地了解了 Delta
    Lake，这是一个基于文件的开源存储格式，与 Apache Spark 一起是构建 Lakehouse 的优秀基础模块。正如你所看到的，它提供以下功能：
- en: Transactional guarantees and schema management, like databases
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像数据库一样的事务保证和架构管理
- en: Scalability and openness, like data lakes
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可伸缩性和开放性，如数据湖
- en: Support for concurrent batch and streaming workloads with ACID guarantees
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持具有 ACID 保证的并发批处理和流处理工作负载
- en: Support for transformation of existing data using update, delete, and merge
    operations that ensure ACID guarantees
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持使用更新、删除和合并操作对现有数据进行转换，以确保 ACID 保证。
- en: Support for versioning, auditing of operation history, and querying of previous
    versions
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持版本控制、操作历史的审计和查询以前的版本
- en: In the next chapter, we’ll explore how to begin building ML models using Spark’s
    MLlib.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何开始使用 Spark 的 MLlib 构建 ML 模型。
- en: ^([1](ch09.html#ch01fn12-marker)) A full view of the data is available at [this
    Excel file](https://oreil.ly/Rgtn1).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.html#ch01fn12-marker)) 可以在[此 Excel 文件](https://oreil.ly/Rgtn1)中查看完整的数据视图。
