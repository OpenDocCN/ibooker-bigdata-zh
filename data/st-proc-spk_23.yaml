- en: Chapter 17\. The Spark Streaming Programming Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第17章。Spark Streaming编程模型
- en: In [Chapter 16](ch16.xhtml#sps-intro), you learned about Spark Streaming’s central
    abstraction, the DStream, and how it blends a microbatch execution model with
    a functional programming API to deliver a complete foundation for stream processing
    on Spark.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第16章](ch16.xhtml#sps-intro)中，您学习了Spark Streaming的核心抽象——DStream，以及它如何将微批执行模型与函数式编程API融合，以提供Spark上流处理的完整基础。
- en: In this chapter, we explore the API offered by the DStream abstraction that
    enables the implementation of arbitrarily complex business logic in a streaming
    fashion. From an API perspective, DStreams delegate much of their work to the
    underlying data structure in Spark, the Resilient Distributed Dataset (RDD). Before
    we delve into the details of the DStream API, we are going to take a quick tour
    of the RDD abstraction. A good understanding of the RDD concept and API is essential
    to comprehend how DStreams work.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探索了DStream抽象提供的API，该API允许以流式方式实现任意复杂的业务逻辑。从API的角度来看，DStreams将他们的大部分工作委托给Spark中的底层数据结构——弹性分布式数据集（RDD）。在我们深入了解DStream
    API的细节之前，我们将快速浏览RDD抽象。理解RDD的概念和API对理解DStreams的工作方式至关重要。
- en: RDDs as the Underlying Abstraction for DStreams
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD作为DStreams的基础抽象
- en: 'Spark has one single data structure as a base element of its API and libraries:
    RDD. This is a polymorphic collection that represents a bag of elements, in which
    the data to be analyzed is represented as an arbitrary Scala type. The dataset
    is distributed across the executors of the cluster and processed using those machines.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在其API和库中只有一个数据结构作为基本元素：RDD。这是一个多态集合，表示一袋元素，其中要分析的数据表示为任意Scala类型。数据集分布在集群的执行器上，并使用这些机器进行处理。
- en: Note
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since the introduction of Spark SQL, the `DataFrame` and `Dataset` abstractions
    are the recommended programming interfaces to Spark. In the most recent versions,
    only library programmers are required to know about the RDD API and its runtime.
    Although RDDs are not often visible at the interface level, they still drive the
    core engine’s distributed computing capabilities.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 自引入Spark SQL以来，`DataFrame`和`Dataset`抽象已成为推荐的Spark编程接口。在最新版本中，只有库程序员需要了解RDD API及其运行时。尽管在接口级别上很少看到RDD，但它们仍驱动核心引擎的分布式计算能力。
- en: To understand how Spark works, some basic knowledge of RDD-level programming
    is highly recommended. In the following section, we cover only the most notable
    elements. For a more in-depth treatment of the RDD programming model, we refer
    you to [[Karau2015]](app01.xhtml#Karau2015).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解Spark的工作原理，强烈建议掌握RDD级别编程的基本知识。在接下来的章节中，我们仅涵盖最显著的元素。如果想更深入地了解RDD编程模型，请参考[[Karau2015]](app01.xhtml#Karau2015)。
- en: 'Using those RDDs involves (mostly) calling functions on the RDD collection
    type. The functions in this API are higher-order functions. In that sense, programming
    in Spark involves functional programming at its core: indeed, a programming language
    is considered to be functional when, in particular, it’s able to define a function
    anywhere: as an argument, as a variable, or more generally as a syntax element.
    But, more importantly, on the programming languages theory level, a language becomes
    a functional programming language only when it is able to pass functions as arguments.
    In the example that follows, we see how Spark lets you use an implementation of
    `map` to transform all of the values of a collection by applying an arbitrary
    function to every single element.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这些RDD时，主要是在RDD集合类型上调用函数。该API中的函数是高阶函数。从这个意义上讲，使用Spark编程核心涉及到函数式编程：实际上，当一个编程语言能够在任何地方定义函数时，它被认为是函数式的：作为参数、作为变量，或更一般地作为语法元素。但更重要的是，在编程语言理论层面上，只有当它能够将函数作为参数传递时，一个语言才成为函数式编程语言。在接下来的示例中，我们将看到Spark如何让你使用`map`的实现来通过将任意函数应用于每个单独元素来转换集合的所有值。
- en: 'In the following example, we read a text file containing frequent last names
    from census data as an `RDD[String]` (read as *RDD of Strings*). Then, we obtain
    the length of those names using a `map` operation that transforms the initial
    name, represented as a `String` into its length:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们读取一个包含人口普查数据中频繁姓氏的文本文件作为`RDD[String]`（读作*字符串的RDD*）。然后，我们使用`map`操作获取这些名字的长度，该操作将初始名称表示为`String`并转换为其长度：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Spark also provides us with a `reduce` function, which lets us aggregate key
    elements of a collection into another result, obtained through iterative composition.
    We are also going to use the `count` operation, which computes the number of elements
    in an RDD. Let’s have a look:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 还为我们提供了 `reduce` 函数，它允许我们将集合的关键元素聚合成另一个结果，通过迭代组合获得。我们还将使用 `count` 操作，它计算
    RDD 中元素的数量。让我们来看一下：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It’s worth noting that `reduce` requires that the RDD is not empty. Otherwise,
    it will throw a `java.lang.UnsupportedOperationException` exception with the message:
    `empty collection`. This might seem like an extreme case given that we are in
    the midst of a discussion about processing large datasets, but it becomes necessary
    when we want to process incoming data in real-time.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，`reduce` 要求 RDD 不为空。否则，它会抛出 `java.lang.UnsupportedOperationException`
    异常，并显示消息：`empty collection`。虽然我们讨论的是处理大型数据集，这似乎是个极端情况，但在我们想要实时处理传入数据时，这变得必要。
- en: To overcome this limitation, we could use `fold`, an aggregator similar to `reduce`.
    Besides the reduction function, `fold` lets us define an initial “zero” element
    to use with the aggregation function, therefore working properly even with empty
    RDDs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个限制，我们可以使用 `fold`，这是一个类似于 `reduce` 的聚合器。除了减少函数，`fold` 还允许我们定义一个初始的“零”元素，以便与聚合函数一起正常工作，即使
    RDD 为空。
- en: '`fold` and `reduce` both use an aggregation function closed over the RDD type.
    Hence, we could sum an RDD of `Int`s or calculate the `min` or `max` of an RDD
    of cartesian coordinates according to a measure. There are cases for which we
    would like to return a different type than the data represented by the RDD. The
    more general `aggregate` function lets you determine how to combine disparate
    input and output types in an intermediate step:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`fold` 和 `reduce` 都使用了一个封闭于 RDD 类型的聚合函数。因此，我们可以对 `Int` 类型的 RDD 进行求和，或者根据度量计算
    RDD 的笛卡尔坐标的 `min` 或 `max`。有些情况下，我们希望返回与 RDD 所表示的数据类型不同的类型。更一般的 `aggregate` 函数让你确定如何在中间步骤中组合不同的输入和输出类型：'
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It is the ease of use of this API that has won Spark RDDs the nickname of "*the
    ultimate Scala collections*.” This reference to Spark’s original implementation
    programming language points at the collections library of Scala, which already
    lets us benefit from functional programming on a single machine with a rich API.
    It lets us expand our data-processing vocabulary from the basic `map` and `reduce`
    from the original MapReduce model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此 API 的易用性使得 Spark RDD 被赋予了“*终极 Scala 集合*”的绰号。这个提到 Spark 原始实现编程语言的引用指向 Scala
    的集合库，该库已经让我们能够在单台机器上享受到丰富 API 的函数式编程。它让我们可以从最初的 MapReduce 模型的基本 `map` 和 `reduce`
    扩展我们的数据处理词汇。
- en: 'The real genius of Spark is that it reproduces the ease of use of the Scala
    API, and scales it up to operate over a cluster of computing resources.The RDD
    API defines two broad families of functions: transformations and actions. Transformations,
    like `map` or `filter`, let us work on the immutable data contained by the RDD
    by creating new RDDs as the result of applying a transformation to its *parent*.
    This chain of RDD transformations forms a directed acyclic graph or DAG that informs
    Spark where the raw data is and how it needs to be transformed into the desired
    result. All transformations are declarative and lazy. That means that they do
    not result in data being actually processed. To obtain results, we need to materialize
    the chain of transformations by issuing an *action*. Actions trigger the distributed
    execution of data operations and produce a tangible result. It can be writing
    to a file or printing samples on the screen.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的真正的天才之处在于，它复制了 Scala API 的易用性，并将其扩展到可以在计算资源集群上运行。RDD API 定义了两类广泛的函数：转换和操作。转换，如
    `map` 或 `filter`，让我们通过对其 *parent* 应用转换的结果创建新的 RDD 来处理 RDD 中包含的不可变数据。这些 RDD 转换链形成了一个指向
    Spark 原始数据在哪里以及如何将其转换为期望结果的有向无环图或 DAG。所有的转换都是声明式和延迟执行的。这意味着它们不会导致数据实际被处理。要获取结果，我们需要通过发出
    *action* 来实现转换链的材料化。操作触发了数据操作的分布式执行，并产生具体的结果。它可以是写入文件或在屏幕上打印样本。
- en: Additional functional operations, such as `flatMap`, `groupBy`, and `zip` are
    also available. You can also find RDD combinators, like `join` and `cogroup`,
    that allow you to merge two or more existing RDDs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他的函数式操作，例如 `flatMap`、`groupBy` 和 `zip` 也是可用的。你还可以找到 RDD 的组合子，比如 `join` 和
    `cogroup`，允许你合并两个或多个现有的 RDD。
- en: Understanding DStream Transformations
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 DStream 转换
- en: The DStream programming API consists of transformations or higher-order functions
    that take another function as an argument. At the API level, a `DStream[T]` is
    a strongly typed data structure that represents a stream of data of type `T`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DStream 编程 API 包括转换或高阶函数，这些函数以另一个函数作为参数。在 API 层面上，`DStream[T]` 是一个强类型数据结构，表示类型为
    `T` 的数据流。
- en: DStreams are immutable. This means that we cannot mutate their contents in place.
    Instead, we implement our intended application logic by applying a series of *transformations*
    to the input DStream. Each transformation creates a new DStream that represents
    the transformed data from the parent DStream. DStream transformations are lazy,
    meaning that the underlying data is not actually processed until the system needs
    to materialize a result. In the case of DStreams, this materialization process
    is triggered when we want to produce results to a stream sink through a particular
    operation known as an *output operation*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DStreams 是不可变的。这意味着我们不能直接在其内容中进行突变。相反，我们通过对输入 DStream 应用一系列*转换*来实现我们预期的应用逻辑。每个转换都会创建一个新的
    DStream，表示从父 DStream 转换后的数据。DStream 转换是惰性的，这意味着在系统需要实现结果之前，底层数据实际上不会被处理。对于 DStreams，这个实现过程是通过触发特定操作（称为*输出操作*）来产生结果到流式汇中。
- en: Note
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For readers coming from a functional programming background, it is evident that
    DStream transformations can be considered pure functions, whereas output operations
    are side-effecting functions that produce a result to an external system.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于来自函数式编程背景的读者，显然可以将 DStream 转换视为纯函数，而输出操作则是产生结果到外部系统的具有副作用的函数。
- en: 'Let’s review these concepts using the code we used earlier in the introduction:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们在介绍中早期使用的代码来复习这些概念：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, the `textDStream` contains lines of text. Using a `map` transformation,
    we apply a fairly naive error-counting function to each line in the original `DStream[String]`,
    resulting in a new `DStream[(Long, Long)]` of `long` tuples. In this case, `map`
    is a DStream transformation and it takes a function applicable to its contents,
    `String`s in this case, to produce another DStream with the transformed contents.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`textDStream` 包含文本行。使用 `map` 转换，我们对原始的 `DStream[String]` 中的每一行应用一个相当朴素的错误计数函数，结果得到一个新的
    `DStream[(Long, Long)]`，包含 `long` 元组。在这种情况下，`map` 是一个 DStream 转换，它接受一个可应用于其内容的函数，本例中是
    `String`，以产生另一个转换后的 DStream。
- en: DStreams are a streaming abstraction in which the elements of the stream are
    grouped into microbatches over a time dimension, as we illustrate in [Figure 17-1](#sps-streaming-model).
    In turn, each microbatch is represented by an RDD. At the execution level, the
    main task of Spark Streaming is to schedule and manage the timely collection and
    delivery of data blocks to Spark. In turn, the Spark core engine will apply the
    programmed sequence of operations to the data that constitute our application
    logic.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: DStreams 是一种流抽象，其中流的元素在时间维度上被分组为微批次，正如我们在 [图 17-1](#sps-streaming-model) 中展示的那样。每个微批次由一个
    RDD 表示。在执行层面上，Spark Streaming 的主要任务是调度和管理数据块的及时收集和传递给 Spark。然后，Spark 核心引擎将应用程序逻辑中的编程操作序列应用于数据。
- en: '![spas 1701](Images/spas_1701.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![spas 1701](Images/spas_1701.png)'
- en: Figure 17-1\. Streaming model mapped on Spark Streaming
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-1\. 映射到 Spark Streaming 上的流模型
- en: Going back to how this is reflected in the API, we see that there are operators,
    like the classical `map`, `filter`, and so on that operate on a single element.
    These operations follow the same principles of distributed execution and abide
    by the same serialization constraints as their batch Spark counterparts.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 回到如何在 API 中反映这一点，我们看到有像经典的 `map`、`filter` 等操作符，这些操作符作用于单个元素。这些操作遵循分布式执行的相同原则，并且遵循批量
    Spark 对应项的相同序列化约束。
- en: There are also operators, like `transform` and `foreachRDD`, that operate on
    an RDD instead of an element. These operators are executed by the Spark Streaming
    scheduler and the functions provided to them run in the context of the driver.
    It is within the scope of these operators that we can implement logic that crosses
    the boundary of microbatches, like bookkeeping history or maintaining application-level
    counters. They also provide access to all the Spark execution contexts. Within
    these operators, we can interact with Spark SQL, Spark ML, or even manage the
    life cycle of the streaming application. These operations are true bridges between
    the recurrent streaming microbatch scheduling, the element-level transformations,
    and the Spark runtime context.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些操作符，如`transform`和`foreachRDD`，它们操作的是RDD而不是单个元素。这些操作符由Spark Streaming调度器执行，并且提供给它们的函数在驱动程序的上下文中运行。在这些操作符的范围内，我们可以实现跨微批次边界的逻辑，如历史记录或维护应用程序级计数器。它们还提供对所有Spark执行上下文的访问。在这些操作符内部，我们可以与Spark
    SQL、Spark ML甚至管理流应用程序的生命周期进行交互。这些操作是重复流微批次调度、元素级转换和Spark运行时上下文之间真正的桥梁。
- en: 'Using this distinction, we can observe two broad groups of transformations:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种区分，我们可以观察到两种广义上的转换组：
- en: Element-centric DStream transformations
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以元素为中心的DStream转换
- en: Transformations that apply to a single element of the stream.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于流的单个元素的转换。
- en: RDD-centric DStream transformations
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以RDD为中心的DStream转换。
- en: Transformations that apply to the underlying RDD of each microbatch.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于每个微批次的基础RDD的转换。
- en: 'Besides this general classification, we also find two other classes of transformations
    in the DStream API:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这一般分类外，我们还发现DStream API中的另外两类转换：
- en: Counting transformations
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 计数转换
- en: Dedicated operations to count elements in a stream.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 专门的操作来统计流中的元素。
- en: Structure-modifying transformations
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 修改结构的转换
- en: Transformations that change the internal structure or organization of the DStream
    but do not change the contents.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 改变DStream的内部结构或组织但不改变内容的转换。
- en: In the rest of this chapter, we study these transformations in detail.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们详细研究这些转换。
- en: Element-Centric DStream Transformations
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以元素为中心的DStream转换
- en: In general, the element-centric transformations on the DStream API mirror functions
    also available in the RDD API, unifying the development experience among batch
    and streaming modes in Spark.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，DStream API上的以元素为中心的转换与RDD API中同样可用的函数相对应，统一了Spark中批处理和流处理模式的开发体验。
- en: 'The most commonly used transformations follow:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的转换如下所示：
- en: Note
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The signature of each transformation has been simplified by removing implicit
    parameters to make the signature more concise, where applicable.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个转换的签名都已经简化，去掉了隐式参数，以使签名更加简洁，适用时。
- en: '`map`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`map`'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `map` function on a DStream takes a function `T => U` and applies it to
    every element of a `DStream[T]`, leaving the RDD structure unchanged. As in RDDs,
    it is the appropriate choice to make for a massively parallel operation, for which
    it is of no importance whether its input is in a particular position with respect
    to the rest of the data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在DStream上的`map`函数接受一个函数`T => U`并将其应用于`DStream[T]`的每个元素，保持RDD结构不变。与RDD类似，它是进行大规模并行操作的适当选择，对于它的输入是否与数据的其余部分在特定位置上没有关系。
- en: '`flatMap`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatMap`'
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`flatmap` is the usual companion of `map`, which instead of returning elements
    of type `U`, returns the type of a `TraversableOnce` container of `U`. These containers
    are coalesced in a single one before returning. All Scala collections implement
    the `TraversableOnce` interface, making them all usable as target types for this
    function.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatMap`通常是`map`的伴侣，它不返回类型为`U`的元素，而是返回`TraversableOnce`容器类型的元素。这些容器在返回之前会合并成一个单一的容器。所有Scala集合都实现了`TraversableOnce`接口，使它们都可以作为此函数的目标类型使用。'
- en: The common use cases for `flatMap` are when we want to create zero or more target
    elements from a single element. We can use `flatMap` to explode a record into
    many elements or, when used in combination with the `Option` type, it can be applied
    to filter out input records that do not meet certain criteria.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatMap`的常见用例是当我们希望从单个元素创建零个或多个目标元素时。我们可以使用`flatMap`将记录展开为多个元素，或者与`Option`类型结合使用时，可以用它来过滤不符合某些条件的输入记录。'
- en: 'An important remark is that this version of `flatMap` does not follow the strict
    monadic definition, which would be: `flatMap[U](flatMapFunc: (T) => DStream[U]):
    DStream[U]`. This is often a cause of confusion to newcomers with a functional
    programming background.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '一个重要的备注是，这个版本的`flatMap`不遵循严格的单子定义，其定义应该是：`flatMap[U](flatMapFunc: (T) => DStream[U]):
    DStream[U]`。这经常会让有函数编程背景的新手感到困惑。'
- en: '`mapPartitions`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapPartitions`'
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This function, like the homonymous function defined on RDD, allows us to directly
    apply a `map` operation on each of the partitions of an RDD. The result is a new
    `DStream[U]` where the elements are mapped. As with the `mapPartitions` call as
    defined on an RDD, this function is useful because it allows us to have executor-specific
    behavior; that is, some logic that will *not* be repeated for every element but
    will rather be executed once for each executor where the data is processed.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数与RDD上定义的同名函数类似，允许我们直接在RDD的每个分区上应用`map`操作。结果是一个新的`DStream[U]`，其中的元素被映射。与RDD上定义的`mapPartitions`调用一样，这个函数非常有用，因为它允许我们具有执行器特定的行为；即，某些逻辑不会为每个元素重复执行，而是每个处理数据的执行器执行一次。
- en: One classic example is initializing a random number generator that is then used
    in the processing of every element of the partition, accessible through the `Iterator[T]`.
    Another useful case is to amortize the creation of expensive resources, such as
    a server or database connection, and reuse such resources to process every input
    element. An additional advantage is that the initialization code is run directly
    on the executors, letting us use nonserializable libraries in a distributed computing
    process.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的例子是初始化一个随机数生成器，然后在每个分区的处理中使用，可以通过`Iterator[T]`访问。另一个有用的情况是分摊昂贵资源的创建成本，例如服务器或数据库连接，并重复使用这些资源来处理每个输入元素。另一个优点是初始化代码直接在执行器上运行，使我们能够在分布式计算过程中使用不可序列化的库。
- en: '`filter`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter`'
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This function selects some elements of the DStream according to the predicate
    passed as an argument. As for `map`, the predicate is checked on every element
    of the DStream. Beware that this can generate empty RDDs if no element verifying
    the predicate is received during a particular batch interval.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数根据作为参数传递的谓词选择DStream的一些元素。与`map`一样，谓词在DStream的每个元素上都被检查。请注意，如果在特定批处理间隔期间未接收到验证谓词的元素，则可能生成空RDD。
- en: '`glom`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`glom`'
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This function, like the homonymous function defined on RDD, allows us to coalesce
    elements in an array. In fact, as the `glom` call on an RDD returns arrays of
    elements—as many as there are partitions—the DStream equivalent returns the result
    of calling the `glom` function on each of its constituent RDDs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数与RDD上定义的同名函数类似，允许我们合并数组中的元素。实际上，作为RDD上`glom`调用的结果返回元素数组（与分区数相同），DStream的等效函数返回对其每个组成RDD调用`glom`函数的结果。
- en: '`reduce`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce`'
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is the equivalent of the `reduce` function on an RDD. It allows us to
    aggregate the elements of an RDD using the provided aggregation function. `reduce`
    takes a function of two arguments, the *accumulator* and a new element of the
    RDD, and returns the new value for the accumulator. The result of applying `reduce`
    to a `DStream[T]` is hence a DStream of the same type `T` of which each `batch
    interval` will contain an RDD with only one element: the final value of the accumulator.
    Note in particular that you should use `reduce` with care: it cannot deal with
    an empty RDD on its own, and in a streaming application, an empty batch of data
    can always happen, like when data production or ingestion is stalled.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是RDD上`reduce`函数的等效版本。它允许我们使用提供的聚合函数聚合RDD的元素。`reduce`接受两个参数的函数，*累加器*和RDD的新元素，并返回累加器的新值。因此，将`reduce`应用于`DStream[T]`的结果是相同类型`T`的DStream，每个`批处理间隔`将包含仅一个元素的RDD：累加器的最终值。特别需要注意的是，使用`reduce`时应谨慎：它不能单独处理空RDD，在流应用程序中，始终可能发生空数据批次，例如数据生产或摄取停滞时。
- en: We can summarize these various transformations in the following way, according
    to what type of action they have on the source DStream. In [Table 17-1](#transformations_streaming)
    we can see whether any operation works on a whole RDD rather than element-wise,
    and whether it has constraints on the output RDD.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据这些不同的转换方式进行总结，根据它们对源DStream的操作类型。在[Table 17-1](#transformations_streaming)中，我们可以看到是否有任何操作作用于整个RDD而不是逐个元素，以及它是否对输出RDD有约束。
- en: Table 17-1\. The computation model and output of some essential DStream operations
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17-1\. 一些关键的 DStream 操作的计算模型和输出
- en: '| Operation | Effect | Output RDD’s structure |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 效果 | 输出 RDD 的结构 |'
- en: '| --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `map` ,`filter` | element-wise | Unchanged (as many elements as original)
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| `map` ,`filter` | 逐元素 | 未改变（与原始元素数量相同） |'
- en: '| `glom` | partition-wise | As many arrays as there are partitions in the original
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `glom` | 按分区 | 与原始分区数相同的数组 |'
- en: '| `mapPartitions` | partition-wise | As many partitions as original RDD |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `mapPartitions` | 按分区 | 与原始 RDD 相同数量的分区 |'
- en: '| `reduce` ,`fold` | aggregating | One element |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `reduce` ,`fold` | 聚合 | 一个元素 |'
- en: '| `flatMap` | element-wise | As many elements as the size of the output container
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `flatMap` | 逐元素 | 输出容器的大小个元素 |'
- en: RDD-Centric DStream Transformations
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于 RDD 的 DStream 转换
- en: These operations give us direct access to the underlying RDD of a DStream. What
    makes these operations special is that they execute in the context of the Spark
    driver. Therefore, we can have access to the facilities provided by the Spark
    Session (or Spark context) as well as to the execution context of the driver program.
    In this local execution context, we can have access to local variables, data stores,
    or API calls to external services that may influence the way you want to process
    the data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作让我们直接访问 `DStream` 的底层 RDD。这些操作之所以特殊，是因为它们在 Spark 驱动程序的上下文中执行。因此，我们可以访问由
    Spark Session（或 Spark context）提供的功能，以及驱动程序程序的执行上下文。在这个本地执行环境中，我们可以访问局部变量、数据存储或
    API 调用外部服务，这可能会影响您希望处理数据的方式。
- en: 'The most commonly used transformations follow:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的转换如下：
- en: '`transform`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`transform`'
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`transform` allows us to reuse a transformation function of type `RDD[T] =>
    RDD[U]` and apply it on each constituent RDD of a DStream. It is often used to
    take advantage of some processing written for a batch job—or more simply in another
    context—to yield a streaming process.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`transform` 允许我们重用一个类型为 `RDD[T] => RDD[U]` 的转换函数，并将其应用于 DStream 的每个组成 RDD。它通常用于利用为批处理作业编写的一些处理，或更简单地在另一个上下文中，产生一个流处理过程。'
- en: '`transform` also has a timed version, with signature `(RDD[T], Time) => RDD[U]`.
    As we will remark soon for the `foreachRDD` action, this can be very useful for
    tagging the data in a DStream with the time of the batch of which this data was
    part.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`transform` 也有一个定时版本，签名为 `(RDD[T], Time) => RDD[U]`。正如我们将很快在 `foreachRDD` 操作中提到的，这对于将
    DStream 中的数据与其所属批次的时间标记起来非常有用。'
- en: '`transformWith`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformWith`'
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`transformWith` lets us combine this DStream with another DStream using an
    arbitrary transformation function. We can use it to implement custom `join` functions
    between the two DStreams where the `join` function is not based on the key. For
    example, we could apply a similarity function and combine those elements that
    are *close enough*. Like `transform`, `transformWith` offers an overload that
    provides access to the batch time to provide a differentiation of timestamp mechanism
    to the incoming data.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformWith` 让我们可以使用任意转换函数将该 DStream 与另一个 DStream 结合起来。我们可以用它来实现两个 DStream
    之间的自定义 `join` 函数，其中 `join` 函数不基于键。例如，我们可以应用一个相似性函数，并组合那些足够“接近”的元素。与 `transform`
    类似，`transformWith` 提供了一个重载，提供对批次时间的访问，以提供一个时间戳机制的不同化入口数据。'
- en: Counting
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计数
- en: Because the contents of a stream often comprise data whose cardinality is important,
    such as counting errors in a log, or hashtags in a tweet, counting is a frequent
    operation that has been optimized enough in Spark to warrant a specific API function.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因为流的内容通常包含重要的基数数据，例如计算日志中的错误数量或推文中的哈希标签，因此计数是一个频繁操作，在 Spark 中已经被优化到足以支持特定的 API
    函数。
- en: It is interesting to note that although `count` is a materializing action in
    the RDD API because it directly produces a result, in the `DStream` API, it is
    a transformation that produces a new DStream with the counts at each microbatch
    interval.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，尽管 `count` 在 RDD API 中是一个具有物化操作的函数，因为它直接产生一个结果，在 `DStream` API 中，它是一个转换操作，生成一个新的
    `DStream`，其每个微批次间隔都带有计数。
- en: Spark Streaming has several counting functions for a given `DStream`, which
    are better seen with an example.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 为给定的 `DStream` 提供了几个计数函数，最好通过示例来理解。
- en: 'Let’s assume that we have a DStream consisting of names keyed by their first
    letter and that this DStream “repeats” itself: each RDD, on each batch interval,
    consists of 10 distinct such names per alphabet letter:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个由名字按其首字母键控的 DStream，并且该 DStream “重复”自己：每个 RDD，在每个批次间隔中，由每个字母的10个不同名字组成：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This would lead to the results shown in [Table 17-2](#counts_streaming).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致 [表 17-2](#counts_streaming) 中显示的结果。
- en: Table 17-2\. Count operations
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17-2\. 计数操作
- en: '| Operation | Return type | Result |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 返回类型 | 结果 |'
- en: '| --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `keyedNames.count()` | `DStream[Long]` | 260 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `keyedNames.count()` | `DStream[Long]` | 260 |'
- en: '| `keyedNames.countByWindow(60s)` | `DStream[Long]` | 260 (because the same
    `RDD` repeats each time ) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `keyedNames.countByWindow(60s)` | `DStream[Long]` | 因为相同的 `RDD` 每次都重复，所以为
    260 |'
- en: '| `keyedNames.countByValue()` | `DStream[((Char, String), Long)]` | 1 for each
    of the 260 distinct (1st character, name) pairs |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `keyedNames.countByValue()` | `DStream[((Char, String), Long)]` | 每个 260
    个不同的（第一个字符，名称）对分别为 1 |'
- en: '| `keyedNames.countByKey()` | `DStream[(Char, Long)]` | 10 for each of the
    26 letters |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `keyedNames.countByKey()` | `DStream[(Char, Long)]` | 每个 26 个字母分别为 10 |'
- en: '| `keyednames.countByValueAndWindow(60s)` | `DStream[((Char, String), Long)]`
    | 12 for each of the 260 distinct (1st character, name) pairs |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| `keyednames.countByValueAndWindow(60s)` | `DStream[((Char, String), Long)]`
    | 每个 260 个不同的（第一个字符，名称）对分别为 12 |'
- en: Structure-Changing Transformations
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改变结构的转换操作
- en: 'The previous operations are all transformations; that is, they always return
    a DStream after applying a function to the content of the stream. There are other
    transformations that do not transform the data in the stream but rather the structure
    of the DStream and in some cases, the flow of data through the Spark cluster:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的操作都是转换操作；也就是说，它们在将函数应用于流的内容后始终返回一个 DStream。还有其他一些不会转换流中数据，而是转换 DStream 结构以及在某些情况下通过
    Spark 集群的数据流动：
- en: '`repartition`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`repartition`'
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`repartition` results in a new DStream with an increased or decreased partitioning
    of the underlying RDDs in this DStream. Repartitioning allows us to change the
    parallelism characteristics of some streaming computation. For example, we might
    want to increase partitioning when a few input streams deliver a high volume of
    data that we want to distribute to a large cluster for some CPU-intensive computation.
    Decreasing the partitioning might be interesting when we want to ensure that we
    have a few partitions with a large number of elements before writing to a database,
    for example. Note that the parameter provided is the absolute number of the target
    number of partitions. Whether this operation results in creating or reducing the
    partitioning depends on the original number of partitions of this DStream, which
    in turn might be dependent on the source parallelism.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`repartition` 会生成一个具有增加或减少底层 RDD 中分区的新 DStream。重新分区允许我们改变某些流计算的并行特性。例如，当少量输入流提供大量数据时，我们可能希望将其分发到一个大集群进行一些
    CPU 密集型计算时增加分区。减少分区可能在写入数据库之前确保有少量分区有大量元素时很有用。注意，提供的参数是目标分区数量的绝对数。此操作是否会增加或减少分区取决于该
    DStream 的原始分区数量，而原始分区数量又可能依赖于源的并行性。'
- en: Unlike data-oriented transformations, correct usage of `repartition` requires
    knowledge of the input parallelism, the complexity of the distributed computation,
    and the size of the cluster where the streaming job will be running.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与面向数据的转换不同，正确使用 `repartition` 需要了解输入并行性、分布式计算的复杂性以及流作业将运行的集群大小。
- en: '`union`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`union`'
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`union` adds two DStreams of the same type into a single stream. The result
    is a DStream that contains the elements found on both input DStreams. An alternative
    usage form is calling `union` on the `streamingContext` instance with a collection
    of the DStreams to be joined. This second form allows us to unite many DStreams
    at once:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`union` 将两个相同类型的 DStreams 合并为一个流。结果是一个 DStream，其中包含两个输入 DStreams 中找到的元素。另一种用法是在
    `streamingContext` 实例上调用 `union` 并传入一组要连接的 DStreams。这种形式允许我们一次性合并多个 DStreams：'
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Summary
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we have learned about the API offered by the DStream to implement
    streaming applications. We saw the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了 DStream 提供的 API，以实现流应用程序。我们看到了以下内容：
- en: DStreams are immutable and we operate on them using transformations.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DStreams 是不可变的，我们通过转换操作来操作它们。
- en: Transformations are lazy. They need to be materialized by a special output operation.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换是惰性的。它们需要通过特殊的输出操作来实现物化。
- en: DStreams offer a rich functional API that allows the transformation of elements.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DStreams 提供了丰富的函数 API，允许对元素进行转换。
- en: Some transformations expose the underlying RDD, providing access to the full
    extent of the rich Spark core API.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些转换会暴露底层的 RDD，从而提供对丰富的 Spark 核心 API 的全面访问。
- en: In the next chapters, you learn how to source data from external systems by
    creating DStreams. You also learn about a particular set of operations, called
    output operations in Spark Streaming jargon, that trigger the execution of transformations
    over our data streams and are able to produce data to other systems or secondary
    storage.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，您将学习如何通过创建 DStreams 从外部系统获取数据。您还将了解到一组特定的操作，在 Spark Streaming 行话中称为输出操作，这些操作触发对我们数据流的转换执行，并能够将数据生成到其他系统或者辅助存储中。
