- en: Chapter 12\. Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章\. 流处理
- en: Our stories aren’t over yet.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的故事还没有结束。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Arya Stark
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —艾亚·史塔克
- en: Looking back at the previous chapters, we’ve covered a good deal, but not everything.
    We’ve analyzed tabular datasets, performed unsupervised learning over raw text,
    analyzed graphs and geographic datasets, and even transformed data with custom
    R code! So now what?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾之前的章节，我们已经涵盖了很多内容，但并非所有内容。我们分析了表格数据集，在原始文本上执行了无监督学习，分析了图形和地理数据集，甚至使用自定义R代码转换了数据！现在怎么办呢？
- en: Though we weren’t explicit about this, we’ve assumed until this point that your
    data is static, and didn’t change over time. But suppose for a moment your job
    is to analyze traffic patterns to give recommendations to the department of transportation.
    A reasonable approach would be to analyze historical data and then design predictive
    models that compute forecasts overnight. Overnight? That’s very useful, but traffic
    patterns change by the hour and even by the minute. You could try to preprocess
    and predict faster and faster, but eventually this model breaks—you can’t load
    large-scale datasets, transform them, score them, unload them, and repeat this
    process by the second.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们没有明确提到，但到目前为止，我们假设您的数据是静态的，并且随时间不变。但是假设你的工作是分析交通模式以向交通部门提供建议。一个合理的方法是分析历史数据，然后设计能够在夜间计算预测的预测模型。过夜？这非常有用，但交通模式每小时甚至每分钟都在变化。您可以尝试预处理并更快地进行预测，但最终这种模型会崩溃——您无法加载大规模数据集、转换它们、评分它们、卸载它们，然后每秒重复这个过程。
- en: Instead, we need to introduce a different kind of dataset—one that is not static
    but rather dynamic, one that is like a table but is growing constantly. We will
    refer to such datasets as *streams*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们需要引入一种不同类型的数据集——一种不是静态而是动态的，一种像表格但不断增长的数据集。我们将这种数据集称为*流*。
- en: Overview
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: We know how to work with large-scale static datasets, but how can we reason
    about large-scale real-time datasets? Datasets with an infinite amount of entries
    are known as *streams*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道如何处理大规模静态数据集，但如何处理大规模实时数据集呢？具有无限条目的数据集被称为*流*。
- en: For static datasets, if we were to do real-time scoring using a pretrained topic
    model, the entries would be lines of text; for real-time datasets, we would perform
    the same scoring over an infinite number of lines of text. Now, in practice, you
    will never process an infinite number of records. You will eventually stop the
    stream—or this universe might end, whichever comes first. Regardless, thinking
    of the datasets as infinite makes it much easier to reason about them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于静态数据集，如果我们使用预先训练的主题模型进行实时评分，条目将是文本行；对于实时数据集，我们将在无限数量的文本行上执行相同的评分。现实中，您永远不会处理无限数量的记录。最终您会停止流——或者这个宇宙可能会结束，看哪个先到。不管怎样，将数据集视为无限使得推理更容易。
- en: Streams are most relevant when processing real-time data—for example, when analyzing
    a Twitter feed or stock prices. Both examples have well-defined columns, like
    “tweet” or “price,” but there are always new rows of data to be analyzed.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理实时数据时，流最相关——例如分析Twitter动态或股票价格。这两个示例都有明确定义的列，如“推文”或“价格”，但总会有新的数据行需要分析。
- en: '*Spark Streaming* provides scalable and fault-tolerant data processing over
    streams of data. That means you can use many machines to process multiple streaming
    sources, perform joins with other streams or static sources, and recover from
    failures with at-least-once guarantees (each message is certain to be delivered,
    but may do so multiple times).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*Spark Streaming* 提供了可伸缩和容错的数据处理，适用于数据流。这意味着您可以使用多台机器处理多个数据流源，与其他流或静态源进行连接，并通过至少一次保证从故障中恢复（确保每条消息被传递，但可能会多次传递）。'
- en: In Spark, you create streams by defining a *source*, a *transformation*, and
    a *sink*; you can think of these steps as reading, transforming, and writing a
    stream, as shown in [Figure 12-1](#streaming-working).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，您通过定义*源*、*转换*和*汇*来创建流；您可以将这些步骤视为读取、转换和写入流，如[图12-1](#streaming-working)所示。
- en: '![Working with Spark Streams](assets/mswr_1201.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark Streaming](assets/mswr_1201.png)'
- en: Figure 12-1\. Working with Spark Streaming
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1\. 使用Spark Streaming
- en: 'Let’s take a look at each of these a little more closely:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看每一个：
- en: Reading
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读
- en: Streams read data using any of the `stream_read_*()` functions; the read operation
    defines the *source* of the stream. You can define one or multiple sources from
    which to read.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 流使用任何`stream_read_*()`函数读取数据；读操作定义了流的*源*。您可以定义一个或多个要读取的源。
- en: Transforming
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 转换
- en: A stream can perform one or multiple transformations using `dplyr`, `SQL`, feature
    transformers, scoring pipelines, or distributed R code. Transformations can not
    only be applied to one or more streams, but can also use a combination of streams
    and static data sources; for instance, those loaded into Spark with `spark_read_()`
    functions—this means that you can combine static data and real-time data sources
    with ease.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 流可以使用`dplyr`、`SQL`、特征转换器、评分管道或分布式R代码执行一个或多个转换。转换不仅可以应用于一个或多个流，还可以使用流和静态数据源的组合；例如，使用`spark_read_()`函数加载到Spark中——这意味着您可以轻松地结合静态数据和实时数据源。
- en: Writing
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 写入
- en: The write operations are performed with the family of `stream_write_*()` functions,
    while the read operation defined the sink of the stream. You can specify a single
    sink or multiple ones to write data to.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 写操作使用`stream_write_*()`函数系列执行，而读操作则定义了流的接收端。您可以指定一个或多个接收端来写入数据。
- en: 'You can read and write to streams in several different file formats: CSV, JSON,
    Parquet, Optimized Row Columnar (ORC), and text (see [Table 12-1](#spark_functions)).
    You also can read and write from and to Kafka, which we will introduce later on.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在多种不同的文件格式（如CSV、JSON、Parquet、优化行列式（ORC）和文本）中读取和写入流；另请参阅[表 12-1](#spark_functions)。您还可以从Kafka读取和写入数据，我们稍后将介绍。
- en: Table 12-1\. Spark functions to read and write streams
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12-1\. 用于读取和写入流的Spark函数
- en: '| Format | Read | Write |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | 读取 | 写入 |'
- en: '| --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| CSV | `stream_read_csv` | `stream_write_csv` |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| CSV | `stream_read_csv` | `stream_write_csv` |'
- en: '| JSON | `stream_read_json` | `stream_write_json` |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| JSON | `stream_read_json` | `stream_write_json` |'
- en: '| Kafka | `stream_read_kafka` | `stream_write_kafka` |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Kafka | `stream_read_kafka` | `stream_write_kafka` |'
- en: '| ORC | `stream_read_orc` | `stream_write_orc` |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| ORC | `stream_read_orc` | `stream_write_orc` |'
- en: '| Parquet | `stream_read_parquet` | `stream_write_parquet` |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| Parquet | `stream_read_parquet` | `stream_write_parquet` |'
- en: '| Text | `stream_read_text` | `stream_write_text` |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | `stream_read_text` | `stream_write_text` |'
- en: '| Memory |  | `stream_write_memory` |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 内存 |  | `stream_write_memory` |'
- en: Since the transformation step is optional, the simplest stream we can define
    is one that continuously copies text files between source and destination.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于转换步骤是可选的，我们可以定义的最简单的流是一个持续复制文本文件的流，从源到目的地。
- en: First, install the `future` package using `install.packages("future")` and connect
    to Spark.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`install.packages("future")`安装`future`包并连接到Spark。
- en: 'Since a stream requires the source to exist, create a `_source_` folder:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于流需要存在的源，创建一个`_source_`文件夹：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We are now ready to define our first stream!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备定义我们的第一个流！
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The streams starts running with `stream_write_*()`; once executed, the stream
    will monitor the *`source`* path and process data into the *`destination /`* path
    as it arrives.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 流使用`stream_write_*()`开始运行；执行后，流将监视*`source`*路径，并在数据到达时将数据处理到*`destination /`*路径。
- en: 'We can use `stream_generate_test()` to produce a file every second containing
    lines of text that follow a given distribution; you can read more about this in
    [Appendix A](app01.html#appendix). In practice, you would connect to existing
    sources without having to generate data artificially. We can then use `view_stream()`
    to track the rows per second (rps) being processed in the source, and in the destination,
    and their latest values over time:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`stream_generate_test()`每秒生成一个文件，其中包含遵循给定分布的文本行；您可以在[附录 A](app01.html#appendix)中详细了解。在实践中，您将连接到现有的源，而不必人工生成数据。然后，我们可以使用`view_stream()`来跟踪在源和目的地中每秒处理的行数（rps）及其随时间变化的最新值：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The result is shown in [Figure 12-2](#streaming-view-stream).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在[图 12-2](#streaming-view-stream)中。
- en: '![Monitoring a stream generating rows following a binomial distribution](assets/mswr_1202.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![监控生成符合二项分布的行的流](assets/mswr_1202.png)'
- en: Figure 12-2\. Monitoring a stream generating rows following a binomial distribution
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-2\. 监控生成符合二项分布的行的流
- en: Notice that the rps rate in the destination stream is higher than that in the
    source stream. This is expected and desirable since Spark measures incoming rates
    from the source stream, but also actual row-processing times in the destination
    stream. For example, if 10 rows per second are written to the *`source/`* path,
    the incoming rate is 10 rps. However, if it takes Spark only 0.01 seconds to write
    all those 10 rows, the output rate is 100 rps.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意目标流中的rps速率高于源流中的速率。这是预期且希望的，因为Spark会测量从源流接收的入站速率，同时还会测量目标流中的实际行处理时间。例如，如果每秒写入`source/`路径的行数为10行，则入站速率为10
    rps。但是，如果Spark仅花费0.01秒就可以写完这10行，则输出速率为100 rps。
- en: 'Use `stream_stop()` to properly stop processing data from this stream:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`stream_stop()`来正确停止从此流程中处理数据：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This exercise introduced how we can easily start a Spark stream that reads and
    writes data based on a simulated stream. Let’s do something more interesting than
    just copying data with proper transformations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习介绍了如何轻松启动一个Spark流，根据模拟流读取和写入数据。让我们做些比仅仅复制数据更有趣的事情，进行适当的转换。
- en: Transformations
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换
- en: In a real-life scenario, the incoming data from a stream would not be written
    as is to the output. The Spark Streaming job would make transformations to the
    data, and then write the transformed data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际情况中，从流中接收的数据不会直接写入输出。Spark Streaming作业将对数据进行转换，然后写入转换后的数据。
- en: Streams can be transformed using `dplyr`, SQL queries, ML pipelines, or R code.
    We can use as many transformations as needed in the same way that Spark DataFrames
    can be transformed with `sparklyr`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`dplyr`、SQL查询、ML管道或R代码对流进行转换。可以像对Spark DataFrames进行转换一样，使用任意数量的转换。
- en: The source of the transformation can be a stream or DataFrame, but the output
    is always a stream. If needed, you can always take a snapshot from the destination
    stream and then save the output as a DataFrame. That is what `sparklyr` will do
    for you if a destination stream is not specified.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 转换的源可以是流或DataFrame，但输出始终是流。如果需要，可以从目标流中获取快照，然后将输出保存为DataFrame。如果未指定目标流，则`sparklyr`将为您完成这一操作。
- en: Each of the following subsections covers an option provided by `sparklyr` to
    perform transformations on a stream.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各小节涵盖了由`sparklyr`提供的选项，用于对流进行转换。
- en: Analysis
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析
- en: 'You can analyze streams with `dplyr` verbs and SQL using `DBI`. As a quick
    example, we will filter rows and add columns over a stream. We won’t explicitly
    call `stream_generate_test()`, but you can call it on your own through the `later`
    package if you feel the urge to verify that data is being processed continuously:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`dplyr`动词和使用`DBI`的SQL来分析流。作为一个快速示例，我们将在流上过滤行并添加列。我们不会显式调用`stream_generate_test()`，但如果您感到有必要验证数据是否持续处理，可以通过`later`包自行调用它：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It’s also possible to perform aggregations over the entire history of the stream.
    The history could be filtered or not:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以对流的整个历史执行聚合。历史可以进行过滤或不进行过滤：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Grouped aggregations of the latest data in the stream require a timestamp.
    The timestamp will note when the reading function (in this case `stream_read_csv()`)
    first “saw” that specific record. In Spark Streaming terminology, the timestamp
    is called a *watermark*. The `spark_watermark()` function adds the timestamp.
    In this example, the watermark will be the same for all records, since the five
    files were read by the stream after they were created. Note that only Kafka and
    memory *outputs* support watermarks:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 流中最新数据的分组聚合需要一个时间戳。时间戳会记录读取函数（在本例中是`stream_read_csv()`）第一次“看到”该特定记录的时间。在Spark
    Streaming术语中，时间戳被称为*水印*。`spark_watermark()`函数会添加这个时间戳。在这个例子中，由于这五个文件是在流读取它们创建后被读取的，因此所有记录的水印都是相同的。请注意，只有Kafka和内存*输出*支持水印：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After the watermark is created, you can use it in the `group_by()` verb. You
    can then pipe it into a `summarise()` function to get some stats of the stream:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了水印之后，可以在`group_by()`动词中使用它。然后可以将其输入到`summarise()`函数中以获取流的一些统计信息：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Modeling
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模
- en: Spark streams currently don’t support training on real-time datasets. Aside
    from the technical challenges, even if it were possible, it would be quite difficult
    to train models since the model itself would need to adapt over time. Known as
    *online learning*, this is perhaps something that Spark will support in the future.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 目前Spark流不支持在实时数据集上进行训练。除了技术上的挑战之外，即使可能，训练模型也将是相当困难的，因为模型本身需要随时间而适应。作为*在线学习*的一种形式，这可能是Spark未来将支持的内容。
- en: That said, there are other modeling concepts we can use with streams, like feature
    transformers and scoring. Let’s try out a feature transformer with streams, and
    leave scoring for the next section, since we will need to train a model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们可以利用流使用其他建模概念，比如特征转换器和评分。让我们尝试在流中使用一个特征转换器，并且将评分留到下一节，因为我们需要训练一个模型。
- en: 'The next example uses the `ft_bucketizer()` feature transformer to modify the
    stream followed by regular `dplyr` functions, which you can use just as you would
    with static datasets:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例使用`ft_bucketizer()`特征转换器修改流，然后使用常规的`dplyr`函数，您可以像处理静态数据集一样使用它们：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Pipelines
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流水线
- en: Spark pipelines can be used for scoring streams, but not to train over streaming
    data. The former is fully supported, while the latter is a feature under active
    development by the Spark community.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Spark流水线可以用于对流进行评分，但不能用于在流数据上进行训练。前者得到了完全支持，而后者是Spark社区正在积极开发的功能。
- en: 'To score a stream, it’s necessary to first create our model. So let’s build,
    fit, and save a simple pipeline:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要对流进行评分，首先需要创建我们的模型。因此，让我们构建、拟合并保存一个简单的流水线：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Tip
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you choose to, you can make use of other concepts presented in [Chapter 5](ch05.html#pipelines),
    like saving and reloading pipelines through `ml_save()` and `ml_load()` before
    scoring streams.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择，可以利用[第5章](ch05.html#pipelines)中介绍的其他概念，如通过`ml_save()`和`ml_load()`保存和重新加载流水线，然后对流进行评分。
- en: 'We can then generate a stream based on `mtcars` using `stream_generate_test()`,
    and score the model using `ml_transform()`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`stream_generate_test()`基于`mtcars`生成流，并使用`ml_transform()`对模型进行评分：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Though this example was put together with a few lines of code, what we just
    accomplished is actually quite impressive. You copied data into Spark, performed
    feature engineering, trained a model, and scored the model over a real-time dataset,
    with just seven lines of code! Let’s try now to use custom transformations, in
    real time.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个例子只用了几行代码，但我们刚刚完成的实际上相当令人印象深刻。您复制数据到Spark中，进行特征工程，训练了一个模型，并对实时数据集进行了评分，仅仅用了七行代码！现在让我们尝试使用自定义转换，实时进行。
- en: Distributed R
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式R
- en: 'Arbitrary R code can also be used to transform a stream with the use of `spark_apply()`.
    This approach follows the same principles discussed in [Chapter 11](ch11.html#distributed),
    where `spark_apply()` runs R code over each executor in the cluster where data
    is available. This enables processing high-throughput streams and fulfills low-latency
    requirements:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用任意的R代码来使用`spark_apply()`对流进行转换。这种方法遵循了在[第11章](ch11.html#distributed)中讨论的相同原则，其中`spark_apply()`在可用数据的集群中的每个执行程序上运行R代码。这使得可以处理高吞吐量的流，并满足低延迟的要求：
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'which, as you would expect, processes data from `cars-stream` into `cars-round`
    by running the custom `round()` R function. Let’s peek into the output sink:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您期望的那样，通过运行自定义的`round()` R函数，它从`cars-stream`处理数据到`cars-round`。让我们窥探一下输出的目标：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Again, make sure you apply the concepts you already know about `spark_apply()`
    when using streams; for instance, you should consider using `arrow` to significantly
    improve performance. Before we move on, disconnect from Spark:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 再次确保在使用流时应用您已经了解的`spark_apply()`的概念；例如，您应该考虑使用`arrow`来显著提高性能。在我们继续之前，断开与Spark的连接：
- en: '[PRE21]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This was our last transformation for streams. We’ll now learn how to use Spark
    Streaming with Kafka.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们对流的最后一个转换。现在我们将学习如何使用Spark Streaming与Kafka。
- en: Kafka
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka
- en: Apache Kafka is an open source stream-processing software platform developed
    by LinkedIn and donated to the Apache Software Foundation. It is written in Scala
    and Java. To describe it using an analogy, Kafka is to real-time storage what
    Hadoop is to static storage.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka是由LinkedIn开发并捐赠给Apache软件基金会的开源流处理软件平台。它用Scala和Java编写。用类比来描述它，Kafka对于实时存储来说就像Hadoop对于静态存储一样。
- en: Kafka stores the stream as records, which consist of a key, a value, and a timestamp.
    It can handle multiple streams that contain different information, by categorizing
    them by topic. Kafka is commonly used to connect multiple real-time applications.
    A *producer* is an application that streams data into Kafka, while a *consumer*
    is the one that reads from Kafka; in Kafka terminology, a consumer application
    *subscribes* to topics. Therefore, the most basic workflow we can accomplish with
    Kafka is one with a single producer and a single consumer; this is illustrated
    in [Figure 12-3](#streaming-kafka-apis).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka将流存储为记录，记录由键、值和时间戳组成。它可以处理包含不同信息的多个流，通过主题对它们进行分类。Kafka通常用于连接多个实时应用程序。*生产者*是将数据流入Kafka的应用程序，而*消费者*则是从Kafka读取数据的应用程序；在Kafka术语中，消费者应用程序*订阅*主题。因此，我们可以通过Kafka实现的最基本的工作流程是一个具有单一生产者和单一消费者的工作流程；这在[图12-3](#streaming-kafka-apis)中有所说明。
- en: '![Basic workflow](assets/mswr_1203.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![基本工作流程](assets/mswr_1203.png)'
- en: Figure 12-3\. A basic Kafka workflow
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3\. 基本Kafka工作流程
- en: If you are new to Kafka, we don’t recommend you run the code from this section.
    However, if you’re really motivated to follow along, you will first need to install
    Kafka as explained in [Appendix A](app01.html#appendix) or deploy it in your cluster.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是Kafka的新手，我们不建议你从本节运行代码。但是，如果你真的有兴趣跟随，你首先需要按照附录[A](app01.html#appendix)中的说明安装Kafka或在你的集群中部署它。
- en: 'Using Kafka also requires you to have the Kafka package when connecting to
    Spark. Make sure this is specified in your connection `config`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kafka还需要在连接到Spark时具备Kafka包。确保在你的连接`config`中指定了这一点：
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Once connected, it’s straightforward to read data from a stream:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦连接成功，从流中读取数据就非常简单：
- en: '[PRE23]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: However, notice that you need to properly configure the `options` list; `kafka.bootstrap.server`
    expects a list of Kafka hosts, while `topic` and `subscribe` define which topic
    should be used when writing or reading from Kafka, respectively.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你需要正确配置`options`列表；`kafka.bootstrap.server`需要Kafka主机的列表，而`topic`和`subscribe`分别定义了在写入或从Kafka读取时应该使用的主题。
- en: Though we’ve started by presenting a simple single-producer and single-consumer
    use case, Kafka also allows much more complex interactions. We will next read
    from one topic, process its data, and then write the results to a different topic.
    Systems that are producers and consumers from the same topic are referred to as
    *stream processors*. In [Figure 12-4](#streaming-kafka-two-outputs), the stream
    processor reads topic A and then writes results to topic B. This allows for a
    given consumer application to read results instead of “raw” feed data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们开始时介绍了一个简单的单一生产者和单一消费者的用例，但Kafka也允许更复杂的交互。接下来，我们将从一个主题中读取数据，处理其数据，然后将结果写入到另一个主题。从同一个主题生产者和消费者的系统被称为*流处理器*。在[图12-4](#streaming-kafka-two-outputs)中，流处理器读取主题A，然后将结果写入主题B。这允许给定的消费者应用程序读取结果而不是“原始”提要数据。
- en: '![Kafka workflow](assets/mswr_1204.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![Kafka工作流程](assets/mswr_1204.png)'
- en: Figure 12-4\. A Kafka workflow using stream processors
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-4\. 使用流处理器的Kafka工作流程
- en: 'Three modes are available when processing Kafka streams in Spark: *`complete`*,
    *`update`*, and *`append`*. The `complete` mode provides the totals for every
    group every time there is a new batch; `update` provides totals for only the groups
    that have updates in the latest batch; and `append` adds raw records to the target
    topic. The `append` mode is not meant for aggregates, but works well for passing
    a filtered subset to the target topic.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 处理Kafka流时，Spark提供了三种模式：*`complete`*，*`update`*和*`append`*。`complete`模式在每次有新批次时为每个组提供总计；`update`仅为最新批次中有更新的组提供总计；而`append`则将原始记录添加到目标主题。`append`模式不适用于聚合，但对于将过滤后的子集传递到目标主题非常有效。
- en: 'In our next example, the producer streams random letters into Kafka under a
    `letters` topic. Then, Spark will act as the stream processor, reading the `letters`
    topic and computing unique letters, which are then written back to Kafka under
    the `totals` topic. We’ll use the `update` mode when writing back into Kafka;
    that is, only the totals that changed will be sent to Kafka. This change is determined
    after each batch from the `letters` topic:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一个示例中，生产者将随机字母流入`letters`主题的Kafka。然后，Spark将作为流处理器，读取`letters`主题并计算唯一字母，然后将其写回到`totals`主题的Kafka。我们将在写回到Kafka时使用`update`模式；也就是说，仅发送变化的总计到Kafka。这些变化是在每个`letters`主题的批次之后确定的。
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can take a quick look at totals by reading from Kafka:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过从Kafka中读取来快速查看总计：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Using a new terminal session, use Kafka’s command-line tool to manually add
    single letters into the `letters` topic:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新的终端会话，使用Kafka的命令行工具手动将单个字母添加到`letters`主题：
- en: '[PRE26]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The letters that you input are pushed to Kafka, read by Spark, aggregated within
    Spark, and pushed back into Kafka, Then, finally, they are consumed by Spark again
    to give you a glimpse into the `totals` topic. This was quite a setup, but also
    a realistic configuration commonly found in real-time processing projects.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您输入的字母被推送到Kafka，由Spark读取，在Spark内聚合，然后再次推送回Kafka，最后由Spark消费，向您展示`totals`主题的一瞥。这确实是一个相当复杂的设置，但也是实时处理项目中常见的现实配置。
- en: Next, we will use the Shiny framework to visualize streams, in real time!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用Shiny框架实时可视化流！
- en: Shiny
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Shiny
- en: Shiny’s reactive framework is well suited to support streaming information,
    which you can use to display real-time data from Spark using `reactiveSpark()`.
    There is far more to learn about Shiny than we could possibly present here. However,
    if you’re already familiar with Shiny, this example should be quite easy to understand.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Shiny的响应式框架非常适合支持流信息，您可以使用它来显示来自Spark的实时数据，使用`reactiveSpark()`。关于Shiny，我们这里可能呈现的内容远远不止这些。然而，如果您已经熟悉Shiny，这个例子应该很容易理解。
- en: We have a modified version of the *k*-means Shiny example that, instead of getting
    the data from the static `iris` dataset, is generated with `stream_generate_test()`,
    consumed by Spark, retrieved to Shiny through `reactiveSpark()`, and then displayed
    as shown in [Figure 12-5](#streaming-shiny-app).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个修改过的*k*-means Shiny示例，不是从静态`iris`数据集获取数据，而是使用`stream_generate_test()`生成数据，由Spark消费，通过`reactiveSpark()`检索到Shiny，并显示如[图12-5](#streaming-shiny-app)所示：
- en: 'To run this example, store the following Shiny app under `shiny/shiny-stream.R`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，请将以下Shiny应用程序存储在`shiny/shiny-stream.R`下：
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This Shiny application can then be launched with `runApp()`, like so:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以使用`runApp()`启动这个Shiny应用程序：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'While the Shiny app is running, launch a new R session from the same directory
    and create a test stream with `stream_generate_test()`. This will generate a stream
    of continuous data that Spark can process and Shiny can visualize (as illustrated
    in [Figure 12-5](#streaming-shiny-app)):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在Shiny应用程序运行时，从同一目录启动一个新的R会话，并使用`stream_generate_test()`创建一个测试流。这将生成一系列连续数据，Spark可以处理，并且Shiny可以可视化（如[图12-5](#streaming-shiny-app)所示）：
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![Progression of Spark reactive loading data into Shiny app](assets/mswr_1205.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![Spark响应式加载数据到Shiny应用程序的进展](assets/mswr_1205.png)'
- en: Figure 12-5\. Progression of Spark reactive loading data into the Shiny app
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-5\. Spark响应式加载数据进入Shiny应用程序的进展
- en: In this section you learned how easy it is to create a Shiny app that can be
    used for several different purposes, such as monitoring and dashboarding.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您学习了创建一个可以用于监控和仪表板等多种不同目的的Shiny应用程序是多么容易。
- en: In a more complex implementation, the source would more likely be a Kafka stream.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在更复杂的实现中，源更可能是一个Kafka流。
- en: 'Before we transition, disconnect from Spark and clear the folders that we used:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在过渡之前，断开与Spark的连接并清除我们使用的文件夹：
- en: '[PRE30]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Recap
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: From static datasets to real-time datasets, you’ve truly mastered many of the
    large-scale computing techniques. Specifically, in this chapter, you learned how
    static data can be generalized to real time if we think of it as an infinite table.
    We were then able to create a simple stream, without any data transformations,
    that copies data from point A to point B.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 从静态数据集到实时数据集，您真正掌握了许多大规模计算技术。特别是在本章中，您学习了如果将静态数据想象为无限表格，可以将其泛化到实时数据。然后，我们能够创建一个简单的流，不进行任何数据转换，仅将数据从A点复制到B点。
- en: This humble start became quite useful when you learned about the several different
    transformations you can apply to streaming data—from data analysis transformations
    using the `dplyr` and `DBI` packages, to feature transformers introduced while
    modeling, to fully fledged pipelines capable of scoring in real time, to, last
    but not least, transforming datasets with custom R code. This was a lot to digest,
    for sure.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当您了解到可以应用于流数据的几种不同转换时，从使用`dplyr`和`DBI`包进行数据分析转换，到在建模时引入的特征转换器，再到能够进行实时评分的完整管道，最后再到使用自定义R代码转换数据集，这些都是需要消化的大量内容。
- en: We then presented Apache Kafka as a reliable and scalable solution for real-time
    data. We showed you how a real-time system could be structured by introducing
    you to consumers, producers, and topics. These, when properly combined, create
    powerful abstractions to process real-time data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们介绍了Apache Kafka作为实时数据可靠且可扩展的解决方案。我们向您展示了如何通过介绍消费者、生产者和主题来构建实时系统。当这些要素正确结合时，它们可以创建强大的抽象来处理实时数据。
- en: 'Then we closed with “a cherry on top of the sundae”: presenting how to use
    Spark Streaming in Shiny. Since a stream can be transformed into a reactive (which
    is the lingua franca of the world of reactivity), the ease of this approach was
    a nice surprise.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接着我们“樱桃在冰淇淋上”的结尾：展示如何在Shiny中使用Spark Streaming。由于流可以转换为响应式（这是响应性世界的通用语言），这种方法的简易性让人感到惊喜。
- en: It’s time now to move on to our very last (and quite short) chapter, [Chapter 13](ch13.html#contributing);
    there we’ll try to persuade you to use your newly acquired knowledge for the benefit
    of the Spark and R communities at large.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候进入我们的最后一个（而且相当简短的）章节了，[第13章](ch13.html#contributing)；在那里，我们将试图说服您利用您新获得的知识造福于Spark和R社区。
