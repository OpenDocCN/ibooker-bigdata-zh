- en: Chapter 23\. Working with Spark SQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第23章。使用Spark SQL
- en: So far, we have seen how Spark Streaming can work as a standalone framework
    to process streams of many sources and produce results that can be sent or stored
    for further consumption.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了Spark Streaming如何作为一个独立的框架来处理多源流，并生成可以进一步发送或存储以供后续使用的结果。
- en: Data in isolation has limited value. We often want to combine datasets to explore
    relationships that become evident only when data from different sources are merged.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 孤立的数据具有有限的价值。我们经常希望合并数据集以探索仅在合并来自不同来源的数据时才显现的关系。
- en: In the particular case of streaming data, the data we see at each batch interval
    is merely a sample of a potentially infinite dataset. Therefore, to increase the
    value of the observed data at a given point in time, it’s imperative that we have
    the means to combine it with the knowledge we already have. It might be historical
    data that we have in files or a database, a model that we created based on data
    from the previous day, or even earlier streaming data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在流数据的特定情况下，我们在每个批处理间隔看到的数据仅仅是潜在无限数据集的一个样本。因此，为了增加在给定时间点观察到的数据的价值，我们必须有办法将其与我们已经拥有的知识结合起来。这可能是我们在文件或数据库中拥有的历史数据，是基于前一天数据创建的模型，或者甚至是早期流数据。
- en: One of the key value propositions of Spark Streaming is its seamless interoperability
    with other Spark frameworks. This synergy among the Spark modules increases the
    spectrum of data-oriented applications that we can create, resulting in applications
    with a lower complexity than combining arbitrary—and often incompatible—libraries
    by ourselves. This translates to increased development efficiency, which in turn
    improves the business value delivered by the application.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming的一个关键价值主张是其与其他Spark框架的无缝互操作性。这种Spark模块之间的协同作用增加了我们可以创建的面向数据的应用程序的范围，导致应用程序的复杂性低于自行组合任意——而且通常是不兼容——库的复杂性。这转化为增加的开发效率，进而提高了应用程序提供的业务价值。
- en: In this chapter, we explore how you can combine Spark Streaming applications
    with Spark SQL.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何将Spark Streaming应用程序与Spark SQL结合使用。
- en: Note
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As we saw in [Part II](part02.xhtml#str-str), Structured Streaming is the native
    approach in Spark to use the Spark SQL abstractions for streaming. The techniques
    described in this chapter apply when we have a Spark Streaming job that we would
    like to complement with Spark SQL functions for a specific purpose.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第二部分](part02.xhtml#str-str)中看到的，结构化流处理是Spark中使用Spark SQL抽象进行流处理的本地方法。本章描述的技术适用于当我们有一个Spark
    Streaming作业时，希望为特定目的使用Spark SQL函数。
- en: For a pure Spark SQL approach to stream processing, consider Structured Streaming
    first.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行纯Spark SQL方法的流处理，请首先考虑结构化流处理。
- en: Spark SQL
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Spark SQL is the Spark module that works with structured data. It implements
    functions and abstractions traditionally found in the realms of databases, such
    as a query analyzer, optimizer, and an execution planner, to enable a table-like
    manipulation of arbitrarily structured data sources on top of the Spark engine.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是与结构化数据一起工作的Spark模块。它实现了传统数据库领域中通常找到的函数和抽象，如查询分析器、优化器和执行计划器，以在Spark引擎之上对任意结构化数据源进行类似表格的操作。
- en: 'Spark SQL introduces three important features:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL引入了三个重要的特性：
- en: The use of the SQL query language to represent data operations
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SQL查询语言来表示数据操作
- en: Datasets, a type-safe data-processing *domain specific language* (DSL) that
    resembles SQL
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Datasets，一种类型安全的数据处理*领域特定语言*（DSL），类似于SQL
- en: DataFrames, the dynamically typed counterpart of Datasets
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrames，Datasets的动态类型对应物
- en: For the purpose of this chapter, we assume the reader’s familiarity with Spark
    SQL, Datasets, and DataFrames. For a more in-depth Spark SQL discussion, we refer
    the reader to [[Karau2015]](app01.xhtml#Karau2015).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是假设读者对Spark SQL、Datasets和DataFrames有所了解。要深入了解Spark SQL，请参阅[[Karau2015]](app01.xhtml#Karau2015)。
- en: With the combination of Spark Streaming and Spark SQL, we get access to the
    significant data wrangling capabilities of Spark SQL from the context of a Spark
    Streaming job. We could efficiently enrich an incoming stream using reference
    data loaded from a database through DataFrames. Or apply advanced summary computations
    using the available SQL functions. We could also write the incoming or resulting
    data to one of the supported writer formats, such as Parquet, ORC, or to an external
    database through Java Database Connectivity (JDBC). The possibilities are endless.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 结合Spark Streaming和Spark SQL，我们可以从Spark Streaming作业的上下文中获得Spark SQL的重要数据操作能力。我们可以使用从数据库加载的数据框架来高效地丰富传入流，或者使用可用的SQL函数进行高级摘要计算。我们还可以将传入或生成的数据写入支持的写入格式之一，如Parquet、ORC，或通过Java数据库连接（JDBC）写入外部数据库。可能性是无限的。
- en: As we saw in [Part II](part02.xhtml#str-str), Apache Spark offers a native streaming
    API that uses the Dataset/DataFrame API abstractions and concepts. Structured
    Streaming should be our first option when it comes to tapping into the native
    SQL capabilities of Spark. But there are cases for which we want to access those
    capabilities from the context of Spark Streaming. In this chapter, we explore
    techniques that you can use when you need to combine Spark Streaming with Spark
    SQL.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第二部分](part02.xhtml#str-str)中看到的，Apache Spark提供了一个使用Dataset/DataFrame API抽象和概念的本机流API。当涉及到利用Spark的本机SQL功能时，结构化流应该是我们的首选。但是，在需要从Spark
    Streaming上下文中访问这些功能的情况下，也存在一些情况。在本章中，我们探讨了在需要将Spark Streaming与Spark SQL结合使用时可以使用的技术。
- en: Accessing Spark SQL Functions from Spark Streaming
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Spark Streaming访问Spark SQL函数
- en: The most common use case of augmenting Spark Streaming with Spark SQL is to
    gain access to the query capabilities and write access to the structured data
    formats supported, such as relational databases, comma-separated values (CSV),
    and Parquet files.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增强Spark Streaming与Spark SQL的结合，最常见的用例是访问查询功能，并将结构化数据格式的写入访问到支持的格式，如关系数据库、逗号分隔值（CSV）和Parquet文件。
- en: 'Example: Writing Streaming Data to Parquet'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：将流数据写入Parquet
- en: Our streaming dataset will consist of our running sensor information, containing
    the `sensorId`, a timestamp, and a value. For the sake of simplicity in this self-contained
    example, we are going to generate a randomized dataset, using a scenario that
    simulates a real Internet of Things (IoT) use case. The timestamp will be the
    time of execution, and each record will be formatted as a string coming from “the
    field” of comma-separated values.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的流数据集将包含我们运行的传感器信息，包括`sensorId`、时间戳和值。在这个自包含示例中，为了简化起见，我们将使用模拟真实物联网（IoT）用例的场景生成一个随机数据集。时间戳将是执行时间，每个记录将被格式化为来自逗号分隔值字段的字符串。
- en: Online Resources
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线资源
- en: For this example, we will use the `enriching-streaming-data` notebook in the
    online resources for the book, located at [*https://github.com/stream-processing-with-spark*](https://github.com/stream-processing-with-spark).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将使用书籍在线资源中的`enriching-streaming-data`笔记本，位于[*https://github.com/stream-processing-with-spark*](https://github.com/stream-processing-with-spark)。
- en: 'We also add a bit of real-world chaos to the data: due to weather conditions,
    some sensors publish corrupt data.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还向数据中添加了一点真实世界的混乱：由于天气条件的影响，一些传感器发布了损坏的数据。
- en: 'We begin by defining our data-generation functions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义我们的数据生成函数：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Caution
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We use a particular trick that requires a moment of attention. Note how the
    values in the preceding example are functions. Instead of creating an RDD of text
    records, we create an RDD of record-generating functions. Then, each time the
    RDD is evaluated, the record function will generate a new random record. This
    way we can simulate a realistic load of random data that delivers a different
    set on each batch.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个特别的技巧，需要稍微注意一下。注意前面示例中的值是函数。我们不是创建文本记录的RDD，而是创建生成记录函数的RDD。然后，每次评估RDD时，记录函数将生成一个新的随机记录。这样我们就可以模拟出一个真实的随机数据负载，每个批次都会提供不同的数据集。
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We sample some data:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们抽样一些数据：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And we create the Streaming Context:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建流处理上下文：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Our stream source will be a `ConstantInputDStream` fed by a record-generating
    RDD. By combining a `ConstantInputDStream` with the record-generating RDD, we
    create a self-generating stream of fresh, random data to process in our example.
    This method makes the example self-contained. It removes the need for an external
    stream-generating process:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的流源将是由生成记录的RDD提供的`ConstantInputDStream`。通过将`ConstantInputDStream`与生成记录的RDD结合使用，我们在示例中创建了一个自动生成的流，用于处理新鲜的随机数据。这种方法使示例自包含，不需要外部流生成过程：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We must provide the schema information for our streaming data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须为我们的流数据提供模式信息。
- en: 'Now that we have a DStream of fresh data processed in a two-second interval,
    we can begin focusing on the gist of this example. First, we want to define and
    apply a schema to the data that we are receiving. In Scala, we define a schema
    with a `case class`, as shown here:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了以两秒间隔处理的新数据的DStream，我们可以开始专注于此示例的主要内容。首先，我们想要定义并应用于我们接收到的数据的模式。在Scala中，我们使用`case
    class`定义模式，如下所示：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we need to apply the schema to the DStream using the `flatMap` function:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要使用`flatMap`函数将模式应用于DStream：
- en: Note
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We use `flatMap` instead of a `map` because there might be cases for which the
    incoming data is incomplete or corrupted.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`flatMap`而不是`map`，因为可能存在传入数据不完整或损坏的情况。
- en: If we use `map`, we would need to provide a resulting value for each transformed
    record. This is something that we cannot do for invalid records. With `flatMap`
    in combination with `Option`, we can represent valid records as `Some(recordValue)`,
    and invalid records as `None`. By the virtue of `flatMap` the internal `Option`
    container is flattened and our resulting stream will contain only valid `recordValue`s.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`map`，则需要为每个转换后的记录提供一个结果值。这对于无效记录是无法做到的。使用`flatMap`结合`Option`，我们可以将有效记录表示为`Some(recordValue)`，无效记录表示为`None`。通过`flatMap`，内部的`Option`容器被展平，因此我们的结果流将仅包含有效的`recordValue`。
- en: During the parsing of the comma-separated records, we not only protect ourselves
    against missing fields, but also parse the numeric values to their expected types.
    The surrounding `Try` captures any `NumberFormatException` that might arise from
    invalid records.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在解析逗号分隔记录时，我们不仅防止丢失字段，还将数值类型解析为其期望的类型。周围的`Try`捕获可能由无效记录引起的任何`NumberFormatException`。
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Saving DataFrames
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存数据帧
- en: 'With the schema stream in place, we can proceed to transform the underlying
    RDDs into DataFrames. We do this in the context of the general-purpose action
    `foreachRDD`. It’s impossible to use transformation at this point because a `DStream[DataFrame]`
    is undefined. This also means that any further operations we would like to apply
    to the DataFrame (or Dataset) need to be contained in the scope of the `foreachRDD`
    closure, as demonstrated here:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有了模式流之后，我们可以继续将底层的RDD转换为数据帧（DataFrames）。我们在通用操作`foreachRDD`的上下文中执行此操作。此时无法使用转换，因为`DStream[DataFrame]`未定义。这也意味着我们希望应用于数据帧（或数据集）的任何进一步操作都需要包含在`foreachRDD`闭包的范围内，如下所示：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we start the stream process:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们启动流处理过程：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We then can inspect the target directory to see the resulting data streaming
    into the Parquet file.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以检查目标目录，查看数据流入Parquet文件的情况。
- en: Now, let’s head to the URL `http://<spark-host>:4040` to inspect the Spark Console.
    There, you can see that both the SQL and Streaming tabs are present, as depicted
    in [Figure 23-1](#spark_console_streaming_parquet). In particular, the Streaming
    tab is of interest to us.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转到URL `http://<spark-host>:4040` 查看Spark控制台。在那里，您可以看到SQL和Streaming选项卡都存在，如[图
    23-1](#spark_console_streaming_parquet)所示。特别是，我们对Streaming选项卡感兴趣。
- en: '![spas 2301](Images/spas_2301.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2301](Images/spas_2301.png)'
- en: Figure 23-1\. Saving a stream to Parquet
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 23-1\. 将流保存为Parquet
- en: In [Figure 23-1](#spark_console_streaming_parquet), you can see how the writing
    time of Parquet quickly increases with time. Appending operations to Parquet files
    becomes more expensive with time. For long-lived processes like a Spark Streaming
    job, one way to work around this limitation is to append to a fresh file every
    so often.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 23-1](#spark_console_streaming_parquet)中，您可以看到Parquet写入时间随时间快速增加。随着时间的推移，向Parquet文件追加操作变得更加昂贵。对于像Spark
    Streaming作业这样的长期运行进程，解决此限制的一种方法是定期向新文件追加。
- en: In [Example 23-1](#snippet_timestamp), we change the suffix of the file every
    hour (3,600 seconds), measuring from the moment the streaming job started.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在[例子 23-1](#snippet_timestamp)中，我们每小时（3,600秒）更改文件的后缀，从流作业启动的时刻开始计算。
- en: Example 23-1\. Timestamping a file destination
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 23-1\. 为文件目的地添加时间戳
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In the Streaming UI, you might notice that the Input Rate chart remains flat
    on zero. This chart collects its information from the receiver implementation.
    Because we are using a `ConstantInputDStream`, there’s no actual incoming record
    count for this chart.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在流处理界面中，您可能会注意到输入速率图表仍然保持零平面。该图表从接收器实现中收集信息。因为我们使用的是 `ConstantInputDStream`，所以对于此图表，没有实际的传入记录计数。
- en: Dealing with Data at Rest
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理静态数据
- en: A typical question that arises during the design and implementation of streaming
    applications is how to use existing data to enrich the events in the stream. This
    data “at rest” could be historical records in files, lookup tables, tables on
    a database, or any other static data that we can use to enhance to the data “in
    flight.”
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计和实现流处理应用程序时，通常会出现一个问题，即如何使用现有数据来丰富流中的事件。这些“静态”的数据可以是文件中的历史记录、查找表、数据库中的表，或者是我们可以用来增强“在途”数据的任何其他静态数据。
- en: We can use the capabilities of Spark SQL to load “resting” datasets that can
    be combined with the incoming streaming data. A key advantage of Spark SQL in
    this scenario is that the loaded data is structured. This reduces the effort that
    the streaming developer needs to invest in preparing the data into the right format
    before performing a join operation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用 Spark SQL 的能力加载“静态”数据集，这些数据集可以与传入的流数据进行合并。在这种情况下，Spark SQL 的一个关键优势是加载的数据是结构化的。这减少了流处理开发人员在执行连接操作之前需要投入的准备数据格式的工作量。
- en: This example illustrates the use of `join` on a fixed dataset in the form of
    a DataFrame and the streaming data that is consumed continuously by our application.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例说明了在 DataFrame 形式的固定数据集和我们应用程序连续消耗的流数据上使用 `join` 操作的用法。
- en: Using Join to Enrich the Input Stream
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用连接操作来丰富输入流
- en: In our previous example, we processed an incoming stream of sensor data, parsed
    it into valid records, and stored it directly into a Parquet file. But to interpret
    the values in the reported data points, we first need to know the sensor type,
    its working range, and the units of each recorded value.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的示例中，我们处理了传入的传感器数据流，将其解析为有效的记录，并直接存储到 Parquet 文件中。但是要解释报告数据点中的值，我们首先需要了解传感器类型、其工作范围以及每个记录值的单位。
- en: In our IoT application, sensors are initially registered before they are deployed.
    The registration process captures the information that we require. Luckily for
    us, this information is available as a CSV file export that we can import in our
    streaming application, as we can see in [Figure 23-2](#sensor_reference_data).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的物联网应用程序中，传感器在部署之前会首先进行注册。注册过程捕获了我们需要的信息。幸运的是，这些信息以 CSV 文件的形式导出，我们可以在我们的流处理应用程序中导入，正如我们在
    [图 23-2](#sensor_reference_data) 中所看到的。
- en: Note
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For this example, we are going to discuss only the relevant differences with
    the previous program. The [complete notebook is available online](https://github.com/LearningSparkStreaming/notebooks)
    for your own exploration.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们只讨论与前一个程序的相关差异。[完整的笔记本可在线查看](https://github.com/LearningSparkStreaming/notebooks)，供您自行探索。
- en: 'Let’s define some constants with the locations of our data:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一些常量，指定我们数据的位置：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we load the reference data from a Parquet file (see [Figure 23-2](#sensor_reference_data)).
    We also cache the data to keep it in memory and improve the performance of our
    streaming application:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们从 Parquet 文件中加载参考数据（参见 [图 23-2](#sensor_reference_data)）。我们还将数据缓存到内存中，以提高我们流处理应用程序的性能：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![spas 2302](Images/spas_2302.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2302](Images/spas_2302.png)'
- en: Figure 23-2\. Sample reference data
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 23-2\. 示例参考数据
- en: Next, we enrich the streaming data. With the schema stream in place, we can
    proceed to transform the underlying RDDs into DataFrames. This time, we are going
    to use the reference data to add the specific sensor information. We are also
    going to denormalize the recorded value according to the sensor range so that
    we don’t need to repeat that data in the resulting dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将丰富流数据。有了模式流的基础，我们可以继续将底层 RDD 转换为 DataFrame。这一次，我们将使用参考数据添加特定的传感器信息。我们还将根据传感器的范围去规范化记录的值，这样我们就不需要在结果数据集中重复这些数据。
- en: 'As before, we do this in the context of the general-purpose action `foreachRDD`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往一样，在通用操作 `foreachRDD` 的上下文中进行：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Warning
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The seemingly weird construct for the `stableSparkSession` is necessary because
    in the SparkNotebook, the `sparkSession` reference is a mutable variable, and
    we cannot issue an import from a non-stable reference.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`stableSparkSession`看似奇怪的结构是必要的，因为在Spark笔记本中，`sparkSession`引用是一个可变变量，我们不能从一个非稳定的引用中导入。'
- en: 'Go ahead and inspect the result. We can use the current Spark Session concurrently
    with the running Spark Streaming job in order to inspect the resulting data, as
    shown in [Figure 23-3](#sample_enriched_records):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 继续检查结果。我们可以同时使用当前的Spark Session与运行中的Spark Streaming作业来检查结果数据，如[图 23-3](#sample_enriched_records)所示：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![spas 2303](Images/spas_2303.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2303](Images/spas_2303.png)'
- en: Figure 23-3\. Sample from the enrichedRecords DataFrame
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 23-3\. 从enrichedRecords DataFrame中的样本
- en: 'At this point, we can see the record count evolving. We can issue a `count`
    on the resulting dataset to see how the streaming process increments the dataset.
    We wait a few moments between the two executions to observe the difference:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以看到记录计数在不断增加。我们可以对结果数据集执行`count`以查看流处理如何增加数据集。在两次执行之间等待片刻以观察差异：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Join Optimizations
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接优化
- en: 'Our current solution has a major drawback: it will drop incoming data from
    unregistered sensors. Because we are loading the reference data only once at the
    start of our process, sensors registered after that moment will be silently dropped.
    We can improve this situation by using a different kind of `join` operation. Let’s
    remember that within the `foreachRDD`, we have full access to the capabilities
    of other Spark libraries. In this particular case, the `join` operation we are
    using comes from Spark SQL, and we can make use of options in that package to
    enhance our streaming process. In particular, we are going to use an *outer* join
    to differentiate between the IoT sensor IDs that are known to the system from
    those that are unknown. We can then write the data from the unknown devices to
    a separate file for later reconciliation.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的解决方案有一个主要缺点：它会丢弃来自未注册传感器的传入数据。因为我们在过程开始时只加载一次参考数据，之后注册的传感器将被悄悄地丢弃。我们可以通过使用不同类型的`join`操作来改善这种情况。让我们记住，在`foreachRDD`中，我们可以充分利用其他Spark库的功能。在这种特殊情况下，我们使用的`join`操作来自Spark
    SQL，并且我们可以利用该包中的选项来增强我们的流处理过程。特别是，我们将使用*outer* join来区分系统已知的IoT传感器ID和未知的IoT传感器ID。然后，我们可以将未知设备的数据写入一个单独的文件，以便后续对账。
- en: The rest of the program remains the same except for the `foreachRDD` call, in
    which we add the new logic, as demonstrated in [Example 23-2](#broadcast-outer-join).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的其余部分保持不变，除了`foreachRDD`调用之外，在此我们添加了新的逻辑，如[示例 23-2](#broadcast-outer-join)所示。
- en: Example 23-2\. Using broadcast optimization with an outer join
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 23-2\. 使用广播优化进行外连接
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Sharp eyes will notice that we have introduced two changes to the join operation.
    [Example 23-3](#rightouter-detail) focuses on them.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 细心的人会注意到，我们对连接操作进行了两处改动。[示例 23-3](#rightouter-detail)专注于这些改动。
- en: Example 23-3\. Detail of the join operation
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 23-3\. 连接操作的详细信息
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The difference is that we changed the order of the join. Instead of joining
    the incoming data with the reference dataset, we do the opposite. We need that
    change in direction for a particular reason: we are adding a `broadcast` hint
    to the join expression to instruct Spark to perform a *broadcast join*.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 不同之处在于我们改变了连接的顺序。我们不再将传入数据与参考数据集连接，而是反过来。我们需要这种方向的改变有一个特定的原因：我们在连接表达式中添加了一个`broadcast`提示，以指示Spark执行*广播连接*。
- en: Broadcast joins, also known as *map-side joins* in Hadoop jargon, are useful
    when there’s a large difference in size between the two datasets participating
    in the join and one of them is small enough to be sent to the memory of every
    executor. Instead of performing an I/O-heavy shuffle-based join, the small dataset
    can be used as an in-memory lookup table on each executor, in parallel. This results
    in lower I/O overhead and hence faster execution time.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop行话中被称为*map-side joins*的广播连接，在参与连接的两个数据集之间大小存在显著差异并且其中一个数据集足够小以至于能够发送到每个执行器的内存时非常有用。与执行I/O密集型的基于shuffle的连接不同，小数据集可以在每个执行器上作为内存中的查找表使用，以并行方式进行。这样可以减少I/O开销，从而提高执行速度。
- en: In our scenario, we swap the order of the participating datasets because we
    know that our reference dataset is much larger than the received device data at
    each interval. Whereas our reference dataset contains a record for every device
    known to us, the streaming data contains only a sample of the total population.
    Therefore, in terms of performance, it’s better to broadcast the data from the
    stream to execute the broadcast join with the reference data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的场景中，我们交换了参与数据集的顺序，因为我们知道我们的参考数据集比每个间隔接收的设备数据要大得多。虽然我们的参考数据集包含我们所知道的每个设备的记录，但流数据只包含总体样本的一部分。因此，从性能上讲，最好将流中的数据广播以执行与参考数据的广播连接。
- en: The last remark in this code fragment is the join direction, given by the join
    type `rightouter`. This join type preserves all records from the right side, our
    incoming sensor data, and adds the fields from the left side if the join condition
    matches. In our case, that is a matching `sensorId`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段中的最后一条备注是连接方向，由连接类型 `rightouter` 指定。此连接类型保留右侧的所有记录，即我们传入的传感器数据，并在匹配的连接条件下添加左侧的字段。在我们的情况下，这是匹配的
    `sensorId`。
- en: 'We store the results in two files: one with the enriched sensor data for the
    known `SensorId`s (see [Figure 23-4](#sample_enriched_records2)), and another
    with the raw data of the sensors that we don’t know at this time (see [Figure 23-5](#sample_unknown_devices)).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将结果存储在两个文件中：一个是已知 `SensorId` 的丰富传感器数据（参见 [图 23-4](#sample_enriched_records2)），另一个是我们当前不知道的传感器的原始数据（参见
    [图 23-5](#sample_unknown_devices)）。
- en: '![spas 2304](Images/spas_2304.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2304](Images/spas_2304.png)'
- en: Figure 23-4\. Sample of enriched records
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 23-4\. 丰富记录样本
- en: '![spas 2305](Images/spas_2305.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2305](Images/spas_2305.png)'
- en: Figure 23-5\. Sample of records from unknown devices
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 23-5\. 未知设备记录样本
- en: For an in-depth discussion of the different join options in Spark and their
    particular characteristics, read [[Karau2017]](app03.xhtml#Karau2017).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入讨论 Spark 中不同连接选项及其特性，请阅读 [[Karau2017]](app03.xhtml#Karau2017)。
- en: Updating Reference Datasets in a Streaming Application
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新流应用程序中的参考数据集
- en: In the previous section, you saw how we could load a static dataset to enrich
    an incoming stream.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，您看到了我们如何加载静态数据集来丰富传入的数据流。
- en: Although some datasets are fairly static, like last year’s synthetic profile
    for Smart Grids, calibration data for IoT sensors, or population distribution
    following a census, oftentimes we will also find that the dataset that we need
    to combine with our streaming data changes, as well. Those changes might be at
    a different, much slower pace than our streaming application and hence do not
    need to be considered a stream on their own.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管某些数据集相当静态，例如去年的智能电网合成配置文件、物联网传感器的校准数据或人口分布在人口普查后的分布，我们通常也会发现，我们需要与流数据组合的数据集也在发生变化。这些变化可能比我们的流应用程序慢得多，因此不需要单独考虑为流。
- en: In their digital evolution, organizations often have a mix of processes of different
    cadences. Slow-paced output processes such as data exports or daily reports are
    valid inputs of our streaming system that require regular, but not continuous
    updates.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在数字化进程中，组织通常拥有不同节奏的混合过程。诸如数据导出或每日报告等慢节奏输出过程是我们流处理系统的有效输入，需要定期更新，但不需要连续更新。
- en: We are going to explore a Spark Streaming technique to integrate a static dataset
    (“slow data”) into our streaming (“fast data”) track.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨一种 Spark Streaming 技术，将静态数据集（“慢数据”）集成到我们的流数据（“快数据”）轨道中。
- en: At its core, Spark Streaming is a high-performance scheduling and coordination
    application. To ensure data integrity, Spark Streaming schedules the different
    output operations of a streaming job in sequence. The order of declaration in
    the application code becomes the execution sequence at runtime.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，Spark Streaming 是一个高性能调度和协调应用程序。为了确保数据完整性，Spark Streaming 按顺序调度流作业的不同输出操作。应用程序代码中声明的顺序在运行时成为执行顺序。
- en: '[Chapter 18](ch18.xhtml#dstream-fund) looks at the `batch interval` and how
    it provides the synchronization point between the collection of data and the submission
    of previously collected data to the Spark engine for further processing. We can
    hook onto the Spark Streaming scheduler in order to execute Spark operations other
    than stream processing.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 18 章](ch18.xhtml#dstream-fund) 讨论了 `batch interval` 及其如何提供数据收集和之前收集的数据提交到
    Spark 引擎进行进一步处理的同步点。我们可以钩入 Spark Streaming 调度程序，以执行除流处理之外的 Spark 操作。'
- en: In particular, we will use a `ConstantInputDStream` with an empty input as the
    essential building block to schedule our additional operations within the streaming
    application. We combine the empty DStream with the general-purpose operation `foreachRDD`
    to have Spark Streaming take care of the regular execution of those additional
    operations we require in the context of our application.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们将使用一个 `ConstantInputDStream` 和一个空输入作为在流应用程序中调度额外操作的基本构建块。我们将空的 DStream
    与通用操作 `foreachRDD` 结合使用，以便 Spark Streaming 处理我们应用程序上下文中所需的常规执行额外操作。
- en: Enhancing Our Example with a Reference Dataset
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用参考数据集增强我们的示例
- en: To better understand the dynamics of this technique, let’s continue with our
    running example.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一技术的动态，请继续我们的运行示例。
- en: In the previous section, we used a reference dataset that contained the description
    of each sensor known to the system. That data was used to enrich the streaming
    sensor data with the parameters needed for its further processing. A critical
    limitation of that approach is that after the streaming application is started,
    we cannot add, update, or remove any sensor present in that list.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们使用了一个包含系统已知传感器描述的参考数据集。该数据用于将流式传感器数据与其进一步处理所需的参数进行丰富。该方法的一个关键限制是，在启动流应用程序后，我们无法添加、更新或删除列表中存在的任何传感器。
- en: In the context of our example, we get an update of that list every hour. We
    would like that our streaming application would use the new reference data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例上下文中，我们每小时获取该列表的更新。我们希望我们的流应用程序使用新的参考数据。
- en: Loading the reference data from a Parquet file
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 Parquet 文件加载参考数据
- en: 'As in the previous example, we load the reference data from the Parquet file.
    We also cache the data to keep it in memory and improve the performance of our
    streaming application. The only difference that we observe in this step is that
    we now use a `variable` instead of a `value` to keep the reference to our reference
    data. We need to make this reference mutable because we will be updating it with
    new data as the streaming application runs:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，我们从 Parquet 文件加载参考数据。我们还将数据缓存到内存中，以提高流应用程序的性能。在这一步中唯一观察到的区别是，我们现在使用一个
    `variable` 而不是 `value` 来保持对参考数据的引用。我们需要将此引用设置为可变的，因为随着流应用程序的运行，我们将使用新数据更新它：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Setting up the refreshing mechanism
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置刷新机制
- en: To periodically load the reference data, we are going to *hook* onto the Spark
    Streaming scheduler. We can do that only in terms of the `batch interval`, which
    serves as the internal tick of the clock for all operations. Hence, we express
    the refresh interval as a window over the base batch interval. In practical terms,
    every `x` batches, we are going to refresh our reference data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定期加载参考数据，我们将 *hook* 到 Spark Streaming 调度程序上。我们只能通过 `batch interval` 来实现这一点，它作为所有操作的内部时钟基础。因此，我们将刷新间隔表示为基本批处理间隔的窗口。实际上，每
    `x` 个批次，我们将刷新我们的参考数据。
- en: We use a `ConstantInputDStream` with an empty RDD. This ensures that at all
    times we have an empty DStream whose only function will be to give us access to
    the scheduler through the `foreachRDD` function. At each window interval, we update
    the variable that points to the current `DataFrame`. This is a safe construction
    because the Spark Streaming scheduler will linearly execute the scheduled operations
    that are due at each `batch interval`. Therefore, the new data will be available
    for the upstream operations that make use of it.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个 `ConstantInputDStream` 和一个空的 RDD。这确保我们始终有一个空的 DStream，其唯一的功能是通过 `foreachRDD`
    函数让我们访问调度程序。在每个窗口间隔内，我们更新指向当前 `DataFrame` 的变量。这是一个安全的构造，因为 Spark Streaming 调度程序将按照每个
    `batch interval` 应执行的调度操作的线性顺序执行。因此，新数据将可供使用它的上游操作使用。
- en: 'We use caching to ensure that the reference dataset is loaded only once over
    the intervals that it’s used in the streaming application. It’s also important
    to `unpersist` the expiring data that was previously cached in order to free up
    resources in the cluster and ensure that we have a stable system from the perspective
    of resource consumption:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用缓存确保参考数据集仅在流应用程序中使用的间隔期间加载一次。重要的是，为了释放集群中的资源并确保从资源消耗的角度看，我们具有稳定的系统，需要 `unpersist`
    先前缓存的过期数据：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We use a tumbling window of 60 seconds for our refresh process.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的刷新过程使用一个 60 秒的滚动窗口。
- en: Runtime implications
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行时影响
- en: Loading a large dataset incurs a cost in time and resources and therefore it
    has an operational impact. [Figure 23-6](#reference-data-loading-chart) depicts
    the recurrent spikes in processing time that correspond to the loading of the
    reference dataset.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 加载大型数据集会消耗大量时间和资源，因此会对操作产生影响。[图 23-6](#reference-data-loading-chart) 描述了处理时间中的周期性峰值，这些峰值对应于参考数据集的加载。
- en: '![spas 2306](Images/spas_2306.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2306](Images/spas_2306.png)'
- en: Figure 23-6\. Loading reference data has a visible effect on the runtime
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 23-6\. 加载参考数据对运行时有显著影响
- en: By scheduling the load of reference data at a rate much lower than the cadence
    of the streaming application, the cost is amortized over a relatively large number
    of microbatches. Nevertheless, you need to take this additional load into consideration
    when planning for cluster resources in order to have a stable execution over time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以比流应用程序的频率低得多的速率调度加载参考数据，可以使成本在相对大量的微批次上摊销。然而，在规划集群资源时，你需要考虑这种额外负载，以确保长时间内的稳定执行。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Even though SQL capabilities are native to Structured Streaming, in this chapter
    you saw how to use Spark SQL features from Spark Streaming and the possibilities
    that this combination opens up. You learned about the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 SQL 能力是结构化流处理的本地特性，但在本章中，你看到了如何从 Spark Streaming 中使用 Spark SQL 特性以及这种组合开放的可能性。你了解到了以下内容：
- en: Accessing the Spark SQL Context from output operations in Spark Streaming
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Spark Streaming 的输出操作中访问 Spark SQL 上下文
- en: Loading and reloading data at rest to enrich our streaming data
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载和重新加载静态数据以丰富我们的流数据
- en: How to join data with different join modes
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用不同的连接模式连接数据
