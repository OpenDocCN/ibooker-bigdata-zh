- en: Chapter 8\. Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 数据
- en: Has it occurred to you that she might not have been a reliable source of information?
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你有没有想过她可能并不是一个可靠的信息来源？
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Jon Snow
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —琼恩·雪诺
- en: With the knowledge acquired in previous chapters, you are now equipped to start
    doing analysis and modeling at scale! So far, however, we haven’t really explained
    much about how to read data into Spark. We’ve explored how to use `copy_to()`
    to upload small datasets or functions like `spark_read_csv()` or `spark_write_csv()`
    without explaining in detail how and why.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 借助前几章所学，你现在具备了开始规模化分析和建模的知识！然而，到目前为止，我们还没有详细解释如何将数据读入Spark。我们探讨了如何使用`copy_to()`上传小数据集或者像`spark_read_csv()`或`spark_write_csv()`这样的函数，但并没有详细解释其具体操作和原因。
- en: So, you are about to learn how to read and write data using Spark. And, while
    this is important on its own, this chapter will also introduce you to the *data
    lake*—a repository of data stored in its natural or raw format that provides various
    benefits over existing storage architectures. For instance, you can easily integrate
    data from external systems without transforming it into a common format and without
    assuming those sources are as reliable as your internal data sources.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你即将学习如何使用Spark读取和写入数据。而且，虽然这本身很重要，但本章还将向你介绍*数据湖*——一个以其自然或原始格式存储数据的仓库，相比现有的存储架构，它提供了各种好处。例如，你可以轻松地集成来自外部系统的数据，而无需将其转换为通用格式，也无需假设这些来源与你内部数据源一样可靠。
- en: In addition, we will also discuss how to extend Spark’s capabilities to work
    with data not accessible out of the box and make several recommendations focused
    on improving performance for reading and writing data. Reading large datasets
    often requires you to fine-tune your Spark cluster configuration, but that’s the
    topic of [Chapter 9](ch09.html#tuning).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将讨论如何扩展Spark的能力，以处理默认情况下无法访问的数据，并提出几条关于优化读写数据性能的建议。要读取大型数据集通常需要微调你的Spark集群配置，但这是[第9章](ch09.html#tuning)的主题。
- en: Overview
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In [Chapter 1](ch01.html#intro), you learned that beyond big data and big compute,
    you can also use Spark to improve velocity, variety, and veracity in data tasks.
    While you can use the learnings of this chapter for any task requiring loading
    and storing data, it is particularly interesting to present this chapter in the
    context of dealing with a variety of data sources. To understand why, we should
    first take a quick detour to examine how data is currently processed in many organizations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.html#intro)中，你了解到除了大数据和大计算之外，你还可以使用Spark来提高数据任务中的速度、多样性和真实性。虽然你可以将本章的学习应用于任何需要加载和存储数据的任务，但将本章放在处理多种数据源的背景下尤为有趣。要理解原因，我们首先应该快速了解一下当前许多组织如何处理数据。
- en: 'For several years, it’s been a common practice to store large datasets in a
    relational *database*, a system first proposed in 1970 by Edgar F. Codd.^([1](ch08.html#idm46099145684872))
    You can think of a database as a collection of tables that are related to one
    another, where each table is carefully designed to hold specific data types and
    relationships to other tables. Most relational database systems use *Structured
    Query Language* (SQL) for querying and maintaining the database. Databases are
    still widely used today, with good reason: they store data reliably and consistently;
    in fact, your bank probably stores account balances in a database and that’s a
    good practice.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，将大型数据集存储在关系型*数据库*中已经成为一种常见做法，这个系统最早由Edgar F. Codd在1970年提出。^([1](ch08.html#idm46099145684872))
    你可以将数据库视为一个相互关联的表集合，每个表都精心设计以容纳特定的数据类型和与其他表的关系。大多数关系型数据库系统使用*结构化查询语言*（SQL）进行查询和维护数据库。数据库至今仍然被广泛使用，理由很充分：它们可靠而一致地存储数据；事实上，你的银行可能正是将账户余额存储在数据库中，这是一个良好的实践。
- en: However, databases have also been used to store information from other applications
    and systems. For instance, your bank might also store data produced by other banks,
    such as incoming checks. To accomplish this, the external data needs to be extracted
    from the external system, transformed into something that fits the current database,
    and finally be loaded into it. This is known as *Extract, Transform, and Load*
    (ETL), a general procedure for copying data from one or more sources into a destination
    system that represents the data differently from the source. The ETL process became
    popular in the 1970s.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据库还被用来存储来自其他应用程序和系统的信息。例如，您的银行还可能存储其他银行产生的数据，如进账支票。为了完成这个任务，外部数据需要从外部系统中提取出来，转换为适合当前数据库的形式，最后加载到其中。这被称为*提取、转换和加载*（ETL），这是将数据从一个或多个源复制到表示与源数据不同的目标系统的一般过程。ETL过程在1970年代变得流行起来。
- en: Aside from databases, data is often also loaded into a *data warehouse*, a system
    used for reporting and data analysis. The data is usually stored and indexed in
    a format that increases data analysis speed but that is often not suitable for
    modeling or running custom distributed code. The challenge is that changing databases
    and data warehouses is usually a long and delicate process, since data needs to
    be reindexed and the data from multiple data sources needs to be carefully transformed
    into single tables that are shared across data sources.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据库外，数据通常还加载到*数据仓库*中，这是用于报告和数据分析的系统。数据通常以增加数据分析速度的格式存储和索引，但通常不适合建模或运行定制分布式代码。挑战在于改变数据库和数据仓库通常是一个漫长而微妙的过程，因为需要重新索引数据，并且来自多个数据源的数据需要小心地转换为在所有数据源之间共享的单一表格。
- en: Instead of trying to transform all data sources into a common format, you can
    embrace this variety of data sources in a *data lake*, a system or repository
    of data stored in its natural format (see [Figure 8-1](#data-data-lake)). Since
    data lakes make data available in its original format, there is no need to carefully
    transform it in advance; anyone can use it for analysis, which adds significant
    flexibility over ETL. You then can use Spark to unify data processing from data
    lakes, databases, and data warehouses through a single interface that is scalable
    across all of them. Some organizations also use Spark to replace their existing
    ETL process; however, this falls in the realm of data engineering, which is well
    beyond the scope of this book. We illustrate this with dotted lines in [Figure 8-1](#data-data-lake).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是试图将所有数据源转换为通用格式，您可以在*数据湖*中接受这种多样化的数据源，即一种以其自然格式存储的数据系统或数据存储库（参见[图8-1](#data-data-lake)）。由于数据湖使数据以其原始格式可用，因此无需事先仔细转换它；任何人都可以用它进行分析，这增加了与ETL相比的显著灵活性。然后，您可以使用Spark通过一个可在所有这些数据源上扩展的单一接口统一来自数据湖、数据库和数据仓库的数据处理。一些组织还使用Spark替换其现有的ETL过程；然而，这超出了本书的范围，我们在[图8-1](#data-data-lake)中用虚线来说明这一点。
- en: '![Spark processing raw data from a data lakes, databases, and data warehouses](assets/mswr_0801.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Spark处理来自数据湖、数据库和数据仓库的原始数据](assets/mswr_0801.png)'
- en: Figure 8-1\. Spark processing raw data from a data lakes, databases, and data
    warehouses
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. Spark处理来自数据湖、数据库和数据仓库的原始数据
- en: To support a broad variety of data sources, Spark needs to be able to read and
    write data in several different file formats (CSV, JSON, Parquet, and others),
    and access them while stored in several file systems (HDFS, S3, DBFS, and more)
    and, potentially, interoperate with other storage systems (databases, data warehouses,
    etc.). We will get to all of that, but first, we will start by presenting how
    to read, write, and copy data using Spark.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持广泛的数据源，Spark需要能够读取和写入多种不同的文件格式（CSV、JSON、Parquet等），并且在存储在多个文件系统中的数据（HDFS、S3、DBFS等）中访问它们，并且可能与其他存储系统（数据库、数据仓库等）进行互操作。我们会讲到所有这些内容，但首先，我们将开始介绍如何使用Spark读取、写入和复制数据。
- en: Reading Data
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取数据
- en: If you are new to Spark, it is highly recommended to review this section before
    you start working with large datasets. We will introduce several techniques that
    improve the speed and efficiency of reading data. Each subsection presents specific
    ways to take advantage of how Spark reads files, such as the ability to treat
    entire folders as datasets as well as being able to describe them to read datasets
    faster in Spark.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是 Spark 的新手，在开始处理大型数据集之前，强烈建议您先审查本节内容。我们将介绍几种提高读取数据速度和效率的技术。每个子节都介绍了利用 Spark
    读取文件的特定方法，例如能够将整个文件夹视为数据集以及能够描述它们以在 Spark 中更快地读取数据集。
- en: Paths
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 路径
- en: 'When analyzing data, loading multiple files into a single data object is a
    common scenario. In R, we typically use a loop or a functional programming directive
    to accomplish this. This is because R must load each file individually into your
    R session. Let’s create a few CSV files inside a folder and read them with R first:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析数据时，将多个文件加载到单个数据对象中是一种常见情况。在 R 中，我们通常使用循环或函数式编程指令来完成这个任务。这是因为 R 必须将每个文件单独加载到您的
    R 会话中。让我们先在一个文件夹中创建几个 CSV 文件，然后用 R 读取它们：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In Spark, there is the notion of a folder as a dataset. Instead of enumerating
    each file, simply pass the path containing all the files. Spark assumes that every
    file in that folder is part of the same dataset. This implies that the target
    folder should be used only for data purposes. This is especially important since
    storage systems like HDFS store files across multiple machines, but, conceptually,
    they are stored in the same folder; when Spark reads the files from this folder,
    it’s actually executing distributed code to read each file within each machine—no
    data is transferred between machines when distributed files are read:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，有一个将文件夹视为数据集的概念。而不是枚举每个文件，只需传递包含所有文件的路径。Spark 假定该文件夹中的每个文件都是同一个数据集的一部分。这意味着目标文件夹应仅用于数据目的。这一点尤为重要，因为像
    HDFS 这样的存储系统会将文件存储在多台机器上，但从概念上讲，它们存储在同一个文件夹中；当 Spark 从此文件夹读取文件时，实际上是在执行分布式代码以在每台机器上读取每个文件
    —— 在分布式文件读取时，不会在机器之间传输数据：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The “folder as a table” idea is found in other open source technologies as well.
    Under the hood, Hive tables work the same way. When you query a Hive table, the
    mapping is done over multiple files within the same folder. The folder’s name
    usually matches the name of the table visible to the user.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: “文件夹作为表格”的想法在其他开源技术中也可以找到。在底层，Hive 表的工作方式相同。当您查询 Hive 表时，映射是在同一文件夹中的多个文件上完成的。该文件夹的名称通常与用户可见的表格名称相匹配。
- en: Next, we will present a technique that allows Spark to read files faster as
    well as to reduce read failures by describing the structure of a dataset in advance.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍一种技术，允许 Spark 更快地读取文件，并通过提前描述数据集的结构来减少读取失败的情况。
- en: Schema
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式
- en: When reading data, Spark is able to determine the data source’s column names
    and column types, also known as the *schema*. However, guessing the schema comes
    at a cost; Spark needs to do an initial pass on the data to guess what it is.
    For a large dataset, this can add a significant amount of time to the data ingestion
    process, which can become costly even for medium-size datasets. For files that
    are read over and over again, the additional read time accumulates over time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取数据时，Spark 能够确定数据源的列名和列类型，也称为*模式*。然而，猜测模式会带来成本；Spark 需要对数据进行初始扫描来猜测数据类型。对于大型数据集，这可能会为数据摄取过程增加显著的时间成本，即使对于中等大小的数据集也是如此。对于反复读取的文件，额外的读取时间会随着时间的推移累积。
- en: 'To avoid this, Spark allows you to provide a column definition by providing
    a `columns` argument to describe your dataset. You can create this schema by sampling
    a small portion of the original file yourself:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，Spark 允许您通过提供 `columns` 参数来定义列，描述您的数据集。您可以通过自己取样原始文件的一个小部分来创建这个模式：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Or, you can set the column specification to a vector containing the column
    types explicitly. The vector’s values are named to match the field names:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以将列规范设置为包含显式列类型的向量。向量的值被命名以匹配字段名称：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The accepted variable types are: `integer`, `character`, `logical`, `double`,
    `numeric`, `factor`, `Date`, and `POSIXct`.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接受的变量类型包括：`integer`、`character`、`logical`、`double`、`numeric`、`factor`、`Date`
    和 `POSIXct`。
- en: Then, when reading using `spark_read_csv()`, you can pass `spec_with_r` to the
    `columns` argument to match the names and types of the original file. This helps
    to improve performance since Spark will not need to determine the column types.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在使用`spark_read_csv()`读取时，可以将`spec_with_r`传递给`columns`参数，以匹配原始文件的名称和类型。这有助于提高性能，因为Spark无需确定列类型。
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following example shows how to set the field type to something different.
    However, the new field type needs to be a compatible type in the original dataset.
    For example, you cannot set a `character` field to `numeric`. If you use an incompatible
    type, the file read will fail with an error. Additionally, the following example
    also changes the names of the original fields:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例显示如何将字段类型设置为不同的内容。但是，新字段类型必须是原始数据集中的兼容类型。例如，您不能将`character`字段设置为`numeric`。如果使用不兼容的类型，文件读取将失败并显示错误。此外，以下示例还更改了原始字段的名称：
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In Spark, malformed entries can cause errors during reading, particularly for
    non-character fields. To prevent such errors, we can use a file specification
    that imports them as characters and then use `dplyr` to coerce the field into
    the desired type.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，格式错误的条目可能会在读取时引发错误，特别是对于非字符字段。为了避免这种错误，我们可以使用一个文件规范，将它们导入为字符，然后使用`dplyr`将字段强制转换为所需的类型。
- en: This subsection reviewed how we can read files faster and with fewer failures,
    which lets us start our analysis more quickly. Another way to accelerate our analysis
    is by loading less data into Spark memory, which we examine in the next section.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节回顾了如何更快地读取文件并减少失败，这使我们能够更快地开始分析。加速分析的另一种方法是将更少的数据加载到Spark内存中，这将在下一节中进行探讨。
- en: Memory
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存
- en: By default, when using Spark with R, when you read data, it is copied into Spark’s
    distributed memory, making data analysis and other operations very fast. There
    are cases, such as when the data is too big, for which loading all the data might
    not be practical or even necessary. For those cases, Spark can just “map” the
    files without copying data into memory.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，在使用R语言与Spark时，当您读取数据时，数据会被复制到Spark的分布式内存中，使得数据分析和其他操作非常快速。有些情况下，比如数据量太大时，加载所有数据可能并不切实际，甚至是不必要的。对于这些情况，Spark可以只是“映射”文件，而不将数据复制到内存中。
- en: The mapping creates a sort of virtual table in Spark. The implication is that
    when a query runs against that table, Spark needs to read the data from the files
    at that time. Any consecutive reads after that will do the same. In effect, Spark
    becomes a pass-through for the data. The advantage of this method is that there
    is almost no up-front time cost to “reading” the file; the mapping is very fast.
    The downside is that running queries that actually extract data will take longer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 映射在Spark中创建了一种虚拟表。其含义是，当针对该表运行查询时，Spark需要在那时从文件中读取数据。之后的任何连续读取也将执行相同操作。实际上，Spark成为数据的传递。该方法的优点是几乎没有“读取”文件的前期时间成本；映射非常快速。缺点是实际提取数据的查询将需要更长时间。
- en: 'This is controlled by the `memory` argument of the read functions. Setting
    it to `FALSE` prevents the data copy (the default is `TRUE`):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这由读取函数的`memory`参数控制。将其设置为`FALSE`可以防止数据复制（默认为`TRUE`）：
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'There are good use cases for this method, one of which is when not all columns
    of a table are needed. For example, take a very large file that contains many
    columns. Assuming this is not the first time you interact with this data, you
    would know what columns are needed for the analysis. When you know which columns
    you need, the files can be read using `memory = FALSE`, and then the needed columns
    can be selected with `dplyr`. The resulting `dplyr` variable can then be cached
    into memory, using the `compute()` function. This will make Spark query the file(s),
    pull the selected fields, and copy only that data into memory. The result is an
    in-memory table that took comparatively less time to ingest:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有很好的使用案例，其中之一是当不需要表的所有列时。例如，假设有一个包含许多列的非常大的文件。假设这不是您第一次与这些数据互动，您将知道分析所需的列。当您知道需要哪些列时，可以使用`memory
    = FALSE`读取文件，然后使用`dplyr`选择所需的列。然后可以将生成的`dplyr`变量缓存到内存中，使用`compute()`函数。这将使Spark查询文件（们），提取所选字段，并仅将该数据复制到内存中。结果是一个内存中的表，相对较少的时间用于摄入：
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The next section covers a short technique to make it easier to carry the original
    field names of imported data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节介绍了一个简短的技术，使得更容易携带导入数据的原始字段名称。
- en: Columns
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列
- en: 'Spark 1.6 required that column names be sanitized, so R does that by default.
    There might be cases when you would like to keep the original names intact, or
    when working with Spark version 2.0 or above. To do that, set the `sparklyr.sanitize.column.names`
    option to `FALSE`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 1.6 要求列名进行过滤，所以 R 默认会这样做。也许有些情况下你希望保留原始的列名，或者在使用 Spark 2.0 或更高版本时。要实现这一点，将
    `sparklyr.sanitize.column.names` 选项设置为 `FALSE`：
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With this review of how to read data into Spark, we move on to look at how we
    can write data from our Spark session.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次对如何将数据读入 Spark 进行回顾之后，我们继续探讨如何从 Spark 会话中写入数据。
- en: Writing Data
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 写入数据
- en: Some projects require that new data generated in Spark be written back to a
    remote source. For example, the data could be new predicted values returned by
    a Spark model. The job processes the mass generation of predictions, but then
    the predictions need to be stored. This section focuses on how you should use
    Spark for moving the data from Spark into an external destination.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一些项目要求在 Spark 中生成的新数据被写回到远程源。例如，数据可能是由 Spark 模型返回的新预测值。作业处理了大量的预测生成，但接下来需要存储这些预测结果。本节重点讨论了如何使用
    Spark 将数据从 Spark 移动到外部目的地。
- en: Many new users start by downloading Spark data into R, and then upload it to
    a target, as illustrated in [Figure 8-2](#data-avoid-approach). It works for smaller
    datasets, but it becomes inefficient for larger ones. The data typically grows
    in size to the point that it is no longer feasible for R to be the middle point.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 许多新用户开始通过下载 Spark 数据到 R，然后再上传到目标位置，如 [图 8-2](#data-avoid-approach) 所示。这对较小的数据集有效，但对于较大的数据集则效率低下。数据通常会增长到无法让
    R 成为中间环节的规模。
- en: '![Incorrect use of Spark when writing large datasets](assets/mswr_0802.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![在写入大型数据集时误用 Spark](assets/mswr_0802.png)'
- en: Figure 8-2\. Incorrect use of Spark when writing large datasets
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-2\. 在写入大型数据集时误用 Spark
- en: All efforts should be made to have Spark connect to the target location. This
    way, reading, processing, and writing happens within the same Spark session.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 应尽一切努力让 Spark 连接到目标位置。这样一来，读取、处理和写入就都在同一个 Spark 会话中完成。
- en: As [Figure 8-3](#data-recommended-approach) shows, a better approach is to use
    Spark to read, process, and write to the target. This approach is able to scale
    as big as the Spark cluster allows, and prevents R from becoming a choke point.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 8-3](#data-recommended-approach) 所示，一个更好的方法是使用 Spark 来读取、处理和写入目标位置。这种方法能够扩展到
    Spark 集群允许的规模，并且避免了 R 成为瓶颈。
- en: '![Correct use of Spark when writing large datasets](assets/mswr_0803.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![在写入大型数据集时正确使用 Spark](assets/mswr_0803.png)'
- en: Figure 8-3\. Correct use of Spark when writing large datasets
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-3\. 在写入大型数据集时正确使用 Spark
- en: 'Consider the following scenario: a Spark job just processed predictions for
    a large dataset, resulting in a considerable amount of predictions. Choosing a
    method to write results will depend on the technology infrastructure you are working
    on. More specifically, it will depend on Spark and the target running, or not,
    in the same cluster.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下情景：一个 Spark 作业刚刚处理了一个大型数据集的预测结果，生成了大量的预测值。选择如何写入结果将取决于你所工作的技术基础设施。更具体地说，它将取决于
    Spark 和目标是否在同一个集群中运行。
- en: Back to our scenario, we have a large dataset in Spark that needs to be saved.
    When Spark and the target are in the same cluster, copying the results is not
    a problem; the data transfer is between RAM and disk of the same cluster or efficiently
    shuffled through a high-bandwidth connection.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的情景，我们有一个需要保存的大型 Spark 数据集。当 Spark 和目标位于同一个集群中时，复制结果并不成问题；数据传输可以在同一集群的内存和磁盘之间高效地进行或通过高带宽连接进行有效的洗牌。
- en: 'But what to do if the target is not within the Spark cluster? There are two
    options, and choosing one will depend on the size of the data and network speed:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果目标不在 Spark 集群内怎么办？有两个选择，选择其中一个将取决于数据的大小和网络速度：
- en: Spark transfer
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 传输
- en: In this case, Spark connects to the remote target location and copies the new
    data. If this is done within the same datacenter, or cloud provider, the data
    transfer could be fast enough to have Spark write the data directly.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Spark 连接到远程目标位置并复制新数据。如果这是在同一个数据中心或云服务提供商内完成的，数据传输可能足够快，以便让 Spark 直接写入数据。
- en: External transfer and otherwise
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 外部传输和其他
- en: Spark can write the results to disk and transfers them via a third-party application.
    Spark writes the results as files and then a separate job copies the files over.
    In the target location, you would use a separate process to transfer the data
    into the target location.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以将结果写入磁盘，并通过第三方应用程序传输它们。Spark将结果写入文件，然后单独的作业将文件复制过去。在目标位置，你将使用单独的进程将数据转移到目标位置。
- en: It is best to recognize that Spark, R, and any other technology are tools. No
    tool can do everything, nor should you expect it to. Next we describe how to copy
    data into Spark or collect large datasets that don’t fit in memory, which you
    can use to transfer data across clusters or help initialize your distributed datasets.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最好认识到Spark、R和其他技术都只是工具。没有工具可以做到所有事情，也不应该期望它可以。接下来我们描述如何将数据复制到Spark或收集不适合内存的大型数据集，这可以用于跨集群传输数据或帮助初始化分布式数据集。
- en: Copying Data
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复制数据
- en: Previous chapters used `copy_to()` as a handy helper to copy data into Spark;
    however, you can use `copy_to()` only to transfer in-memory datasets that are
    already loaded in memory. These datasets tend to be much smaller than the kind
    of datasets you would want to copy into Spark.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的章节使用 `copy_to()` 作为一个方便的辅助工具将数据复制到Spark中；然而，你只能使用 `copy_to()` 来传输已加载到内存中的内存数据集。这些数据集通常比你想要复制到Spark中的数据集小得多。
- en: 'For instance, suppose that we have a 3 GB dataset generated as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个生成的3 GB数据集：
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If we had only 2 GB of memory in the driver node, we would not be able to load
    this 3 GB file into memory using `copy_to()`. Instead, when using the HDFS as
    storage in your cluster, you can use the `hadoop` command-line tool to copy files
    from disk into Spark from the terminal as follows. Notice that the following code
    works only in clusters using HDFS, not in local environments.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果驱动节点只有2 GB内存，我们将无法使用 `copy_to()` 将这个3 GB文件加载到内存中。相反，当在你的集群中使用HDFS作为存储时，你可以使用
    `hadoop` 命令行工具从终端将文件从磁盘复制到Spark中。请注意，以下代码仅在使用HDFS的集群中有效，而不适用于本地环境。
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You then can read the uploaded file, as described in the [“File Formats”](#data-file-formats)
    section; for text files, you would run:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以按照 [“文件格式”](#data-file-formats) 部分中描述的方式读取上传的文件；对于文本文件，你可以运行：
- en: '[PRE18]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`collect()` has a similar limitation in that it can collect only datasets that
    fit your driver memory; however, if you had to extract a large dataset from Spark
    through the driver node, you could use specialized tools provided by the distributed
    storage. For HDFS, you would run the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`collect()` 存在一个类似的限制，即它只能收集适合驱动器内存的数据集；然而，如果你需要从Spark中通过驱动节点提取大型数据集，可以使用分布式存储提供的专用工具。对于HDFS，你可以运行以下命令：'
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Alternatively, you can also collect datasets that don’t fit in memory by providing
    a callback to `collect()`. A callback is just an R function that will be called
    over each Spark partition. You then can write this dataset to disk or push to
    other clusters over the network.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以通过向 `collect()` 提供回调来收集不适合内存的数据集。回调只是一个将在每个Spark分区上调用的R函数。然后你可以将这些数据集写入磁盘或通过网络推送到其他集群。
- en: 'You could use the following code to collect 3 GB even if the driver node collecting
    this dataset had less than 3 GB of memory. That said, as [Chapter 3](ch03.html#analysis)
    explains, you should avoid collecting large datasets into a single machine since
    this creates a significant performance bottleneck. For conciseness, we will collect
    only the first million rows; feel free to remove `head(10^6)` if you have a few
    minutes to spare:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 即使驱动节点收集此数据集的内存少于3 GB，你仍然可以使用以下代码收集3 GB数据。尽管如此，正如[第3章](ch03.html#analysis)所解释的那样，应避免将大型数据集收集到单台机器上，因为这会导致显著的性能瓶颈。为了简洁起见，我们只收集前一百万行；如果你有几分钟可以花费，可以删除
    `head(10^6)`：
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Make sure you clean up these large files and empty your recycle bin as well:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 确保清理这些大文件并清空回收站：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In most cases, data will already be stored in the cluster, so you should not
    need to worry about copying large datasets; instead, you can usually focus on
    reading and writing different file formats, which we describe next.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，数据已经存储在集群中，因此你不需要担心复制大型数据集；相反，你通常可以专注于读取和写入不同的文件格式，我们接下来会描述。
- en: File Formats
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件格式
- en: Out of the box, Spark is able to interact with several file formats, like CSV,
    JSON, LIBSVM, ORC, and Parquet. [Table 8-1](#Table0801) maps the file format to
    the function you should use to read and write data in Spark.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以直接与多种文件格式交互，如CSV、JSON、LIBSVM、ORC和Parquet。[表 8-1](#Table0801)将文件格式映射到应在Spark中使用的函数。
- en: Table 8-1\. Spark functions to read and write file formats
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-1\. Spark读写文件格式的函数
- en: '| Format | Read | Write |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | 读取 | 写入 |'
- en: '| --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| CSV | `spark_read_csv()` | `spark_write_csv()` |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| CSV | `spark_read_csv()` | `spark_write_csv()` |'
- en: '| JSON | `spark_read_json()` | `spark_write_json()` |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| JSON | `spark_read_json()` | `spark_write_json()` |'
- en: '| LIBSVM | `spark_read_libsvm()` | `spark_write_libsvm()` |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| LIBSVM | `spark_read_libsvm()` | `spark_write_libsvm()` |'
- en: '| ORC | `spark_read_orc()` | `spark_write_orc()` |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ORC | `spark_read_orc()` | `spark_write_orc()` |'
- en: '| Apache Parquet | `spark_read_parquet()` | `spark_write_parquet()` |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Apache Parquet | `spark_read_parquet()` | `spark_write_parquet()` |'
- en: '| Text | `spark_read_text()` | `spark_write_text()` |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | `spark_read_text()` | `spark_write_text()` |'
- en: The following sections will describe special considerations particular to each
    file format as well as some of the strengths and weaknesses of some popular file
    formats, starting with the well-known CSV file format.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的部分将描述特定于每种文件格式的特殊考虑事项，以及一些流行文件格式的优缺点，从广为人知的CSV文件格式开始。
- en: CSV
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CSV
- en: 'The CSV format might be the most common file type in use today. It is defined
    by a text file separated by a given character, usually a comma. It should be pretty
    straightforward to read CSV files; however, it’s worth mentioning a couple techniques
    that can help you process CSVs that are not fully compliant with a well-formed
    CSV file. Spark offers the following modes for addressing parsing issues:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: CSV格式可能是当今最常用的文件类型。它由一个文本文件组成，以给定字符分隔，通常是逗号。读取CSV文件应该相当简单；然而，值得一提的是一些可以帮助您处理不完全符合规范的CSV文件的技术。Spark提供了以下处理解析问题的模式：
- en: Permissive
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 宽容模式
- en: Inserts NULL values for missing tokens
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 插入NULL值以代替缺失的令牌
- en: Drop Malformed
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 放弃格式不正确的行
- en: Drops lines that are malformed
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 删除格式不正确的行
- en: Fail Fast
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 快速失败
- en: Aborts if it encounters any malformed line
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到任何格式不正确的行则中止
- en: 'You can use these in `sparklyr` by passing them inside the `options` argument.
    The following example creates a file with a broken entry. It then shows how it
    can be read into Spark:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在`sparklyr`中，您可以通过将它们传递给`options`参数来使用这些。以下示例创建了一个带有损坏条目的文件。然后展示了如何将其读入Spark：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Spark provides an issue tracking column, which was hidden by default. To enable
    it, add `_corrupt_record` to the `columns` list. You can combine this with the
    use of the `PERMISSIVE` mode. All rows will be imported, invalid entries will
    receive an `NA`, and the issue will be tracked in the `_corrupt_record` column:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了一个默认隐藏的问题追踪列。要启用它，请将`_corrupt_record`添加到`columns`列表中。您可以将其与使用`PERMISSIVE`模式结合使用。所有行将被导入，无效条目将接收`NA`，并在`_corrupt_record`列中跟踪问题：
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Reading and storing data as CSVs is quite common and supported across most systems.
    For tabular datasets, it is still a popular option, but for datasets containing
    nested structures and nontabular data, JSON is usually preferred.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据读取和存储为CSV文件是相当常见的，并且在大多数系统中都得到支持。对于表格数据集，这仍然是一个流行的选项，但对于包含嵌套结构和非表格数据的数据集，通常更喜欢使用JSON。
- en: JSON
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JSON
- en: 'JSON is a file format originally derived from JavaScript that has grown to
    be language-independent and very popular due to its flexibility and ubiquitous
    support. Reading and writing JSON files is quite straightforward:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: JSON是最初源自JavaScript并因其灵活性和普遍支持而变得与语言无关且非常流行的文件格式。读写JSON文件非常简单：
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'However, when you deal with a dataset containing nested fields like the one
    from this example, it is worth pointing out how to extract nested fields. One
    approach is to use a JSON path, which is a domain-specific syntax commonly used
    to extract and query JSON files. You can use a combination of `get_json_object()`
    and `to_json()` to specify the JSON path you are interested in. To extract `f1`
    you would run the following transformation:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当处理包含像这个例子中那样的嵌套字段的数据集时，值得指出如何提取嵌套字段。一种方法是使用JSON路径，这是一种常用的领域特定语法，用于提取和查询JSON文件。您可以使用`get_json_object()`和`to_json()`的组合来指定您感兴趣的JSON路径。要提取`f1`，您可以运行以下转换：
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Another approach is to install `sparkly.nested` from CRAN with `install.packages("sparklyr.nested")`
    and then unnest nested data with `sdf_unnest()`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是从CRAN安装`sparklyr.nested`，然后使用`sdf_unnest()`展开嵌套数据：
- en: '[PRE31]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: While JSON and CSVs are quite simple to use and versatile, they are not optimized
    for performance; however, other formats like ORC, AVRO, and Parquet are.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 JSON 和 CSV 非常简单且多用途，但它们并非针对性能优化；相比之下，ORC、AVRO 和 Parquet 等其他格式则是。
- en: Parquet
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquet
- en: Apache Parquet, Apache ORC, and Apache AVRO are all file formats designed with
    performance in mind. Parquet and ORC store data in columnar format, while AVRO
    is row-based. All of them are binary file formats, which reduces storage space
    and improves performance. This comes at the cost of making them a bit more difficult
    to read by external systems and libraries; however, this is usually not an issue
    when used as intermediate data storage within Spark.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet、Apache ORC 和 Apache AVRO 都是专为性能设计的文件格式。Parquet 和 ORC 以列格式存储数据，而
    AVRO 是基于行的。它们都是二进制文件格式，可以减少存储空间并提高性能。这样做的代价是它们在外部系统和库中读取时可能稍微复杂一些；但在 Spark 中用作中间数据存储时通常不会成为问题。
- en: 'To illustrate this, [Figure 8-4](#data-file-format-benchmark) plots the result
    of running a 1-million-row write-speed benchmark using the `bench` package; feel
    free to use your own benchmarks over meaningful datasets when deciding which format
    best fits your needs:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，[图 8-4](#data-file-format-benchmark) 展示了使用 `bench` 包运行的百万行写入速度基准测试结果；在决定哪种格式最适合您的需求时，请随意使用您自己的基准测试数据集：
- en: '[PRE33]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'From now on, be sure to disconnect from Spark whenever we present a new `spark_connect()`
    command:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，每当我们出现新的 `spark_connect()` 命令时，请务必断开与 Spark 的连接：
- en: '[PRE34]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This concludes the introduction to some of the out-of-the-box supported file
    formats. Next, we describe how to deal with formats that require external packages
    and customization.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了对一些内置支持的文件格式的介绍。接下来，我们描述需要外部包和定制化处理的格式的处理方法。
- en: '![One-million-rows write benchmark between CSV, JSON, Parquet, and ORC](assets/mswr_0804.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![CSV、JSON、Parquet 和 ORC 之间的百万行写入基准测试](assets/mswr_0804.png)'
- en: Figure 8-4\. One-million-rows write benchmark between CSV, JSON, Parquet, and
    ORC
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-4\. CSV、JSON、Parquet 和 ORC 之间的百万行写入基准测试
- en: Others
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他
- en: Spark is a very flexible computing platform. It can add functionality by using
    extension programs, called packages. You can access a new source type or file
    system by using the appropriate package.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个非常灵活的计算平台。通过使用称为包的扩展程序，它可以添加功能。您可以通过使用适当的包来访问新的数据源类型或文件系统。
- en: Packages need to be loaded into Spark at connection time. To load the package,
    Spark needs its location, which could be inside the cluster, in a file share,
    or the internet.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 包在连接时需要加载到 Spark 中。要加载包，Spark 需要知道其位置，可以是集群内部、文件共享或互联网上的位置。
- en: In `sparklyr`, the package location is passed to `spark_connect()`. All packages
    should be listed in the `sparklyr.connect.packages` entry of the connection configuration.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `sparklyr` 中，包位置通过 `spark_connect()` 传递。所有包应列在连接配置的 `sparklyr.connect.packages`
    项中。
- en: It is possible to access data source types that we didn’t previously list. Loading
    the appropriate default package for Spark is the first of two steps The second
    step is to actually read or write the data. The `spark_read_source()` and `spark_write_source()`
    functions do that. They are generic functions that can use the libraries imported
    by a default package.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 可以访问我们之前未列出的数据源类型。加载 Spark 的适当默认包是两个步骤中的第一步。第二步是实际读取或写入数据。`spark_read_source()`
    和 `spark_write_source()` 函数就是这样做的。它们是通用函数，可以使用默认包导入的库。
- en: 'For instance, we can read XML files as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以如下读取 XML 文件：
- en: '[PRE35]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'which you can also write back to XML with ease, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以轻松地将数据写回 XML，如下所示：
- en: '[PRE37]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In addition, there are a few extensions developed by the R community to load
    additional file formats, such as `sparklyr.nested` to assist with nested data,
    `spark.sas7bdat` to read data from SAS, `sparkavro` to read data in AVRO format,
    and `sparkwarc` to read WARC files, which use extensibility mechanisms introduced
    in [Chapter 10](ch10.html#extensions). [Chapter 11](ch11.html#distributed) presents
    techniques to use R packages to load additional file formats, and [Chapter 13](ch13.html#contributing)
    presents techniques to use Java libraries to complement this further. But first,
    let’s explore how to retrieve and store files from several different file systems.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，R社区还开发了一些扩展来加载额外的文件格式，例如用于嵌套数据的`sparklyr.nested`，用于从SAS读取数据的`spark.sas7bdat`，用于AVRO格式数据的`sparkavro`，以及用于读取WARC文件的`sparkwarc`，这些都使用了[第10章](ch10.html#extensions)中引入的可扩展机制。[第11章](ch11.html#distributed)介绍了使用R包加载额外文件格式的技术，而[第13章](ch13.html#contributing)介绍了使用Java库进一步补充这一点的技术。但首先，让我们探讨如何从几种不同的文件系统中检索和存储文件。
- en: File Systems
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件系统
- en: Spark defaults to the file system on which it is currently running. In a YARN
    managed cluster, the default file system will be HDFS. An example path of */home/user/file.csv*
    will be read from the cluster’s HDFS folders, not the Linux folders. The operating
    system’s file system will be accessed for other deployments, such as Standalone,
    and `sparklyr`’s local.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Spark默认使用当前运行的文件系统。在YARN管理的集群中，默认文件系统将是HDFS。例如路径*/home/user/file.csv*将从集群的HDFS文件夹中读取，而不是Linux文件夹。操作系统的文件系统将用于其他部署，如独立部署和`sparklyr`的本地部署。
- en: The file system protocol can be changed when reading or writing. You do this
    via the `path` argument of the `sparklyr` function. For example, a full path of
    *file://home/user/file.csv* forces the use of the local operating system’s file
    system.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取或写入时可以更改文件系统协议。您可以通过`sparklyr`函数的`path`参数来实现这一点。例如，完整路径*file://home/user/file.csv*将强制使用本地操作系统的文件系统。
- en: There are many other file system protocols, such as `_dbfs://_` for Databricks’
    file system, `_s3a://_` for Amazon’s S3 service, `_wasb://_` for Microsoft Azure
    storage, and `_gs://_` for Google storage.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他的文件系统协议，如用于Databricks文件系统的`_dbfs://_`，用于Amazon S3服务的`_s3a://_`，用于Microsoft
    Azure存储的`_wasb://_`，以及用于Google存储的`_gs://_`。
- en: Spark does not provide support for all them directly; instead, they are configured
    as needed. For instance, accessing the “s3a” protocol requires adding a package
    to the `sparklyr.connect.packages` configuration setting, while connecting and
    specifying appropriate credentials might require using the `AWS_ACCESS_KEY_ID`
    and `AWS_SECRET_ACCESS_KEY` environment variables.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Spark并没有直接提供对所有这些功能的支持；相反，它们根据需要进行配置。例如，访问“s3a”协议需要将一个包添加到`sparklyr.connect.packages`配置设置中，而连接和指定适当的凭据可能需要使用`AWS_ACCESS_KEY_ID`和`AWS_SECRET_ACCESS_KEY`环境变量。
- en: '[PRE38]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Accessing other file protocols requires loading different packages, although,
    in some cases, the vendor providing the Spark environment might load the package
    for you. Please refer to your vendor’s documentation to find out whether that
    is the case.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 访问其他文件协议需要加载不同的包，虽然在某些情况下，提供Spark环境的供应商可能会为您加载这些包。请参考您供应商的文档以了解是否适用。
- en: Storage Systems
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储系统
- en: A data lake and Spark usually go hand-in-hand, with optional access to storage
    systems like databases and data warehouses. Presenting all the different storage
    systems with appropriate examples would be quite time-consuming, so instead we
    present some of the commonly used storage systems.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖和Spark通常是密切相关的，可选择访问诸如数据库和数据仓库之类的存储系统。展示所有不同的存储系统并提供适当的示例将非常耗时，因此我们选择展示一些常用的存储系统。
- en: As a start, Apache *Hive* is a data warehouse software that facilitates reading,
    writing, and managing large datasets residing in distributed storage using SQL.
    In fact, Spark has components from Hive built directly into its sources. It is
    very common to have installations of Spark or Hive side-by-side, so we will start
    by presenting Hive, followed by Cassandra, and then close by looking at JDBC connections.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个开始，Apache *Hive* 是一个数据仓库软件，利用SQL方便地读取、写入和管理分布式存储中的大型数据集。事实上，Spark直接内置了来自Hive的组件。在安装了Spark或Hive并存的情况下非常常见，所以我们将从介绍Hive开始，接着是Cassandra，最后看一下JDBC连接。
- en: Hive
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hive
- en: In YARN managed clusters, Spark provides a deeper integration with Apache Hive.
    Hive tables are easily accessible after opening a Spark connection.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在YARN管理的集群中，Spark与Apache Hive有更深入的集成。在打开Spark连接后，可以轻松访问Hive表。
- en: 'You can access a Hive table’s data using `DBI` by referencing the table in
    a SQL statement:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`DBI`引用SQL语句中的Hive表数据：
- en: '[PRE39]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Another way to reference a table is with `dplyr` using the `tbl()` function,
    which retrieves a reference to the table:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种引用表的方式是使用`dplyr`的`tbl()`函数，该函数检索表的引用：
- en: '[PRE40]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'It is important to reiterate that no data is imported into R; the `tbl()` function
    only creates a reference. You then can pipe more `dplyr` verbs following the `tbl()`
    command:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 强调重要的是，不会将任何数据导入R；`tbl()`函数仅创建引用。然后，您可以在`tbl()`命令之后使用更多的`dplyr`动词：
- en: '[PRE41]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Hive table references assume a default database source. Often, the needed table
    is in a different database within the metastore. To access it using SQL, prefix
    the database name to the table. Separate them using a period, as demonstrated
    here:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Hive表引用假定默认的数据库源。通常，所需的表位于元数据存储中的不同数据库中。要通过SQL访问它，请在表前缀数据库名称。使用一个句点将它们分开，如本例所示：
- en: '[PRE42]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In `dplyr`, the `in_schema()` function can be used. The function is used inside
    the `tbl()` call:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在`dplyr`中，可以使用`in_schema()`函数。该函数用于`tbl()`调用内部：
- en: '[PRE43]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You can also use the `tbl_change_db()` function to set the current session’s
    default database. Any subsequent call via `DBI` or `dplyr` will use the selected
    name as the default database:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`tbl_change_db()`函数来设置当前会话的默认数据库。随后通过`DBI`或`dplyr`调用将使用所选名称作为默认数据库：
- en: '[PRE44]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The following examples require additional Spark packages and databases which
    might be difficult to follow unless you happen to have a JDBC driver or Cassandra
    database accessible to you.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例需要额外的Spark包和数据库，可能难以理解，除非您恰好可以访问JDBC驱动程序或Cassandra数据库。
- en: Next, we explore a less structured storage system, often referred to as a *NoSQL
    database*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们探索一个不那么结构化的存储系统，通常称为*NoSQL数据库*。
- en: Cassandra
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cassandra
- en: Apache *Cassandra* is a free and open source, distributed, wide-column store,
    NoSQL database management system designed to handle large amounts of data across
    many commodity servers. While there are many other database systems beyond Cassandra,
    taking a quick look at how Cassandra can be used from Spark will give you insight
    into how to make use of other database and storage systems like Solr, Redshift,
    Delta Lake, and others.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Apache *Cassandra*是一个免费开源的分布式宽列存储NoSQL数据库管理系统，设计用于处理大量数据，跨多个廉价服务器。虽然除了Cassandra之外还有许多其他数据库系统，但快速了解如何从Spark使用Cassandra将使您了解如何利用其他数据库和存储系统，如Solr、Redshift、Delta
    Lake等。
- en: 'The following example code shows how to use the `datastax:spark-cassandra-connector`
    package to read from Cassandra. The key is to use the `org.apache.spark.sql.cassandra`
    library as the `source` argument. It provides the mapping Spark can use to make
    sense of the data source. Unless you have a Cassandra database, skip executing
    the following statement:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例代码展示了如何使用`datastax:spark-cassandra-connector`包从Cassandra读取数据。关键是使用`org.apache.spark.sql.cassandra`库作为`source`参数。它提供了Spark可以用来理解数据源的映射。除非你有一个Cassandra数据库，否则请跳过执行以下语句：
- en: '[PRE45]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: One of the most useful features of Spark when dealing with external databases
    and data warehouses is that Spark can push down computation to the database, a
    feature known as *pushdown predicates*. In a nutshell, pushdown predicates improve
    performance by asking remote databases smart questions. When you execute a query
    that contains the `filter(age > 20)` expression against a remote table referenced
    through `spark_read_source()` and not loaded in memory, rather than bringing the
    entire table into Spark, it will be passed to the remote database and only a subset
    of the remote table is retrieved.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 处理外部数据库和数据仓库时，Spark的一个最有用的功能是它可以将计算推送到数据库，这一功能称为*推送谓词*。简而言之，推送谓词通过向远程数据库提出智能问题来提高性能。当您对通过`spark_read_source()`引用的远程表执行包含`filter(age
    > 20)`表达式的查询时，而不是将整个表带入Spark内存中，它将被传递到远程数据库，并且仅检索远程表的子集。
- en: While it is ideal to find Spark packages that support the remote storage system,
    there will be times when a package is not available and you need to consider vendor
    JDBC drivers.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管理想情况下是找到支持远程存储系统的Spark包，但有时会没有可用的包，您需要考虑供应商的JDBC驱动程序。
- en: JDBC
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JDBC
- en: When a Spark package is not available to provide connectivity, you can consider
    a JDBC connection. JDBC is an interface for the programming language Java, which
    defines how a client can access a database.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有Spark包可用以提供连接时，您可以考虑使用JDBC连接。JDBC是Java编程语言的接口，定义了客户端如何访问数据库。
- en: It is quite easy to connect to a remote database with `spark_read_jdbc()`, and
    `spark_write_jdbc()`; as long as you have access to the appropriate JDBC driver,
    which at times is trivial and other times is quite an adventure. To keep this
    simple, we can briefly consider how a connection to a remote MySQL database could
    be accomplished.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`spark_read_jdbc()`和`spark_write_jdbc()`连接到远程数据库非常简单；只要您有适当的JDBC驱动程序访问权限，有时是微不足道的，有时则是一次相当的冒险。为了保持简单，我们可以简要考虑如何连接到远程MySQL数据库。
- en: 'First, you would need to download the appropriate JDBC driver from MySQL’s
    developer portal and specify this additional driver as a `sparklyr.shell.driver-class-path`
    connection option. Since JDBC drivers are Java-based, the code is contained within
    a *JAR* (Java ARchive) file. As soon as you’re connected to Spark with the appropriate
    driver, you can use the *jdbc://* protocol to access particular drivers and databases.
    Unless you are willing to download and configure MySQL on your own, skip executing
    the following statement:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要从MySQL的开发者门户网站下载适当的JDBC驱动程序，并将此附加驱动程序指定为`sparklyr.shell.driver-class-path`连接选项。由于JDBC驱动程序是基于Java的，代码包含在一个*JAR*（Java
    ARchive）文件中。一旦使用适当的驱动程序连接到Spark，您就可以使用*jdbc://*协议访问特定的驱动程序和数据库。除非您愿意自行下载和配置MySQL，否则请跳过执行以下语句：
- en: '[PRE46]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: If you are a customer of particular database vendors, making use of the vendor-provided
    resources is usually the best place to start looking for appropriate drivers.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是特定数据库供应商的客户，则利用供应商提供的资源通常是开始寻找适当驱动程序的最佳方式。
- en: Recap
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter expanded on how and why you should use Spark to connect and process
    a variety of data sources through a new data storage model known as data lakes—a
    storage pattern that provides more flexibility than standard ETL processes by
    enabling you to use raw datasets with, potentially, more information to enrich
    data analysis and modeling.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细介绍了使用Spark连接和处理各种数据源的方法及其原因，通过称为数据湖的新数据存储模型——这种存储模式比标准ETL过程提供了更大的灵活性，使您能够使用原始数据集，可能具有更多信息来丰富数据分析和建模。
- en: 'We also presented best practices for reading, writing, and copying data into
    and from Spark. We then returned to exploring the components of a data lake: file
    formats and file systems, with the former representing how data is stored, and
    the latter where the data is stored. You then learned how to tackle file formats
    and storage systems that require additional Spark packages, reviewed some of the
    performance trade-offs across file formats, and learned the concepts required
    to make use of storage systems (databases and warehouses) in Spark.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提出了有关在Spark中读取、写入和复制数据的最佳实践。然后，我们回顾了数据湖的组成部分：文件格式和文件系统，前者表示数据存储方式，后者表示数据存储位置。然后，您学习了如何处理需要额外Spark包的文件格式和存储系统，审查了不同文件格式之间的性能权衡，并学习了在Spark中使用存储系统（数据库和数据仓库）所需的概念。
- en: While reading and writing datasets should come naturally to you, you might still
    hit resource restrictions while reading and writing large datasets. To handle
    these situations, [Chapter 9](ch09.html#tuning) shows you how Spark manages tasks
    and data across multiple machines, which in turn allows you to further improve
    the performance of your analysis and modeling tasks.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然阅读和写入数据集对您来说应该是自然而然的，但在读写大型数据集时，您可能仍会遇到资源限制。为了处理这些情况，[第9章](ch09.html#tuning)向您展示了Spark如何跨多台机器管理任务和数据，从而进一步提高分析和建模任务的性能。
- en: ^([1](ch08.html#idm46099145684872-marker)) Codd EF (1970). “A relational model
    of data for large shared data banks.”
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.html#idm46099145684872-marker)) Codd EF（1970）。"大型共享数据库的数据关系模型。"
