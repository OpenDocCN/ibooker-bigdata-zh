- en: Chapter 3\. Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章 分析
- en: 'First lesson: stick them with the pointy end.'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第一课：用尖锐的一端戳它们。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Jon Snow
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —琼·雪诺
- en: Previous chapters focused on introducing Spark with R, getting you up to speed
    and encouraging you to try basic data analysis workflows. However, they have not
    properly introduced what data analysis means, especially with Spark. They presented
    the tools you will need throughout this book—tools that will help you spend more
    time learning and less time troubleshooting.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章重点介绍了使用R与Spark，使您快速上手，并鼓励您尝试基本的数据分析工作流程。然而，它们并没有恰当地介绍数据分析的含义，特别是在Spark中。它们呈现了您在本书中需要的工具——这些工具将帮助您花更多时间学习，而不是疑难解答。
- en: 'This chapter introduces tools and concepts to perform data analysis in Spark
    from R. Spoiler alert: these are the same tools you use with plain R! This is
    not a mere coincidence; rather, we want data scientists to live in a world where
    technology is hidden from them, where you can use the R packages you know and
    love, and they “just work” in Spark! Now, we are not quite there yet, but we are
    also not that far. Therefore, in this chapter you learn widely used R packages
    and practices to perform data analysis—`dplyr`, `ggplot2`, formulas, `rmarkdown`,
    and so on—which also happen to work in Spark.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了在R中使用Spark进行数据分析的工具和概念。剧透警告：这些工具与您在纯R中使用的工具相同！这不仅仅是巧合；我们希望数据科学家可以在一个技术被隐藏的世界中使用他们熟悉和喜爱的R包，并且它们在Spark中“只是工作”！现在，我们还没有完全实现这一点，但我们也不远了。因此，在本章中，您将学习广泛使用的R包和实践，以执行数据分析——如`dplyr`、`ggplot2`、公式、`rmarkdown`等，这些在Spark中同样适用。
- en: '[Chapter 4](ch04.html#modeling) will focus on creating statistical models to
    predict, estimate, and describe datasets, but first, let’s get started with analysis!'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[第四章](ch04.html#modeling) 将专注于创建统计模型来预测、估计和描述数据集，但首先，让我们开始分析吧！'
- en: Overview
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In a data analysis project, the main goal is to understand what the data is
    trying to “tell us,” hoping that it provides an answer to a specific question.
    Most data analysis projects follow a set of steps, as shown in [Figure 3-1](#analysis-steps).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析项目中，主要目标是理解数据试图“告诉我们”的内容，希望它能回答特定的问题。大多数数据分析项目遵循一套步骤，如[图 3-1](#analysis-steps)所示。
- en: As the diagram illustrates, we first *import* data into our analysis stem, where
    we *wrangle* it by trying different data transformations, such as aggregations.
    We then *visualize* the data to help us perceive relationships and trends. To
    gain deeper insight, we can fit one or multiple statistical *models* against sample
    data. This will help us find out whether the patterns hold true when new data
    is applied to them. Lastly, the results are communicated publicly or privately
    to colleagues and stakeholders.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图所示，我们首先将数据*导入*到我们的分析流程中，通过尝试不同的数据转换（如聚合）来*整理*它。然后，我们*可视化*数据以帮助我们感知关系和趋势。为了深入了解，我们可以对样本数据拟合一个或多个统计*模型*。这将帮助我们找出这些模式在应用新数据时是否成立。最后，结果会公开或私下与同事和利益相关者交流。
- en: '![The general steps of a data analysis](assets/mswr_0301.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![数据分析的一般步骤](assets/mswr_0301.png)'
- en: Figure 3-1\. The general steps of a data analysis
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1 数据分析的一般步骤
- en: When working with not-large-scale datasets—as in datasets that fit in memory—we
    can perform all those steps from R, without using Spark. However, when data does
    not fit in memory or computation is simply too slow, we can slightly modify this
    approach by incorporating Spark. But how?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理不大规模数据集（即适合内存的数据集）时，我们可以使用R执行所有这些步骤，而无需使用Spark。然而，当数据量过大无法放入内存或计算速度太慢时，我们可以略微调整此方法，引入Spark。但是如何做呢？
- en: For data analysis, the ideal approach is to let Spark do what it’s good at.
    Spark is a parallel computation engine that works at a large scale and provides
    a SQL engine and modeling libraries. You can use these to perform most of the
    same operations R performs. Such operations include data selection, transformation,
    and modeling. Additionally, Spark includes tools for performing specialized computational
    work like graph analysis, stream processing, and many others. For now, we will
    skip those non-rectangular datasets and present them in later chapters.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据分析，理想的方法是让Spark发挥其所擅长的功能。Spark是一个并行计算引擎，能够在大规模上工作，并提供SQL引擎和建模库。您可以使用这些功能执行大多数与R相同的操作，包括数据选择、转换和建模。此外，Spark还包括用于执行特殊计算工作的工具，如图分析、流处理等等。在此，我们将跳过那些非矩形数据集，并在后续章节中介绍它们。
- en: You can perform data *import*, *wrangling*, and *modeling* within Spark. You
    can also partly do *visualization* with Spark, which we cover later in this chapter.
    The idea is to use R to tell Spark what data operations to run, and then only
    bring the results into R. As illustrated in [Figure 3-2](#analysis-approach),
    the ideal method *pushes compute* to the Spark cluster and then *collects results*
    into R.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark内部可以执行数据*导入*，*整理*和*建模*。您也可以部分使用Spark进行*可视化*，这在本章稍后讨论。其思想是使用R告诉Spark要运行哪些数据操作，然后只将结果带入R。如[图 3-2](#analysis-approach)所示，理想的方法*将计算推送*到Spark集群，然后*收集结果*到R中。
- en: '![Spark computes while R collects results](assets/mswr_0302.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![Spark在计算时收集结果](assets/mswr_0302.png)'
- en: Figure 3-2\. Spark computes while R collects results
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. Spark在计算时收集结果
- en: The `sparklyr` package aids in using the “push compute, collect results” principle.
    Most of its functions are wrappers on top of Spark API calls. This allows us to
    take advantage of Spark’s analysis components, instead of R’s. For example, when
    you need to fit a linear regression model, instead of using R’s familiar `lm()`
    function, you would use Spark’s `ml_linear_regression()` function. This R function
    then calls Spark to create this model. [Figure 3-3](#analysis-scala) depicts this
    specific example.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`sparklyr`包有助于使用“推送计算，收集结果”的原则。它的大多数功能都是在Spark API调用的顶部进行封装。这使我们能够利用Spark的分析组件，而不是R的。例如，当您需要拟合线性回归模型时，您不会使用R熟悉的`lm()`函数，而是使用Spark的`ml_linear_regression()`函数。然后，这个R函数调用Spark来创建这个模型。[图 3-3](#analysis-scala)描绘了这个具体的例子。'
- en: '![R functions call Spark functionality](assets/mswr_0303.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![R函数调用Spark功能](assets/mswr_0303.png)'
- en: Figure 3-3\. R functions call Spark functionality
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. R函数调用Spark功能
- en: For more common data manipulation tasks, `sparklyr` provides a backend for `dplyr`.
    This means you can use `dplyr` verbs with which you’re already familiar in R,
    and then `sparklyr` and `dplyr` will translate those actions into Spark SQL statements,
    which are generally more compact and easier to read than SQL statements (see [Figure 3-4](#unnamed-chunk-1)).
    So, if you are already familiar with R and `dplyr`, there is nothing new to learn.
    This might feel a bit anticlimactic—indeed, it is—but it’s also great since you
    can focus that energy on learning other skills required to do large-scale computing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更常见的数据处理任务，`sparklyr`为`dplyr`提供了后端支持。这意味着您可以在R中使用您已经熟悉的`dplyr`动词，然后`sparklyr`和`dplyr`将这些操作转换为Spark
    SQL语句，通常比SQL语句更简洁且更易读（参见[图 3-4](#unnamed-chunk-1)）。因此，如果您已经熟悉R和`dplyr`，则没有什么新东西需要学习。这可能会感到有点令人失望——确实如此——但这也很好，因为您可以将精力集中在学习进行大规模计算所需的其他技能上。
- en: '![dplyr writes SQL](assets/mswr_0304.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![dplyr在Spark中写SQL](assets/mswr_0304.png)'
- en: Figure 3-4\. dplyr writes SQL in Spark
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-4\. dplyr在Spark中写SQL
- en: To practice as you learn, the rest of this chapter’s code uses a single exercise
    that runs in the *local* Spark master. This way, you can replicate the code on
    your personal computer. Make sure `sparklyr` is already working, which should
    be the case if you completed [Chapter 2](ch02.html#starting).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在学习过程中进行实践，本章其余部分的代码使用在*本地* Spark主节点上运行的单个练习。这样，您可以在个人计算机上复制代码。确保`sparklyr`已经可以工作，如果您已经完成了[第2章](ch02.html#starting)，那么应该可以工作。
- en: 'This chapter will make use of packages that you might not have installed. So,
    first, make sure the following packages are installed by running these commands:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用您可能尚未安装的包。因此，请首先确保通过运行以下命令安装这些包：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First, load the `sparklyr` and `dplyr` packages and then open a new *local*
    connection.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载`sparklyr`和`dplyr`包，然后打开一个新的*本地*连接。
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The environment is ready to be used, so our next task is to import data that
    we can later analyze.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 环境已准备就绪，因此我们下一个任务是导入稍后可以分析的数据。
- en: Import
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入
- en: When using Spark with R, you need to approach importing data differently. Usually,
    importing means that R will read files and load them into memory; when you are
    using Spark, the data is imported into Spark, not R. In [Figure 3-5](#analysis-access),
    notice how the data source is connected to Spark instead of being connected to
    R.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Spark与R时，您需要以不同方式处理数据导入。通常，导入意味着R将读取文件并将其加载到内存中；当您使用Spark时，数据将导入到Spark而不是R中。在[图 3-5](#analysis-access)中，请注意数据源如何连接到Spark而不是连接到R。
- en: '![Import data to Spark not R](assets/mswr_0305.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![将数据导入Spark而非R](assets/mswr_0305.png)'
- en: Figure 3-5\. Import data to Spark not R
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-5\. 将数据导入Spark而非R
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When you’re performing analysis over large-scale datasets, the vast majority
    of the necessary data will already be available in your Spark cluster (which is
    usually made available to users via Hive tables or by accessing the file system
    directly). [Chapter 8](ch08.html#data) will cover this extensively.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当您对大规模数据集执行分析时，大多数必要的数据已经可用于您的Spark集群（通常通过Hive表或直接访问文件系统向用户提供）。[第8章](ch08.html#data)将详细介绍这一点。
- en: Rather than importing all data into Spark, you can request Spark to access the
    data source without importing it—this is a decision you should make based on speed
    and performance. Importing all of the data into the Spark session incurs a one-time
    up-front cost, since Spark needs to wait for the data to be loaded before analyzing
    it. If the data is not imported, you usually incur a cost with every Spark operation
    since Spark needs to retrieve a subset from the cluster’s storage, which is usually
    disk drives that happen to be much slower than reading from Spark’s memory. More
    on this topic will be covered in [Chapter 9](ch09.html#tuning).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将所有数据导入Spark，您可以请求Spark访问数据源而不导入它——这是基于速度和性能进行的决策。将所有数据导入Spark会产生一次性的前期成本，因为Spark需要等待数据加载才能进行分析。如果数据没有导入，通常在每次Spark操作时会产生成本，因为Spark需要从集群存储中检索子集，这通常是磁盘驱动器，速度比从Spark内存读取慢得多。关于这个话题的更多内容将在[第9章](ch09.html#tuning)中讨论。
- en: Let’s prime the session with some data by importing `mtcars` into Spark using
    `copy_to()`; you can also import data from distributed files in many different
    file formats, which we look at in [Chapter 8](ch08.html#data).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用`copy_to()`将`mtcars`导入Spark来准备一些数据会话；您还可以从多种不同的文件格式中导入分布式文件中的数据，我们将在[第8章](ch08.html#data)中讨论这些。
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When using real clusters, you should use `copy_to()` to transfer only small
    tables from R; large data transfers should be performed with specialized data
    transfer tools.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用真实集群时，应使用`copy_to()`从R中仅传输小表；大数据传输应使用专门的数据传输工具进行。
- en: The data is now accessible to Spark and you can now apply transformations with
    ease; the next section covers how to wrangle data by running transformations inside
    Spark, using `dplyr`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以访问数据到Spark，并且可以轻松应用转换；下一节将介绍如何在Spark中运行转换以整理数据，使用`dplyr`。
- en: Wrangle
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整理
- en: Data wrangling uses transformations to understand the data. It is often referred
    to as the process of transforming data from one “raw” data form into another format
    with the intent of making it more appropriate for data analysis.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理使用转换来理解数据。通常被称为将数据从一种“原始”形式转换为另一种适合数据分析的格式的过程。
- en: Malformed or missing values and columns with multiple attributes are common
    data problems you might need to fix, since they prevent you from understanding
    your dataset. For example, a “name” field contains the last and first name of
    a customer. There are two attributes (first and last name) in a single column.
    To be usable, we need to *transform* the “name” field, by *changing* it into “first_name”
    and “last_name” fields.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中常见的问题包括格式错误或缺失值以及具有多个属性的列，这可能需要修复，因为它们会阻碍你理解数据集。例如，“name”字段包含客户的姓和名。一个单独的列中包含两个属性（姓和名）。为了能够使用，我们需要*转换*“name”字段，*改变*它成为“first_name”和“last_name”字段。
- en: After the data is cleaned, you still need to understand the basics about its
    content. Other transformations such as aggregations can help with this task. For
    example, the result of requesting the average balance of all customers will return
    a single row and column. The value will be the average of all customers. That
    information will give us context when we see individual, or grouped, customer
    balances.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据清理完成后，您仍然需要了解其内容的基础知识。其他转换如聚合可以帮助完成此任务。例如，请求所有客户平均余额的结果将返回一行和一列。该值将是所有客户的平均值。当我们查看单个或分组客户余额时，这些信息将为我们提供背景。
- en: The main goal is to write the data transformations using R syntax as much as
    possible. This saves us from the cognitive cost of having to switch between multiple
    computer technologies to accomplish a single task. In this case, it is better
    to take advantage of `dplyr` instead of writing Spark SQL statements for data
    exploration.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 主要目标是尽可能使用R语法编写数据转换。这样可以避免在完成单一任务时需要在多个计算技术之间切换所带来的认知成本。在这种情况下，最好利用`dplyr`而不是编写Spark
    SQL语句进行数据探索。
- en: 'In the R environment, `cars` can be treated as if it were a local DataFrame,
    so you can use `dplyr` verbs. For instance, we can find out the mean of all columns
    by using `summarise_all()`:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 环境中，`cars` 可以被视为本地 DataFrame，因此您可以使用 `dplyr` 动词。例如，我们可以通过使用 `summarise_all()`
    找出所有列的均值：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'While this code is exactly the same as the code you would run when using `dplyr`
    without Spark, a lot is happening under the hood. The data is *not* being imported
    into R; instead, `dplyr` converts this task into SQL statements that are then
    sent to Spark. The `show_query()` command makes it possible to peer into the SQL
    statement that `sparklyr` and `dplyr` created and sent to Spark. We can also use
    this time to introduce the pipe operator (`%>%`), a custom operator from the `magrittr`
    package that pipes a computation into the first argument of the next function,
    making your data analysis much easier to read:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这段代码与在没有 Spark 的情况下使用 `dplyr` 运行的代码完全相同，但在幕后发生了很多事情。数据并没有被导入到 R 中；相反，`dplyr`
    将此任务转换为 SQL 语句，然后将其发送到 Spark。`show_query()` 命令使我们能够查看 `sparklyr` 和 `dplyr` 创建并发送给
    Spark 的 SQL 语句。我们还可以利用这个时间来介绍管道操作符 (`%>%`)，这是 `magrittr` 包中的自定义操作符，将计算管道化到下一个函数的第一个参数中，使得数据分析变得更加易读：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As is evident, `dplyr` is much more concise than SQL, but rest assured, you
    will not need to see or understand SQL when using `dplyr`. Your focus can remain
    on obtaining insights from the data, as opposed to figuring out how to express
    a given set of transformations in SQL. Here is another example that groups the
    `cars` dataset by `transmission` type:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，`dplyr` 比 SQL 更加简洁，但请放心，在使用 `dplyr` 时您不需要查看或理解 SQL。您的焦点可以继续集中在从数据中获取洞察力上，而不是弄清楚如何在
    SQL 中表达给定的转换集。这里还有一个示例，按 `transmission` 类型对 `cars` 数据集进行分组：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Most of the data transformation operations made available by `dplyr` to work
    with local DataFrames are also available to use with a Spark connection. This
    means that you can focus on learning `dplyr` first and then reuse that skill when
    working with Spark. Chapter 5 from the book [*R for Data Science*](https://r4ds.had.co.nz/)
    by Hadley Wickham and Garrett Grolemund (O’Reilly) is a great resource to learn
    `dplyr` in depth. If proficiency with `dplyr` is not an issue for you, we recommend
    that you take some time to experiment with different `dplyr` functions against
    the `cars` table.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`dplyr` 提供的大多数数据转换操作都可以与本地 DataFrame 一起使用 Spark 连接。这意味着您可以先专注于学习 `dplyr`，然后在与
    Spark 工作时重复使用这些技能。由 Hadley Wickham 和 Garrett Grolemund (O’Reilly) 合著的 [*R for
    Data Science*](https://r4ds.had.co.nz/) 第 5 章是深入学习 `dplyr` 的好资源。如果您对 `dplyr` 的熟练掌握不成问题，我们建议您花些时间尝试对
    `cars` 表使用不同的 `dplyr` 函数。'
- en: Sometimes, we might need to perform an operation not yet available through `dplyr`
    and `sparklyr`. Instead of downloading the data into R, there is usually a Hive
    function within Spark to accomplish what we need. The next section covers this
    scenario.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们可能需要执行 `dplyr` 和 `sparklyr` 尚未提供的操作。与其将数据下载到 R 中，通常在 Spark 中有一个 Hive 函数可以完成我们所需的任务。下一节将介绍这种情况。
- en: Built-in Functions
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内置函数
- en: Spark SQL is based on Hive’s SQL conventions and functions, and it is possible
    to call all these functions using `dplyr` as well. This means that we can use
    any Spark SQL functions to accomplish operations that might not be available via
    `dplyr`. We can access the functions by calling them as if they were R functions.
    Instead of failing, `dplyr` passes functions it does not recognize as is to the
    query engine. This gives us a lot of flexibility on the functions we can use.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 基于 Hive 的 SQL 约定和函数，可以使用 `dplyr` 调用所有这些函数。这意味着我们可以使用任何 Spark SQL 函数来执行可能不通过
    `dplyr` 提供的操作。我们可以通过调用它们就像调用 R 函数一样访问这些函数。`dplyr` 不识别的函数会原样传递给查询引擎，而不是失败。这使我们在使用函数方面具有了很大的灵活性。
- en: 'For instance, the `percentile()` function returns the exact percentile of a
    column in a group. The function expects a column name, and either a single percentile
    value or an array of percentile values. We can use this Spark SQL function from
    `dplyr`, as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`percentile()` 函数返回组中列的精确百分位数。该函数期望一个列名，以及一个单一的百分位值或百分位值数组。我们可以使用 `dplyr`
    中的这个 Spark SQL 函数，如下所示：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'There is no `percentile()` function in R, so `dplyr` passes that portion of
    the code as-is to the resulting SQL query:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 中没有 `percentile()` 函数，因此 `dplyr` 将代码部分原样传递给生成的 SQL 查询：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To pass multiple values to `percentile()`, we can call another Hive function
    called `array()`. In this case, `array()` would work similarly to R’s `list()`
    function. We can pass multiple values separated by commas. The output from Spark
    is an array variable, which is imported into R as a list variable column:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要传递多个值给 `percentile()`，我们可以调用另一个名为 `array()` 的 Hive 函数。在这种情况下，`array()` 的工作方式类似于
    R 的 `list()` 函数。我们可以通过逗号分隔的方式传递多个值。Spark 的输出是一个数组变量，它作为一个列表变量列导入到 R 中：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can use the `explode()` function to separate Spark’s array value results
    into their own record. To do this, use `explode()` within a `mutate()` command,
    and pass the variable containing the results of the percentile operation:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `explode()` 函数将 Spark 数组值结果分隔成它们自己的记录。为此，请在 `mutate()` 命令中使用 `explode()`，并传递包含百分位操作结果的变量：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We have included a comprehensive list of all the Hive functions in the section
    [“Hive Functions”](app01.html#hive-functions). Glance over them to get a sense
    of the wide range of operations that you can accomplish with them.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [“Hive 函数”](app01.html#hive-functions) 部分包含了所有 Hive 函数的详尽列表。快速浏览一下，了解您可以使用它们完成的各种操作范围。
- en: Correlations
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关性
- en: 'A very common exploration technique is to calculate and visualize correlations,
    which we often calculate to find out what kind of statistical relationship exists
    between paired sets of variables. Spark provides functions to calculate correlations
    across the entire dataset and returns the results to R as a DataFrame object:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 非常常见的探索技术是计算和可视化相关性，我们经常计算它们以找出成对变量之间存在的统计关系。Spark 提供了计算整个数据集相关性的函数，并将结果作为 DataFrame
    对象返回给 R：
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `corrr` R package specializes in correlations. It contains friendly functions
    to prepare and visualize the results. Included inside the package is a backend
    for Spark, so when a Spark object is used in `corrr`, the actual computation also
    happens in Spark. In the background, the `correlate()` function runs `sparklyr::ml_corr()`,
    so there is no need to collect any data into R prior to running the command:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`corrr` R 包专门用于相关性。它包含友好的函数来准备和可视化结果。在该包中还包含了一个用于 Spark 的后端，因此当在 `corrr` 中使用
    Spark 对象时，实际的计算也发生在 Spark 中。在后台，`correlate()` 函数运行 `sparklyr::ml_corr()`，因此在运行命令之前无需将任何数据收集到
    R 中：'
- en: '[PRE19]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can pipe the results to other `corrr` functions. For example, the `shave()`
    function turns all of the duplicated results into `NAs`. Again, while this feels
    like standard R code using existing R packages, Spark is being used under the
    hood to perform the correlation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将结果传输给其他 `corrr` 函数。例如，`shave()` 函数将所有重复的结果转换为 `NAs`。再次强调，虽然这感觉像是使用现有 R
    包的标准 R 代码，但 Spark 实际上在执行相关性计算。
- en: 'Additionally, as shown in [Figure 3-6](#analysis-corrr-rplot), the results
    can be easily visualized using the `rplot()` function, as shown here:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，如图 [Figure 3-6](#analysis-corrr-rplot) 所示，可以使用 `rplot()` 函数轻松地进行可视化，如下所示：
- en: '[PRE21]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Using rplot() to visualize correlations](assets/mswr_0306.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![使用 rplot() 可视化相关性](assets/mswr_0306.png)'
- en: Figure 3-6\. Using rplot() to visualize correlations
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-6\. 使用 rplot() 可视化相关性
- en: 'It is much easier to see which relationships are positive or negative: positive
    relationships are in gray, and negative relationships are black. The size of the
    circle indicates how significant their relationship is. The power of visualizing
    data is in how much easier it makes it for us to understand results. The next
    section expands on this step of the process.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 看出哪些关系是正向的或负向的要容易得多：正向关系是灰色的，负向关系是黑色的。圆圈的大小表示它们的关系有多显著。数据可视化的威力在于它能让我们更容易地理解结果。下一节将扩展这个过程的步骤。
- en: Visualize
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化
- en: Visualizations are a vital tool to help us find patterns in the data. It is
    easier for us to identify outliers in a dataset of 1,000 observations when plotted
    in a graph, as opposed to reading them from a list.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化是帮助我们在数据中找到模式的重要工具。当在图表中绘制 1000 条观察数据集时，我们更容易识别异常值，而不是从列表中读取它们。
- en: R is great at data visualizations. Its capabilities for creating plots are extended
    by the many R packages that focus on this analysis step. Unfortunately, the vast
    majority of R functions that create plots depend on the data already being in
    local memory within R, so they fail when using a remote table within Spark.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: R 在数据可视化方面非常出色。通过许多专注于此分析步骤的 R 包，它的绘图能力得到了扩展。不幸的是，大多数创建图表的 R 函数都依赖于已经在 R 本地内存中的数据，因此在使用
    Spark 中的远程表时会失败。
- en: It is possible to create visualizations in R from data sources that exist in
    Spark. To understand how to do this, let’s first break down how computer programs
    build plots. To begin, a program takes the raw data and performs some sort of
    transformation. The transformed data is then mapped to a set of coordinates. Finally,
    the mapped values are drawn in a plot. [Figure 3-7](#analysis-plot) summarizes
    each of the steps.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从存在于 Spark 中的数据源创建 R 中的可视化。要理解如何做到这一点，让我们首先分解计算机程序如何构建图表。首先，程序获取原始数据并执行某种转换。然后，转换后的数据被映射到一组坐标上。最后，映射的值被绘制成图表。[Figure 3-7](#analysis-plot)
    总结了每个步骤。
- en: '![Stages of an R plot](assets/mswr_0307.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![R 绘图的阶段](assets/mswr_0307.png)'
- en: Figure 3-7\. Stages of an R plot
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-7\. R 绘图的阶段
- en: 'In essence, the approach for visualizing is the same as in wrangling: push
    the computation to Spark, and then collect the results in R for plotting. As illustrated
    in [Figure 3-8](#analysis-spark-plot), the heavy lifting of preparing the data,
    such as aggregating the data by groups or bins, can be done within Spark, and
    then the much smaller dataset can be collected into R. Inside R, the plot becomes
    a more basic operation. For example, for a histogram, the bins are calculated
    in Spark, and then plotted in R using a simple column plot, as opposed to a histogram
    plot, because there is no need for R to recalculate the bins.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，可视化的方法与数据整理的方法相同：将计算推送到 Spark，然后在 R 中收集结果进行绘制。正如在 [Figure 3-8](#analysis-spark-plot)
    中所示，准备数据的繁重工作（例如按组或区间聚合数据）可以在 Spark 中完成，然后将较小的数据集收集到 R 中。在 R 中，绘制图表变成了更基本的操作。例如，对于直方图，Spark
    计算了区间，然后在 R 中使用简单的柱状图进行绘制，而不是直方图，因为无需重新计算区间。
- en: '![Plotting with Spark and R](assets/mswr_0308.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Spark 和 R 进行绘图](assets/mswr_0308.png)'
- en: Figure 3-8\. Plotting with Spark and R
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-8\. 使用 Spark 和 R 进行绘图
- en: Let’s apply this conceptual model when using `ggplot2`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `ggplot2` 时，让我们应用这个概念模型。
- en: Using ggplot2
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 ggplot2
- en: 'To create a bar plot using `ggplot2`, we simply call a function:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `ggplot2` 创建条形图，我们只需调用一个函数：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In this case, the `mtcars` raw data was *automatically* transformed into three
    discrete aggregated numbers. Next, each result was mapped into an `x` and `y`
    plane. Then the plot was drawn. As R users, all of the stages of building the
    plot are conveniently abstracted for us.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`mtcars` 的原始数据被 *自动* 转换为三个离散聚合的数字。接下来，每个结果被映射到 `x` 和 `y` 平面上。然后绘制图表。作为
    R 用户，构建图表的所有阶段都为我们方便地抽象化了。
- en: In Spark, there are a couple of key steps when codifying the “push compute,
    collect results” approach. First, ensure that the transformation operations happen
    within Spark. In the example that follows, `group_by()` and `summarise()` will
    run inside Spark. The second is to bring the results back into R after the data
    has been transformed. Be sure to transform and then collect, in that order; if
    `collect()` is run first, R will try to ingest the entire dataset from Spark.
    Depending on the size of the data, collecting all of the data will slow down or
    can even bring down your system.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，编码“推送计算，收集结果”的关键步骤有几个。首先，确保转换操作发生在 Spark 内部。在接下来的示例中，`group_by()`
    和 `summarise()` 将在 Spark 内部运行。第二个步骤是在数据被转换后将结果带回 R。确保按照转换然后收集的顺序进行；如果先运行 `collect()`，R
    将尝试从 Spark 中摄取整个数据集。根据数据大小，收集所有数据将减慢系统速度，甚至可能使系统崩溃。
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In this example, now that the data has been preaggregated and collected into
    R, only three records are passed to the plotting function:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，现在数据已经预先聚合并收集到 R 中，只传递了三条记录给绘图函数：
- en: '[PRE25]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[Figure 3-9](#analysis-viz1) shows the resulting plot.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 3-9](#analysis-viz1) 展示了生成的图表。'
- en: '![Plot with aggregation in Spark](assets/mswr_0309.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![在 Spark 中进行聚合绘图](assets/mswr_0309.png)'
- en: Figure 3-9\. Plot with aggregation in Spark
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-9\. 在 Spark 中进行聚合绘图
- en: Any other `ggplot2` visualization can be made to work using this approach; however,
    this is beyond the scope of the book. Instead, we recommend that you read [*R
    Graphics Cookbook*](https://oreil.ly/bIF4), by Winston Chang (O’Reilly) to learn
    additional visualization techniques applicable to Spark. Now, to ease this transformation
    step before visualizing, the `dbplot` package provides a few ready-to-use visualizations
    that automate aggregation in Spark.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此方法可以制作任何其他 `ggplot2` 可视化；但是，这超出了本书的范围。相反，我们建议您阅读 [*R Graphics Cookbook*](https://oreil.ly/bIF4)，作者
    Winston Chang（O’Reilly），以了解适用于 Spark 的其他可视化技术。现在，在可视化之前简化此转换步骤，`dbplot` 包提供了一些自动化聚合在
    Spark 中使用的准备就绪的可视化。
- en: Using dbplot
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 dbplot
- en: The `dbplot` package provides helper functions for plotting with remote data.
    The R code `dbplot` that’s used to transform the data is written so that it can
    be translated into Spark. It then uses those results to create a graph using the
    `ggplot2` package where data transformation and plotting are both triggered by
    a single function.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`dbplot` 包提供了用于处理远程数据的辅助函数。R 代码 `dbplot` 被编写成可以转换为 Spark。然后使用这些结果使用 `ggplot2`
    包创建图表，数据转换和绘图都由一个单一函数触发。'
- en: 'The `dbplot_histogram()` function makes Spark calculate the bins and the count
    per bin and outputs a `ggplot` object, which we can further refine by adding more
    steps to the plot object. `dbplot_histogram()` also accepts a `binwidth` argument
    to control the range used to compute the bins:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`dbplot_histogram()` 函数使 Spark 计算 bins 和每个 bin 的计数，并输出一个 `ggplot` 对象，我们可以通过添加更多步骤来进一步优化绘图对象。`dbplot_histogram()`
    还接受一个 `binwidth` 参数来控制用于计算 bins 的范围：'
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Figure 3-10](#analysis-visualizations-histogram) presents the resulting plot.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-10](#analysis-visualizations-histogram) 展示了生成的绘图。'
- en: '![Histogram created by dbplot](assets/mswr_0310.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![由 dbplot 创建的直方图](assets/mswr_0310.png)'
- en: Figure 3-10\. Histogram created by dbplot
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-10\. 由 dbplot 创建的直方图
- en: Histograms provide a great way to analyze a single variable. To analyze two
    variables, a scatter or raster plot is commonly used.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图提供了分析单个变量的绝佳方式。要分析两个变量，通常使用散点图或光栅图。
- en: 'Scatter plots are used to compare the relationship between two continuous variables.
    For example, a scatter plot will display the relationship between the weight of
    a car and its gas consumption. The plot in [Figure 3-11](#analysis-point) shows
    that the higher the weight, the higher the gas consumption because the dots clump
    together into almost a line that goes from the upper left toward the lower right.
    Here’s the code to generate the plot:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图用于比较两个连续变量之间的关系。例如，散点图将显示汽车重量与其燃油消耗之间的关系。[图 3-11](#analysis-point) 中的图表显示，重量越大，燃油消耗越高，因为点几乎聚集成一条从左上到右下的线。以下是生成该图表的代码：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Scatter plot example in Spark](assets/mswr_0311.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 中的散点图示例](assets/mswr_0311.png)'
- en: Figure 3-11\. Scatter plot example in Spark
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-11\. Spark 中的散点图示例
- en: However, for scatter plots, no amount of “pushing the computation” to Spark
    will help with this problem because the data must be plotted in individual dots.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于散点图来说，无论如何“将计算推向” Spark 都无法解决这个问题，因为数据必须以单个点的形式绘制。
- en: The best alternative is to find a plot type that represents the x/y relationship
    and concentration in a way that it is easy to perceive and to “physically” plot.
    The *raster* plot might be the best answer. A raster plot returns a grid of x/y
    positions and the results of a given aggregation, usually represented by the color
    of the square.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的替代方案是找到一种能够以易于感知且“物理”绘制的方式表示 x/y 关系和浓度的图表类型。*光栅*图可能是最佳答案。光栅图返回一个 x/y 位置网格和给定聚合结果，通常由方块的颜色表示。
- en: 'You can use `dbplot_raster()` to create a scatter-like plot in Spark, while
    only retrieving (collecting) a small subset of the remote dataset:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `dbplot_raster()` 在 Spark 中创建类似散点图的图表，同时只检索（收集）远程数据集的小部分：
- en: '[PRE28]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As shown in [Figure 3-12](#analysis-visualizations-raster), the resulting plot
    returns a grid no bigger than 5 x 5\. This limits the number of records that need
    to be collected into R to 25.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 3-12](#analysis-visualizations-raster) 所示，生成的绘图返回的网格不超过 5 x 5。这限制了需要收集到
    R 中的记录数为 25。
- en: '![A raster plot using Spark](assets/mswr_0312.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Spark 创建的光栅图](assets/mswr_0312.png)'
- en: Figure 3-12\. A raster plot using Spark
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-12\. 使用 Spark 的光栅图
- en: Tip
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can also use `dbplot` to retrieve the raw data and visualize by other means;
    to retrieve the aggregates, but not the plots, use `db_compute_bins()`, `db_compute_count()`,
    `db_compute_raster()`, and `db_compute_boxplot()`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 `dbplot` 通过其他方式检索原始数据并进行可视化；要检索聚合数据但不是绘图，请使用 `db_compute_bins()`、`db_compute_count()`、`db_compute_raster()`
    和 `db_compute_boxplot()`。
- en: While visualizations are indispensable, you can complement data analysis using
    statistical models to gain even deeper insights into our data. The next section
    describes how we can prepare data for modeling with Spark.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可视化是不可或缺的，但您可以通过统计模型来补充数据分析，以深入了解我们的数据。下一节描述了如何准备数据以用于 Spark 建模。
- en: Model
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: The next two chapters focus entirely on modeling, so rather than introducing
    modeling in too much detail in this chapter, we want to cover how to interact
    with models while doing data analysis.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两章完全专注于建模，因此我们不打算在本章中详细介绍建模，而是想覆盖在进行数据分析时如何与模型进行交互。
- en: 'First, an analysis project goes through many transformations and models to
    find the answer. That’s why the first data analysis diagram we introduced in [Figure 3-2](#analysis-approach)
    illustrates a cycle of: visualizing, wrangling, and modeling—we know you don’t
    end with modeling, not in R, nor when using Spark.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，分析项目经历了多次转换和模型以找到答案。这就是为什么我们在[图3-2](#analysis-approach)中介绍了第一个数据分析图表，展示了可视化、数据处理和建模的循环——我们知道你不仅在R中，甚至在使用Spark时也不仅仅停留在建模。
- en: Therefore, the ideal data analysis language enables you to quickly adjust over
    each wrangle-visualize-model iteration. Fortunately, this is the case when using
    Spark and R.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，理想的数据分析语言使您能够在每个数据处理-可视化-建模迭代中快速调整。幸运的是，使用Spark和R时就是这种情况。
- en: 'To illustrate how easy it is to iterate over wrangling and modeling in Spark,
    consider the following example. We will start by performing a linear regression
    against all features and predict miles per gallon:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要说明在Spark中迭代数据处理和建模有多容易，请考虑以下示例。我们将从针对所有特征执行线性回归开始，并预测每加仑英里数：
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'At this point, it is very easy to experiment with different features, we can
    simply change the R formula from `mpg ~ .` to, say, `mpg ~ hp + cyl` to only use
    horsepower and cylinders as features:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，试验不同特征非常容易，我们可以简单地更改R公式，例如从`mpg ~ .`到`mpg ~ hp + cyl`，仅使用马力和汽缸作为特征：
- en: '[PRE31]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Additionally, it is also very easy to iterate with other kinds of models. The
    following one replaces the linear model with a generalized linear model:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，使用其他类型的模型进行迭代也非常容易。以下示例将线性模型替换为广义线性模型：
- en: '[PRE33]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Usually, before fitting a model you would need to use multiple `dplyr` transformations
    to get it ready to be consumed by a model. To make sure the model can be fitted
    as efficiently as possible, you should cache your dataset before fitting it, as
    described next.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在拟合模型之前，您需要使用多个`dplyr`转换来准备数据，以便模型能够有效使用。为了确保模型能够尽可能高效地拟合，您应该在拟合之前缓存数据集，如下所述。
- en: Caching
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存
- en: The examples in this chapter are built using a very small dataset. In real-life
    scenarios, large amounts of data are used for models. If the data needs to be
    transformed first, the volume of the data could exact a heavy toll on the Spark
    session. Before fitting the models, it is a good idea to save the results of all
    the transformations in a new table loaded in Spark memory.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例是使用一个非常小的数据集构建的。在实际场景中，会使用大量数据来进行建模。如果需要先转换数据，则数据的数量可能会对Spark会话产生重大影响。在拟合模型之前，将所有转换的结果保存在新表中，并加载到Spark内存中是个好主意。
- en: 'The `compute()` command can take the end of a `dplyr` command and save the
    results to Spark memory:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute()`命令可以接受`dplyr`命令的末尾，并将结果保存到Spark内存中：'
- en: '[PRE35]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As more insights are gained from the data, more questions might be raised. That
    is why we expect to iterate through the data wrangle, visualize, and model cycle
    multiple times. Each iteration should provide incremental insights into what the
    data is “telling us.” There will be a point when we reach a satisfactory level
    of understanding. It is at this point that we will be ready to share the results
    of the analysis. This is the topic of the next section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 随着从数据中获得更多见解，可能会提出更多问题。这就是为什么我们希望通过数据处理、可视化和建模的迭代多次来获得更多见解。每次迭代应该为我们解读数据提供增量见解。当我们达到满意的理解水平时，我们将准备好分享分析结果。这是下一节的主题。
- en: Communicate
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 沟通
- en: It is important to clearly communicate the analysis results—as important as
    the analysis work itself! The public, colleagues, or stakeholders need to understand
    what you found out and how.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 清楚地传达分析结果同样重要——正如分析工作本身一样重要！公众、同事或利益相关者需要理解您发现的内容及其意义。
- en: To communicate effectively, we need to use artifacts such as reports and presentations;
    these are common output formats that we can create in R, using *R Markdown*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要有效地进行沟通，我们需要使用报告和演示文稿等工件；这些是我们可以使用*R Markdown*在R中创建的常见输出格式。
- en: R Markdown documents allow you to weave narrative text and code together. The
    variety of output formats provides a very compelling reason to learn and use R
    Markdown. There are many available output formats like HTML, PDF, PowerPoint,
    Word, web slides, websites, books, and so on.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: R Markdown 文档允许你将叙述文本和代码交织在一起。多种输出格式提供了非常强大的理由去学习和使用 R Markdown。有许多可用的输出格式，如
    HTML、PDF、PowerPoint、Word、Web 幻灯片、网站、书籍等等。
- en: 'Most of these outputs are available in the core R packages of R Markdown: `knitr`
    and `rmarkdown`. You can extend R Markdown with other R packages. For example,
    this book was written using R Markdown thanks to an extension provided by the
    `bookdown` package. The best resource to delve deeper into R Markdown is the official
    book.^([1](ch03.html#idm46099157387896))'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些输出格式都包含在 R Markdown 的核心 R 包 `knitr` 和 `rmarkdown` 中。你可以通过其他的 R 包扩展 R Markdown。例如，本书就是使用了
    `bookdown` 包提供的扩展来撰写的 R Markdown。深入了解 R Markdown 的最佳资源是官方书籍。^([1](ch03.html#idm46099157387896))
- en: In R Markdown, one singular artifact could potentially be rendered in different
    formats. For example, you could render the same report in HTML or as a PDF file
    by changing a setting within the report itself. Conversely, multiple types of
    artifacts could be rendered as the same output. For example, a presentation deck
    and a report could be rendered in HTML.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R Markdown 中，一个单一的文档可能以不同的格式呈现。例如，你可以通过更改报告本身的设置，将同一报告渲染为 HTML 或 PDF 文件。反之亦然，多种类型的文档也可以渲染为相同的输出。例如，一个演示文稿和一个报告都可以渲染为
    HTML。
- en: Creating a new R Markdown report that uses Spark as a computer engine is easy.
    At the top, R Markdown expects a YAML header. The first and last lines are three
    consecutive dashes (`---`). The content in between the dashes varies depending
    on the type of document. The only required field in the YAML header is the `output`
    value. R Markdown needs to know what kind of output it needs to render your report
    into. This YAML header is called *frontmatter*. Following the frontmatter are
    sections of code, called *code chunks*. These code chunks can be interlaced with
    the narratives. There is nothing particularly interesting to note when using Spark
    with R Markdown; it is just business as usual.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的使用 Spark 作为计算引擎的 R Markdown 报告很容易。在顶部，R Markdown 需要一个 YAML 头部。第一行和最后一行是三个连字符
    (`---`)。介于连字符之间的内容因文档类型不同而异。YAML 头部中唯一必需的字段是 `output` 值。R Markdown 需要知道需要将报告渲染成什么样的输出。这个
    YAML 头部称为 *frontmatter*。在 frontmatter 之后是代码块的部分，称为 *代码块*。这些代码块可以与叙述文本交织在一起。当使用
    Spark 和 R Markdown 时没有特别需要注意的地方；一切都像往常一样。
- en: 'Since an R Markdown document is self-contained and meant to be reproducible,
    before rendering documents, we should first disconnect from Spark to free resources:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 R Markdown 文档是自包含的且旨在可复制，渲染文档之前，我们应首先断开与 Spark 的连接以释放资源：
- en: '[PRE38]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following example shows how easy it is to create a fully reproducible report
    that uses Spark to process large-scale datasets. The narrative, code, and, most
    important, the output of the code is recorded in the resulting HTML file. You
    can copy and paste the following code in a file. Save the file with a *.Rmd* extension,
    and choose whatever name you would like:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例展示了创建一个完全可复制的报告并使用 Spark 处理大规模数据集是多么容易。叙述性文字、代码以及最重要的是代码输出都记录在生成的 HTML
    文件中。你可以复制并粘贴以下代码到一个文件中。将文件保存为 *.Rmd* 扩展名，并选择任何你喜欢的名称：
- en: '[PRE39]{r, setup, include = FALSE}'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE39]{r, setup, include = FALSE}'
- en: library(sparklyr)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: library(sparklyr)
- en: library(dplyr)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: library(dplyr)
- en: sc <- spark_connect(master = "local", version = "2.3")
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: sc <- spark_connect(master = "local", version = "2.3")
- en: cars <- copy_to(sc, mtcars)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: cars <- copy_to(sc, mtcars)
- en: '[PRE40]{r  fig.align=''center'', warning=FALSE}'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE40]{r  fig.align=''center'', warning=FALSE}'
- en: library(ggplot2)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: library(ggplot2)
- en: cars %>%
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: cars %>%
- en: group_by(cyl) %>% summarise(mpg = mean(mpg)) %>%
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: group_by(cyl) %>% summarise(mpg = mean(mpg)) %>%
- en: ggplot(aes(cyl, mpg)) + geom_bar(stat="identity")
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ggplot(aes(cyl, mpg)) + geom_bar(stat="identity")
- en: '[PRE41]{r}'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE41]{r}'
- en: cars %>%
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: cars %>%
- en: ml_linear_regression(wt ~ mpg) %>%
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ml_linear_regression(wt ~ mpg) %>%
- en: summary()
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: summary()
- en: '[PRE42]{r, include = FALSE}'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE42]{r, include = FALSE}'
- en: spark_disconnect(sc)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: spark_disconnect(sc)
- en: '[PRE43]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: To knit this report, save the file with a *.Rmd* extension such as *report.Rmd*,
    and run `render()` from R. The output should look like that shown in [Figure 3-13](#visualize-analysis-communicate).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 要编织此报告，请将文件保存为 *.Rmd* 扩展名，例如 *report.Rmd*，并从 R 中运行 `render()`。输出应如 [图 3-13](#visualize-analysis-communicate)
    所示。
- en: '[PRE44]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![R Markdown HTML output](assets/mswr_0313.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![R Markdown HTML 输出](assets/mswr_0313.png)'
- en: Figure 3-13\. R Markdown HTML output
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-13\. R Markdown HTML 输出
- en: You can now easily share this report, and viewers of won’t need Spark or R to
    read and consume its contents; it’s just a self-contained HTML file, trivial to
    open in any browser.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以轻松分享此报告，而查看者无需使用Spark或R来阅读和消化其内容；它只是一个自包含的HTML文件，在任何浏览器中打开都很容易。
- en: 'It is also common to distill insights of a report into many other output formats.
    Switching is quite easy: in the top frontmatter, change the `output` option to
    `powerpoint_presentation`, `pdf_document`, `word_document`, or the like. Or you
    can even produce multiple output formats from the same report:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 将报告的见解提炼到许多其他输出格式中也很常见。切换非常容易：在前言的顶部，将`output`选项更改为`powerpoint_presentation`、`pdf_document`、`word_document`或类似选项。或者，您甚至可以从同一份报告中生成多个输出格式：
- en: '[PRE45]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The result will be a PowerPoint presentation, a Word document, and a PDF. All
    of the same information that was displayed in the original HTML report is computed
    in Spark and rendered in R. You’ll likely need to edit the PowerPoint template
    or the output of the code chunks.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是一个PowerPoint演示文稿、一个Word文档和一个PDF。所有原始HTML报告中显示的信息都是在Spark中计算并在R中渲染的。您可能需要编辑PowerPoint模板或代码块的输出。
- en: This minimal example shows how easy it is to go from one format to another.
    Of course, it will take some more editing on the R user’s side to make sure the
    slides contain only the pertinent information. The main point is that it does
    not require that you learn a different markup or code conventions to switch from
    one artifact to another.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单示例展示了从一种格式转换到另一种格式有多么容易。当然，需要在R用户的一侧进行更多编辑，以确保幻灯片只包含相关信息。主要问题在于，不需要学习不同的标记或代码约定就能从一个文档转换到另一个文档。
- en: Recap
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter presented a solid introduction to data analysis with R and Spark.
    Many of the techniques presented looked quite similar to using just R and no Spark,
    which, while anticlimactic, is the right design to help users already familiar
    with R to easily transition to Spark. For users unfamiliar with R, this chapter
    also served as a very brief introduction to some of the most popular (and useful!)
    packages available in R.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了使用R和Spark进行数据分析的基础知识。许多介绍的技术看起来与仅使用R而不是Spark非常相似，虽然有些平淡，但确实是帮助已熟悉R的用户轻松过渡到Spark的正确设计。对于不熟悉R的用户，本章还是对R中一些最流行（和有用！）包的简要介绍。
- en: It should now be quite obvious that, together, R and Spark are a powerful combination—a
    large-scale computing platform, along with an incredibly robust ecosystem of R
    packages, makes for an ideal analysis platform.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该很明显，R和Spark一起是一个强大的组合——一个大规模计算平台，以及一个非常强大的R包生态系统，使其成为一个理想的分析平台。
- en: While doing analysis in Spark with R, remember to push computation to Spark
    and focus on collecting results in R. This paradigm should set up a successful
    approach to data manipulation, visualization and communication through sharing
    your results in a variety of outputs.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中使用R进行分析时，请记住将计算推送到Spark并专注于在R中收集结果。这种范式应该为通过在各种输出中分享结果来进行数据处理、可视化和沟通设立一个成功的方法。
- en: '[Chapter 4](ch04.html#modeling) will dive deeper into how to build statistical
    models in Spark using a much more interesting dataset (what’s more interesting
    than dating datasets?). You will also learn many more techniques that we did not
    even mention in the brief modeling section from this chapter.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[第四章](ch04.html#modeling)将深入探讨如何使用更有趣的数据集（比约会数据集更有趣的是什么？）在Spark中构建统计模型。您还将从本章的简要建模部分学习更多技术，我们甚至没有提及。'
- en: '^([1](ch03.html#idm46099157387896-marker)) Xie Allaire G (2018). *R Markdown:
    The Definite Guide*, 1st edition. CRC Press.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch03.html#idm46099157387896-marker)) Xie Allaire G（2018）。《R Markdown:
    The Definite Guide》，第1版。CRC Press。'
