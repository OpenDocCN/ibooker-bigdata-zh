- en: Foreword
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: 'Apache Spark is a distributed computing platform built on extensibility: Spark’s
    APIs make it easy to combine input from many data sources and process it using
    diverse programming languages and algorithms to build a data application. R is
    one of the most powerful languages for data science and statistics, so it makes
    a lot of sense to connect R to Spark. Fortunately, R’s rich language features
    enable simple APIs for calling Spark from R that look similar to running R on
    local data sources. With a bit of background about both systems, you will be able
    to invoke massive computations in Spark or run your R code in parallel from the
    comfort of your favorite R programming environment.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个建立在可扩展性之上的分布式计算平台：Spark的API使得从多个数据源获取输入并使用不同的编程语言和算法进行处理来构建数据应用变得容易。R语言是数据科学和统计学中最强大的语言之一，因此将R与Spark连接起来非常有意义。幸运的是，R丰富的语言特性使得从R调用Spark的API变得简单，看起来就像在本地数据源上运行R一样。只需了解这两个系统的一些背景知识，你就能够在Spark中调用大规模计算，或者从你喜爱的R编程环境中并行运行你的R代码。
- en: This book explores using Spark from R in detail, focusing on the `sparklyr`
    package that enables support for `dplyr` and other packages known to the R community.
    It covers all of the main use cases in detail, ranging from querying data using
    the Spark engine to exploratory data analysis, machine learning, parallel execution
    of R code, and streaming. It also has a self-contained introduction to running
    Spark and monitoring job execution. The authors are exactly the right people to
    write about this topic—Javier, Kevin, and Edgar have been involved in `sparklyr`
    development since the project started. I was excited to see how well they’ve assembled
    this clear and focused guide about using Spark with R.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本书详细探讨了如何使用Spark进行R编程，重点介绍了`sparklyr`包，该包支持`dplyr`和其他为R社区所熟知的包。它详细介绍了所有主要的使用案例，从使用Spark引擎查询数据到探索性数据分析、机器学习、R代码的并行执行以及流处理。它还提供了一个自包含的介绍，讲述了如何运行Spark并监控作业的执行情况。书中的作者——Javier、Kevin和Edgar，自项目开始以来一直参与了`sparklyr`的开发。我对他们如何精心组织这本关于在R中使用Spark的清晰而专注的指南感到非常兴奋。
- en: I hope that you enjoy this book and use it to scale up your R workloads and
    connect them to the capabilities of the broader Spark ecosystem. And because all
    of the infrastructure here is open source, don’t hesitate to give the developers
    feedback about making these tools better.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢这本书，并用它来扩展你的R工作负载，并将其连接到更广泛的Spark生态系统的能力中去。因为这里所有的基础设施都是开源的，所以请毫不犹豫地向开发人员提供关于改进这些工具的反馈。
- en: Matei Zaharia
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[Matei Zaharia](https://en.wikipedia.org/wiki/Matei_Zaharia)'
- en: Assistant Professor at Stanford University,
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福大学助理教授，
- en: Chief Technologist at Databricks,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks的首席技术专家，
- en: and original creator of Apache Spark
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 和Apache Spark的原创者
