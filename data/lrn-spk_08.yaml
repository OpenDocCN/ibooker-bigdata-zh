- en: Chapter 7\. Optimizing and Tuning Spark Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章\. 优化和调整 Spark 应用程序
- en: In the previous chapter, we elaborated on how to work with Datasets in Java
    and Scala. We explored how Spark manages memory to accommodate Dataset constructs
    as part of its unified and high-level API, and we considered the costs associated
    with using Datasets and how to mitigate those costs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们详细说明了如何在 Java 和 Scala 中处理数据集。我们探讨了 Spark 如何管理内存以适应数据集构造作为其统一和高级 API
    的一部分，以及考虑了使用数据集的成本及其如何减轻这些成本。
- en: Besides mitigating costs, we also want to consider how to optimize and tune
    Spark. In this chapter, we will discuss a set of Spark configurations that enable
    optimizations, look at Spark’s family of join strategies, and inspect the Spark
    UI, looking for clues to bad behavior.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 除了降低成本外，我们还希望考虑如何优化和调整 Spark。在本章中，我们将讨论一组启用优化的 Spark 配置，查看 Spark 的连接策略系列，并检查
    Spark UI，寻找不良行为的线索。
- en: Optimizing and Tuning Spark for Efficiency
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化和调整 Spark 的效率
- en: While Spark has many configurations for [tuning](https://oreil.ly/c7Y2q), this
    book will only cover a handful of the most important and commonly tuned configurations.
    For a comprehensive list grouped by functional themes, you can peruse the [documentation](https://oreil.ly/mifI7).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Spark 有许多用于[调优](https://oreil.ly/c7Y2q)的配置，但本书只涵盖了一些最重要和常调整的配置。要获得按功能主题分组的全面列表，您可以查阅[文档](https://oreil.ly/mifI7)。
- en: Viewing and Setting Apache Spark Configurations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看和设置 Apache Spark 配置
- en: 'There are three ways you can get and set Spark properties. The first is through
    a set of configuration files. In your deployment’s `$SPARK_HOME` directory (where
    you installed Spark), there are a number of config files: *conf/spark-defaults.conf.template*,
    *conf/log4j.properties.template*, and *conf/spark-env.sh.template*. Changing the
    default values in these files and saving them without the .*template* suffix instructs
    Spark to use these new values.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过三种方式获取和设置 Spark 的属性。首先是通过一组配置文件。在你部署的 `$SPARK_HOME` 目录（即你安装 Spark 的地方），有一些配置文件：*conf/spark-defaults.conf.template*、*conf/log4j.properties.template*
    和 *conf/spark-env.sh.template*。修改这些文件中的默认值，并去掉 `.template` 后缀后保存，Spark 将使用这些新值。
- en: Note
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Configuration changes in the *conf/spark-defaults.conf* file apply to the Spark
    cluster and all Spark applications submitted to the cluster.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *conf/spark-defaults.conf* 文件中的配置更改适用于 Spark 集群和提交到集群的所有 Spark 应用程序。
- en: 'The second way is to specify Spark configurations directly in your Spark application
    or on the command line when submitting the application with `spark-submit`, using
    the `--conf` flag:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是直接在 Spark 应用程序中或在使用 `spark-submit` 提交应用程序时的命令行中指定 Spark 配置，使用 `--conf`
    标志：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here’s how you would do this in the Spark application itself:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是在 Spark 应用程序中如何操作：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The third option is through a programmatic interface via the Spark shell. As
    with everything else in Spark, APIs are the primary method of interaction. Through
    the `SparkSession` object, you can access most Spark config settings.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种选项是通过 Spark shell 的程序化接口。与 Spark 中的其他一切一样，API 是主要的交互方法。通过 `SparkSession`
    对象，您可以访问大多数 Spark 配置设置。
- en: 'In a Spark REPL, for example, this Scala code shows the Spark configs on a
    local host where Spark is launched in local mode (for details on the different
    modes available, see [“Deployment modes”](ch01.html#deployment_modes) in [Chapter 1](ch01.html#introduction_to_apache_spark_a_unified_a)):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 Spark REPL 中，以下 Scala 代码显示了在本地主机上以本地模式启动 Spark（有关可用的不同模式的详细信息，请参见 [“部署模式”](ch01.html#deployment_modes)
    在 [第 1 章](ch01.html#introduction_to_apache_spark_a_unified_a)）时的 Spark 配置：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can also view only the Spark SQL–specific Spark configs:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以查看仅限于 Spark SQL 的特定 Spark 配置：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Alternatively, you can access Spark’s current configuration through the Spark
    UI’s Environment tab, which we discuss later in this chapter, as read-only values,
    as shown in [Figure 7-1](#the_spark_3dot0_uiapostrophes_environmen).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以通过 Spark UI 的环境标签页访问当前 Spark 的配置，我们将在本章后面讨论，这些值是只读的，如 [图 7-1](#the_spark_3dot0_uiapostrophes_environmen)
    所示。
- en: '![The Spark 3.0 UI’s Environment tab](assets/lesp_0701.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 3.0 UI 的环境标签页](assets/lesp_0701.png)'
- en: Figure 7-1\. The Spark 3.0 UI’s Environment tab
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. Spark 3.0 UI 的环境标签页
- en: 'To set or modify an existing configuration programmatically, first check if
    the property is modifiable. `spark.conf.isModifiable("*<config_name>*")` will
    return `true` or `false`. All modifiable configs can be set to new values using
    the API:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要以编程方式设置或修改现有配置，首先检查属性是否可修改。`spark.conf.isModifiable("*<config_name>*")`将返回`true`或`false`。可以使用API将所有可修改的配置设置为新值：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Among all the ways that you can set Spark properties, an order of precedence
    determines which values are honored. Any values or flags defined in *spark-defaults.conf*
    will be read first, followed by those supplied on the command line with `spark-submit`,
    and finally those set via `SparkSession` in the Spark application. All these properties
    will be merged, with any duplicate properties reset in the Spark application taking
    precedence. Likewise, values supplied on the command line will supersede settings
    in the configuration file, provided they are not overwritten in the application
    itself.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有可以设置Spark属性的方式中，存在一个优先顺序确定哪些值将被采用。首先读取*spark-defaults.conf*中定义的所有值或标志，然后是使用`spark-submit`命令行提供的值，最后是通过SparkSession在Spark应用程序中设置的值。所有这些属性将被合并，重置在Spark应用程序中的重复属性将优先。同样地，通过命令行提供的值将覆盖配置文件中的设置，前提是它们未在应用程序本身中被覆盖。
- en: Tweaking or supplying the right configurations helps with performance, as you’ll
    see in the next section. The recommendations here are derived from practitioners’
    observations in the community and focus on how to maximize cluster resource utilization
    for Spark to accommodate large-scale workloads.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 调整或提供正确的配置有助于提高性能，这一点将在下一节中详细讨论。这里的建议来自社区从业者的观察，专注于如何最大化Spark集群资源利用率，以适应大规模工作负载。
- en: Scaling Spark for Large Workloads
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为大规模工作负载扩展Spark
- en: 'Large Spark workloads are often batch jobs—some run on a nightly basis, while
    some are scheduled at regular intervals during the day. In either case, these
    jobs may process tens of terabytes of data or more. To avoid job failures due
    to resource starvation or gradual performance degradation, there are a handful
    of Spark configurations that you can enable or alter. These configurations affect
    three Spark components: the Spark driver, the executor, and the shuffle service
    running on the executor.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模的Spark工作负载通常是批处理作业——有些在每晚运行，有些则在白天定期调度。无论哪种情况，这些作业可能处理数十TB甚至更多的数据。为了避免由于资源匮乏或性能逐渐下降而导致作业失败，有几个Spark配置可以启用或修改。这些配置影响三个Spark组件：Spark驱动程序、执行器以及执行器上运行的洗牌服务。
- en: The Spark driver’s responsibility is to coordinate with the cluster manager
    to launch executors in a cluster and schedule Spark tasks on them. With large
    workloads, you may have hundreds of tasks. This section explains a few configurations
    you can tweak or enable to optimize your resource utilization, parallelize tasks,
    and avoid bottlenecks for large numbers of tasks. Some of the optimization ideas
    and insights have been derived from big data companies like Facebook that use
    Spark at terabyte scale, which they shared with the Spark community at the Spark
    + AI Summit.^([1](ch07.html#ch01fn7))
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Spark驱动程序的责任是与集群管理器协调，在集群中启动执行器并调度Spark任务。在大型工作负载下，您可能会有数百个任务。本节解释了您可以调整或启用的一些配置，以优化资源利用率，并行化任务，避免大量任务的瓶颈。一些优化思路和见解来自像Facebook这样的大数据公司，在使用Spark处理TB级数据时分享给了Spark社区，并在Spark
    + AI Summit上进行了交流。^([1](ch07.html#ch01fn7))
- en: Static versus dynamic resource allocation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 静态与动态资源分配
- en: When you specify compute resources as command-line arguments to `spark-submit`,
    as we did earlier, you cap the limit. This means that if more resources are needed
    later as tasks queue up in the driver due to a larger than anticipated workload,
    Spark cannot accommodate or allocate extra resources.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将计算资源作为命令行参数传递给`spark-submit`时，就像我们之前所做的那样，您限制了资源上限。这意味着，如果由于比预期更大的工作负载导致任务在驱动程序中排队，那么后续可能需要更多资源，Spark将无法提供或分配额外的资源。
- en: If instead you use Spark’s [dynamic resource allocation configuration](https://oreil.ly/FX8wl),
    the Spark driver can request more or fewer compute resources as the demand of
    large workloads flows and ebbs. In scenarios where your workloads are dynamic—that
    is, they vary in their demand for compute capacity—using dynamic allocation helps
    to accommodate sudden peaks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用Spark的[动态资源分配配置](https://oreil.ly/FX8wl)，Spark驱动程序可以根据大型工作负载的需求请求更多或更少的计算资源。在您的工作负载动态变化的场景中，即它们在计算容量需求上有所变化时，使用动态分配有助于适应突发的高峰需求。
- en: One use case where this can be helpful is streaming, where the data flow volume
    may be uneven. Another is on-demand data analytics, where you might have a high
    volume of SQL queries during peak hours. Enabling dynamic resource allocation
    allows Spark to achieve better utilization of resources, freeing executors when
    not in use and acquiring new ones when needed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术可以帮助的一个用例是流式处理，在这种情况下，数据流量可能是不均匀的。另一个用例是按需数据分析，在高峰时段可能会有大量的SQL查询。启用动态资源分配允许Spark更好地利用资源，当执行器空闲时释放它们，并在需要时获取新的执行器。
- en: Note
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As well as when working with large or varying workloads, dynamic allocation
    is also useful [in a multitenant environment](https://oreil.ly/Hqtip), where Spark
    may be deployed alongside other applications or services in YARN, Mesos, or Kubernetes.
    Be advised, however, that Spark’s shifting resource demands may impact other applications
    demanding resources at the same time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大型或变化工作负载时，动态分配同样在[多租户环境](https://oreil.ly/Hqtip)中非常有用，此时Spark可能与YARN、Mesos或Kubernetes中的其他应用或服务一同部署。不过需要注意的是，Spark的资源需求变化可能会影响同时需求资源的其他应用程序。
- en: 'To enable and configure dynamic allocation, you can use settings like the following.
    Note that the numbers here are arbitrary; the appropriate settings will depend
    on the nature of your workload and they should be adjusted accordingly. Some of
    these configs cannot be set inside a Spark REPL, so you will have to set them
    programmatically:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用和配置动态分配，您可以使用以下设置。请注意，这里的数字是任意的；适当的设置取决于您的工作负载的性质，并且应相应调整。某些配置不能在Spark REPL内设置，因此您必须通过编程方式设置：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By default `spark.dynamicAllocation.enabled` is set to `false`. When enabled
    with the settings shown here, the Spark driver will request that the cluster manager
    create two executors to start with, as a minimum (`spark.dynamicAllocation.minExecutors`).
    As the task queue backlog increases, new executors will be requested each time
    the backlog timeout (`spark.dynamicAllocation.schedulerBacklogTimeout`) is exceeded.
    In this case, whenever there are pending tasks that have not been scheduled for
    over 1 minute, the driver will request that a new executor be launched to schedule
    backlogged tasks, up to a maximum of 20 (`spark.dynamicAllocation.maxExecutors`).
    By contrast, if an executor finishes a task and is idle for 2 minutes (`spark.dynamicAllocation.executorIdleTimeout`),
    the Spark driver will terminate it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`spark.dynamicAllocation.enabled`被设置为`false`。启用后，Spark驱动程序将请求集群管理器创建至少两个执行器作为起始值（`spark.dynamicAllocation.minExecutors`）。随着任务队列积压增加，每当积压超过超时时间（`spark.dynamicAllocation.schedulerBacklogTimeout`）时，将请求新的执行器。在本例中，每当有未安排超过1分钟的挂起任务时，驱动程序将请求启动新的执行器以安排积压任务，最多不超过20个（`spark.dynamicAllocation.maxExecutors`）。相反地，如果执行器完成任务并且在空闲2分钟后（`spark.dynamicAllocation.executorIdleTimeout`），Spark驱动程序将终止它。
- en: Configuring Spark executors’ memory and the shuffle service
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置Spark执行器的内存和洗牌服务
- en: Simply enabling dynamic resource allocation is not sufficient. You also have
    to understand how executor memory is laid out and used by Spark so that executors
    are not starved of memory or troubled by JVM garbage collection.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅启用动态资源分配是不够的。您还必须了解Spark如何配置和使用执行器内存，以确保执行器不会因为内存不足或JVM垃圾收集而出现问题。
- en: 'The amount of memory available to each executor is controlled by `spark.executor.memory`.
    This is divided into three sections, as depicted in [Figure 7-2](#executor_memory_layout):
    execution memory, storage memory, and reserved memory. The default division is
    60% for execution memory and 40% for storage, after allowing for 300 MB for reserved
    memory, to safeguard against OOM errors. The Spark [documentation](https://oreil.ly/ECABs)
    advises that this will work for most cases, but you can adjust what fraction of
    `spark.executor.memory` you want either section to use as a baseline. When storage
    memory is not being used, Spark can acquire it for use in execution memory for
    execution purposes, and vice versa.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 每个执行器可用的内存量由`spark.executor.memory`控制。这被分为三部分，如[图7-2](#executor_memory_layout)所示：执行内存、存储内存和保留内存。默认分配为60%用于执行内存和40%用于存储内存，并且预留300MB用于保留内存，以防止OOM错误。Spark的[文档](https://oreil.ly/ECABs)建议这适用于大多数情况，但您可以调整`spark.executor.memory`的哪一部分用作基线。当存储内存未被使用时，Spark可以获取它用于执行内存的执行目的，反之亦然。
- en: '![Executor memory layout](assets/lesp_0702.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![执行器内存布局](assets/lesp_0702.png)'
- en: Figure 7-2\. Executor memory layout
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2. 执行器内存布局
- en: Execution memory is used for Spark shuffles, joins, sorts, and aggregations.
    Since different queries may require different amounts of memory, the fraction
    (`spark.memory.fraction` is `0.6` by default) of the available memory to dedicate
    to this can be tricky to tune but it’s easy to adjust. By contrast, storage memory
    is primarily used for caching user data structures and partitions derived from
    DataFrames.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Spark洗牌、连接、排序和聚合使用执行内存。由于不同的查询可能需要不同数量的内存，因此将可用内存的一部分（`spark.memory.fraction`默认为`0.6`）用于此目的可能有些棘手，但很容易进行调整。与之相反，存储内存主要用于缓存用户数据结构和从DataFrame派生的分区。
- en: During map and shuffle operations, Spark writes to and reads from the local
    disk’s shuffle files, so there is heavy I/O activity. This can result in a bottleneck,
    because the default configurations are suboptimal for large-scale Spark jobs.
    Knowing what configurations to tweak can mitigate this risk during this phase
    of a Spark job.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在映射和洗牌操作期间，Spark会读写本地磁盘上的洗牌文件，因此会有大量的I/O活动。这可能导致瓶颈，因为默认配置对于大规模Spark作业来说并不是最优的。了解需要调整哪些配置可以在Spark作业的这个阶段缓解风险。
- en: In [Table 7-1](#spark_configurations_to_tweak_for_isolid), we capture a few
    recommended configurations to adjust so that the map, spill, and merge processes
    during these operations are not encumbered by inefficient I/O and to enable these
    operations to employ buffer memory before writing the final shuffle partitions
    to disk. [Tuning the shuffle service](https://oreil.ly/4o_pV) running on each
    executor can also aid in increasing overall performance for large Spark workloads.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表 7-1](#spark_configurations_to_tweak_for_isolid)中，我们列出了一些建议的配置，以便在这些操作期间进行的映射、溢出和合并过程不受低效的I/O影响，并在将最终的洗牌分区写入磁盘之前使用缓冲内存。调整每个执行器上运行的洗牌服务（[调整洗牌服务](https://oreil.ly/4o_pV)）也可以增强大规模Spark工作负载的整体性能。
- en: Table 7-1\. Spark configurations to tweak for I/O during map and shuffle operations
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-1\. 调整Spark在映射和洗牌操作期间的I/O的配置
- en: '| Configuration | Default value, recommendation, and description |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 配置 | 默认值、推荐值和描述 |'
- en: '| --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `spark.driver.memory` | Default is `1g` (1 GB). This is the amount of memory
    allocated to the Spark driver to receive data from executors. This is often changed
    during `spark-submit` with `--driver-memory`. Only change this if you expect the
    driver to receive large amounts of data back from operations like `collect()`,
    or if you run out of driver memory. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| `spark.driver.memory` | 默认为`1g`（1 GB）。这是分配给Spark驱动程序的内存量，用于从执行器接收数据。在`spark-submit`时使用`--driver-memory`可以更改此值。只有在预期驱动程序将从`collect()`等操作中接收大量数据，或者当驱动程序内存不足时才需要更改此值。
    |'
- en: '| `spark.shuffle.file.buffer` | Default is 32 KB. Recommended is 1 MB. This
    allows Spark to do more buffering before writing final map results to disk. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| `spark.shuffle.file.buffer` | 默认为32 KB。推荐为1 MB。这使得Spark在最终写入磁盘之前能够进行更多的缓冲。
    |'
- en: '| `spark.file.transferTo` | Default is `true`. Setting it to `false` will force
    Spark to use the file buffer to transfer files before finally writing to disk;
    this will decrease the I/O activity. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| `spark.file.transferTo` | 默认为`true`。将其设置为`false`会强制Spark在最终写入磁盘之前使用文件缓冲区传输文件，从而减少I/O活动。
    |'
- en: '| `spark.shuffle.unsafe.file.output.buffer` | Default is 32 KB. This controls
    the amount of buffering possible when merging files during shuffle operations.
    In general, large values (e.g., 1 MB) are more appropriate for larger workloads,
    whereas the default can work for smaller workloads. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| `spark.shuffle.unsafe.file.output.buffer` | 默认为32 KB。这控制了在洗牌操作期间合并文件时可能的缓冲量。一般来说，对于较大的工作负载，较大的值（例如1
    MB）更合适，而默认值适用于较小的工作负载。 |'
- en: '| `spark.io.compression.lz4.blockSize` | Default is 32 KB. Increase to 512
    KB. You can decrease the size of the shuffle file by increasing the compressed
    size of the block. |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| `spark.io.compression.lz4.blockSize` | 默认为32 KB。增加到512 KB。通过增加块的压缩大小可以减小洗牌文件的大小。
    |'
- en: '| `spark.shuffle.service.​index.cache.size` | Default is 100m. Cache entries
    are limited to the specified memory footprint in byte. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| `spark.shuffle.service.index.cache.size` | 默认为100m。缓存条目受限于指定的内存占用（以字节为单位）。
    |'
- en: '| `spark.shuffle.registration.​timeout` | Default is 5000 ms. Increase to 120000
    ms. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `spark.shuffle.registration.timeout` | 默认为5000 ms。增加到120000 ms。 |'
- en: '| `spark.shuffle.registration.maxAttempts` | Default is 3\. Increase to 5 if
    needed. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `spark.shuffle.registration.maxAttempts` | 默认为3。如有需要增加到5。 |'
- en: Note
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The recommendations in this table won’t work for all situations, but they should
    give you an idea of how to adjust these configurations based on your workload.
    Like with everything else in performance tuning, you have to experiment until
    you find the right balance.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此表中的建议并不适用于所有情况，但它们应该让您了解如何根据工作负载调整这些配置。与性能调整中的其他所有事物一样，您必须进行实验，直到找到适合的平衡点。
- en: Maximizing Spark parallelism
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大化 Spark 的并行性
- en: Much of Spark’s efficiency is due to its ability to run multiple tasks in parallel
    at scale. To understand how you can maximize parallelism—i.e., read and process
    as much data in parallel as possible—you have to look into how Spark reads data
    into memory from storage and what partitions mean to Spark.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的高效性很大程度上归因于其在规模化处理中能够并行运行多个任务的能力。要理解如何最大化并行性——即尽可能并行读取和处理数据——您必须深入了解
    Spark 如何从存储中将数据读入内存，以及分区对 Spark 的意义。
- en: In data management parlance, a partition is a way to arrange data into a subset
    of configurable and readable chunks or blocks of contiguous data on disk. These
    subsets of data can be read or processed independently and in parallel, if necessary,
    by more than a single thread in a process. This independence matters because it
    allows for massive parallelism of data processing.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据管理术语中，分区是一种将数据排列成可配置和可读块或连续数据的子集的方法。这些数据子集可以独立读取或并行处理，如果需要，可以由一个进程中的多个线程处理。这种独立性很重要，因为它允许数据处理的大规模并行性。
- en: 'Spark is embarrassingly efficient at processing its tasks in parallel. As you
    learned in [Chapter 2](ch02.html#downloading_apache_spark_and_getting_sta), for
    large-scale workloads a Spark job will have many stages, and within each stage
    there will be many tasks. Spark will at best schedule a thread per task per core,
    and each task will process a distinct partition. To optimize resource utilization
    and maximize parallelism, the ideal is at least as many partitions as there are
    cores on the executor, as depicted in [Figure 7-3](#relationship_of_spark_taskscomma_coresco).
    If there are more partitions than there are cores on each executor, all the cores
    are kept busy. You can think of partitions as atomic units of parallelism: a single
    thread running on a single core can work on a single partition.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 在并行处理任务方面表现出色。正如您在 [第二章](ch02.html#downloading_apache_spark_and_getting_sta)
    中了解到的那样，对于大规模工作负载，一个 Spark 作业将包含许多阶段，在每个阶段中将有许多任务。Spark 最多会为每个核心的每个任务安排一个线程，并且每个任务将处理一个独立的分区。为了优化资源利用和最大化并行性，理想情况是每个执行器的核心数至少与分区数相同，如
    [图 7-3](#relationship_of_spark_taskscomma_coresco) 所示。如果分区数超过每个执行器的核心数，所有核心都将保持忙碌状态。您可以将分区视为并行性的原子单位：在单个核心上运行的单个线程可以处理单个分区。
- en: '![Relationship of Spark tasks, cores, partitions, and parallelism](assets/lesp_0703.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 任务、核心、分区和并行性之间的关系](assets/lesp_0703.png)'
- en: Figure 7-3\. Relationship of Spark tasks, cores, partitions, and parallelism
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. Spark 任务、核心、分区和并行性之间的关系
- en: How partitions are created
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何创建分区
- en: As mentioned previously, Spark’s tasks process data as partitions read from
    disk into memory. Data on disk is laid out in chunks or contiguous file blocks,
    depending on the store. By default, file blocks on data stores range in size from
    64 MB to 128 MB. For example, on HDFS and S3 the default size is 128 MB (this
    is configurable). A contiguous collection of these blocks constitutes a partition.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，Spark 的任务将从磁盘读取的数据作为分区处理到内存中。磁盘上的数据根据存储的不同而以块或连续文件块的形式排列。默认情况下，数据存储上的文件块大小范围从
    64 MB 到 128 MB 不等。例如，在 HDFS 和 S3 上，默认大小为 128 MB（这是可配置的）。这些块的连续集合构成一个分区。
- en: The size of a partition in Spark is dictated by `spark.sql.files.maxPartitionBytes`.
    The default is 128 MB. You can decrease the size, but that may result in what’s
    known as the “small file problem”—many small partition files, introducing an inordinate
    amount of disk I/O and performance degradation thanks to filesystem operations
    such as opening, closing, and listing directories, which on a distributed filesystem
    can be slow.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，分区的大小由 `spark.sql.files.maxPartitionBytes` 决定，默认为 128 MB。您可以减小这个大小，但这可能会导致所谓的“小文件问题”——许多小分区文件，由于文件系统操作（如打开、关闭和列出目录）而引入大量磁盘
    I/O 和性能降低，特别是在分布式文件系统上可能会很慢。
- en: 'Partitions are also created when you explicitly use certain methods of the
    DataFrame API. For example, while creating a large DataFrame or reading a large
    file from disk, you can explicitly instruct Spark to create a certain number of
    partitions:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当你明确使用DataFrame API的一些方法时，也会创建分区。例如，在创建大型DataFrame或从磁盘读取大型文件时，可以明确地指示Spark创建某个数量的分区：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Finally, *shuffle partitions* are created during the shuffle stage. By default,
    the number of shuffle partitions is set to 200 in `spark.sql.shuffle.partitions`.
    You can adjust this number depending on the size of the data set you have, to
    reduce the amount of small partitions being sent across the network to executors’
    tasks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*shuffle partitions*是在shuffle阶段创建的。默认情况下，`spark.sql.shuffle.partitions`中shuffle
    partitions的数量设置为200。可以根据数据集的大小调整这个数字，以减少通过网络发送到执行器任务的小分区的数量。
- en: Note
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The default value for `spark.sql.shuffle.partitions` is too high for smaller
    or streaming workloads; you may want to reduce it to a lower value such as the
    number of cores on the executors or less.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.sql.shuffle.partitions`的默认值对于较小或流式工作负载来说太高；可能需要将其减少到更低的值，比如执行器核心数或更少。'
- en: Created during operations like `groupBy()` or `join()`, also known as wide transformations,
    shuffle partitions consume both network and disk I/O resources. During these operations,
    the shuffle will spill results to executors’ local disks at the location specified
    in `spark.local.directory`. Having performant SSD disks for this operation will
    boost the performance.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在`groupBy()`或`join()`等操作期间创建的*shuffle partitions*，也被称为宽转换，消耗网络和磁盘I/O资源。在这些操作期间，shuffle将结果溢出到执行器的本地磁盘，位置由`spark.local.directory`指定。对于这些操作，性能良好的SSD硬盘将提高性能。
- en: There is no magic formula for the number of shuffle partitions to set for the
    shuffle stage; the number may vary depending on your use case, data set, number
    of cores, and the amount of executor memory available—it’s a trial-and-error approach.^([2](ch07.html#ch01fn8))
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为shuffle阶段设置shuffle partitions的数量没有一个神奇的公式；这个数字可能根据你的用例、数据集、核心数量以及可用的执行器内存量而变化——这是一个试错的过程。^([2](ch07.html#ch01fn8))
- en: In addition to scaling Spark for large workloads, to boost your performance
    you’ll want to consider caching or persisting your frequently accessed DataFrames
    or tables. We explore various caching and persistence options in the next section.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为了大型工作负载扩展Spark功能外，为了提高性能，你需要考虑缓存或持久化频繁访问的DataFrame或表。我们将在下一节中探讨各种缓存和持久化选项。
- en: Caching and Persistence of Data
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的缓存和持久化
- en: What is the difference between caching and persistence? In Spark they are synonymous.
    Two API calls, `cache()` and `persist()`, offer these capabilities. The latter
    provides more control over how and where your data is stored—in memory and on
    disk, serialized and unserialized. Both contribute to better performance for frequently
    accessed DataFrames or tables.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存和持久化有什么区别？在Spark中，它们是同义词。两个API调用，`cache()`和`persist()`，提供了这些功能。后者在数据存储方面提供了更多的控制能力——可以在内存和磁盘上，序列化和非序列化存储数据。这两者都有助于提高频繁访问的DataFrame或表的性能。
- en: DataFrame.cache()
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame.cache()
- en: '`cache()` will store as many of the partitions read in memory across Spark
    executors as memory allows (see [Figure 7-2](#executor_memory_layout)). While
    a DataFrame may be fractionally cached, partitions cannot be fractionally cached
    (e.g., if you have 8 partitions but only 4.5 partitions can fit in memory, only
    4 will be cached). However, if not all your partitions are cached, when you want
    to access the data again, the partitions that are not cached will have to be recomputed,
    slowing down your Spark job.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`cache()`将尽可能多地将读取的分区存储在Spark执行器的内存中（请参见[Figure 7-2](#executor_memory_layout)）。虽然DataFrame可能只有部分缓存，但分区不能部分缓存（例如，如果有8个分区，但只有4.5个分区可以适合内存，那么只有4个分区会被缓存）。然而，如果没有缓存所有分区，当您再次访问数据时，没有被缓存的分区将需要重新计算，从而减慢Spark作业的速度。'
- en: 'Let’s look at an example of how caching a large DataFrame improves performance
    when accessing a DataFrame:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，看看当缓存一个大型DataFrame时，如何提高访问一个DataFrame的性能：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first `count()` materializes the cache, whereas the second one accesses
    the cache, resulting in a close to 12 times faster access time for this data set.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`count()`方法实例化缓存，而第二个访问缓存，导致这个数据集接近12倍的快速访问时间。
- en: Note
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When you use `cache()` or `persist()`, the DataFrame is not fully cached until
    you invoke an action that goes through every record (e.g., `count()`). If you
    use an action like `take(1)`, only one partition will be cached because Catalyst
    realizes that you do not need to compute all the partitions just to retrieve one
    record.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用`cache()`或`persist()`时，DataFrame不会完全缓存，直到调用一个通过每条记录的动作（例如`count()`）为止。如果使用像`take(1)`这样的动作，只会缓存一个分区，因为Catalyst意识到您不需要计算所有分区就可以检索一条记录。
- en: Observing how a DataFrame is stored across one executor on a local host, as
    displayed in [Figure 7-4](#cache_distributed_across_12_partitions), we can see
    they all fit in memory (recall that at a low level DataFrames are backed by RDDs).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 观察一个DataFrame如何在本地主机的一个执行器上存储，如[图 7-4](#cache_distributed_across_12_partitions)所示，我们可以看到它们都适合于内存中（记住，DataFrames在底层由RDD支持）。
- en: '![Cache distributed across 12 partitions in executor memory](assets/lesp_0704.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![缓存分布在执行器内存的12个分区](assets/lesp_0704.png)'
- en: Figure 7-4\. Cache distributed across 12 partitions in executor memory
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-4\. 缓存分布在执行器内存的12个分区
- en: DataFrame.persist()
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame.persist()
- en: '`persist(StorageLevel.*LEVEL*)` is nuanced, providing control over how your
    data is cached via [`StorageLevel`](https://oreil.ly/gz6Bb). [Table 7-2](#storagelevels)
    summarizes the different storage levels. Data on disk is always serialized using
    either [Java or Kryo serialization](https://oreil.ly/NIL6a).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`persist(StorageLevel.*LEVEL*)` 微妙地提供了对数据如何通过[`StorageLevel`](https://oreil.ly/gz6Bb)进行缓存的控制。[表 7-2](#storagelevels)总结了不同的存储级别。数据在磁盘上始终使用Java或Kryo序列化。'
- en: Table 7-2\. StorageLevels
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-2\. 存储级别
- en: '| StorageLevel | Description |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 存储级别 | 描述 |'
- en: '| --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `MEMORY_ONLY` | Data is stored directly as objects and stored only in memory.
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_ONLY` | 数据直接存储为对象并仅存储在内存中。 |'
- en: '| `MEMORY_ONLY_SER` | Data is serialized as compact byte array representation
    and stored only in memory. To use it, it has to be deserialized at a cost. |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_ONLY_SER` | 数据以紧凑的字节数组表示并仅存储在内存中。要使用它，必须进行反序列化，这会带来一定的成本。 |'
- en: '| `MEMORY_AND_DISK` | Data is stored directly as objects in memory, but if
    there’s insufficient memory the rest is serialized and stored on disk. |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_AND_DISK` | 数据直接存储为对象在内存中，但如果内存不足，则其余部分将序列化并存储在磁盘上。 |'
- en: '| `DISK_ONLY` | Data is serialized and stored on disk. |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `DISK_ONLY` | 数据进行序列化并存储在磁盘上。 |'
- en: '| `OFF_HEAP` | Data is stored off-heap. Off-heap memory is used in Spark for
    [storage and query execution](https://oreil.ly/a69L0); see [“Configuring Spark
    executors’ memory and the shuffle service”](#configuring_spark_executorsapostrophe_me).
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `OFF_HEAP` | 数据存储在堆外。在Spark中，堆外内存用于[存储和查询执行](https://oreil.ly/a69L0)，详见[“配置Spark执行器内存和洗牌服务”](#configuring_spark_executorsapostrophe_me)。
    |'
- en: '| `MEMORY_AND_DISK_SER` | Like `MEMORY_AND_DISK`, but data is serialized when
    stored in memory. (Data is always serialized when stored on disk.) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_AND_DISK_SER` | 类似于`MEMORY_AND_DISK`，但在存储在内存中时将数据序列化。（数据始终在存储在磁盘上时进行序列化。）
    |'
- en: Note
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Each `StorageLevel` (except `OFF_HEAP`) has an equivalent `LEVEL_NAME_2`*,*
    which means replicate twice on two different Spark executors: `MEMORY_ONLY_2`,
    `MEMORY_AND_DISK_SER_2`, etc. While this option is expensive, it allows data locality
    in two places, providing fault tolerance and giving Spark the option to schedule
    a task local to a copy of the data.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`StorageLevel`（除了`OFF_HEAP`）都有一个相应的`LEVEL_NAME_2`，这意味着在两个不同的Spark执行器上复制两次：`MEMORY_ONLY_2`，`MEMORY_AND_DISK_SER_2`等。虽然这种选项很昂贵，但它允许在两个位置提供数据局部性，提供容错性，并给Spark提供在数据副本处调度任务的选项。
- en: 'Let’s look at the same example as in the previous section, but using the `persist()`
    method:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看与前一节相同的例子，但使用`persist()`方法：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see from [Figure 7-5](#cache_distributed_across_12_partitions_i),
    the data is persisted on disk, not in memory. To unpersist your cached data, just
    call `DataFrame.unpersist()`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从[图 7-5](#cache_distributed_across_12_partitions_i)中看到的那样，数据存储在磁盘上，而不是内存中。要取消持久化缓存的数据，只需调用`DataFrame.unpersist()`。
- en: '![Cache distributed across 12 partitions in executor disk](assets/lesp_0705.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![缓存分布在执行器磁盘上的12个分区](assets/lesp_0705.png)'
- en: Figure 7-5\. Cache distributed across 12 partitions in executor disk
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5\. 缓存分布在执行器磁盘上的12个分区
- en: 'Finally, not only can you cache DataFrames, but you can also cache the tables
    or views derived from DataFrames. This gives them more readable names in the Spark
    UI. For example:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您不仅可以缓存DataFrames，还可以缓存由DataFrames派生的表或视图。这使它们在Spark UI中具有更可读的名称。例如：
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: When to Cache and Persist
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时缓存和持久化
- en: 'Common use cases for caching are scenarios where you will want to access a
    large data set repeatedly for queries or transformations. Some examples include:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存的常见用例是需要重复访问大型数据集以进行查询或转换的情景。一些例子包括：
- en: DataFrames commonly used during iterative machine learning training
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在迭代式机器学习训练期间常用的数据框
- en: DataFrames accessed commonly for doing frequent transformations during ETL or
    building data pipelines
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 ETL 过程中或构建数据管道期间经常访问的数据框进行频繁转换
- en: When Not to Cache and Persist
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时不要缓存和持久化
- en: 'Not all use cases dictate the need to cache. Some scenarios that may not warrant
    caching your DataFrames include:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有用例都需要缓存。一些可能不需要缓存您的数据框的情况包括：
- en: DataFrames that are too big to fit in memory
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法完全放入内存的数据框
- en: An inexpensive transformation on a DataFrame not requiring frequent use, regardless
    of size
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要频繁使用的数据框进行廉价转换，无论其大小如何
- en: As a general rule you should use memory caching judiciously, as it can incur
    resource costs in serializing and deserializing, depending on the `StorageLevel`
    used.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，您应该谨慎使用内存缓存，因为它可能会导致资源成本的增加，具体取决于使用的`StorageLevel`。
- en: Next, we’ll shift our focus to discuss a couple of common Spark join operations
    that trigger expensive movement of data, demanding compute and network resources
    from the cluster, and how we can alleviate this movement by organizing the data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转向讨论几种常见的 Spark 连接操作，这些操作会触发昂贵的数据移动，从集群中要求计算和网络资源，并且我们如何通过组织数据来减少这种移动。
- en: A Family of Spark Joins
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 连接家族
- en: 'Join operations are a common type of transformation in big data analytics in
    which two data sets, in the form of tables or DataFrames, are merged over a common
    matching key. Similar to relational databases, the Spark DataFrame and Dataset
    APIs and Spark SQL offer a series of join transformations: inner joins, outer
    joins, left joins, right joins, etc. All of these operations trigger a large amount
    of data movement across Spark executors.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据分析中，连接操作是一种常见的转换类型，其中两个数据集（以表格或数据框的形式）通过共同匹配键合并。类似于关系数据库，Spark 数据框和数据集 API
    以及 Spark SQL 提供一系列的连接转换：内连接、外连接、左连接、右连接等。所有这些操作都会触发大量数据在 Spark 执行器之间的移动。
- en: At the heart of these transformations is how Spark computes what data to produce,
    what keys and associated data to write to the disk, and how to transfer those
    keys and data to nodes as part of operations like `groupBy()`, `join()`, `agg()`,
    `sortBy()`, and `reduceByKey()`. This movement is commonly referred to as the
    *shuffle*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这些转换的核心在于 Spark 如何计算要生成的数据、写入磁盘的键和相关数据以及如何将这些键和数据作为 `groupBy()`、`join()`、`agg()`、`sortBy()`
    和 `reduceByKey()` 等操作的一部分传输到节点。这种移动通常称为*洗牌*。
- en: 'Spark has [five distinct join strategies](https://oreil.ly/q-KvH) by which
    it exchanges*,* moves, sorts, groups, and merges data across executors: the broadcast
    hash join (BHJ), shuffle hash join (SHJ), shuffle sort merge join (SMJ), broadcast
    nested loop join (BNLJ), and shuffle-and-replicated nested loop join (a.k.a. Cartesian
    product join). We’ll focus on only two of these here (BHJ and SMJ), because they’re
    the most common ones you’ll encounter.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 有 [五种不同的连接策略](https://oreil.ly/q-KvH)，通过这些策略，在执行器之间交换、移动、排序、分组和合并数据：广播哈希连接（BHJ）、洗牌哈希连接（SHJ）、洗牌排序合并连接（SMJ）、广播嵌套循环连接（BNLJ）和洗牌与复制嵌套循环连接（又称笛卡尔积连接）。我们将仅关注其中的两种（BHJ
    和 SMJ），因为它们是您最常遇到的。
- en: Broadcast Hash Join
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广播哈希连接
- en: Also known as a *map-side-only join*, the broadcast hash join is employed when
    two data sets, one small (fitting in the driver’s and executor’s memory) and another
    large enough to ideally be spared from movement, need to be joined over certain
    conditions or columns. Using a Spark [broadcast variable](https://oreil.ly/ersei),
    the smaller data set is broadcasted by the driver to all Spark executors, as shown
    in [Figure 7-6](#bhj_the_smaller_data_set_is_broadcast_to), and subsequently joined
    with the larger data set on each executor. This strategy avoids the large exchange.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为*仅地图侧连接*，广播哈希连接用于当两个数据集需要根据某些条件或列进行连接时，一个较小的数据集（适合驱动程序和执行器内存）和另一个足够大的数据集需要避免移动。使用
    Spark [广播变量](https://oreil.ly/ersei)，驱动程序将较小的数据集广播到所有 Spark 执行器，如 [图 7-6](#bhj_the_smaller_data_set_is_broadcast_to)
    所示，然后在每个执行器上与较大的数据集进行连接。该策略避免了大量的数据交换。
- en: '![BHJ: the smaller data set is broadcast to all executors](assets/lesp_0706.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![BHJ：较小的数据集广播到所有执行器](assets/lesp_0706.png)'
- en: 'Figure 7-6\. BHJ: the smaller data set is broadcast to all executors'
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. BHJ：较小的数据集广播到所有执行器
- en: By default Spark will use a broadcast join if the smaller data set is less than
    10 MB. This configuration is set in `spark.sql.autoBroadcastJoinThreshold`; you
    can decrease or increase the size depending on how much memory you have on each
    executor and in the driver. If you are confident that you have enough memory you
    can use a broadcast join with DataFrames larger than 10 MB (even up to 100 MB).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，如果较小的数据集大小小于 10 MB，Spark 将使用广播连接。此配置在 `spark.sql.autoBroadcastJoinThreshold`
    中设置；根据每个执行器和驱动程序中的内存量，您可以减少或增加大小。如果您确信您有足够的内存，即使对大于 10 MB 的 DataFrame，您也可以使用广播连接（甚至可达到
    100 MB）。
- en: 'A common use case is when you have a common set of keys between two DataFrames,
    one holding less information than the other, and you need a merged view of both.
    For example, consider a simple case where you have a large data set of soccer
    players around the world, `playersDF`, and a smaller data set of soccer clubs
    they play for, `clubsDF`, and you wish to join them over a common key:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的用例是当您有两个 DataFrame 之间的公共键集，一个持有比另一个少的信息，并且您需要一个合并视图。例如，考虑一个简单的情况，您有一个大数据集
    `playersDF` 包含全球足球运动员的信息，以及一个较小的数据集 `clubsDF` 包含他们所属的足球俱乐部的信息，您希望根据一个共同的键将它们合并：
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In this code we are forcing Spark to do a broadcast join, but it will resort
    to this type of join by default if the size of the smaller data set is below the
    `spark.sql.autoBroadcastJoinThreshold`.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们强制 Spark 进行广播连接，但如果较小的数据集大小低于 `spark.sql.autoBroadcastJoinThreshold`，它将默认采用此类连接。
- en: The BHJ is the easiest and fastest join Spark offers, since it does not involve
    any shuffle of the data set; all the data is available locally to the executor
    after a broadcast. You just have to be sure that you have enough memory both on
    the Spark driver’s and the executors’ side to hold the smaller data set in memory.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: BHJ 是 Spark 提供的最简单和最快的连接，因为它不涉及数据集的任何洗牌；所有数据在广播后都可在执行器上本地使用。您只需确保 Spark 驱动程序和执行器的内存足够大，以将较小的数据集保存在内存中。
- en: 'At any time after the operation, you can see in the physical plan what join
    operation was performed by executing:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作之后的任何时间，您可以通过执行以下操作查看物理计划执行的连接操作：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In Spark 3.0, you can use `joinedDF.explain('*mode*')` to display a readable
    and digestible output. The modes include `'simple'`, `'extended'`, `'codegen'`,
    `'cost'`, and `'formatted'`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 3.0 中，您可以使用 `joinedDF.explain('*mode*')` 来显示可读且易于理解的输出。模式包括 `'simple'`、`'extended'`、`'codegen'`、`'cost'`
    和 `'formatted'`。
- en: When to use a broadcast hash join
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时使用广播哈希连接
- en: 'Use this type of join under the following conditions for maximum benefit:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下条件下使用此类连接以获取最大效益：
- en: When each key within the smaller and larger data sets is hashed to the same
    partition by Spark
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 Spark 将较小和较大的数据集内的每个键都哈希到同一个分区时
- en: When one data set is much smaller than the other (and within the default config
    of 10 MB, or more if you have sufficient memory)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个数据集比另一个数据集小得多（并且在默认配置下小于 10 MB，如果有足够的内存则更多）
- en: When you only want to perform an equi-join, to combine two data sets based on
    matching unsorted keys
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您仅希望执行等值连接时，基于匹配的未排序键来组合两个数据集
- en: When you are not worried by excessive network bandwidth usage or OOM errors,
    because the smaller data set will be broadcast to all Spark executors
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您不担心过多的网络带宽使用或内存溢出错误，因为较小的数据集将广播到所有 Spark 执行器
- en: Specifying a value of `-1` in `spark.sql.autoBroadcastJoinThreshold` will cause
    Spark to always resort to a shuffle sort merge join, which we discuss in the next
    section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 指定在 `spark.sql.autoBroadcastJoinThreshold` 中值为 `-1` 将导致 Spark 总是采用洗牌排序合并连接，这将在下一节中讨论。
- en: Shuffle Sort Merge Join
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 洗牌排序合并连接
- en: The sort-merge algorithm is an efficient way to merge two large data sets over
    a common key that is sortable, unique, and can be assigned to or stored in the
    same partition—that is, two data sets with a common hashable key that end up being
    on the same partition. From Spark’s perspective, this means that all rows within
    each data set with the same key are hashed on the same partition on the same executor.
    Obviously, this means data has to be colocated or exchanged between executors.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 排序合并算法是合并两个大数据集的有效方法，这两个数据集具有可排序、唯一且可分配或存储在同一分区的公共键。从 Spark 的角度来看，这意味着具有相同键的每个数据集内的所有行都在相同执行器上的相同分区上进行哈希。显然，这意味着数据必须在执行器之间共享或交换。
- en: 'As the name indicates, this join scheme has two phases: a sort phase followed
    by a merge phase. The sort phase sorts each data set by its desired join key;
    the merge phase iterates over each key in the row from each data set and merges
    the rows if the two keys match.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名称所示，此连接方案有两个阶段：排序阶段和合并阶段。排序阶段根据每个数据集的所需连接键对数据集进行排序；合并阶段迭代每个数据集中的每个键，并在两个键匹配时合并行。
- en: By default, the `SortMergeJoin` is enabled via `spark.sql.join.preferSortMergeJoin`.
    Here is a code snippet from a notebook of standalone applications available for
    this chapter in the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2).
    The main idea is to take two large DataFrames, with one million records, and join
    them on two common keys, `uid == users_id`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，通过`spark.sql.join.preferSortMergeJoin`启用`SortMergeJoin`。以下是书籍的此章节中可用的独立应用程序笔记本的代码片段，位于其[GitHub存储库](https://github.com/databricks/LearningSparkV2)。主要思想是使用两个拥有一百万条记录的大型DataFrame，按两个共同的键`uid
    == users_id`进行连接。
- en: 'This data is synthetic but illustrates the point:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据为合成数据，但说明了这一点：
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Examining our final execution plan, we notice that Spark employed a `SortMergeJoin`,
    as expected, to join the two DataFrames. The `Exchange` operation is the shuffle
    of the results of the map operation on each executor:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 检查我们的最终执行计划时，我们注意到Spark使用了预期的`SortMergeJoin`来连接这两个DataFrame。如预期那样，`Exchange`操作是在每个执行器上的映射操作结果之间的洗牌：
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Furthermore, the Spark UI (which we will discuss in the next section) shows
    three stages for the entire job: the `Exchange` and `Sort` operations happen in
    the final stage, followed by merging of the results, as depicted in Figures [7-7](#before_bucketing_stages_of_the_spark)
    and [7-8](#before_bucketing_exchange_is_required). The `Exchange` is expensive
    and requires partitions to be shuffled across the network between executors.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark UI（我们将在下一节中讨论）显示整个作业的三个阶段：`Exchange`和`Sort`操作发生在最终阶段，随后合并结果，如图[7-7](#before_bucketing_stages_of_the_spark)和[7-8](#before_bucketing_exchange_is_required)所示。`Exchange`操作代价高昂，并要求分区在执行器之间通过网络进行洗牌。
- en: '![Before bucketing: stages of the Spark](assets/lesp_0707.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![桶前：Spark的阶段](assets/lesp_0707.png)'
- en: 'Figure 7-7\. Before bucketing: stages of the Spark'
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7\. 桶前：Spark的阶段
- en: '![Before bucketing: Exchange is required](assets/lesp_0708.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![桶前：需要交换](assets/lesp_0708.png)'
- en: 'Figure 7-8\. Before bucketing: Exchange is required'
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8\. 桶前：需要交换
- en: Optimizing the shuffle sort merge join
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化洗牌排序合并连接
- en: We can eliminate the `Exchange` step from this scheme if we create partitioned
    buckets for common sorted keys or columns on which we want to perform frequent
    equi-joins. That is, we can create an explicit number of buckets to store specific
    sorted columns (one key per bucket). Presorting and reorganizing data in this
    way boosts performance, as it allows us to skip the expensive `Exchange` operation
    and go straight to `WholeStageCodegen`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们为常见的排序键或要执行频繁等连接的列创建分区桶，我们可以从此方案中消除`Exchange`步骤。也就是说，我们可以创建显式数量的桶来存储特定排序列（每个桶一个键）。通过这种方式预排序和重新组织数据可提升性能，因为它允许我们跳过昂贵的`Exchange`操作并直接进入`WholeStageCodegen`。
- en: 'In the following code snippet from the notebook for this chapter (available
    in the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2)) we
    sort and bucket by the `users_id` and `uid` columns on which we’ll join, and save
    the buckets as Spark managed tables in Parquet format:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在此章节的笔记本中的以下代码片段（在书籍的[GitHub存储库](https://github.com/databricks/LearningSparkV2)中提供）中，我们按照将要连接的`users_id`和`uid`列进行排序和分桶，并将桶保存为Parquet格式的Spark托管表：
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The joined output is sorted by `uid` and `users_id`, because we saved the tables
    sorted in ascending order. As such, there’s no need to sort during the `SortMergeJoin`.
    Looking at the Spark UI ([Figure 7-9](#after_bucketing_exchange_is_not_required)),
    we can see that we skipped the `Exchange` and went straight to `WholeStageCodegen`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 连接输出按`uid`和`users_id`排序，因为我们保存的表按升序排序。因此，在`SortMergeJoin`期间不需要排序。查看Spark UI（[图 7-9](#after_bucketing_exchange_is_not_required)），我们可以看到我们跳过了`Exchange`并直接进入了`WholeStageCodegen`。
- en: 'The physical plan also shows no `Exchange` was performed, compared to the physical
    plan before bucketing:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 物理计划还显示没有执行`Exchange`操作，与桶前的物理计划相比：
- en: '[PRE17]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![After bucketing: Exchange is not required](assets/lesp_0709.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![桶后：不需要交换](assets/lesp_0709.png)'
- en: 'Figure 7-9\. After bucketing: Exchange is not required'
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9\. 桶后：不需要交换
- en: When to use a shuffle sort merge join
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时使用洗牌排序合并连接
- en: 'Use this type of join under the following conditions for maximum benefit:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下条件下使用这种类型的连接以获得最大效益：
- en: When each key within two large data sets can be sorted and hashed to the same
    partition by Spark
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 Spark 能够将两个大数据集中的每个键按排序和哈希方式分到同一分区时
- en: When you want to perform only equi-joins to combine two data sets based on matching
    sorted keys
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您希望仅执行基于匹配排序键的两个数据集的等连接以合并时
- en: When you want to prevent `Exchange` and `Sort` operations to save large shuffles
    across the network
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您希望防止 `Exchange` 和 `Sort` 操作以节省跨网络的大型洗牌时
- en: So far we have covered operational aspects related to tuning and optimizing
    Spark, and how Spark exchanges data during two common join operations. We also
    demonstrated how you can boost the performance of a shuffle sort merge join operation
    by using bucketing to avoid large exchanges of data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了与调整和优化 Spark 相关的操作方面，以及 Spark 在两种常见连接操作期间如何交换数据。我们还演示了如何通过使用分桶来避免大数据交换来提高
    shuffle sort merge join 操作的性能。
- en: As you’ve seen in the preceding figures, the Spark UI is a useful way to visualize
    these operations. It shows collected metrics and the state of the program, revealing
    a wealth of information and clues about possible performance bottlenecks. In the
    final section of this chapter, we discuss what to look for in the Spark UI.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的图表中所看到的，Spark UI 是可视化这些操作的有用方式。它显示收集的指标和程序的状态，揭示了大量关于可能性能瓶颈的信息和线索。在本章的最后一节中，我们将讨论在
    Spark UI 中寻找什么。
- en: Inspecting the Spark UI
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查 Spark UI
- en: Spark provides an elaborate web UI that allows us to inspect various components
    of our applications. It offers details on memory usage, jobs, stages, and tasks,
    as well as event timelines, logs, and various metrics and statistics that can
    give you insight into what transpires in your Spark applications, both at the
    Spark driver level and in individual executors.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了一个精心设计的 Web UI，允许我们检查应用程序的各个组件。它提供有关内存使用情况、作业、阶段和任务的详细信息，以及事件时间线、日志和各种指标和统计信息，这些可以帮助您深入了解
    Spark 应用程序在 Spark 驱动程序级别和单个执行器中的运行情况。
- en: A `spark-submit` job will launch the Spark UI, and you can connect to it on
    the local host (in local mode) or through the Spark driver (in other modes) at
    the default port 4040.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `spark-submit` 作业将启动 Spark UI，您可以在本地主机上（在本地模式下）或通过 Spark 驱动程序（在其他模式下）连接到默认端口
    4040。
- en: Journey Through the Spark UI Tabs
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 Spark UI 选项卡的旅程
- en: The Spark UI has six tabs, as shown in [Figure 7-10](#spark_ui_tabs), each providing
    opportunities for exploration. Let’s take a look at what each tab reveals to us.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Spark UI 有六个选项卡，如 [图 7-10](#spark_ui_tabs) 所示，每个选项卡都提供了探索的机会。让我们看看每个选项卡向我们揭示了什么。
- en: '![Spark UI tabs](assets/lesp_0710.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![Spark UI 选项卡](assets/lesp_0710.png)'
- en: Figure 7-10\. Spark UI tabs
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-10\. Spark UI 选项卡
- en: This discussion applies to Spark 2.x and Spark 3.0\. While much of the UI is
    the same in Spark 3.0, it also adds a seventh tab, Structured Streaming. This
    is previewed in [Chapter 12](ch12.html#epilogue_apache_spark_3dot0).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这个讨论适用于 Spark 2.x 和 Spark 3.0。虽然 Spark 3.0 中的 UI 大部分相同，但它还增加了第七个选项卡，结构化流处理。这在
    [第 12 章](ch12.html#epilogue_apache_spark_3dot0) 中进行了预览。
- en: Jobs and Stages
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作业与阶段
- en: As you learned in [Chapter 2](ch02.html#downloading_apache_spark_and_getting_sta),
    Spark breaks an application down into jobs, stages, and tasks. The Jobs and Stages
    tabs allow you to navigate through these and drill down to a granular level to
    examine the details of individual tasks. You can view their completion status
    and review metrics related to I/O, memory consumption, duration of execution,
    etc.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在 [第 2 章](ch02.html#downloading_apache_spark_and_getting_sta) 中学到的，Spark
    将应用程序分解为作业、阶段和任务。作业和阶段选项卡允许您浏览这些内容，并深入到细粒度级别以检查个别任务的详细信息。您可以查看它们的完成状态，并查看与 I/O、内存消耗、执行持续时间等相关的指标。
- en: '[Figure 7-11](#the_jobs_tab_offers_a_view_of_the_event) shows the Jobs tab
    with the expanded Event Timeline, showing when executors were added to or removed
    from the cluster. It also provides a tabular list of all completed jobs in the
    cluster. The Duration column indicates the time it took for each job (identified
    by the Job Id in the first column) to finish. If this time is high, it’s a good
    indication that you might want to investigate the stages in that job to see what
    tasks might be causing delays. From this summary page you can also access a details
    page for each job, including a DAG visualization and list of completed stages.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-11](#the_jobs_tab_offers_a_view_of_the_event) 显示了作业选项卡及其扩展的事件时间轴，显示了执行者何时添加到或从集群中移除。它还提供了集群中所有已完成作业的表格列表。持续时间列显示了每个作业完成所需的时间（由第一列中的作业
    ID 标识）。如果这段时间很长，则可能需要调查导致延迟的任务阶段。从此摘要页面，您还可以访问每个作业的详细页面，包括 DAG 可视化和已完成阶段列表。'
- en: '![The Jobs tab offers a view of the event timeline and list of all completed
    jobs](assets/lesp_0711.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![作业选项卡提供了事件时间轴视图和所有已完成作业的列表](assets/lesp_0711.png)'
- en: Figure 7-11\. The Jobs tab offers a view of the event timeline and list of all
    completed jobs
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-11\. 作业选项卡提供了事件时间轴视图和所有已完成作业的列表
- en: The Stages tab provides a summary of the current state of all stages of all
    jobs in the application. You can also access a details page for each stage, providing
    a DAG and metrics on its tasks ([Figure 7-12](#the_stages_tab_provides_details_on_stage)).
    As well as some other optional statistics, you can see the average duration of
    each task, time spent in garbage collection (GC), and number of shuffle bytes/records
    read. If shuffle data is being read from remote executors, a high Shuffle Read
    Blocked Time can signal I/O issues. A high GC time signals too many objects on
    the heap (your executors may be memory-starved). If a stage’s max task time is
    much larger than the median, then you probably have data skew caused by uneven
    data distribution in your partitions. Look for these tell-tale signs.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段选项卡提供了应用程序中所有作业所有阶段当前状态的摘要。您还可以访问每个阶段的详细页面，其中包含 DAG 和任务的指标（[图 7-12](#the_stages_tab_provides_details_on_stage)）。除了一些可选的统计信息外，您还可以查看每个任务的平均持续时间、GC
    花费的时间以及洗牌字节/记录读取的数量。如果从远程执行者读取洗牌数据，则高 Shuffle Read Blocked Time 可能会提示 I/O 问题。高
    GC 时间表明堆上有太多对象（可能是内存不足的执行者）。如果一个阶段的最大任务时间远大于中位数，则可能存在数据分区不均匀导致的数据倾斜问题。请留意这些显著迹象。
- en: '![The Stages tab provides details on stages and their tasks](assets/lesp_0712.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![阶段选项卡提供了有关阶段及其任务的详细信息](assets/lesp_0712.png)'
- en: Figure 7-12\. The Stages tab provides details on stages and their tasks
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-12\. 阶段选项卡提供了有关阶段及其任务的详细信息
- en: You can also see aggregated metrics for each executor and a breakdown of the
    individual tasks on this page.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以查看每个执行者的聚合指标以及此页面上各个任务的详细信息。
- en: Executors
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行者
- en: The Executors tab provides information on the executors created for the application.
    As you can see in [Figure 7-13](#the_executors_tab_shows_granular_statist), you
    can drill down into the minutiae of details about resource usage (disk, memory,
    cores), time spent in GC, amount of data written and read during shuffle, etc.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 执行者选项卡提供了有关为应用程序创建的执行者的信息。正如您在 [图 7-13](#the_executors_tab_shows_granular_statist)
    中所看到的，您可以深入了解有关资源使用情况（磁盘、内存、核心）、GC 时间、洗牌期间写入和读取的数据量等细节。
- en: '![The Executors tab shows granular statistics and metrics on the executors
    used by your Spark application](assets/lesp_0713.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![执行者选项卡显示了 Spark 应用程序使用的执行者的详细统计数据和指标](assets/lesp_0713.png)'
- en: Figure 7-13\. The Executors tab shows granular statistics and metrics on the
    executors used by your Spark application
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-13\. 执行者选项卡显示了 Spark 应用程序使用的执行者的详细统计数据和指标
- en: In addition to the summary statistics, you can view how memory is used by each
    individual executor, and for what purpose. This also helps to examine resource
    usage when you have used the `cache()` or `persist()` method on a DataFrame or
    managed table, which we discuss next.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 除了摘要统计信息外，您还可以查看每个单独执行者的内存使用情况及其用途。当您在 DataFrame 或管理表上使用 `cache()` 或 `persist()`
    方法时，这也有助于检查资源使用情况，接下来我们将讨论这些。
- en: Storage
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储
- en: In the Spark code in “Shuffle Sort Merge Join” we cached two managed tables
    after bucketing. The Storage tab, shown in [Figure 7-14](#the_storage_tab_shows_details_on_memory),
    provides information on any tables or DataFrames cached by the application as
    a result of the `cache()` or `persist()` method.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在“Shuffle Sort Merge Join”中的 Spark 代码中，我们对桶分区后管理了两个表的缓存。在[图 7-14](#the_storage_tab_shows_details_on_memory)中显示的存储选项卡提供了有关该应用程序缓存的任何表或
    DataFrame 的信息，这是由`cache()`或`persist()`方法的结果产生的。
- en: '![The Storage tab shows details on memory usage](assets/lesp_0714.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![存储选项卡显示内存使用情况的详细信息](assets/lesp_0714.png)'
- en: Figure 7-14\. The Storage tab shows details on memory usage
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-14\. 存储选项卡显示内存使用情况的详细信息
- en: Going a bit further by clicking on the link “In-memory table `UsersTbl`” in
    [Figure 7-14](#the_storage_tab_shows_details_on_memory) displays how the table
    is cached in memory and on disk across 1 executor and 8 partitions—this number
    corresponds to the number of buckets we created for this table (see [Figure 7-15](#spark_ui_showing_cache_table_distributio)).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 点击链接“内存表`UsersTbl`”在[图 7-14](#the_storage_tab_shows_details_on_memory)中，可以进一步了解表在内存和磁盘上的缓存情况，以及在1个执行器和8个分区上的分布情况，这个数字对应我们为这个表创建的桶的数量（参见[图 7-15](#spark_ui_showing_cache_table_distributio)）。
- en: '![Spark UI showing cache table distribution across executor memory](assets/lesp_0715.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![Spark UI显示表在执行器内存中的缓存分布](assets/lesp_0715.png)'
- en: Figure 7-15\. Spark UI showing cached table distribution across executor memory
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-15\. Spark UI显示表在执行器内存中的缓存分布
- en: SQL
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SQL
- en: The effects of Spark SQL queries that are executed as part of your Spark application
    are traceable and viewable through the SQL tab. You can see when the queries were
    executed and by which jobs, and their duration. For example, in our `SortMergeJoin`
    example we executed some queries; all of them are displayed in [Figure 7-16](#the_sql_tab_shows_details_on_the_complet),
    with links to drill further down.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 作为您的 Spark 应用程序执行的一部分执行的 Spark SQL 查询的影响可以通过 SQL 选项卡进行跟踪和查看。您可以看到查询何时执行以及由哪个作业执行，以及它们的持续时间。例如，在我们的`SortMergeJoin`示例中，我们执行了一些查询；所有这些都显示在[图 7-16](#the_sql_tab_shows_details_on_the_complet)中，并附带了进一步深入了解的链接。
- en: '![The SQL tab shows details on the completed SQL queries](assets/lesp_0716.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![SQL选项卡显示已完成的SQL查询的详细信息](assets/lesp_0716.png)'
- en: Figure 7-16\. The SQL tab shows details on the completed SQL queries
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-16\. SQL 选项卡显示已完成的SQL查询的详细信息
- en: Clicking on the description of a query displays details of the execution plan
    with all the physical operators, as shown in [Figure 7-17](#spark_ui_showing_detailed_statistics_on).
    Under each physical operator of the plan—here, `Scan In-memory table`, `HashAggregate`,
    and `Exchange`—are SQL metrics.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 点击查询的描述会显示执行计划的细节，包括所有物理运算符，如[图 7-17](#spark_ui_showing_detailed_statistics_on)所示。在计划的每个物理运算符下面—例如，在此处`扫描内存表`，`哈希聚合`和`Exchange`—都有
    SQL 指标。
- en: 'These metrics are useful when we want to inspect the details of a physical
    operator and discover what transpired: how many rows were scanned, how many shuffle
    bytes were written, etc.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要检查物理运算符的细节并了解发生了什么时，这些指标就非常有用：有多少行被扫描了，写了多少洗牌字节等等。
- en: '![Spark UI showing detailed statistics on a SQL query](assets/lesp_0717.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![Spark UI显示SQL查询的详细统计信息](assets/lesp_0717.png)'
- en: Figure 7-17\. Spark UI showing detailed statistics on a SQL query
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-17\. Spark UI显示SQL查询的详细统计信息
- en: Environment
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境
- en: The Environment tab, shown in [Figure 7-18](#the_environment_tab_shows_the_runtime_pr),
    is just as important as the others. Knowing about the environment in which your
    Spark application is running reveals many clues that are useful for troubleshooting.
    In fact, it’s imperative to know what environment variables are set, what jars
    are included, what Spark properties are set (and their respective values, especially
    if you tweaked some of the configs mentioned in [“Optimizing and Tuning Spark
    for Efficiency”](#optimizing_and_tuning_spark_for_efficien)), what system properties
    are set, what runtime environment (such as JVM or Java version) is used, etc.
    All these read-only details are a gold mine of information supplementing your
    investigative efforts should you notice any abnormal behavior in your Spark application.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他选项卡一样，[图 7-18](#the_environment_tab_shows_the_runtime_pr)中显示的环境选项卡同样重要。了解您的
    Spark 应用程序所在的环境可以揭示许多有用于故障排除的线索。事实上，了解已设置了哪些环境变量，包括了哪些 jar 包，设置了哪些 Spark 属性（及其相应的值，特别是如果您调整了[“优化和调整
    Spark 的效率”](#optimizing_and_tuning_spark_for_efficien)中提到的一些配置），设置了哪些系统属性，使用了哪个运行时环境（如
    JVM 或 Java 版本）等等对于您在 Spark 应用程序中注意到任何异常行为时的调查工作非常有帮助。
- en: '![The Environment tab shows the runtime properties of your Spark cluster](assets/lesp_0718.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![环境标签显示了你的 Spark 集群的运行时属性](assets/lesp_0718.png)'
- en: Figure 7-18\. The Environment tab shows the runtime properties of your Spark
    cluster
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-18\. 环境标签显示了你的 Spark 集群的运行时属性。
- en: Debugging Spark applications
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调试 Spark 应用程序
- en: In this section, we have navigated through the various tabs in the Spark UI.
    As you’ve seen, the UI provides a wealth of information that you can use for debugging
    and troubleshooting issues with your Spark applications. In addition to what we’ve
    covered here, it also provides access to both driver and executor stdout/stderr
    logs, where you might have logged debugging information.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经浏览了 Spark UI 中的各种标签。正如你所见，UI 提供了大量信息，可用于调试和解决 Spark 应用程序的问题。除了我们在这里涵盖的内容之外，它还提供了访问驱动程序和执行器的
    stdout/stderr 日志的方式，你可以在这里记录调试信息。
- en: Debugging through the UI is a different process than stepping through an application
    in your favorite IDE—more like sleuthing, following trails of bread crumbs—though
    if you prefer that approach, you can also debug a Spark application in an IDE
    such as [IntelliJ IDEA](https://oreil.ly/HkbIv) on a local host.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 UI 进行调试是与在你喜欢的 IDE 中逐步执行应用程序不同的过程 —— 更像是侦探，追随面包屑的线索 —— 虽然如果你喜欢那种方法，你也可以在本地主机上的
    IDE（如 [IntelliJ IDEA](https://oreil.ly/HkbIv)）中调试 Spark 应用程序。
- en: The [Spark 3.0 UI tabs](https://oreil.ly/3X46q) reveal insightful bread crumbs
    about what happened, along with access to both driver and executor stdout/stderr
    logs, where you might have logged debugging information.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[Spark 3.0 UI 标签](https://oreil.ly/3X46q)展示了关于发生情况的见解性线索，以及访问驱动程序和执行器的 stdout/stderr
    日志，你可能在这里记录了调试信息。'
- en: Initially, this plethora of information can be overwhelming to a novice. But
    with time you’ll gain an understanding of what to look for in each tab, and you’ll
    begin to be able to detect and diagnose anomalies more quickly. Patterns will
    become clear, and by frequently visiting these tabs and getting familiar with
    them after running some Spark examples, you’ll get accustomed to tuning and inspecting
    your Spark applications via the UI.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，这些大量的信息对新手来说可能是压倒性的。但随着时间的推移，你会逐渐理解每个标签中要寻找的内容，并且能够更快地检测和诊断异常。模式会变得清晰，通过频繁访问这些标签并在运行一些
    Spark 示例后熟悉它们，你会习惯通过 UI 调优和检查你的 Spark 应用程序。
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we have discussed a number of optimization techniques for tuning
    your Spark applications. As you saw, by adjusting some of the default Spark configurations,
    you can improve scaling for large workloads, enhance parallelism, and minimize
    memory starvation among Spark executors. You also got a glimpse of how you can
    use caching and persisting strategies with appropriate levels to expedite access
    to your frequently used data sets, and we examined two commonly used joins Spark
    employs during complex aggregations and demonstrated how by bucketing DataFrames
    by sorted keys, you can skip over expensive shuffle operations.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了多种优化技术，用于调优你的 Spark 应用程序。正如你所见，通过调整一些默认的 Spark 配置，你可以改善大型工作负载的扩展性，增强并行性，并减少
    Spark 执行器之间的内存饥饿。你还一瞥了如何使用缓存和持久化策略以适当的级别加速访问你经常使用的数据集，并且我们检查了 Spark 在复杂聚合过程中使用的两种常见连接方式，并演示了通过按排序键分桶
    DataFrame 如何跳过昂贵的洗牌操作。
- en: Finally, to get a visual perspective on performance, the Spark UI completed
    the picture. Informative and detailed though the UI is, it’s not equivalent to
    step-debugging in an IDE; yet we showed how you can become a Spark sleuth by examining
    and gleaning insights from the metrics and statistics, compute and memory usage
    data, and SQL query execution traces available on the half-dozen Spark UI tabs.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过 Spark UI，你可以从视觉角度看到性能的情况完整呈现出来。尽管 UI 提供了详细和丰富的信息，但它并不等同于在 IDE 中逐步调试；然而我们展示了如何通过检查和从半打
    Spark UI 标签中获得的度量和统计信息、计算和内存使用数据以及 SQL 查询执行跟踪来成为 Spark 的侦探。
- en: In the next chapter, we’ll dive into Structured Streaming and show you how the
    Structured APIs that you learned about in earlier chapters allow you to write
    both streaming and batch applications in a continuous manner, enabling you to
    build reliable data lakes and pipelines.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入讲解结构化流式处理，并向你展示在前几章学习过的结构化 API 如何让你连续编写流式和批处理应用程序，使你能够构建可靠的数据湖和管道。
- en: ^([1](ch07.html#ch01fn7-marker)) See [“Tuning Apache Spark for Large Scale Workloads”](https://oreil.ly/cT8Az)
    and [“Hive Bucketing in Apache Spark”](https://oreil.ly/S2hTU).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.html#ch01fn7-marker)) 参见 [“调整 Apache Spark 以应对大规模工作负载”](https://oreil.ly/cT8Az)
    和 [“Apache Spark 中的 Hive 分桶”](https://oreil.ly/S2hTU)。
- en: ^([2](ch07.html#ch01fn8-marker)) For some tips on configuring shuffle partitions,
    see [“Tuning Apache Spark for Large Scale Workloads”](https://oreil.ly/QpVyf),
    [“Hive Bucketing in Apache Spark”](https://oreil.ly/RmiTd), and [“Why You Should
    Care about Data Layout in the Filesystem”](https://oreil.ly/RQQFf).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch07.html#ch01fn8-marker)) 想要了解一些有关配置洗牌分区的技巧，请参见 [“调整 Apache Spark 以应对大规模工作负载”](https://oreil.ly/QpVyf)，[“Apache
    Spark 中的 Hive 分桶”](https://oreil.ly/RmiTd)，以及 [“为什么你应该关注文件系统中的数据布局”](https://oreil.ly/RQQFf)。
