- en: 11  Constant Model, Loss, and Transformations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11  恒定模型、损失和转换
- en: 原文：[https://ds100.org/course-notes/constant_model_loss_transformations/loss_transformations.html](https://ds100.org/course-notes/constant_model_loss_transformations/loss_transformations.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://ds100.org/course-notes/constant_model_loss_transformations/loss_transformations.html](https://ds100.org/course-notes/constant_model_loss_transformations/loss_transformations.html)
- en: '*Learning Outcomes* ***   Derive the optimal model parameters for the constant
    model under MSE and MAE cost functions.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习成果* ***   推导出在MSE和MAE成本函数下恒定模型的最佳模型参数。'
- en: Evaluate the differences between MSE and MAE risk.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估MSE和MAE风险之间的差异。
- en: 'Understand the need for linearization of variables and apply the Tukey-Mosteller
    bulge diagram for transformations.**  **Last time, we introduced the modeling
    process. We set up a framework to predict target variables as functions of our
    features, following a set workflow:'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解变量线性化的必要性，并应用图基-莫斯特勒凸图进行转换。**  **上次，我们介绍了建模过程。我们建立了一个框架，根据一套工作流程，预测目标变量作为我们特征的函数：
- en: Choose a model - how should we represent the world?
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择模型 - 我们应该如何表示世界？
- en: Choose a loss function - how do we quantify prediction error?
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择损失函数 - 我们如何量化预测误差？
- en: Fit the model - how do we choose the best parameter of our model given our data?
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型 - 我们如何根据我们的数据选择最佳模型参数？
- en: Evaluate model performance - how do we evaluate whether this process gave rise
    to a good model?
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型性能 - 我们如何评估这个过程是否产生了一个好模型？
- en: 'To illustrate this process, we derived the optimal model parameters under simple
    linear regression (SLR) with mean squared error (MSE) as the cost function. A
    summary of the SLR modeling process is shown below:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个过程，我们推导了简单线性回归（SLR）下均方误差（MSE）作为成本函数的最佳模型参数。SLR建模过程的摘要如下所示：
- en: '![error](../Images/79a6ff489df4dc5d865a1eb74d7ca2fd.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![error](../Images/79a6ff489df4dc5d865a1eb74d7ca2fd.png)'
- en: In this lecture, we’ll dive deeper into step 4 - evaluating model performance
    - using SLR as an example. Additionally, we’ll also explore the modeling process
    with new models, continue familiarizing ourselves with the modeling process by
    finding the best model parameters under a new model, the constant model, and test
    out two different loss functions to understand how our choice of loss influences
    model design. Later on, we’ll consider what happens when a linear model isn’t
    the best choice to capture trends in our data and what solutions there are to
    create better models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本讲座中，我们将深入探讨步骤4 - 评估模型性能 - 以SLR为例。此外，我们还将通过新模型探索建模过程，继续通过在新模型下找到最佳模型参数来熟悉建模过程，并测试两种不同的损失函数，以了解我们选择的损失如何影响模型设计。稍后，我们将考虑当线性模型不是捕捉数据趋势的最佳选择时会发生什么，以及有哪些解决方案可以创建更好的模型。
- en: '11.1 Step 4: Evaluating the SLR Model'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 步骤4：评估SLR模型
- en: 'Now that we’ve explored the mathematics behind (1) choosing a model, (2) choosing
    a loss function, and (3) fitting the model, we’re left with one final question
    – how “good” are the predictions made by this “best” fitted model? To determine
    this, we can:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了（1）选择模型、（2）选择损失函数和（3）拟合模型背后的数学原理，我们还剩下一个最后的问题 - 这个“最佳”拟合模型的预测有多“好”？为了确定这一点，我们可以：
- en: 'Visualize data and compute statistics:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化数据并计算统计数据：
- en: Plot the original data.
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制原始数据。
- en: Compute each column’s mean and standard deviation. If the mean and standard
    deviation of our predictions are close to those of the original observed \(y_i\)s,
    we might be inclined to say that our model has done well.
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每一列的均值和标准差。如果我们的预测的均值和标准差接近于原始观察到的\(y_i\)，我们可能会倾向于说我们的模型做得不错。
- en: (If we’re fitting a linear model) compute the correlation \(r\). A large magnitude
    for the correlation coefficient between the feature and response variables could
    also indicate that our model has done well.
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: （如果我们拟合线性模型）计算相关性\(r\)。特征和响应变量之间的相关系数的大幅度也可能表明我们的模型做得不错。
- en: 'Performance metrics:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 性能指标：
- en: We can take the **Root Mean Squared Error (RMSE)**.
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以采用**均方根误差（RMSE）**。
- en: It’s the square root of the mean squared error (MSE), which is the average loss
    that we’ve been minimizing to determine optimal model parameters.
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是均方误差（MSE）的平方根，它是我们一直在最小化以确定最佳模型参数的平均损失。
- en: RMSE is in the same units as \(y\).
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSE与\(y\)的单位相同。
- en: A lower RMSE indicates more “accurate” predictions, as we have a lower “average
    loss” across the data.
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较低的RMSE表示更“准确”的预测，因为我们在数据中有更低的“平均损失”。
- en: \[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}\]
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}\]
- en: 'Visualization:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化：
- en: Look at the residual plot of \(e_i = y_i - \hat{y_i}\) to visualize the difference
    between actual and predicted values. The good residual plot should not show any
    pattern between input/features \(x_i\) and residual values \(e_i\).
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看\(e_i = y_i - \hat{y_i}\)的残差图，以可视化实际值和预测值之间的差异。良好的残差图不应显示输入/特征\(x_i\)和残差值\(e_i\)之间的任何模式。
- en: To illustrate this process, let’s take a look at **Anscombe’s quartet**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个过程，让我们看看**安斯库姆的四重奏**。
- en: 11.1.1 Four Mysterious Datasets (Anscombe’s quartet)
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 四个神秘的数据集（安斯库姆的四重奏）
- en: Let’s take a look at four different datasets.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看四个不同的数据集。
- en: <details><summary>Code</summary>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE0]</details> <details><summary>Code</summary>'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE0]</details> <details><summary>代码</summary>'
- en: '[PRE1]</details> <details><summary>Code</summary>'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1]</details> <details><summary>代码</summary>'
- en: '[PRE2]</details> <details><summary>Code</summary>'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE2]</details> <details><summary>代码</summary>'
- en: '[PRE3]</details>'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]</details>'
- en: '![](../Images/58bbf11fc68de63b93c59eb24b0be4a3.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58bbf11fc68de63b93c59eb24b0be4a3.png)'
- en: While these four sets of datapoints look very different, they actually all have
    identical \(\bar x\), \(\bar y\), \(\sigma_x\), \(\sigma_y\), correlation \(r\),
    and RMSE! If we only look at these statistics, we would probably be inclined to
    say that these datasets are similar.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这四组数据点看起来非常不同，但它们实际上都具有相同的\(\bar x\)、\(\bar y\)、\(\sigma_x\)、\(\sigma_y\)、相关性\(r\)和RMSE！如果我们只看这些统计数据，我们可能会倾向于说这些数据集是相似的。
- en: <details><summary>Code</summary>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE4]</details>'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE4]</details>'
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We may also wish to visualize the model’s **residuals**, defined as the difference
    between the observed and predicted \(y_i\) value (\(e_i = y_i - \hat{y}_i\)).
    This gives a high-level view of how “off” each prediction is from the true observed
    value. Recall that you explored this concept in [Data 8](https://inferentialthinking.com/chapters/15/5/Visual_Diagnostics.html?highlight=heteroscedasticity#detecting-heteroscedasticity):
    a good regression fit should display no clear pattern in its plot of residuals.
    The residual plots for Anscombe’s quartet are displayed below. Note how only the
    first plot shows no clear pattern to the magnitude of residuals. This is an indication
    that SLR is not the best choice of model for the remaining three sets of points.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能还希望可视化模型的**残差**，定义为观察值和预测的\(y_i\)值之间的差异（\(e_i = y_i - \hat{y}_i\)）。这提供了每个预测与真实观察值的“偏差”的高层视图。回想一下，你在[Data
    8](https://inferentialthinking.com/chapters/15/5/Visual_Diagnostics.html?highlight=heteroscedasticity#detecting-heteroscedasticity)中探讨过这个概念：一个好的回归拟合在其残差图中不应显示出明显的模式。Anscombe的四重奏的残差图如下所示。请注意，只有第一个图显示出残差大小没有明显模式。这表明SLR不是剩下的三组点的最佳模型的指示。
- en: <details><summary>Code</summary>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE6]</details>'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE6]</details>'
- en: '![](../Images/b741f6b981adc05a9800d5f93e6af038.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b741f6b981adc05a9800d5f93e6af038.png)'
- en: 11.1.2 Prediction vs. Estimation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.2 预测 vs. 估计
- en: The terms prediction and estimation are often used somewhat interchangeably,
    but there is a subtle difference between them. **Estimation** is the task of using
    data to calculate model parameters. **Prediction** is the task of using a model
    to predict outputs for unseen data. In our simple linear regression model
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 术语预测和估计通常在某种程度上可以互换使用，但它们之间有微妙的区别。**估计**是使用数据计算模型参数的任务。**预测**是使用模型预测未见数据的输出的任务。在我们的简单线性回归模型中
- en: \[\hat{y} = \hat{\theta_0} + \hat{\theta_1}\]
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat{y} = \hat{\theta_0} + \hat{\theta_1}\]
- en: we **estimate** the parameters by minimizing average loss; then, we **predict**
    using these estimations. **Least Squares Estimation** is when we choose the parameters
    that minimize MSE.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过最小化平均损失来**估计**参数；然后，我们使用这些估计来**预测**。**最小二乘估计**是选择最小化MSE的参数。
- en: 11.2 Constant Model + MSE
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 常数模型 + MSE
- en: Now, we’ll shift from the SLR model to the **constant model**, also known as
    a summary statistic. The constant model is slightly different from the simple
    linear regression model we’ve explored previously. Rather than generating predictions
    from an inputted feature variable, the constant model always *predicts the same
    constant number*. This ignores any relationships between variables. For example,
    let’s say we want to predict the number of drinks a boba shop sells in a day.
    Boba tea sales likely depend on the time of year, the weather, how the customers
    feel, whether school is in session, etc., but the constant model ignores these
    factors in favor of a simpler model. In other words, the constant model employs
    a **simplifying assumption**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将从SLR模型转换为**常数模型**，也称为汇总统计。常数模型与我们之前探索过的简单线性回归模型略有不同。常数模型不是从输入的特征变量生成预测，而是始终*预测相同的常数数字*。这忽略了变量之间的任何关系。例如，假设我们想要预测一家波霸店一天卖出的饮料数量。波霸茶的销售可能取决于一年中的时间、天气、顾客的感觉、学校是否在上课等等，但常数模型忽略了这些因素，而更倾向于一个更简单的模型。换句话说，常数模型采用了一个**简化的假设**。
- en: 'It is also a parametric, statistical model:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 它也是一个参数化的统计模型：
- en: \[\hat{y}_i = \theta_0\]
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat{y}_i = \theta_0\]
- en: \(\theta_0\) is the parameter of the constant model, just as \(\theta_0\) and
    \(\theta_1\) were the parameters in SLR. Since our parameter \(\theta_0\) is 1-dimensional
    (\(\theta_0 \in \mathbb{R}\)), we now have no input to our model and will always
    predict \(\hat{y}_i = \theta_0\).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \(\theta_0\)是常数模型的参数，就像\(\theta_0\)和\(\theta_1\)是SLR中的参数一样。由于我们的参数\(\theta_0\)是一维的（\(\theta_0
    \in \mathbb{R}\)），我们现在的模型没有输入，将始终预测\(\hat{y}_i = \theta_0\)。
- en: 11.2.1 Deriving the optimal \(\theta_0\)
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 推导最优的\(\theta_0\)
- en: Our task now is to determine what value of \(\theta_0\) best represents the
    optimal model – in other words, what number should we guess each time to have
    the lowest possible **average loss** on our data?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的任务是确定什么值的\(\theta_0\)最能代表最佳模型 - 换句话说，每次猜测什么数字可以在我们的数据上获得最低可能的**平均损失**？
- en: Like before, we’ll use Mean Squared Error (MSE). Recall that the MSE is average
    squared loss (L2 loss) over the data \(D = \{y_1, y_2, ..., y_n\}\).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 像以前一样，我们将使用均方误差（MSE）。回想一下，MSE是数据\(D = \{y_1, y_2, ..., y_n\}\)上的平均平方损失（L2损失）。
- en: \[R(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \hat{y_i})^2 \]
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: \[R(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \hat{y_i})^2 \]
- en: 'Our modeling process now looks like this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的建模过程现在看起来像这样：
- en: 'Choose a model: constant model'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择模型：常数模型
- en: 'Choose a loss function: L2 loss'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择损失函数：L2损失
- en: Fit the model
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型
- en: Evaluate model performance
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型性能
- en: Given the **constant model** \(\hat{y}_i = \theta_0\), we can rewrite the MSE
    equation as
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 给定**常数模型**\(\hat{y}_i = \theta_0\)，我们可以将MSE方程重写为
- en: \[R(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2 \]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \[R(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2 \]
- en: We can fit **the model** by finding the optimal \(\theta_0\) that minimizes
    the MSE using a calculus approach.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过找到最优的\(\theta_0\)来拟合**模型**，从而最小化MSE，使用微积分方法。
- en: Differentiate with respect to \(\theta_0\)
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对\(\theta_0\)求导
- en: \[ \begin{align} \frac{d}{d\theta_0}\text{R}(\theta) & = \frac{d}{d\theta_0}\frac{1}{n}\sum^{n}_{i=1}
    (y_i - \theta_0)^2 \\ &= {n}\sum^{n}_{i=1} \frac{d}{d\theta_0} (y_i - \theta_0)^2
    \quad \quad \text{derivative of sum is a sum of derivatives} \\ &= {n}\sum^{n}_{i=1}
    2 (y_i - \theta_0) (-1) \quad \quad \text{chain rule} \\ &= {\frac{-2}{n}}\sum^{n}_{i=1}
    (y_i - \theta_0) \quad \quad \text{simply constants} \end{align} \]
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align} \frac{d}{d\theta_0}\text{R}(\theta) & = \frac{d}{d\theta_0}\frac{1}{n}\sum^{n}_{i=1}
    (y_i - \theta_0)^2 \\ &= {n}\sum^{n}_{i=1} \frac{d}{d\theta_0} (y_i - \theta_0)^2
    \quad \quad \text{求和的导数是导数的和} \\ &= {n}\sum^{n}_{i=1} 2 (y_i - \theta_0) (-1)
    \quad \quad \text{链式法则} \\ &= {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \theta_0) \quad
    \quad \text{简单的常数} \end{align} \]
- en: Set equal to 0 \[ 0 = {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \theta_0) \]
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等于0 \[ 0 = {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \theta_0) \]
- en: Solve for \(\theta_0\)
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解出 \(\theta_0\)
- en: \[ \begin{align} 0 &= {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \theta_0) \\ &= \sum^{n}_{i=1}
    (y_i - \theta_0) \quad \quad \text{divide both sides by} \frac{-2}{n} \\ &= \sum^{n}_{i=1}
    y_i - \sum^{n}_{i=1} \theta_0 \quad \quad \text{separate sums} \\ &= \sum^{n}_{i=1}
    y_i - n * \theta_0 \quad \quad \text{c + c + … + c = nc} \\ n * \theta_0 &= \sum^{n}_{i=1}
    y_i \\ \theta_0 &= \frac{1}{n} \sum^{n}_{i=1} y_i \\ \theta_0 &= \bar{y} \end{align}
    \]
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align} 0 &= {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \theta_0) \\ &= \sum^{n}_{i=1}
    (y_i - \theta_0) \quad \quad \text{两边同时除以} \frac{-2}{n} \\ &= \sum^{n}_{i=1} y_i
    - \sum^{n}_{i=1} \theta_0 \quad \quad \text{分开求和} \\ &= \sum^{n}_{i=1} y_i - n
    * \theta_0 \quad \quad \text{c + c + … + c = nc} \\ n * \theta_0 &= \sum^{n}_{i=1}
    y_i \\ \theta_0 &= \frac{1}{n} \sum^{n}_{i=1} y_i \\ \theta_0 &= \bar{y} \end{align}
    \]
- en: Let’s take a moment to interpret this result. \(\hat{\theta} = \bar{y}\) is
    the optimal parameter for constant model + MSE. It holds true regardless of what
    data sample you have, and it provides some formal reasoning as to why the mean
    is such a common summary statistic.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间解释一下这个结果。 \(\hat{\theta} = \bar{y}\) 是常数模型 + MSE 的最佳参数。无论你有什么样的数据样本，它都是成立的，并且它提供了一些正式的推理，解释了为什么均值是如此常见的摘要统计量。
- en: 'Our optimal model parameter is the value of the parameter that minimizes the
    cost function. This minimum value of the cost function can be expressed:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最佳模型参数是使成本函数最小化的参数值。成本函数的最小值可以表示为：
- en: \[R(\hat{\theta}) = \min_{\theta} R(\theta)\]
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: \[R(\hat{\theta}) = \min_{\theta} R(\theta)\]
- en: 'To restate the above in plain English: we are looking at the value of the cost
    function when it takes the best parameter as input. This optimal model parameter,
    \(\hat{\theta}\), is the value of \(\theta\) that minimizes the cost \(R\).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 用简单的英语重新陈述上面的内容：当成本函数以最佳参数作为输入时，我们正在查看成本函数的值。这个最佳模型参数 \(\hat{\theta}\) 是使成本
    \(R\) 最小化的 \(\theta\) 的值。
- en: 'For modeling purposes, we care less about the minimum value of cost, \(R(\hat{\theta})\),
    and more about the *value of \(\theta\)* that results in this lowest average loss.
    In other words, we concern ourselves with finding the best parameter value such
    that:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于建模目的，我们更关心成本的最小值 \(R(\hat{\theta})\)，而不是导致这种最低平均损失的 * \(\theta\) 的值。换句话说，我们关心找到最佳参数值，使得：
- en: \[\hat{\theta} = \underset{\theta}{\operatorname{\arg\min}}\:R(\theta)\]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat{\theta} = \underset{\theta}{\operatorname{\arg\min}}\:R(\theta)\]
- en: That is, we want to find the **arg**ument \(\theta\) that **min**imizes the
    cost function.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们想要找到使成本函数最小化的参数 \(\theta\)。
- en: 11.2.2 Comparing Two Different Models, Both Fit with MSE
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.2 比较两个不同的模型，都使用MSE进行拟合
- en: 'Now that we’ve explored the constant model with an L2 loss, we can compare
    it to the SLR model that we learned last lecture. Consider the dataset below,
    which contains information about the ages and lengths of dugongs. Supposed we
    wanted to predict dugong ages:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了带有L2损失的常数模型，我们可以将其与上一讲学到的SLR模型进行比较。考虑下面的数据集，其中包含嘴海牛的年龄和长度信息。假设我们想要预测嘴海牛的年龄：
- en: '|  | Constant Model | Simple Linear Regression |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | 常数模型 | 简单线性回归 |'
- en: '| --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| model | \(\hat{y} = \theta_0\) | \(\hat{y} = \theta_0 + \theta1 x\) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | \(\hat{y} = \theta_0\) | \(\hat{y} = \theta_0 + \theta1 x\) |'
- en: '| data | sample of ages \(D = \{y_1, y_2, ..., y_m\}\) | sample of ages \(D
    = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}\) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 数据 | 年龄样本 \(D = \{y_1, y_2, ..., y_m\}\) | 年龄样本 \(D = \{(x_1, y_1), (x_2,
    y_2), ..., (x_n, y_n)\}\) |'
- en: '| dimensions | \(\hat{\theta_0}\) is 1-D | \(\hat{\theta} = [\hat{\theta_0},
    \hat{\theta_1}]\) is 2-D |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 维度 | \(\hat{\theta_0}\) 是1-D | \(\hat{\theta} = [\hat{\theta_0}, \hat{\theta_1}]\)
    是2-D |'
- en: '| loss surface | 2-D ![](../Images/77b8b80a40f792278ba31e5a65f89638.png) |
    3-D ![](../Images/8e9ef675bd93b6efa6ace8cbed115fb1.png) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 损失曲面 | 2-D ![](../Images/77b8b80a40f792278ba31e5a65f89638.png) | 3-D ![](../Images/8e9ef675bd93b6efa6ace8cbed115fb1.png)
    |'
- en: '| Loss Model | \(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2\)
    | \(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - (\theta_0 + \theta_1 x))^2\)
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 损失模型 | \(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2\)
    | \(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - (\theta_0 + \theta_1 x))^2\)
    |'
- en: '| RMSE | 7.72 | 4.31 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| RMSE | 7.72 | 4.31 |'
- en: '| predictions visualized | rug plot ![](../Images/a5b069ce0a04699ffdf7164128075d90.png)
    | scatter plot ![](../Images/5220425f2d1084934b29ed218e328706.png) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 可视化预测 | 地毯图 ![](../Images/a5b069ce0a04699ffdf7164128075d90.png) | 散点图 ![](../Images/5220425f2d1084934b29ed218e328706.png)
    |'
- en: (Notice how the points for our SLR scatter plot are visually not a great linear
    fit. We’ll come back to this).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: （注意我们的SLR散点图的点在视觉上并不是一个很好的线性拟合。我们会回到这个问题）。
- en: The code for generating the graphs and models is included below, but we won’t
    go over it in too much depth.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 生成图形和模型的代码如下，但我们不会深入讨论。
- en: <details><summary>Code</summary>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE7]</details> <details><summary>Code</summary>'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE7]</details> <details><summary>代码</summary>'
- en: '[PRE8]</details>'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE8]</details>'
- en: '![](../Images/ba957469d571034c251e860a1b172ec9.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba957469d571034c251e860a1b172ec9.png)'
- en: <details><summary>Code</summary>
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE9]</details>'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE9]</details>'
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/e09f41290ab421da63d0cd1100d32a64.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e09f41290ab421da63d0cd1100d32a64.png)'
- en: <details><summary>Code</summary>
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE11]</details> <details><summary>Code</summary>'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE11]</details> <details><summary>代码</summary>'
- en: '[PRE12]</details>'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE12]</details>'
- en: '![](../Images/9a6741e27d7f14854d80693e9190e15e.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a6741e27d7f14854d80693e9190e15e.png)'
- en: <details><summary>Code</summary>
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE13]</details>'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE13]</details>'
- en: '![](../Images/282dea8446c284207be5a81f0905a3f1.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/282dea8446c284207be5a81f0905a3f1.png)'
- en: 'Interpreting the RMSE (Root Mean Squared Error): * The constant error is HIGHER
    than the linear error.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 解释RMSE（均方根误差）：* 常数误差高于线性误差。
- en: Hence, * The constant model is WORSE than the linear model (at least for this
    metric).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，* 常数模型比线性模型更差（至少对于这个度量）。
- en: 11.3 Constant Model + MAE
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 常数模型 + MAE
- en: We see now that changing the model used for prediction leads to a wildly different
    result for the optimal model parameter. What happens if we instead change the
    loss function used in model evaluation?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在看到，改变用于预测的模型会导致最佳模型参数的结果大不相同。如果我们改变模型评估中使用的损失函数会发生什么？
- en: This time, we will consider the constant model with L1 (absolute loss) as the
    loss function. This means that the average loss will be expressed as the **Mean
    Absolute Error (MAE)**.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们将考虑具有 L1（绝对损失）作为损失函数的常数模型。这意味着平均损失将被表示为**平均绝对误差（MAE）**。
- en: 'Choose a model: constant model'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择模型：常数模型
- en: 'Choose a loss function: L1 loss'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择损失函数：L1 损失
- en: Fit the model
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型
- en: Evaluate model performance
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型性能
- en: 11.3.1 Deriving the optimal \(\theta_0\)
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 求解最优 \(\theta_0\)
- en: Recall that the MAE is average **absolute** loss (L1 loss) over the data \(D
    = \{y_1, y_2, ..., y_m\}\).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，MAE 是数据 \(D = \{y_1, y_2, ..., y_m\}\) 上的平均**绝对**损失（L1 损失）。
- en: \[\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \hat{y_i}| \]
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \hat{y_i}| \]
- en: 'Given the constant model \(\hat{y} = \theta_0\), we can write the MAE as:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 给定常数模型 \(\hat{y} = \theta_0\)，我们可以将 MAE 写成：
- en: \[\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta_0| \]
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta_0| \]
- en: 'To fit the model, we find the optimal parameter value \(\hat{\theta}\) by differentiating
    using a calculus approach:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了拟合模型，我们通过微积分方法找到最优参数值 \(\hat{\theta}\)：
- en: Differentiate with respect to \(\hat{\theta_0}\).
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 \(\hat{\theta_0}\) 求导数。
- en: \[\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta| \]
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta| \]
- en: \[\frac{d}{d\theta} R(\theta) = \frac{d}{d\theta} \left(\frac{1}{n} \sum^{n}_{i=1}
    |y_i - \theta| \right)\]
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: \[\frac{d}{d\theta} R(\theta) = \frac{d}{d\theta} \left(\frac{1}{n} \sum^{n}_{i=1}
    |y_i - \theta| \right)\]
- en: \[= \frac{1}{n} \sum^{n}_{i=1} \frac{d}{d\theta} |y_i - \theta| \]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: \[= \frac{1}{n} \sum^{n}_{i=1} \frac{d}{d\theta} |y_i - \theta| \]
- en: 'Here, we seem to have run into a problem: the derivative of an absolute value
    is undefined when the argument is 0 (i.e. when \(y_i = \theta\)). For now, we’ll
    ignore this issue. It turns out that disregarding this case doesn’t influence
    our final result.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里，我们似乎遇到了一个问题：当参数为 0 时（即 \(y_i = \theta\)）绝对值的导数是未定义的。现在，我们将忽略这个问题。事实证明，忽略这种情况不会影响我们的最终结果。
- en: To perform the derivative, consider two cases. When \(\theta\) is *less than
    or equal to* \(y_i\), the term \(y_i - \theta\) will be positive and the absolute
    value has no impact. When \(\theta\) is *greater than* \(y_i\), the term \(y_i
    - \theta\) will be negative. Applying the absolute value will convert this to
    a positive value, which we can express by saying \(-(y_i - \theta) = \theta -
    y_i\).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行导数运算时，考虑两种情况。当 \(\theta\) *小于或等于* \(y_i\) 时，项 \(y_i - \theta\) 将为正值，绝对值不会产生影响。当
    \(\theta\) *大于* \(y_i\) 时，项 \(y_i - \theta\) 将为负值。应用绝对值将其转换为正值，我们可以表示为 \(-(y_i
    - \theta) = \theta - y_i\)。
- en: \[|y_i - \theta| = \begin{cases} y_i - \theta \quad \text{ if } \theta \le y_i
    \\ \theta - y_i \quad \text{if }\theta > y_i \end{cases}\]
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: \[|y_i - \theta| = \begin{cases} y_i - \theta \quad \text{ 如果 } \theta \le y_i
    \\ \theta - y_i \quad \text{如果 }\theta > y_i \end{cases}\]
- en: 'Taking derivatives:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 求导：
- en: \[\frac{d}{d\theta} |y_i - \theta| = \begin{cases} \frac{d}{d\theta} (y_i -
    \theta) = -1 \quad \text{if }\theta < y_i \\ \frac{d}{d\theta} (\theta - y_i)
    = 1 \quad \text{if }\theta > y_i \end{cases}\]
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: \[\frac{d}{d\theta} |y_i - \theta| = \begin{cases} \frac{d}{d\theta} (y_i -
    \theta) = -1 \quad \text{如果 }\theta < y_i \\ \frac{d}{d\theta} (\theta - y_i)
    = 1 \quad \text{如果 }\theta > y_i \end{cases}\]
- en: 'This means that we obtain a different value for the derivative for data points
    where \(\theta < y_i\) and where \(\theta > y_i\). We can summarize this by saying:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这意味着我们对于 \(\theta < y_i\) 和 \(\theta > y_i\) 的数据点得到了不同的导数值。我们可以总结为：
- en: \[\frac{d}{d\theta} R(\theta) = \frac{1}{n} \sum^{n}_{i=1} \frac{d}{d\theta}
    |y_i - \theta| \\ = \frac{1}{n} \left[\sum_{\hat{\theta_0} < y_i} (-1) + \sum_{\hat{\theta_0}
    > y_i} (+1) \right] \]
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: \[\frac{d}{d\theta} R(\theta) = \frac{1}{n} \sum^{n}_{i=1} \frac{d}{d\theta}
    |y_i - \theta| \\ = \frac{1}{n} \left[\sum_{\hat{\theta_0} < y_i} (-1) + \sum_{\hat{\theta_0}
    > y_i} (+1) \right] \]
- en: 'In other words, we take the sum of values for \(i = 1, 2, ..., n\):'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 换句话说，我们取 \(i = 1, 2, ..., n\) 的值的总和：
- en: \(-1\) if our observation \(y_i\) is *greater than* our prediction \(\hat{\theta_0}\)
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的观察值 \(y_i\) *大于* 我们的预测值 \(\hat{\theta_0}\)，则为\(-1\)
- en: \(+1\) if our observation \(y_i\) is *smaller than* our prediction \(\hat{\theta_0}\)
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的观察值 \(y_i\) *小于* 我们的预测值 \(\hat{\theta_0}\)，则为\(+1\)
- en: Set equal to 0\. \[ 0 = \frac{1}{n}\sum_{\hat{\theta_0} < y_i} (-1) + \frac{1}{n}\sum_{\hat{\theta_0}
    > y_i} (+1) \]
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 置为 0。\[ 0 = \frac{1}{n}\sum_{\hat{\theta_0} < y_i} (-1) + \frac{1}{n}\sum_{\hat{\theta_0}
    > y_i} (+1) \]
- en: Solve for \(\hat{\theta_0}\). \[ 0 = -\frac{1}{n}\sum_{\hat{\theta_0} < y_i}
    (1) + \frac{1}{n}\sum_{\hat{\theta_0} > y_i} (1)\]
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 求解 \(\hat{\theta_0}\)。\[ 0 = -\frac{1}{n}\sum_{\hat{\theta_0} < y_i} (1) + \frac{1}{n}\sum_{\hat{\theta_0}
    > y_i} (1)\]
- en: \[\sum_{\hat{\theta_0} < y_i} (1) = \sum_{\hat{\theta_0} > y_i} (1) \]
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: \[\sum_{\hat{\theta_0} < y_i} (1) = \sum_{\hat{\theta_0} > y_i} (1) \]
- en: 'Thus, the constant model parameter \(\theta = \hat{\theta_0}\) that minimizes
    MAE must satisfy:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最小化 MAE 的常数模型参数 \(\theta = \hat{\theta_0}\) 必须满足：
- en: \[ \sum_{\hat{\theta_0} < y_i} (1) = \sum_{\hat{\theta_0} > y_i} (1) \]
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{\hat{\theta_0} < y_i} (1) = \sum_{\hat{\theta_0} > y_i} (1) \]
- en: In other words, the number of observations greater than \(\theta_0\) must be
    equal to the number of observations less than \(\theta_0\); there must be an equal
    number of points on the left and right sides of the equation. This is the definition
    of median, so our optimal value is \[ \hat{\theta_0} = median(y) \]
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，大于 \(\theta_0\) 的观察数量必须等于小于 \(\theta_0\) 的观察数量；方程的左右两侧必须有相等数量的点。这就是中位数的定义，因此我们的最优值是
    \[ \hat{\theta_0} = median(y) \]
- en: '11.4 Summary: Loss Optimization, Calculus, and Critical Points'
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 总结：损失优化、微积分和临界点
- en: First, define the **objective function** as average loss.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将**目标函数**定义为平均损失。
- en: Plug in L1 or L2 loss.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代入 L1 或 L2 损失。
- en: Plug in the model so that the resulting expression is a function of \(\theta\).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代入模型，使得结果表达为 \(\theta\) 的函数。
- en: 'Then, find the minimum of the objective function:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，找到目标函数的最小值：
- en: Differentiate with respect to \(\theta\).
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 \(\theta\) 求导数。
- en: Set equal to 0.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 置为 0。
- en: Solve for \(\hat{\theta}\).
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 求解 \(\hat{\theta}\)。
- en: (If we have multiple parameters) repeat steps 1-3 with partial derivatives.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （如果我们有多个参数）重复步骤 1-3，使用偏导数。
- en: 'Recall critical points from calculus: \(R(\hat{\theta})\) could be a minimum,
    maximum, or saddle point! * We should technically also perform the second derivative
    test, i.e., show \(R''''(\hat{\theta}) > 0\). * MSE has a property—**convexity**—that
    guarantees that \(R(\hat{\theta})\) is a global minimum. * The proof of convexity
    for MAE is beyond this course.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 回想微积分中的临界点：\(R(\hat{\theta})\)可能是一个最小值、最大值或者鞍点！* 从技术上讲，我们还应该进行二阶导数测试，即，展示 \(R''(\hat{\theta})
    > 0\)。* MSE具有一个特性——**凸性**——它保证了 \(R(\hat{\theta})\) 是一个全局最小值。* MAE的凸性证明超出了本课程的范围。
- en: 11.5 Comparing Loss Functions
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 比较损失函数
- en: We’ve now tried our hand at fitting a model under both MSE and MAE cost functions.
    How do the two results compare?
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经尝试了在MSE和MAE成本函数下拟合模型。这两个结果如何比较？
- en: Let’s consider a dataset where each entry represents the number of drinks sold
    at a bubble tea store each day. We’ll fit a constant model to predict the number
    of drinks that will be sold tomorrow.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个数据集，其中每个条目代表了泡泡茶店每天卖出的饮料数量。我们将拟合一个常数模型来预测明天将卖出的饮料数量。
- en: '[PRE14]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: From our derivations above, we know that the optimal model parameter under MSE
    cost is the mean of the dataset. Under MAE cost, the optimal parameter is the
    median of the dataset.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们上面的推导，我们知道MSE成本下的最佳模型参数是数据集的均值。在MAE成本下，最佳参数是数据集的中位数。
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If we plot each empirical risk function across several possible values of \(\theta\),
    we find that each \(\hat{\theta}\) does indeed correspond to the lowest value
    of error:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在几个可能的 \(\theta\) 值上绘制每个经验风险函数，我们会发现每个 \(\hat{\theta}\) 确实对应于最低的错误值：
- en: '![error](../Images/c20a805ff62f8ff4674709f826b8db6a.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![error](../Images/c20a805ff62f8ff4674709f826b8db6a.png)'
- en: Notice that the MSE above is a **smooth** function – it is differentiable at
    all points, making it easy to minimize using numerical methods. The MAE, in contrast,
    is not differentiable at each of its “kinks.” We’ll explore how the smoothness
    of the cost function can impact our ability to apply numerical optimization in
    a few weeks.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 注意上面的MSE是一个**平滑**函数——它在所有点上都是可微的，这使得用数值方法最小化它变得容易。相比之下，MAE在每个“拐点”处都不可微。我们将在几周内探讨成本函数的平滑性如何影响我们应用数值优化的能力。
- en: How do outliers affect each cost function? Imagine we replace the largest value
    in the dataset with 1000\. The mean of the data increases substantially, while
    the median is nearly unaffected.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值如何影响每个成本函数？想象一下，我们用1000替换数据集中的最大值。数据的均值显著增加，而中位数几乎不受影响。
- en: '[PRE18]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This means that under the MSE, the optimal model parameter \(\hat{\theta}\)
    is strongly affected by the presence of outliers. Under the MAE, the optimal parameter
    is not as influenced by outlying data. We can generalize this by saying that the
    MSE is **sensitive** to outliers, while the MAE is **robust** to outliers.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在MSE下，最佳模型参数 \(\hat{\theta}\) 受到异常值的影响。在MAE下，最佳参数不受异常数据的影响。我们可以通过说MSE对异常值**敏感**，而MAE对异常值**稳健**来概括这一点。
- en: Let’s try another experiment. This time, we’ll add an additional, non-outlying
    datapoint to the data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试另一个实验。这一次，我们将向数据中添加一个额外的非异常数据点。
- en: '[PRE21]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'When we again visualize the cost functions, we find that the MAE now plots
    a horizontal line between 22 and 29\. This means that there are *infinitely* many
    optimal values for the model parameter: any value \(\hat{\theta} \in [22, 29]\)
    will minimize the MAE. In contrast, the MSE still has a single best value for
    \(\hat{\theta}\). In other words, the MSE has a **unique** solution for \(\hat{\theta}\);
    the MAE is not guaranteed to have a single unique solution.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们再次可视化成本函数时，我们发现MAE现在在22和29之间绘制了一条水平线。这意味着模型参数有*无数*个最佳值：任何值 \(\hat{\theta}
    \in [22, 29]\) 都将最小化MAE。相比之下，MSE仍然有一个最佳的 \(\hat{\theta}\) 值。换句话说，MSE有一个**唯一**的
    \(\hat{\theta}\) 解；MAE不能保证有一个唯一的解。
- en: '![](../Images/59c20d96f6055ae13c894a1da00e926e.png) ![](../Images/d06ac30e4b87e6dd5aa8cd831b5f641d.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59c20d96f6055ae13c894a1da00e926e.png) ![](../Images/d06ac30e4b87e6dd5aa8cd831b5f641d.png)'
- en: To summarize our example,
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 总结我们的例子，
- en: '| – | MSE (Mean Squared Loss) | MAE (Mean Absolute Loss) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| – | MSE（均方损失） | MAE（平均绝对损失） |'
- en: '| --- | --- | --- |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Loss Function | \(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2\)
    | \(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} &#124;y_i - \theta_0&#124;\) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 损失函数 | \(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2\)
    | \(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} &#124;y_i - \theta_0&#124;\) |'
- en: '| optimal \(\hat{\theta_0}\) | \(\hat{\theta_0} = mean(y) = \bar{y}\) | \(\hat{\theta_0}
    = median(y)\) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 最佳 \(\hat{\theta_0}\) | \(\hat{\theta_0} = mean(y) = \bar{y}\) | \(\hat{\theta_0}
    = median(y)\) |'
- en: '| loss surface | ![](../Images/59c20d96f6055ae13c894a1da00e926e.png) | ![](../Images/d06ac30e4b87e6dd5aa8cd831b5f641d.png)
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 损失曲面 | ![](../Images/59c20d96f6055ae13c894a1da00e926e.png) | ![](../Images/d06ac30e4b87e6dd5aa8cd831b5f641d.png)
    |'
- en: '| shape | **Smooth** - easy to minimize using numerical methods (in a few weeks)
    | **Piecewise** - at each of the “kinks,” it’s not differentiable. Harder to minimize.
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 形状 | **平滑** - 容易使用数值方法最小化（在几周内） | **分段** - 在每个“拐点”处，它不可微。更难最小化。'
- en: '| outliers | **Sensitive** to outliers (since they change mean substantially).
    Sensitivity also depends on the dataset size. | **More robust** to outliers. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: 异常值 | 对异常值敏感（因为它们会显著改变均值）。敏感性还取决于数据集的大小。 | 对异常值更加稳健。 |
- en: '| \(\hat{\theta_0}\) uniqueness | **unique** \(\hat{\theta_0}\) | **Infinitely
    many** \(\hat{\theta_0}\) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| \(\hat{\theta_0}\) 唯一性 | **唯一** \(\hat{\theta_0}\) | **无数个** \(\hat{\theta_0}\)
    |'
- en: 11.6 Transformations to fit Linear Models
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.6 转换以拟合线性模型
- en: At this point, we have an effective method of fitting models to predict linear
    relationships. Given a feature variable and target, we can apply our four-step
    process to find the optimal model parameters.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经有了一种有效的方法来拟合模型以预测线性关系。给定一个特征变量和目标，我们可以应用我们的四步过程来找到最佳的模型参数。
- en: A key word above is *linear*. When we computed parameter estimates earlier,
    we assumed that \(x_i\) and \(y_i\) shared a roughly linear relationship. Data
    in the real world isn’t always so straightforward, but we can transform the data
    to try and obtain linearity.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的关键词是*线性*。当我们之前计算参数估计时，我们假设\(x_i\)和\(y_i\)之间存在大致线性的关系。现实世界中的数据并不总是那么简单，但我们可以对数据进行转换以尝试获得线性关系。
- en: The **Tukey-Mosteller Bulge Diagram** is a useful tool for summarizing what
    transformations can linearize the relationship between two variables. To determine
    what transformations might be appropriate, trace the shape of the “bulge” made
    by your data. Find the quadrant of the diagram that matches this bulge. The transformations
    shown on the vertical and horizontal axes of this quadrant can help improve the
    fit between the variables.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tukey-Mosteller Bulge Diagram**是一个总结两个变量之间关系线性化的变换的有用工具。要确定哪些变换可能合适，追踪数据形成的“凸起”的形状。找到与此凸起匹配的图表象限。该象限的垂直和水平轴上显示的变换可以帮助改善变量之间的拟合。'
- en: '![bulge](../Images/0cf7f011f717e86a1e3bf7a2d18fb002.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![bulge](../Images/0cf7f011f717e86a1e3bf7a2d18fb002.png)'
- en: 'Note that:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: There are multiple solutions. Some will fit better than others.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有多种解决方案。有些比其他的拟合效果更好。
- en: sqrt and log make a value “smaller.”
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sqrt和log使值“变小”。
- en: Raising to a power makes a value “bigger.”
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高到幂会使值“变大”。
- en: Each of these transformations equates to increasing or decreasing the scale
    of an axis.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些变换中的每一个都等同于增加或减少轴的比例。
- en: Other goals in addition to linearity are possible, for example, making data
    appear more symmetric. Linearity allows us to fit lines to the transformed data.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 除了线性之外，还有其他可能的目标，例如使数据看起来更对称。线性允许我们对转换后的数据进行拟合。
- en: 'Let’s revisit our dugongs example. The lengths and ages are plotted below:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新看一下我们的儒艮示例。长度和年龄如下图所示：
- en: <details><summary>Code</summary>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE23]</details>'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE23]</details>'
- en: '![](../Images/4a616e7927a46dbcf7cceb3400e1ea48.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a616e7927a46dbcf7cceb3400e1ea48.png)'
- en: Looking at the plot on the left, we see that there is a slight curvature to
    the data points. Plotting the SLR curve on the right results in a poor fit.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在左边的图中，我们看到数据点有轻微的曲线。在右边绘制SLR曲线会导致拟合效果不佳。
- en: For SLR to perform well, we’d like there to be a rough linear trend relating
    `"Age"` and `"Length"`. What is making the raw data deviate from a linear relationship?
    Notice that the data points with `"Length"` greater than 2.6 have disproportionately
    high values of `"Age"` relative to the rest of the data. If we could manipulate
    these data points to have lower `"Age"` values, we’d “shift” these points downwards
    and reduce the curvature in the data. Applying a logarithmic transformation to
    \(y_i\) (that is, taking \(\log(\) `"Age"` \()\) ) would achieve just that.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使SLR表现良好，我们希望“年龄”和“长度”之间存在粗略的线性趋势。是什么导致原始数据偏离线性关系？注意到“长度”大于2.6的数据点相对于其他数据有着不成比例的高“年龄”值。如果我们能够操纵这些数据点使其具有较低的“年龄”值，我们将“移动”这些点向下并减少数据中的曲率。对\(y_i\)应用对数变换（即取\(\log(\)“年龄”\()\)）就可以实现这一点。
- en: 'An important word on \(\log\): in Data 100 (and most upper-division STEM courses),
    \(\log\) denotes the natural logarithm with base \(e\). The base-10 logarithm,
    where relevant, is indicated by \(\log_{10}\).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 关于\(\log\)的重要说明：在Data 100（以及大多数高年级STEM课程）中，\(\log\)表示以\(e\)为底的自然对数。在相关情况下，以10为底的对数用\(\log_{10}\)表示。
- en: <details><summary>Code</summary>
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE24]</details>'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE24]</details>'
- en: '![](../Images/239f4cc4315a0f7c90a4856d21a94984.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/239f4cc4315a0f7c90a4856d21a94984.png)'
- en: 'Our SLR fit looks a lot better! We now have a new target variable: the SLR
    model is now trying to predict the *log* of `"Age"`, rather than the untransformed
    `"Age"`. In other words, we are applying the transformation \(z_i = \log{(y_i)}\).
    Notice that the resulting model is still **linear in the parameters** \(\theta
    = [\theta_0, \theta_1]\). The SLR model becomes:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的SLR拟合看起来好多了！我们现在有了一个新的目标变量：SLR模型现在试图预测“年龄”的*对数*，而不是未经转换的“年龄”。换句话说，我们应用了变换\(z_i
    = \log{(y_i)}\)。注意到得到的模型仍然是**参数线性**的\(\theta = [\theta_0, \theta_1]\)。SLR模型变为：
- en: \[\log{\hat{(y_i)}} = \theta_0 + \theta_1 x_i\] \[\hat{z}_i = \theta_0 + \theta_1
    x_i\]
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: \[\log{\hat{(y_i)}} = \theta_0 + \theta_1 x_i\] \[\hat{z}_i = \theta_0 + \theta_1
    x_i\]
- en: 'It turns out that this linearized relationship can help us understand the underlying
    relationship between \(x_i\) and \(y_i\). If we rearrange the relationship above,
    we find: \[ \log{(y_i)} = \theta_0 + \theta_1 x_i \\ y_i = e^{\theta_0 + \theta_1
    x_i} \\ y_i = (e^{\theta_0})e^{\theta_1 x_i} \\ y_i = C e^{k x_i} \]'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，这种线性化关系可以帮助我们理解\(x_i\)和\(y_i\)之间的基本关系。如果我们重新排列上面的关系，我们会发现：\[ \log{(y_i)}
    = \theta_0 + \theta_1 x_i \\ y_i = e^{\theta_0 + \theta_1 x_i} \\ y_i = (e^{\theta_0})e^{\theta_1
    x_i} \\ y_i = C e^{k x_i} \]
- en: For some constants \(C\) and \(k\).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些常数\(C\)和\(k\)。
- en: \(y_i\) is an *exponential* function of \(x_i\). Applying an exponential fit
    to the untransformed variables corroborates this finding.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: \(y_i\)是\(x_i\)的*指数*函数。对未经转换的变量应用指数拟合可以证实这一发现。
- en: <details><summary>Code</summary>
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE25]</details>'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE25]</details>'
- en: '![](../Images/d80d91584d74363d2075a5d2ad748649.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d80d91584d74363d2075a5d2ad748649.png)'
- en: 'You may wonder: why did we choose to apply a log transformation specifically?
    Why not some other function to linearize the data?'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：为什么我们选择特别应用对数变换？为什么不使用其他函数来线性化数据？
- en: Practically, many other mathematical operations that modify the relative scales
    of `"Age"` and `"Length"` could have worked here.**
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，许多其他修改“年龄”和“长度”相对比例的数学运算在这里都可以起作用。**
