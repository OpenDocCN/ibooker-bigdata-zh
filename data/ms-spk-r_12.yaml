- en: Chapter 11\. Distributed R
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章 分布式 R
- en: Not like this. Not like this. Not like this.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不像这样。不像这样。不像这样。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Cersei Lannister
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —瑟曦·兰尼斯特
- en: In previous chapters, you learned how to perform data analysis and modeling
    in local Spark instances and proper Spark clusters. In [Chapter 10](ch10.html#extensions)
    specifically, we examined how to make use of the additional functionality provided
    by the Spark and R communities at large. In most cases, the combination of Spark
    functionality and extensions is more than enough to perform almost any computation.
    However, for those cases in which functionality is lacking in Spark and their
    extensions, you could consider distributing R computations to worker nodes yourself.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，您学习了如何在本地 Spark 实例和适当的 Spark 集群中进行数据分析和建模。具体来说，在 [第10章](ch10.html#extensions)
    中，我们讨论了如何利用 Spark 和 R 社区提供的额外功能。在大多数情况下，Spark 功能和扩展的组合已经足以执行几乎任何计算。然而，对于那些 Spark
    和它们的扩展功能不足的情况，您可以考虑将 R 计算分发到工作节点。
- en: You can run arbitrary R code in each worker node to run any computation—you
    can run simulations, crawl content from the web, transform data, and so on. In
    addition, you can also make use of any package available in CRAN and private packages
    available in your organization, which reduces the amount of code that you need
    to write to help you remain productive.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在每个工作节点上运行任意的 R 代码来执行任何计算 — 比如运行模拟、从网络上爬取内容、转换数据等。此外，您还可以使用 CRAN 中可用的任何包和您组织中可用的私有包，这样可以减少您需要编写的代码量，帮助您保持高效率。
- en: If you are already familiar with R, you might be tempted to use this approach
    for all Spark operations; however, this is not the recommended use of `spark_apply()`.
    Previous chapters provided more efficient techniques and tools to solve well-known
    problems; in contrast, `spark_apply()` introduces additional cognitive overhead,
    additional troubleshooting steps, performance trade-offs, and, in general, additional
    complexity you should avoid. Not to say that `spark_apply()` should never be used;
    rather, `spark_apply()` is reserved to support use cases for which previous tools
    and techniques fell short.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经熟悉 R，您可能会倾向于将这种方法用于所有 Spark 操作；然而，这并不是推荐使用 `spark_apply()` 的方式。之前的章节提供了更高效的技术和工具来解决已知问题；相比之下，`spark_apply()`
    引入了额外的认知负担、额外的故障排除步骤、性能折衷，以及一般上要避免的额外复杂性。并不是说 `spark_apply()` 绝对不能使用；而是说，`spark_apply()`
    专门用于支持之前工具和技术无法满足的用例。
- en: Overview
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: '[Chapter 1](ch01.html#intro) introduced MapReduce as a technique capable of
    processing large-scale datasets. It also described how Apache Spark provided a
    superset of operations to perform MapReduce computations easily and more efficiently.
    [Chapter 9](ch09.html#tuning) presented insights into how Spark works by applying
    custom transformations over each partition of the distributed datasets. For instance,
    if we multiplied each element of a distributed numeric dataset by 10, Spark would
    apply a mapping operation over each partition through multiple workers. A conceptual
    view of this process is illustrated in [Figure 11-1](#distributed-times-ten).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](ch01.html#intro) 引入了作为处理大规模数据集技术的 MapReduce。还描述了 Apache Spark 提供了一组操作的超集，以便更轻松和更有效地执行
    MapReduce 计算。[第9章](ch09.html#tuning) 通过在分布式数据集的每个分区上应用自定义转换来展示 Spark 的工作原理。例如，如果我们将分布式数值数据集的每个元素乘以
    10，则 Spark 将通过多个工作节点对每个分区应用映射操作。这个过程的概念视图如 [图11-1](#distributed-times-ten) 所示。'
- en: '![Map Operation when Multiplying by Ten](assets/mswr_1101.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![乘以十的映射操作](assets/mswr_1101.png)'
- en: Figure 11-1\. Map operation when multiplying by 10
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-1\. 乘以十的映射操作
- en: 'This chapter presents how to define a custom `f(x)` mapping operation using
    `spark_apply()`; for the previous example, `spark_apply()` provides support to
    define `10 * x`, as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍如何使用 `spark_apply()` 定义自定义 `f(x)` 映射操作；对于前面的示例，`spark_apply()` 支持定义 `10
    * x`，如下所示：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice that `~ 10 * .x` is plain R code executed across all worker nodes. The
    `~` operator is defined in the `rlang` package and provides a compact definition
    of a function equivalent to `function(.x) 10 * .x`; this compact form is also
    known as an *anonymous function*, or *lambda expression*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `~ 10 * .x` 是在所有工作节点上执行的普通 R 代码。`~` 运算符定义在 `rlang` 包中，提供了一个相当于 `function(.x)
    10 * .x` 的紧凑函数定义；这种紧凑形式也被称为*匿名函数*或*lambda 表达式*。
- en: The `f(x)` function must take an R DataFrame (or something that can be automatically
    transformed to one) as input and must also produce an R DataFrame as output, as
    shown in [Figure 11-2](#distributed-spark-apply-input-output).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`f(x)` 函数必须接受 R DataFrame（或可以自动转换为其的内容）作为输入，并且必须输出 R DataFrame，如 [图 11-2](#distributed-spark-apply-input-output)
    所示。'
- en: '![Expected Function Signature in spark_apply() Mappings](assets/mswr_1102.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![spark_apply() 映射中预期的函数签名](assets/mswr_1102.png)'
- en: Figure 11-2\. Expected function signature in spark_apply() mappings
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-2\. spark_apply() 映射中预期的函数签名
- en: We can refer back to the original MapReduce example from [Chapter 1](ch01.html#intro),
    where the map operation was defined to split sentences into words and the total
    unique words were counted as the reduce operation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以回顾来自 [第一章](ch01.html#intro) 的原始 MapReduce 示例，其中定义了将句子拆分为单词并将总唯一单词计数为减少操作。
- en: 'In R, we could make use of the `unnest_tokens()` function from the `tidytext`
    R package, which you would need to install from CRAN before connecting to Spark.
    You can then use `tidytext` with `spark_apply()` to tokenize those sentences into
    a table of words:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 中，我们可以使用 `tidytext` R 包中的 `unnest_tokens()` 函数，在连接到 Spark 之前需要从 CRAN 安装此包。然后，您可以使用
    `spark_apply()` 将这些句子标记为单词表：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can complete this MapReduce example by performing the reduce operation with
    `dplyr`, as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 `dplyr` 完成 MapReduce 示例中的减少操作，如下所示：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The rest of this chapter will explain in detail the use cases, features, caveats,
    considerations, and troubleshooting techniques required when you are defining
    custom mappings through `spark_apply()`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分将详细解释在定义通过 `spark_apply()` 进行自定义映射时所需的用例、特性、注意事项、考虑因素和故障排除技术。
- en: Note
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The previous sentence tokenizer example can be more efficiently implemented
    using concepts from previous chapters, specifically through `sentences %>% ft_tokenizer("text",
    "words") %>% transmute(word = explode(words))`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的句子标记化示例可以通过前几章的概念更有效地实现，具体来说是通过 `sentences %>% ft_tokenizer("text", "words")
    %>% transmute(word = explode(words))`。
- en: Use Cases
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Use Cases
- en: 'Now that we’ve presented an example to help you understand how `spark_apply()`
    works, we’ll cover a few practical use cases for it:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了一个示例，帮助您理解 `spark_apply()` 的工作原理，接下来我们将讨论几个它的实际用例：
- en: Import
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 导入
- en: You can consider using R to import data from external data sources and formats.
    For example, when a file format is not natively supported in Spark or its extensions,
    you can consider using R code to implement a distributed *custom parser* using
    R packages.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以考虑使用 R 从外部数据源和格式导入数据。例如，当 Spark 或其扩展中未本地支持文件格式时，您可以考虑使用 R 代码来实现使用 R 包的分布式
    *自定义解析器*。
- en: Model
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 模型
- en: It is natural to use the rich modeling capabilities already available in R with
    Spark. In most cases, R models can’t be used across large data; however, we will
    present two particular use cases where R models can be useful at scale. For instance,
    when data fits into a single machine, you can use *grid search* to optimize their
    parameters in parallel. In cases where the data can be partitioned to create several
    models over subsets of the data, you can use *partitioned modeling* in R to compute
    models across partitions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 利用已经在 R 中可用的丰富建模功能与 Spark 是很自然的事情。在大多数情况下，R 模型无法跨大数据使用；然而，我们将介绍两个特定的用例，展示 R
    模型在大规模情况下的有用性。例如，当数据适合单台机器时，您可以使用 *网格搜索* 并行优化其参数。在数据可以分区以创建多个模型来跨数据子集进行计算的情况下，您可以使用
    R 中的 *分区建模* 来计算跨分区的模型。
- en: Transform
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 转换
- en: You can use R’s rich data transformation capabilities to complement Spark. We’ll
    present a use case of evaluating data by external systems, and use R to interoperate
    with them by calling them through a *web API*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 R 丰富的数据转换能力来补充 Spark。我们将呈现一个通过 *web API* 调用外部系统评估数据的用例。
- en: Compute
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 计算
- en: When you need to perform large-scale computation in R, or *big compute* as described
    in [Chapter 1](ch01.html#intro), Spark is ideal to distribute this computation.
    We will present *simulations* as a particular use case for large-scale computing
    in R.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要在 R 中执行大规模计算，或者如 [第一章](ch01.html#intro) 中所述的 *大型计算* 时，Spark 是分布此计算的理想选择。我们将展示
    *模拟* 作为 R 中大规模计算的一个特定用例。
- en: As we now explore each use case in detail, we’ll provide a working example to
    help you understand how to use `spark_apply()` effectively.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将详细探讨每个用例，并提供一个工作示例，帮助您有效地使用 `spark_apply()`。
- en: Custom Parsers
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义解析器
- en: Though Spark and its various extensions provide support for many file formats
    (CSVs, JSON, Parquet, AVRO, etc.), you might need other formats to use at scale.
    You can parse these additional formats using `spark_apply()` and many existing
    R packages. In this section, we will look at how to parse logfiles, though similar
    approaches can be followed to parse other file formats.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Spark及其各种扩展支持许多文件格式（CSV、JSON、Parquet、AVRO等），您可能需要其他格式以便规模化使用。您可以使用 `spark_apply()`
    和许多现有的R包解析这些额外的格式。在本节中，我们将看一下如何解析日志文件，虽然可以按照类似的方法解析其他文件格式。
- en: It is common to use Spark to analyze logfiles—for instance, logs that track
    download data from Amazon S3\. The `webreadr` package can simplify the process
    of parsing logs by providing support to load logs stored as Amazon S3, Squid,
    and the Common log format. You should install `webreadr` from CRAN before connecting
    to Spark.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark分析日志文件是很常见的，例如来自Amazon S3跟踪下载数据的日志。 `webreadr` 包可以通过支持加载存储在Amazon S3、Squid和常见日志格式的日志来简化解析日志的过程。在连接到Spark之前，您应该从CRAN安装
    `webreadr` 。
- en: 'For example, an Amazon S3 log looks as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Amazon S3 日志如下所示：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This can be parsed easily with `read_aws()`, as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过 `read_aws()` 轻松解析，如下所示：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To scale this operation, we can make use of `read_aws()` using `spark_apply()`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展此操作，我们可以使用 `spark_apply()` 使用 `read_aws()`：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The code used by plain R and `spark_apply()` is similar; however, with `spark_apply()`,
    logs are parsed in parallel across all the worker nodes available in your cluster.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 纯R和 `spark_apply()` 使用的代码类似；但是，使用 `spark_apply()`，日志会在集群中所有可用的工作节点上并行解析。
- en: This concludes the custom parsers discussion; you can parse many other file
    formats at scale from R following a similar approach. Next we’ll look at present
    partitioned *modeling* as another use case focused on modeling across several
    datasets in parallel.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了自定义解析器讨论；您可以遵循类似的方法从R中扩展解析许多其他文件格式。接下来，我们将看一下分区*建模*作为另一个重点在多个数据集之间并行建模的用例。
- en: Partitioned Modeling
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分区建模
- en: There are many modeling packages available in R that can also be run at scale
    by partitioning the data into manageable groups that fit in the resources of a
    single machine.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: R中有许多建模包也可以通过将数据分区为适合单台计算机资源的可管理组来运行。
- en: For instance, suppose that you have a 1 TB dataset for sales data across multiple
    cities and you are tasked with creating sales predictions over each city. For
    this case, you can consider partitioning the original dataset per city—say, into
    10 GB of data per city—which could be managed by a single compute instance. For
    this kind of partitionable dataset, you can also consider using `spark_apply()`
    by training each model over each city.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您有一个跨多个城市的1 TB销售数据集，并且您的任务是在每个城市上创建销售预测。对于这种情况，您可以考虑按城市对原始数据集进行分区，比如每个城市10
    GB的数据，可以由单个计算实例管理。对于这种可分区数据集，您还可以考虑使用 `spark_apply()`，通过在每个城市上训练每个模型来训练。
- en: 'As a simple example of partitioned modeling, we can run a linear regression
    using the `iris` dataset partitioned by species:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 作为分区建模的一个简单示例，我们可以使用 `iris` 数据集按物种分区运行线性回归：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then you can run a linear regression over each species using `spark_apply()`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用 `spark_apply()` 对每个物种运行线性回归：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As you can see from the `r.squared` results and intuitively in [Figure 11-3](#distributed-r-modeling-species),
    the linear model for `versicolor` better fits to the regression line:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从 `r.squared` 结果和直觉上在 [图11-3](#distributed-r-modeling-species) 中看到的，对于 `versicolor`，线性模型更适合回归线：
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Modeling over species](assets/mswr_1103.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![在物种上建模](assets/mswr_1103.png)'
- en: Figure 11-3\. Modeling over species in the iris dataset
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-3。在iris数据集中对物种建模
- en: This concludes our brief overview on how to perform modeling over several different
    partitionable datasets. A similar technique can be applied to perform modeling
    over the same dataset using different modeling parameters, as we cover next.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们关于如何在几个不同的可分区数据集上执行建模的简要概述。可以应用类似的技术来在相同数据集上使用不同的建模参数执行建模，接下来我们将介绍。
- en: Grid Search
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格搜索
- en: Many R packages provide models that require defining multiple parameters to
    configure and optimize. When the value of these parameters is unknown, we can
    distribute this list of unknown parameters across a cluster of machines to find
    the optimal parameter combination. If the list contains more than one parameter
    to optimize, it is common to test against all the combinations between parameter
    A and parameter B, creating a grid of parameters. The process of searching for
    the best parameter over this parameter grid is commonly known as *grid search*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 R 包提供需要定义多个参数来配置和优化的模型。当这些参数的值未知时，我们可以将此未知参数列表分发到一组机器中，以找到最佳参数组合。如果列表包含多个要优化的参数，则通常会针对参数
    A 和参数 B 之间的所有组合进行测试，创建一个参数网格。在这个参数网格上搜索最佳参数的过程通常称为 *网格搜索*。
- en: 'For example, we can define a grid of parameters to optimize decision tree models
    as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以定义一个参数网格来优化决策树模型，如下所示：
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The grid dataset was copied by using `repartition = 9` to ensure that each
    partition is contained in one machine, since the grid also has nine rows. Now,
    assuming that the original dataset fits in every machine, we can distribute this
    dataset to many machines and perform parameter search to find the model that best
    fits this data:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `repartition = 9` 将网格数据集复制，以确保每个分区都包含在一个机器中，因为网格也有九行。现在，假设原始数据集适合每台机器，我们可以将此数据集分发到多台机器，并进行参数搜索，以找到最适合此数据的模型：
- en: '[PRE18]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For this model, `minsplit = 2` and `maxdepth = 8` produces the most accurate
    results. You can now use this specific parameter combination to properly train
    your model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此模型，`minsplit = 2` 和 `maxdepth = 8` 生成了最准确的结果。现在，你可以使用这个特定的参数组合来正确训练你的模型。
- en: Web APIs
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Web API
- en: A web API is a program that can do something useful through a web interface
    that other programs can reuse. For instance, services like Twitter provide web
    APIs that allow you to automate reading tweets from a program written in R and
    other programming languages. You can make use of web APIs using `spark_apply()`
    by sending programmatic requests to external services using R code.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Web API 是通过 Web 接口提供有用功能的程序，其他程序可以重用它们。例如，像 Twitter 这样的服务提供 Web API，允许你在使用 R
    和其他编程语言编写的程序中自动读取推文。你可以通过使用 `spark_apply()` 发送 R 代码来利用 Web API。
- en: For example, Google provides a web API to label images using deep learning techniques;
    you can use this API from R, but for larger datasets, you need to access its APIs
    from Spark. You can use Spark to prepare data to be consumed by a web API and
    then use `spark_apply()` to perform this call and process all the incoming results
    back in Spark.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Google 提供了一个 Web API 来使用深度学习技术对图像进行标注；你可以从 R 中使用此 API，但对于较大的数据集，你需要从 Spark
    访问其 API。你可以使用 Spark 准备数据以供 Web API 使用，然后使用 `spark_apply()` 调用并在 Spark 中处理所有传入的结果。
- en: 'The next example makes use of the `googleAuthR` package to authenticate to
    Google Cloud, the `RoogleVision` package to perform labeling over the Google Vision
    API, and `spark_apply()` to interoperate between Spark and Google’s deep learning
    service. To run the following example, you’ll first need to disconnect from Spark
    and download your *`cloudml.json`* file from the Google developer portal:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例利用 `googleAuthR` 包对 Google Cloud 进行身份验证，使用 `RoogleVision` 包在 Google Vision
    API 上执行标注，并使用 `spark_apply()` 在 Spark 和 Google 的深度学习服务之间进行交互。要运行以下示例，首先需要断开与 Spark
    的连接，并从 Google 开发者门户下载你的 *`cloudml.json`* 文件：
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: To successfully run a large distributed computation over a web API, the API
    needs to be able to scale to support the load from all the Spark executors. We
    can trust that major service providers are likely to support all the requests
    incoming from your cluster. But when you’re calling internal web APIs, make sure
    the API can handle the load. Also, when you’re using third-party services, consider
    the cost of calling their APIs across all the executors in your cluster to avoid
    potentially expensive and unexpected charges.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功地在 Web API 上运行大规模的分布式计算，API 需要能够扩展以支持来自所有 Spark 执行器的负载。我们可以相信主要服务提供商可能支持来自你的集群的所有请求。但当你调用内部
    Web API 时，请确保 API 能够处理负载。此外，当你使用第三方服务时，考虑在你的集群中的所有执行器上调用其 API 的成本，以避免潜在的昂贵和意外的费用。
- en: Next we’ll describe a use case for big compute where R is used to perform distributed
    rendering.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将描述一个使用案例，其中 R 用于执行分布式渲染。
- en: Simulations
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟
- en: You can use R combined with Spark to perform large-scale computing. The use
    case we explore here is rendering computationally expensive images using the `rayrender`
    package, which uses *ray tracing*, a photorealistic technique commonly used in
    movie production.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以结合使用 R 和 Spark 进行大规模计算。我们在这里探讨的用例是使用 `rayrender` 软件包渲染计算密集型图像，该软件包使用光线追踪技术，这是电影制作中常用的一种逼真技术。
- en: 'Let’s use this package to render a simple scene that includes a few spheres
    (see [Figure 11-4](#distributed-r-ray-tracing)) that use a *lambertian material*,
    a diffusely reflecting material or “matte.” First, install `rayrender` using `install.packages("rayrender")`.
    Then, be sure you’ve disconnected and reconnected Spark:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个软件包来渲染一个包含几个球体的简单场景（见 [Figure 11-4](#distributed-r-ray-tracing)），这些球体使用
    *lambertian material*，一种漫反射材料或“哑光”。首先，使用 `install.packages("rayrender")` 安装 `rayrender`。然后，确保您已断开并重新连接
    Spark：
- en: '[PRE22]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Ray tracing in Spark using R and rayrender](assets/mswr_1104.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![在 Spark 中使用 R 和 rayrender 进行光线追踪](assets/mswr_1104.png)'
- en: Figure 11-4\. Ray tracing in Spark using R and rayrender
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-4\. 在 Spark 中使用 R 和 rayrender 进行光线追踪
- en: 'In higher resolutions, say 1920 x 1080, the previous example takes several
    minutes to render the single frame from [Figure 11-4](#distributed-r-ray-tracing);
    rendering a few seconds at 30 frames per second would take several hours in a
    single machine. However, we can reduce this time using multiple machines by parallelizing
    computation across them. For instance, using 10 machines with the same number
    of CPUs would cut rendering time tenfold:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在更高的分辨率下，比如 1920 x 1080，之前的例子需要几分钟来渲染来自 [Figure 11-4](#distributed-r-ray-tracing)
    的单帧；以每秒 30 帧的速度渲染几秒钟将在单台机器上花费数小时。然而，我们可以通过在多台机器上并行计算来减少这个时间。例如，使用相同数量的 CPU 的 10
    台机器将减少渲染时间十倍：
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After all the images are rendered, the last step is to collect them from HDFS
    and use tools like `ffmpeg` to convert individual images into an animation:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染所有图像之后，最后一步是从 HDFS 中收集它们，并使用 `ffmpeg` 这样的工具将单独的图像转换为动画：
- en: '[PRE24]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This example assumes HDFS is used as the storage technology for Spark and being
    run under a `hadoop` user, you will need to adjust this for your particular storage
    or user.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例假设 HDFS 用作 Spark 的存储技术，并且以 `hadoop` 用户身份运行，您需要根据您的特定存储或用户进行调整。
- en: We’ve covered some common use cases for `spark_apply()`, but you are certainly
    welcome to find other use cases for your particular needs. The next sections present
    technical concepts you’ll need to understand to create additional use cases and
    to use `spark_apply()` effectively.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了一些 `spark_apply()` 的常见用例，但您当然可以找到其他符合您特定需求的用例。接下来的章节将介绍您需要理解的技术概念，以创建额外的用例并有效地使用
    `spark_apply()`。
- en: Partitions
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分区
- en: Most Spark operations that analyze data with `dplyr` or model with MLlib don’t
    require understanding how Spark partitions data; they simply work automatically.
    However, for distributed R computations, this is not the case. For these you will
    have to learn and understand how exactly Spark is partitioning your data and provide
    transformations that are compatible with them. This is required since `spark_apply()`
    receives each partition and allows you to perform any transformation, not the
    entire dataset. You can refresh concepts like partitioning and transformations
    using the diagrams and examples from [Chapter 9](ch09.html#tuning).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数使用 `dplyr` 分析数据或者使用 MLlib 建模的 Spark 操作不需要理解 Spark 如何分区数据；它们会自动运行。然而，对于分布式
    R 计算来说并非如此。对于这些操作，您需要学习和理解 Spark 如何分区您的数据，并提供与之兼容的转换。这是因为 `spark_apply()` 接收每个分区并允许您执行任何转换，而不是整个数据集。您可以通过
    [Chapter 9](ch09.html#tuning) 中的图表和示例刷新分区和转换的概念。
- en: 'To help you understand how partitions are represented in `spark_apply()`, consider
    the following code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要帮助您理解 `spark_apply()` 中如何表示分区，请考虑以下代码：
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Should we expect the output to be the total number of rows? As you can see
    from the results, in general the answer is no; Spark assumes data will be distributed
    across multiple machines, so you’ll often find it already partitioned, even for
    small datasets. Because we should not expect `spark_apply()` to operate over a
    single partition, let’s find out how many partitions `sdf_len(sc, 10)` contains:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该期望输出是总行数吗？从结果中可以看出，一般来说答案是否定的；Spark 假设数据将分布在多台机器上，因此即使是小数据集，你通常也会发现它已经被分区。因为我们不应该期望
    `spark_apply()` 在单个分区上操作，让我们找出 `sdf_len(sc, 10)` 包含多少个分区：
- en: '[PRE27]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This explains why counting rows through `nrow()` under `spark_apply()` retrieves
    two rows since there are two partitions, not one. `spark_apply()` is retrieving
    the count of rows over each partition, and each partition contains 5 rows, not
    10 rows total, as you might have expected.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了为什么在`spark_apply()`下通过`nrow()`计算行数会检索到两行，因为有两个分区，而不是一个。`spark_apply()`正在检索每个分区中的行数计数，每个分区包含5行，而不是总共10行，这可能是您所期望的。
- en: 'For this particular example, we could further aggregate these partitions by
    repartitioning and then adding up—this would resemble a simple MapReduce operation
    using `spark_apply()`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的示例，我们可以通过重新分区并进行累加来进一步聚合这些分区——这将类似于使用`spark_apply()`进行简单的MapReduce操作：
- en: '[PRE29]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: So now that you know about partitions using `spark_apply()`, we’ll move on to
    using `group_by` to control partitions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已了解如何使用`spark_apply()`处理分区，我们将转而使用`group_by`来控制分区。
- en: Grouping
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分组
- en: 'When using `spark_apply()`, we can request explicit partitions from Spark.
    For instance, if we had to process numbers less than four in one partition and
    the remaining ones in a second partition, we could create these groups explicitly
    and then request `spark_apply()` to use them:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`spark_apply()`时，我们可以要求Spark从生成的DataFrame中获取显式分区。例如，如果我们需要在一个分区中处理小于四的数字，而在第二个分区中处理剩余的数字，我们可以显式创建这些组，然后请求`spark_apply()`使用它们：
- en: '[PRE31]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Notice that `spark_apply()` is still processing two partitions, but in this
    case we expect these partitions since we explicitly requested them in `spark_apply()`;
    therefore, you can safely interpret the results as “there are three integers less
    than four.”
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`spark_apply()`仍在处理两个分区，但在这种情况下，我们期望这些分区，因为我们在`spark_apply()`中明确请求了它们；因此，您可以安全地解释结果为“有三个小于四的整数”。
- en: Note
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can only group data by partitions that fit in a single machine; if one of
    the groups is too large, an exception will be thrown. To perform operations over
    groups that exceed the resources of a single node, you can consider partitioning
    to smaller units or use `dplyr::do`, which is currently optimized for large partitions.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您只能按照适合单台机器的分区对数据进行分组；如果其中一个分组过大，将抛出异常。为了在超出单个节点资源的组上执行操作，您可以考虑将其分区为较小单元，或者使用`dplyr::do`，它目前针对大分区进行了优化。
- en: The takeaway from this section is to always consider partitions when dealing
    with `spark_apply()`. Next, we will zoom in to `spark_apply()` to understand how
    columns are interpreted.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的要点是在处理`spark_apply()`时始终考虑分区。接下来，我们将深入研究`spark_apply()`，以了解列的解释方式。
- en: Columns
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 列
- en: 'By default, `spark_apply()` automatically inspects the DataFrame being produced
    to learn column names and types. For example:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`spark_apply()`会自动检查正在生成的DataFrame以学习列名和类型。例如：
- en: '[PRE33]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'However, this is inefficient since `spark_apply()` needs to run twice: first,
    to find columns by computing `spark_apply()` against a subset of all the data,
    and then to compute the actual desired values.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这样做效率低下，因为`spark_apply()`需要运行两次：首先通过计算所有数据的子集来找到列，然后计算实际所需的值。
- en: 'To improve performance, you can explicitly specify the columns through the
    `columns` parameters. This parameter takes a named list of types expected in the
    resulting DataFrame. We can then rewrite the previous example to run only once
    by specifying the correct type for the `numbers` column:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能，可以通过`columns`参数显式指定列。此参数接受一个命名列表，列出预期在生成的DataFrame中的类型。我们可以通过为`numbers`列指定正确的类型，重写之前的示例以仅运行一次：
- en: '[PRE35]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now that we’ve presented how rows and columns interact with `spark_apply()`,
    let’s move on to making use of the contextual information sometimes required when
    processing distributed datasets.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了行和列如何与`spark_apply()`交互，让我们继续利用处理分布式数据集时有时需要的上下文信息。
- en: Context
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文
- en: To process partitions using `spark_apply()`, you might need to include auxiliary
    data that is small enough to fit in each node. This was the case in the [grid
    search](#distributed-grid-search) use case, where the dataset was passed to all
    partitions and remained unpartitioned itself.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`spark_apply()`处理分区，可能需要包含足够小以适应每个节点的辅助数据。这在[网格搜索](#distributed-grid-search)用例中是成立的，数据集传递到所有分区并保持不分区。
- en: 'We can modify the initial `f(x) = 10 * x` example in this chapter to customize
    the multiplier. It was originally set to `10`, but we can make it configurable
    by specifying it as the `context` parameter:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以修改本章中初始的`f(x) = 10 * x`示例以自定义乘数。它最初设置为`10`，但我们可以通过将其指定为`context`参数来使其可配置：
- en: '[PRE37]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[Figure 11-5](#distributed-times-context) illustrates this example conceptually.
    Notice that the data partitions are still variable; however, the contextual parameter
    is distributed to all the nodes.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-5](#distributed-times-context) 从概念上说明了这个例子。请注意，数据分区仍然是可变的；但是，上下文参数分发到所有节点。'
- en: '![Map Operation when Multiplying with Context](assets/mswr_1105.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![在乘以上下文时的映射操作](assets/mswr_1105.png)'
- en: Figure 11-5\. Map operation when multiplying with context
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-5\. 在乘以上下文时的映射操作
- en: The grid search example used this parameter to pass a DataFrame to each worker
    node; however, since the context parameter is serialized as an R object, it can
    contain anything. For instance, if you need to pass multiple values—or even multiple
    datasets—you can pass a list with values.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索示例使用此参数将DataFrame传递给每个工作节点；但是，由于上下文参数被序列化为R对象，它可以包含任何内容。例如，如果需要传递多个值——甚至多个数据集——可以传递一个包含值的列表。
- en: 'The following example defines a `f(x) = m * x + b` function and runs `m = 10`
    and `b = 2`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例定义了一个`f(x) = m * x + b`函数，并运行`m = 10`和`b = 2`：
- en: '[PRE39]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Notice that we’ve renamed `context` to `.y` to shorten the variable name. This
    works because `spark_apply()` assumes context is the second parameter in functions
    and expressions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已将`context`重命名为`.y`以缩短变量名。这是有效的，因为`spark_apply()`假设上下文是函数和表达式中的第二个参数。
- en: You’ll find the `context` parameter extremely useful; for instance, the next
    section presents how to properly construct functions, and `context` is used in
    advanced use cases to construct functions dependent on other functions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 您会发现`context`参数非常有用；例如，下一节将介绍如何正确构建函数，而高级用例中使用`context`来构建依赖于其他函数的函数。
- en: Functions
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 函数
- en: Earlier we presented `spark_apply()` as an operation to perform custom transformations
    using a function or expression. In programming literature, functions with a context
    are also referred to as a *closure*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们介绍了`spark_apply()`作为执行自定义转换的操作，使用函数或表达式。在编程文献中，具有上下文的函数也被称为*闭包*。
- en: Expressions are useful to define short transformations, like `~ 10 * .x`. For
    an expression, `.x` contains a partition and `.y` the context, when available.
    However, it can be hard to define an expression for complex code that spans multiple
    lines. For those cases, functions are more appropriate.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式可用于定义短转换，如`~ 10 * .x`。对于表达式，`.x`包含一个分区，`.y`包含上下文（如果有）。但是，对于跨多行的复杂代码，定义表达式可能很困难。对于这些情况，函数更为合适。
- en: Functions enable complex and multiline transformations, and are defined as `function(data,
    context) {}` where you can provide arbitrary code within the `{}`. We’ve seen
    them in previous sections when using Google Cloud to transform images into image
    captions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 函数使复杂且多行的转换成为可能，并被定义为`function(data, context) {}`，您可以在`{}`中提供任意代码。在使用Google
    Cloud将图像转换为图像标题时，我们在前面的部分中看到了它们。
- en: 'The function passed to `spark_apply()` is serialized using `serialize()`, which
    is described as “a simple low-level interface for serializing to connections.”
    One current limitation of `serialize()` is that it won’t serialize objects being
    referenced outside its environment. For instance, the following function errors
    out since the closure references `external_value`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给`spark_apply()`的函数使用`serialize()`进行序列化，该函数被描述为“用于序列化到连接的简单低级接口”。`serialize()`当前的限制之一是，它不会序列化在其环境外引用的对象。例如，由于闭包引用了`external_value`，因此以下函数会报错：
- en: '[PRE41]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'As workarounds to this limitation, you can add the functions your closure needs
    into the `context` and then assign the functions into the global environment:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解决此限制的变通方法，您可以将闭包所需的函数添加到`context`中，然后将这些函数分配给全局环境：
- en: '[PRE42]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: When this isn’t feasible, you can also create your own R package with the functionality
    you need and then use your package in `spark_apply()`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当这不可行时，您还可以创建自己的R包，其中包含您需要的功能，然后在`spark_apply()`中使用您的包。
- en: You’ve learned all the functionality available in `spark_apply()` using plain
    R code. In the next section we present how to use packages when distributing computations.
    R packages are essential when you are creating useful transformations.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经学会了`spark_apply()`中提供的所有功能，使用纯R代码。在下一节中，我们将介绍在分布计算中使用包时的方法。当您创建有用的转换时，R包至关重要。
- en: Packages
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 包
- en: 'With `spark_apply()` you can use any R package inside Spark. For instance,
    you can use the `broom` package to create a tidy DataFrame from linear regression
    output:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`spark_apply()`，您可以在Spark内部使用任何R包。例如，您可以使用`broom`包从线性回归输出中创建整洁的DataFrame：
- en: '[PRE44]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The first time you call `spark_apply()`, all the contents in your local `.libPaths()`
    (which contains all R packages) will be copied into each Spark worker node. Packages
    are only copied once and persist as long as the connection remains open. It’s
    not uncommon for R libraries to be several gigabytes in size, so be prepared for
    a one-time tax while the R packages are copied over to your Spark cluster. You
    can disable package distribution by setting `packages = FALSE`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次调用 `spark_apply()` 时，将会复制您本地 `.libPaths()` 中的所有内容（其中包含所有 R 包）到每个 Spark 工作节点中。包只会复制一次，并在连接保持打开状态时持续存在。R
    库的大小可能会达到数千兆字节，因此在复制 R 包到 Spark 集群期间需要一次性的时间和资源。您可以通过设置 `packages = FALSE` 来禁用包分发。
- en: Note
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since packages are copied only once for the duration of the `spark_connect()`
    connection, installing additional packages is not supported while the connection
    is active. Therefore, if a new package needs to be installed, `spark_disconnect()`
    the connection, modify packages, and then reconnect. In addition, R packages are
    not copied in local mode, because the packages already exist on the local system.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因为包只在 `spark_connect()` 连接期间复制一次，所以在连接活跃时不支持安装额外的包。因此，如果需要安装新包，需先 `spark_disconnect()`
    断开连接，修改包，然后重新连接。此外，在本地模式下不会复制 R 包，因为这些包已经存在于本地系统中。
- en: Though this section was brief, using packages with distributed R code opens
    up an entire new universe of interesting use cases. Some of those use cases were
    presented in this chapter, but by looking at the rich ecosystem of R packages
    available today you’ll find many more.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本节内容简短，但使用分布式 R 代码的包打开了许多有趣的用例。本章介绍了一些这些用例，但通过查看当今可用的丰富 R 包生态系统，您会发现更多。
- en: This section completes our discussion of the functionality needed to distribute
    R code with R packages. We’ll now cover some of the requirements your cluster
    needs to make use of `spark_apply()`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本节完成了我们讨论分发 R 代码所需功能的部分。现在我们将介绍一些您的集群需要满足以使用 `spark_apply()` 的要求。
- en: Cluster Requirements
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群要求
- en: 'The functionality presented in previous chapters did not require special configuration
    of your Spark cluster—as long as you had a properly configured Spark cluster,
    you could use R with it. However, for the functionality presented here, your cluster
    administrator, cloud provider, or you will have to configure your cluster by installing
    either:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中介绍的功能不需要对 Spark 集群进行特殊配置，只要您的 Spark 集群正确配置，就可以使用 R。然而，对于此处介绍的功能，您的集群管理员、云提供商或您将不得不通过安装以下内容来配置您的集群：
- en: R in every node, to execute R code across your cluster
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整个集群中的每个节点都需要安装 R，以便在您的集群中执行 R 代码。
- en: Apache Arrow in every node when using Spark 2.3 or later (Arrow provides performance
    improvements that bring distributed R code closer to native Scala code)
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用 Spark 2.3 或更高版本时，每个节点都需要安装 Apache Arrow（Arrow 提供的性能改进使得分布式 R 代码更接近本地 Scala
    代码）。
- en: Let’s take a look at each requirement to ensure that you properly consider the
    trade-offs or benefits that they provide.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个审视每个要求，确保您正确考虑到它们提供的权衡或好处。
- en: Installing R
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 R
- en: Starting with the first requirement, the R runtime is expected to be preinstalled
    in *every* node in the cluster; this is a requirement specific to `spark_apply()`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从第一个要求开始，期望在集群的*每个*节点预先安装 R 运行时；这是 `spark_apply()` 特定的要求。
- en: Failure to install R in every node will trigger a `Cannot run program, no such
    file or directory` error when you attempt to use `spark_apply()`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未在每个节点安装 R，则在尝试使用 `spark_apply()` 时将触发 `Cannot run program, no such file or
    directory` 错误。
- en: 'Contact your cluster administrator to consider making the R runtime available
    throughout the entire cluster. If R is already installed, you can specify the
    installation path to use with the `spark.r.command` configuration setting, as
    shown here:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 请联系您的集群管理员，考虑让 R 运行时在整个集群中可用。如果 R 已安装，您可以指定安装路径以供 `spark.r.command` 配置设置使用，如下所示：
- en: '[PRE46]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: A *homogeneous cluster* is required since the driver node distributes, and potentially
    compiles, packages to the workers. For instance, the driver and workers must have
    the same processor architecture, system libraries, and so on. This is usually
    the case for most clusters, but might not be true for yours.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*同质集群*是必需的，因为驱动节点分发并可能编译包到工作节点。例如，驱动程序和工作节点必须具有相同的处理器架构、系统库等。对于大多数集群来说，这通常是符合条件的，但对您的集群可能不一定成立。'
- en: 'Different cluster managers, Spark distributions, and cloud providers support
    different solutions to install additional software (like R) across every node
    in the cluster; follow instructions when installing R over each worker node. Here
    are a few examples:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的集群管理器、Spark分布和云提供商支持安装额外软件（如R）在集群的每个节点上；在安装R时，请遵循相应的说明。以下是一些示例：
- en: Spark Standalone
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Standalone
- en: Requires connecting to each machine and installing R; tools like `pssh` allow
    you to run a single installation command against multiple machines.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 需要连接到每台机器并安装R；像`pssh`这样的工具允许您对多台机器运行单个安装命令。
- en: Cloudera
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Cloudera
- en: Provides an R parcel (see the Cloudera blog post [“How to Distribute Your R
    code with sparklyr and Cloudera Data Science Workbench”](http://bit.ly/33JAfu4)),
    which enables R over each worker node.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 提供一个R包（参见Cloudera博客文章[“如何使用sparklyr和Cloudera数据科学工作台分发您的R代码”](http://bit.ly/33JAfu4)），它允许在每个工作节点上使用R。
- en: Amazon EMR
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon EMR
- en: R is preinstalled when starting an EMR cluster as mentioned in [“Amazon”](ch06.html#clusters-amazon-emr).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当提到[“Amazon”](ch06.html#clusters-amazon-emr)时，启动EMR集群时预安装R。
- en: Microsoft HDInsight
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 微软HDInsight
- en: R is preinstalled and no additional steps are needed.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当启动EMR集群时，R是预安装的，不需要额外的步骤。
- en: Livy
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Livy
- en: Livy connections *do not* support distributing packages because the client machine
    where the libraries are precompiled might not have the same processor architecture
    or operating systems as the cluster machines.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Livy连接*不支持*分发软件包，因为客户机（预编译库的位置）可能与集群机器具有不同的处理器架构或操作系统。
- en: Strictly speaking, this completes the last requirement for your cluster. However,
    we strongly recommend you use Apache Arrow with `spark_apply()` to support large-scale
    computation with minimal overhead.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，这完成了集群的最后一个要求。然而，我们强烈建议您使用`spark_apply()`与Apache Arrow支持大规模计算，以减少额外开销。
- en: Apache Arrow
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Arrow
- en: Before introducing Apache Arrow, we’ll discuss how data is stored and transferred
    between Spark and R. As R was designed from its inception to perform fast numeric
    computations to accomplish this, it’s important to figure out the best way to
    store data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍Apache Arrow之前，我们将讨论数据在Spark和R之间是如何存储和传输的。由于R从一开始就设计用于执行快速的数值计算，因此找出存储数据的最佳方式非常重要。
- en: Some computing systems store data internally by row; however, most interesting
    numerical operations require data to be processed by column. For example, calculating
    the mean of a column requires processing each column on its own, not the entire
    row. Spark stores data by default by row, since it’s easier to partition; in contrast,
    R stores data by column. Therefore, something needs to transform both representations
    when data is transferred between Spark and R, as shown in [Figure 11-6](#distributed-r-transformation).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一些计算系统将数据内部存储为行；但是，大多数有趣的数值操作需要按列处理数据。例如，计算列的平均值需要单独处理每一列，而不是整行。Spark默认按行存储数据，因为它更容易分区；相比之下，R按列存储数据。因此，在数据在Spark和R之间传输时，需要某种方式转换这两种表示，如[图 11-6](#distributed-r-transformation)所示。
- en: '![Data Transformation between Spark and R](assets/mswr_1106.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![Spark和R之间的数据转换](assets/mswr_1106.png)'
- en: Figure 11-6\. Data transformation between Spark and R
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-6\. Spark和R之间的数据转换
- en: This transformation from rows to columns needs to happen for each partition.
    In addition, data also must be transformed from Scala’s internal representation
    to R’s internal representation. Apache Arrow reduces these transformations that
    waste a lot of CPU cycles.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分区都需要进行从行到列的转换。此外，数据还必须从Scala的内部表示转换为R的内部表示。Apache Arrow减少了这些浪费大量CPU周期的转换过程。
- en: Apache Arrow is a cross-language development platform for in-memory data. In
    Spark, it speeds up transferring data between Scala and R by defining a common
    data format compatible with many programming languages. Instead of having to transform
    between Scala’s internal representation and R’s, you can use the same structure
    for both languages. In addition, transforming data from row-based storage to columnar
    storage is performed in parallel in Spark, which can be further optimized by using
    the columnar storage formats presented in [Chapter 8](ch08.html#data). The improved
    transformations are shown in [Figure 11-7](#distributed-r-using-arrow).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Arrow是一个跨语言的内存数据开发平台。在Spark中，它通过定义与许多编程语言兼容的通用数据格式来加快Scala和R之间数据传输的速度。不必转换Scala的内部表示和R的，可以同时使用两种语言相同的结构。此外，将数据从基于行的存储转换为列存储是在Spark中并行进行的，可以通过使用第8章中介绍的列存储格式进一步优化。改进的转换如图11-7所示（#distributed-r-using-arrow）。
- en: '![Data Transformation between Spark and R using Arrow](assets/mswr_1107.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![使用Arrow在Spark和R之间进行数据转换](assets/mswr_1107.png)'
- en: Figure 11-7\. Data transformation between Spark and R using Apache Arrow
  id: totrans-191
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-7。使用Apache Arrow在Spark和R之间进行数据转换
- en: Apache Arrow is not required but is strongly recommended while you are working
    with `spark_apply()`. It has been available since Spark 2.3.0; however, it requires
    system administrators to install the Apache Arrow runtime in every node (see [*http://arrow.apache.org/install/)*](http://arrow.apache.org/install/)).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`spark_apply()`时，虽然不需要Apache Arrow，但强烈推荐使用。自Spark 2.3.0起已经可用，但需要系统管理员在每个节点上安装Apache
    Arrow运行时（参见[*http://arrow.apache.org/install/)*](http://arrow.apache.org/install/)）。
- en: 'In addition, to use Apache Arrow with `sparklyr`, you also need to install
    the `arrow` package:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，要在`sparklyr`中使用Apache Arrow，还需要安装`arrow`包：
- en: '[PRE47]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Before we use `arrow`, let’s take a measurement to validate:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`arrow`之前，让我们进行一次测量以验证：
- en: '[PRE48]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'In our particular system, processing 10,000 rows takes about 8 seconds. To
    enable Arrow, simply include the library and use `spark_apply()` as usual. Let’s
    measure how long it takes `spark_apply()` to process 1 million rows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的特定系统中，处理10,000行大约需要8秒钟。要启用Arrow，只需包含该库并像往常一样使用`spark_apply()`。我们来测量一下`spark_apply()`处理100万行所需的时间：
- en: '[PRE50]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In our system, Apache Arrow can process 100 times more data in half the time:
    just 4 seconds.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的系统中，Apache Arrow可以在半时间内处理100倍数据：仅需4秒。
- en: 'Most functionality in `arrow` simply works in the background, improving performance
    and data serialization; however, there is one setting you should be aware of.
    The `spark.sql.execution.arrow.maxRecordsPerBatch` configuration setting specifies
    the default size of each arrow data transfer. It’s shared with other Spark components
    and defaults to 10,000 rows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数`arrow`功能只需在后台运行，改善性能和数据序列化；但是，有一个设置你应该注意。`spark.sql.execution.arrow.maxRecordsPerBatch`配置设置了每个Arrow数据传输的默认大小。它与其他Spark组件共享，默认为10,000行：
- en: '[PRE52]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'You might need to adjust this number based on how much data your system can
    handle, making it smaller for large datasets or bigger for operations that require
    records to be processed together. We can change this setting to 5,000 rows and
    verify the partitions change appropriately:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要根据系统处理能力调整此数字，对于大数据集可能需要减少，对于需要一起处理记录的操作可能需要增加。我们可以将此设置更改为5,000行，并验证分区是否相应更改：
- en: '[PRE54]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: So far we’ve presented use cases, main operations, and cluster requirements.
    Now we’ll discuss the troubleshooting techniques useful when distributing R code.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了用例、主要操作和集群要求。现在我们将讨论在分发R代码时有用的故障排除技术。
- en: Troubleshooting
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除
- en: 'A custom transformation can fail for many reasons. To learn how to troubleshoot
    errors, let’s simulate a problem by triggering an error ourselves:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义转换可能因多种原因而失败。要了解如何排除错误，请通过触发错误来模拟一个问题：
- en: '[PRE56]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Notice that the error message mentions inspecting the logs. When running in
    local mode, you can simply run the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，错误消息提到检查日志。在本地模式下运行时，您可以简单地运行以下命令：
- en: '[PRE58]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This points to the artificial `stop("force an error")` error we mentioned.
    However, if you’re not working in local mode, you will have to retrieve the worker
    logs from your cluster manager. Since this can be cumbersome, one alternative
    is to rerun `spark_apply()` but return the error message yourself:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这指向我们提到的人为`stop("force an error")`错误。但是，如果您不是在本地模式下工作，则需要从集群管理器中检索工作节点日志。由于这可能很麻烦，一个替代方法是重新运行`spark_apply()`，但返回自己的错误消息：
- en: '[PRE60]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Among the other, more advanced troubleshooting techniques applicable to `spark_apply()`,
    the following sections present these techniques in order. You should try to troubleshoot
    by using worker logs first, then identifying partitioning errors, and finally,
    attempting to debug a worker node.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在适用于`spark_apply()`的其他更高级故障排除技术中，以下部分按顺序介绍了这些技术。您应该首先尝试使用工作节点日志来进行故障排除，然后识别分区错误，最后尝试调试工作节点。
- en: Worker Logs
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作节点日志
- en: Whenever `spark_apply()` is executed, information regarding execution is written
    over each worker node. You can use this log to write custom messages to help you
    diagnose and fine-tune your code.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 每当执行`spark_apply()`时，会在每个工作节点上写入有关执行的信息。您可以使用此日志编写自定义消息来帮助诊断和微调您的代码。
- en: 'For instance, suppose that we don’t know what the first column name of `df`
    is. We can write a custom log message executed from the worker nodes using `worker_log()`
    as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们不知道`df`的第一列名称是什么。我们可以使用`worker_log()`从工作节点编写自定义日志消息如下：
- en: '[PRE62]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'When running locally, we can filter the log entries for the worker as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地运行时，我们可以按以下方式筛选工作节点的日志条目：
- en: '[PRE64]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Notice that the logs print our custom log entry, showing that `id` is the name
    of the first column in the given DataFrame.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，日志打印出我们的自定义日志条目，显示`id`是给定DataFrame中第一列的名称。
- en: 'This functionality is useful when troubleshooting errors; for instance, if
    we force an error using the `stop()` function:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能在故障排除时非常有用；例如，如果我们使用`stop()`函数强制引发错误：
- en: '[PRE66]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We will get an error similar to the following:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将收到类似以下的错误：
- en: '[PRE67]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'As suggested by the error, we can look in the worker logs for the specific
    errors as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 正如错误建议的那样，我们可以查看工作节点日志以获取具体的错误信息，如下所示：
- en: '[PRE68]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'This will show an entry containing the error and the call stack:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示一个包含错误和调用堆栈的条目：
- en: '[PRE69]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Notice that `spark_log(sc)` only retrieves the worker logs when you’re using
    local clusters. When running in proper clusters with multiple machines, you will
    have to use the tools and user interface provided by the cluster manager to find
    these log entries.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只有在使用本地集群时，`spark_log(sc)`才会检索到工作节点的日志。在具有多台机器的正式集群中运行时，您必须使用集群管理器提供的工具和用户界面来查找这些日志条目。
- en: Resolving Timeouts
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决超时问题
- en: When you are running with several hundred executors, it becomes more likely
    that some tasks will hang indefinitely. In this situation, most of the tasks in
    your job complete successfully, but a handful of them are still running and do
    not fail or succeed.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用数百个执行器运行时，某些任务无限期挂起的可能性增加。在这种情况下，您的作业中大多数任务会成功完成，但其中少数任务仍在运行且未失败或成功。
- en: 'Suppose that you need to calculate the size of many web pages. You could use
    `spark_apply()` with something similar to:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您需要计算许多网页的大小。您可以使用类似以下内容的`spark_apply()`：
- en: '[PRE70]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Some web pages might not exist or take too long to download. In this case, most
    tasks will succeed, but a few will hang. To prevent these few tasks from blocking
    all computations, you can use the `spark.speculation` Spark setting. With this
    setting enabled, once 75% of all tasks succeed, Spark will look for tasks taking
    longer than the median task execution time and retry. You can use the `spark.speculation.multiplier`
    setting to configure the time multiplier used to determine when a task is running
    slow.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 有些网页可能不存在或下载时间过长。在这种情况下，大多数任务会成功，但少数任务会挂起。为了防止这些少数任务阻塞所有计算，可以使用`spark.speculation`
    Spark设置。启用此设置后，当所有任务成功完成的比例达到75%时，Spark会查找执行时间超过中位数的任务并进行重试。可以使用`spark.speculation.multiplier`设置来配置用于确定任务运行缓慢的时间倍增器。
- en: 'Therefore, for this example, you could configure Spark to retry tasks that
    take four times longer than the median as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本例中，您可以配置Spark以按照四倍于中位数时间的任务进行重试，如下所示：
- en: '[PRE71]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Inspecting Partitions
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查分区
- en: 'If a particular partition fails, you can detect the broken partition by computing
    a digest and then retrieving that particular partition. As usual, you can install
    `digest` from CRAN before connecting to Spark:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特定分区失败，您可以通过计算摘要来检测损坏的分区，然后检索该特定分区。通常情况下，您可以在连接到Spark之前从CRAN安装`digest`：
- en: '[PRE72]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This will add an entry similar to:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这将添加类似以下条目：
- en: '[PRE73]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'When executing this in your cluster, look in the logs for the task that is
    not finishing. Once you have that digest, you can cancel the job. Then you can
    use that digest to retrieve the specific DataFrame to R with something like this:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中执行此操作时，请查看未完成任务的日志。一旦获取了该摘要，您可以取消作业。然后，您可以使用该摘要从R中检索特定的DataFrame，如下所示：
- en: '[PRE74]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: You can then run this in R to troubleshoot further.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以在R中运行此命令以进一步进行故障排除。
- en: Debugging Workers
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调试工作节点
- en: 'A debugger is a tool that lets you execute your code line by line; you can
    use this to troubleshoot `spark_apply()` for local connections. You can start
    `spark_apply()` in debug mode by using the `debug` parameter and then following
    the instructions:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 调试器是一个工具，允许您逐行执行代码；您可以使用此工具来调试本地连接的`spark_apply()`。您可以通过使用`debug`参数启动调试模式，然后按照以下说明操作：
- en: '[PRE76]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: As these instructions indicate, you’ll need to connect “as the worker node”
    from a different R session and then step through the code. This method is less
    straightforward than previous ones, since you’ll also need to step through some
    lines of `sparklyr` code; thus, we only recommend this as a last resort. (You
    can also try the online resources described in [Chapter 2](ch02.html#starting).)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这些说明所示，您需要从不同的R会话中连接“作为工作节点”，然后逐步执行代码。这种方法比以前的方法更不直接，因为您还需要逐行查看一些`sparklyr`代码；因此，我们仅建议作为最后的手段使用。（您也可以尝试[第2章](ch02.html#starting)中描述的在线资源。）
- en: Let’s now wrap up this chapter with a brief recap of the functionality we presented.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过简要回顾本章节所呈现的功能来总结本章节。
- en: Recap
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾
- en: This chapter presented `spark_apply()` as an advanced technique you can use
    to fill gaps in functionality in Spark or its many extensions. We presented example
    use cases for `spark_apply()` to parse data, model in parallel many small datasets,
    perform a grid search, and call web APIs. You learned how partitions relate to
    `spark_apply()`, and how to create custom groups, distribute contextual information
    across all nodes, and troubleshoot problems, limitations, and cluster configuration
    caveats.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了`spark_apply()`作为一种高级技术，您可以用它来填补Spark或其许多扩展功能中的空白。我们展示了`spark_apply()`的示例用例，用于解析数据、并行建模多个小数据集、执行网格搜索和调用Web
    API。您了解了分区如何与`spark_apply()`相关联，以及如何创建自定义组，将上下文信息分发到所有节点，并解决问题、限制和集群配置注意事项。
- en: We also strongly recommended using Apache Arrow as a library when working with
    Spark with R, and presented installation, use cases, and considerations you should
    be aware of.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还强烈建议在使用Spark与R时使用Apache Arrow作为库，并介绍了您应该注意的安装、用例和注意事项。
- en: Up to this point, we’ve only worked with large datasets of static data, which
    doesn’t change over time and remains invariant while we analyze, model, and visualize
    them. In [Chapter 12](ch12.html#streaming), we will introduce techniques to process
    datasets that, in addition to being large, are also growing in such a way that
    they resemble a stream of information.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅使用静态数据的大型数据集进行了工作，这些数据不会随时间而变化，在我们分析、建模和可视化它们时保持不变。在[第12章](ch12.html#streaming)中，我们将介绍处理数据集的技术，这些数据集不仅大，而且以类似信息流的方式增长。
