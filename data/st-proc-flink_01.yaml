- en: Chapter 1\. Introduction to Stateful Stream Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 介绍有状态流处理
- en: Apache Flink is a distributed stream processor with intuitive and expressive
    APIs to implement stateful stream processing applications. It efficiently runs
    such applications at large scale in a fault-tolerant manner. Flink joined the
    Apache Software Foundation as an incubating project in April 2014 and became a
    top-level project in January 2015\. Since its beginning, Flink has had a very
    active and continuously growing community of users and contributors. To date,
    more than five hundred individuals have contributed to Flink, and it has evolved
    into one of the most sophisticated open source stream processing engines as proven
    by its widespread adoption. Flink powers large-scale, business-critical applications
    in many companies and enterprises across different industries and around the globe.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink是一个分布式流处理器，具有直观和表达力强的API，可用于实现有状态的流处理应用程序。它以容错的方式高效地在大规模上运行此类应用程序。Flink于2014年4月作为孵化项目加入Apache软件基金会，并于2015年1月成为顶级项目。自成立以来，Flink拥有一个非常活跃且不断增长的用户和贡献者社区。迄今为止，已有超过五百名个人为Flink做出了贡献，并且它已经发展成为广泛采用的最复杂的开源流处理引擎之一。Flink驱动许多公司和企业跨越不同行业和全球范围内的大规模、业务关键应用程序。
- en: Stream processing technology is becoming more and more popular with companies
    big and small because it provides superior solutions for many established use
    cases such as data analytics, ETL, and transactional applications, but also facilitates
    novel applications, software architectures, and business opportunities. In this
    chapter, we discuss why stateful stream processing is becoming so popular and
    assess its potential. We start by reviewing conventional data application architectures
    and point out their limitations. Next, we introduce application designs based
    on stateful stream processing that exhibit many interesting characteristics and
    benefits over traditional approaches. Finally, we briefly discuss the evolution
    of open source stream processors and help you run a streaming application on a
    local Flink instance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理技术在大公司和小公司中越来越受欢迎，因为它为许多成熟用例（如数据分析、ETL和事务应用程序）提供了优越的解决方案，并促进了新的应用程序、软件架构和业务机会。在本章中，我们讨论为什么有状态流处理如此受欢迎，并评估其潜力。我们首先回顾传统数据应用程序架构，并指出它们的局限性。接下来，我们介绍基于有状态流处理的应用程序设计，展示其在传统方法上的许多有趣特性和优势。最后，我们简要讨论开源流处理器的发展，并帮助您在本地Flink实例上运行流应用程序。
- en: Traditional Data Infrastructures
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统数据基础设施
- en: 'Data and data processing have been omnipresent in businesses for many decades.
    Over the years the collection and usage of data has grown consistently, and companies
    have designed and built infrastructures to manage that data. The traditional architecture
    that most businesses implement distinguishes two types of data processing: transactional
    processing and analytical processing. In this section, we discuss both types and
    how they manage and process data.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，数据和数据处理在企业中普遍存在。多年来，数据的收集和使用一直在持续增长，公司设计和建立了基础设施来管理这些数据。大多数企业实施的传统架构区分两种数据处理类型：事务处理和分析处理。在本节中，我们讨论了这两种类型以及它们如何管理和处理数据。
- en: Transactional Processing
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事务处理
- en: Companies use all kinds of applications for their day-to-day business activities,
    such as enterprise resource planning (ERP) systems, customer relationship management
    (CRM) software, and web-based applications. These systems are typically designed
    with separate tiers for data processing (the application itself) and data storage
    (a transactional database system) as shown in [Figure 1-1](#fig-transactional-apps).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 公司在日常业务活动中使用各种应用程序，如企业资源规划（ERP）系统、客户关系管理（CRM）软件和基于Web的应用程序。这些系统通常设计有独立的数据处理层（应用程序本身）和数据存储层（事务性数据库系统），如[图 1-1](#fig-transactional-apps)所示。
- en: '![](assets/spaf_0101.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0101.png)'
- en: Figure 1-1\. Traditional design of transactional applications that store data
    in a remote database system
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-1 传统的事务性应用程序设计，将数据存储在远程数据库系统中
- en: Applications are usually connected to external services or face human users
    and continuously process incoming events such as orders, email, or clicks on a
    website. When an event is processed, an application reads its state or updates
    it by running transactions against the remote database system. Often, a database
    system serves multiple applications that sometimes access the same databases or
    tables.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序通常连接到外部服务或面向人类用户，并持续处理诸如订单、电子邮件或网站点击等传入事件。当事件被处理时，应用程序读取其状态或通过对远程数据库系统运行事务来更新状态。通常，数据库系统为多个应用程序提供服务，有时这些应用程序可能访问相同的数据库或表。
- en: This application design can cause problems when applications need to evolve
    or scale. Since multiple applications might work on the same data representation
    or share the same infrastructure, changing the schema of a table or scaling a
    database system requires careful planning and a lot of effort. A recent approach
    to overcoming the tight bundling of applications is the microservices design pattern.
    Microservices are designed as small, self-contained, and independent applications.
    They follow the UNIX philosophy of doing a single thing and doing it well. More
    complex applications are built by connecting several microservices with each other
    that only communicate over standardized interfaces such as RESTful HTTP connections.
    Because microservices are strictly decoupled from each other and only communicate
    over well-defined interfaces, each microservice can be implemented with a different
    technology stack including a programming language, libraries, and datastores.
    Microservices and all the required software and services are typically bundled
    and deployed in independent containers. [Figure 1-2](#fig-microservice-arch) depicts
    a microservices architecture.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序需要发展或扩展时，这种应用程序设计可能引发问题。由于多个应用程序可能在相同的数据表示上工作或共享相同的基础设施，更改表的架构或扩展数据库系统需要谨慎规划和大量努力。克服应用程序紧密捆绑的最新方法是微服务设计模式。微服务被设计为小型、独立和独立的应用程序。它们遵循单一事务并做好的UNIX哲学。更复杂的应用程序是通过连接多个只通过标准化接口（例如RESTful
    HTTP连接）进行通信的微服务构建的。因为微服务彼此之间严格解耦并且只通过定义良好的接口进行通信，因此每个微服务都可以使用不同的技术堆栈实现，包括编程语言、库和数据存储。微服务和所有必需的软件和服务通常捆绑并部署在独立的容器中。图1-2展示了一个微服务架构。
- en: '![](assets/spaf_0102.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0102.png)'
- en: Figure 1-2\. A microservices architecture
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2\. 微服务架构
- en: Analytical Processing
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析处理
- en: The data that is stored in the various transactional database systems of a company
    can provide valuable insights about a company’s business operations. For example,
    the data of an order processing system can be analyzed to obtain sales growth
    over time, to identify reasons for delayed shipments, or to predict future sales
    in order to adjust the inventory. However, transactional data is often distributed
    across several disconnected database systems and is more valuable when it can
    be jointly analyzed. Moreover, the data often needs to be transformed into a common
    format.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 公司各种事务性数据库系统中存储的数据可以提供有关公司业务运营的宝贵见解。例如，订单处理系统的数据可以分析以获得销售增长趋势，识别延迟发货原因，或者预测未来销售以调整库存。然而，事务性数据通常分布在多个不连通的数据库系统中，只有在可以联合分析时才更具价值。此外，这些数据通常需要转换为通用格式。
- en: Instead of running analytical queries directly on the transactional databases,
    the data is typically replicated to a data warehouse, a dedicated datastore for
    analytical query workloads. In order to populate a data warehouse, the data managed
    by the transactional database systems needs to be copied to it. The process of
    copying data to the data warehouse is called extract–transform–load (ETL). An
    ETL process extracts data from a transactional database, transforms it into a
    common representation that might include validation, value normalization, encoding,
    deduplication, and schema transformation, and finally loads it into the analytical
    database. ETL processes can be quite complex and often require technically sophisticated
    solutions to meet performance requirements. ETL processes need to run periodically
    to keep the data in the data warehouse synchronized.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不直接在事务性数据库上运行分析查询，而是通常将数据复制到数据仓库，这是专门用于分析查询工作负载的专用数据存储。为了填充数据仓库，事务性数据库系统管理的数据需要复制到其中。将数据复制到数据仓库的过程称为抽取-转换-加载（ETL）。ETL过程从事务性数据库中提取数据，将其转换为通用表示形式，可能包括验证、值规范化、编码、去重和模式转换，最后将其加载到分析数据库中。ETL过程可能非常复杂，通常需要技术上复杂的解决方案以满足性能要求。ETL过程需要定期运行，以保持数据仓库中的数据同步。
- en: Once the data has been imported into the data warehouse it can be queried and
    analyzed. Typically, there are two classes of queries executed on a data warehouse.
    The first type are periodic report queries that compute business-relevant statistics
    such as revenue, user growth, or production output. These metrics are assembled
    into reports that help the management to assess the business’s overall health.
    The second type are ad-hoc queries that aim to provide answers to specific questions
    and support business-critical decisions, for example a query to collect revenue
    numbers and spending on radio commercials to evaluate the effectiveness of a marketing
    campaign. Both kinds of queries are executed by a data warehouse in a batch processing
    fashion, as shown in [Figure 1-3](#fig-traditional-arch).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据导入数据仓库，就可以查询和分析它。通常，在数据仓库上执行两类查询。第一类是周期性报告查询，计算业务相关统计数据，如收入、用户增长或生产产出。这些指标被汇总成报告，帮助管理层评估企业的整体健康状况。第二类是即席查询，旨在回答特定问题并支持业务关键决策，例如查询以收集收入数据和用于广播广告的支出，以评估营销活动的效果。这两种类型的查询由数据仓库以批处理方式执行，如[图1-3](#fig-traditional-arch)所示。
- en: '![](assets/spaf_0103.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0103.png)'
- en: Figure 1-3\. A traditional data warehouse architecture for data analytics
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3\. 传统数据仓库架构用于数据分析
- en: Today, components of the Apache Hadoop ecosystem are integral parts in the IT
    infrastructures of many enterprises. Instead of inserting all data into a relational
    database system, significant amounts of data, such as log files, social media,
    or web click logs, are written into Hadoop’s distributed filesystem (HDFS), S3,
    or other bulk datastores, like Apache HBase, which provide massive storage capacity
    at a small cost. Data that resides in such storage systems can be queried with
    and processed by a SQL-on-Hadoop engine, for example Apache Hive, Apache Drill,
    or Apache Impala. However, the infrastructure remains basically the same as a
    traditional data warehouse architecture.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，Apache Hadoop生态系统的组件已成为许多企业IT基础设施的重要部分。与将所有数据插入关系数据库系统不同，大量数据（如日志文件、社交媒体或Web点击日志）被写入Hadoop的分布式文件系统（HDFS）、S3或其他大容量数据存储，如Apache
    HBase，这些系统提供了巨大的存储能力以较低的成本。驻留在这些存储系统中的数据可以通过SQL-on-Hadoop引擎查询和处理，例如Apache Hive、Apache
    Drill或Apache Impala。然而，基础设施基本上仍然与传统的数据仓库架构相同。
- en: Stateful Stream Processing
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 状态型流处理
- en: Virtually all data is created as continuous streams of events. Think of user
    interactions on websites or in mobile apps, placements of orders, server logs,
    or sensor measurements; all of these are streams of events. In fact, it is difficult
    to find examples of finite, complete datasets that are generated all at once.
    Stateful stream processing is an application design pattern for processing unbounded
    streams of events and is applicable to many different use cases in the IT infrastructure
    of a company. Before we discuss its use cases, we briefly explain how stateful
    stream processing works.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的数据都是作为连续的事件流创建的。想象一下网站或移动应用中的用户交互、订单的下达、服务器日志或传感器测量；所有这些都是事件流。事实上，很难找到一次性生成的有限完整数据集的例子。有状态的流处理是一种处理无界事件流的应用程序设计模式，适用于公司IT基础设施中的许多不同用例。在我们讨论其用例之前，我们简要解释一下有状态的流处理是如何工作的。
- en: Any application that processes a stream of events and does not just perform
    trivial record-at-a-time transformations needs to be stateful, with the ability
    to store and access intermediate data. When an application receives an event,
    it can perform arbitrary computations that involve reading data from or writing
    data to the state. In principle, state can be stored and accessed in many different
    places including program variables, local files, or embedded or external databases.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 任何处理事件流且不仅执行单个记录转换的应用程序都需要是有状态的，具有存储和访问中间数据的能力。当应用程序接收到事件时，它可以执行涉及从状态读取数据或向状态写入数据的任意计算。原则上，状态可以存储和访问在多个不同的地方，包括程序变量、本地文件或嵌入式或外部数据库。
- en: Apache Flink stores the application state locally in memory or in an embedded
    database. Since Flink is a distributed system, the local state needs to be protected
    against failures to avoid data loss in case of application or machine failure.
    Flink guarantees this by periodically writing a consistent checkpoint of the application
    state to a remote and durable storage. State, state consistency, and Flink’s checkpointing
    mechanism will be discussed in more detail in the following chapters, but, for
    now, [Figure 1-4](#fig-stateful-app) shows a stateful streaming Flink application.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink 将应用程序状态存储在内存中或嵌入式数据库中。由于 Flink 是一个分布式系统，本地状态需要受到保护，以避免在应用程序或机器故障时数据丢失。Flink
    通过定期将应用程序状态的一致性检查点写入远程和持久存储来保证这一点。关于状态、状态一致性以及 Flink 的检查点机制将在接下来的章节中详细讨论，但是现在，[图 1-4](#fig-stateful-app)
    展示了一个有状态的流式 Flink 应用程序。
- en: '![](assets/spaf_0104.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0104.png)'
- en: Figure 1-4\. A stateful streaming application
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-4\. 一个有状态的流式应用程序
- en: Stateful stream processing applications often ingest their incoming events from
    an event log. An event log stores and distributes event streams. Events are written
    to a durable, append-only log, which means that the order of written events cannot
    be changed. A stream that is written to an event log can be read many times by
    the same or different consumers. Due to the append-only property of the log, events
    are always published to all consumers in exactly the same order. There are several
    event log systems available as open source software, Apache Kafka being the most
    popular, or as integrated services offered by cloud computing providers.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有状态的流处理应用程序通常从事件日志中接收其传入事件。事件日志存储和分发事件流。事件被写入持久、只追加的日志，这意味着写入事件的顺序不能被更改。写入到事件日志的流可以被同一个或不同的消费者多次读取。由于日志的只追加属性，事件总是以完全相同的顺序发布给所有消费者。有几种事件日志系统作为开源软件提供，Apache
    Kafka 是最流行的，也可以作为云计算提供商提供的集成服务。
- en: Connecting a stateful streaming application running on Flink and an event log
    is interesting for multiple reasons. In this architecture the event log persists
    the input events and can replay them in deterministic order. In case of a failure,
    Flink recovers a stateful streaming application by restoring its state from a
    previous checkpoint and resetting the read position on the event log. The application
    will replay (and fast forward) the input events from the event log until it reaches
    the tail of the stream. This technique is used to recover from failures but can
    also be leveraged to update an application, fix bugs and repair previously emitted
    results, migrate an application to a different cluster, or perform A/B tests with
    different application versions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 将运行在 Flink 上的有状态流应用与事件日志连接起来是有趣的，原因有多种。在这种架构中，事件日志会持久化输入事件，并能够按确定性顺序重放它们。在发生故障时，Flink
    通过从先前检查点恢复其状态并重置事件日志上的读取位置来恢复有状态流应用。应用程序将会从事件日志中回放（和快进）输入事件，直到达到流的末尾。这种技术用于从故障中恢复，但也可以用于更新应用程序、修复错误和修复先前发出的结果、将应用程序迁移到不同的集群，或使用不同的应用程序版本进行A/B测试。
- en: 'As previously stated, stateful stream processing is a versatile and flexible
    design architecture that can be used for many different use cases. In the following,
    we present three classes of applications that are commonly implemented using stateful
    stream processing: (1) event-driven applications, (2) data pipeline applications,
    and (3) data analytics applications.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，有状态流处理是一种多用途且灵活的设计架构，可用于许多不同的用例。以下我们介绍了三类常用状态流处理实现的应用程序：（1）事件驱动应用程序，（2）数据管道应用程序和（3）数据分析应用程序。
- en: Real-World Streaming Use-Cases and Deployments
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真实世界的流处理用例和部署
- en: If you are interested in learning more about real-world use cases and deployments,
    check out Apache Flink’s [Powered By](http://bit.ly/2FOxayO) page and the talk
    recordings and slide decks of [Flink Forward](https://flink-forward.org/) presentations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣了解更多关于真实世界用例和部署的信息，请查看 Apache Flink 的[Powered By](http://bit.ly/2FOxayO)
    页面以及[Flink Forward](https://flink-forward.org/) 的演讲录音和幻灯片。
- en: We describe the classes of applications as distinct patterns to emphasize the
    versatility of stateful stream processing, but most real-world applications share
    the properties of more than one class.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用程序的这些类描述为不同的模式，以强调有状态流处理的多功能性，但大多数真实世界的应用程序共享多个类的属性。
- en: Event-Driven Applications
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事件驱动应用
- en: Event-driven applications are stateful streaming applications that ingest event
    streams and process the events with application-specific business logic. Depending
    on the business logic, an event-driven application can trigger actions such as
    sending an alert or an email or write events to an outgoing event stream to be
    consumed by another event-driven application.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 事件驱动应用是有状态的流应用，它们摄取事件流并使用特定于应用的业务逻辑处理事件。根据业务逻辑，事件驱动应用可以触发诸如发送警报或电子邮件的操作，或者将事件写入出站事件流以供另一个事件驱动应用消费。
- en: 'Typical use cases for event-driven applications include:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 事件驱动应用的典型用例包括：
- en: Real-time recommendations (e.g., for recommending products while customers browse
    a retailer’s website)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时推荐（例如在客户浏览零售商网站时推荐产品）
- en: Pattern detection or complex event processing (e.g., for fraud detection in
    credit card transactions)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式检测或复杂事件处理（例如用于信用卡交易中的欺诈检测）
- en: Anomaly detection (e.g., to detect attempts to intrude a computer network)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测（例如检测企图入侵计算机网络的尝试）
- en: Event-driven applications are an evolution of microservices. They communicate
    via event logs instead of REST calls and hold application data as local state
    instead of writing it to and reading it from an external datastore, such as a
    relational database or key-value store. [Figure 1-5](#fig-event-driven-apps) shows
    a service architecture composed of event-driven streaming applications.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 事件驱动应用是微服务的一种演进形式。它们通过事件日志进行通信，而不是通过 REST 调用，并将应用数据作为本地状态而不是写入和读取外部数据存储，比如关系型数据库或键值存储。[图 1-5](#fig-event-driven-apps)
    展示了由事件驱动流应用组成的服务架构。
- en: '![](assets/spaf_0105.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0105.png)'
- en: Figure 1-5\. An event-driven application architecture
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-5\. 事件驱动应用架构
- en: The applications in [Figure 1-5](#fig-event-driven-apps) are connected by event
    logs. One application emits its output to an event log and another application
    consumes the events the other application emitted. The event log decouples senders
    and receivers and provides asynchronous, nonblocking event transfer. Each application
    can be stateful and can locally manage its own state without accessing external
    datastores. Applications can also be individually operated and scaled.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-5](#fig-event-driven-apps)中的应用程序通过事件日志连接。一个应用程序将其输出发出到事件日志，另一个应用程序消耗另一个应用程序发出的事件。事件日志解耦了发送方和接收方，并提供了异步、非阻塞的事件传输。每个应用程序可以是有状态的，并且可以在不访问外部数据存储的情况下本地管理其自己的状态。应用程序也可以单独操作和扩展。'
- en: Event-driven applications offer several benefits compared to transactional applications
    or microservices. Local state access provides very good performance compared to
    reading and writing queries against remote datastores. Scaling and fault tolerance
    are handled by the stream processor, and by leveraging an event log as the input
    source the complete input of an application is reliably stored and can be deterministically
    replayed. Furthermore, Flink can reset the state of an application to a previous
    savepoint, making it possible to evolve or rescale an application without losing
    its state.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 事件驱动的应用程序相比事务性应用程序或微服务具有几个优点。与远程数据存储的读写查询相比，本地状态访问提供了非常好的性能。通过流处理器处理扩展性和容错性，通过利用事件日志作为输入源，应用程序的完整输入可靠地存储，并可以确定性地重放。此外，Flink可以将应用程序的状态重置到先前的保存点，从而可以在不丢失状态的情况下演进或重新调整应用程序。
- en: Event-driven applications have quite high requirements on the stream processor
    that runs them. Not all stream processors are equally well-suited to run event-driven
    applications. The expressiveness of the API and the quality of state handling
    and event-time support determine the business logic that can be implemented and
    executed. This aspect depends on the APIs of the stream processor, what kinds
    of state primitives it provides, and the quality of its support for event-time
    processing. Moreover, exactly-once state consistency and the ability to scale
    an application are fundamental requirements for event-driven applications. Apache
    Flink checks all these boxes and is a very good choice to run this class of applications.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 事件驱动的应用程序对运行它们的流处理器有相当高的要求。并非所有流处理器都同样适合运行事件驱动的应用程序。API的表达能力以及状态处理和事件时间支持的质量决定了可以实现和执行的业务逻辑。这一方面取决于流处理器的API、它提供的状态基元以及其对事件时间处理支持的质量。此外，一致的状态一致性和能够扩展应用程序是事件驱动应用程序的基本要求。Apache
    Flink符合所有这些要求，是运行这类应用程序的非常好的选择。
- en: Data Pipelines
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管道
- en: Today’s IT architectures include many different datastores, such as relational
    and special-purpose database systems, event logs, distributed filesystems, in-memory
    caches, and search indexes. All of these systems store data in different formats
    and data structures that provide the best performance for their specific access
    pattern. It is common that companies store the same data in multiple different
    systems to improve the performance of data accesses. For example, information
    for a product that is offered in a webshop can be stored in a transactional database,
    a web cache, and a search index. Due to this replication of data, the data stores
    must be kept in sync.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当今的IT架构包括许多不同的数据存储，例如关系型数据库系统、专用数据库系统、事件日志、分布式文件系统、内存缓存和搜索索引。所有这些系统都以不同的格式和数据结构存储数据，以便为它们特定的访问模式提供最佳性能。公司通常会将相同的数据存储在多个不同的系统中，以提高数据访问的性能。例如，在网店中提供的产品信息可以存储在事务性数据库、Web缓存和搜索索引中。由于数据的复制，数据存储必须保持同步。
- en: A traditional approach to synchronize data in different storage systems is periodic
    ETL jobs. However, they do not meet the latency requirements for many of today’s
    use cases. An alternative is to use an event log to distribute updates. The updates
    are written to and distributed by the event log. Consumers of the log incorporate
    the updates into the affected data stores. Depending on the use case, the transferred
    data may need to be normalized, enriched with external data, or aggregated before
    it is ingested by the target data store.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同存储系统中同步数据的传统方法是定期的ETL作业。然而，它们无法满足当今许多用例的延迟要求。一个替代方案是使用事件日志来分发更新。更新被写入和分发到事件日志。日志的消费者将更新合并到受影响的数据存储中。根据用例的不同，传输的数据可能需要在被目标数据存储摄取之前进行规范化、丰富化外部数据或聚合。
- en: Ingesting, transforming, and inserting data with low latency is another common
    use case for stateful stream processing applications. This type of application
    is called a data pipeline. Data pipelines must be able to process large amounts
    of data in a short time. A stream processor that operates a data pipeline should
    also feature many source and sink connectors to read data from and write data
    to various storage systems. Again, Flink does all of this.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 低延迟摄取、转换和插入数据是有状态流处理应用程序的另一个常见用例。这种应用称为数据管道。数据管道必须能够在短时间内处理大量数据。操作数据管道的流处理器还应具备许多源和汇接器连接器，用于从各种存储系统读取数据和写入数据。同样，Flink可以做到这一切。
- en: Streaming Analytics
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式分析
- en: ETL jobs periodically import data into a datastore and the data is processed
    by ad-hoc or scheduled queries. This is batch processing regardless of whether
    the architecture is based on a data warehouse or components of the Hadoop ecosystem.
    While periodically loading data into a data analysis system has been the state
    of the art for many years, it adds considerable latency to the analytics pipeline.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 定期的ETL作业将数据导入数据存储，并由即席或定期查询处理。无论架构是基于数据仓库还是Hadoop生态系统的组件，这都是批处理。多年来，周期性加载数据到数据分析系统一直是最先进的技术，但会给分析流程增加相当的延迟。
- en: Depending on the scheduling intervals it may take hours or days until a data
    point is included in a report. To some extent, the latency can be reduced by importing
    data into the datastore with a data pipeline application. However, even with continuous
    ETL there will always be a delay until an event is processed by a query. While
    this kind of delay may have been acceptable in the past, applications today must
    be able to collect data in real-time and immediately act on it (e.g., by adjusting
    to changing conditions in a mobile game or by personalizing user experiences for
    an online retailer).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 根据调度间隔，一个数据点可能需要几小时或几天才能包含在报告中。在某种程度上，可以通过使用数据管道应用程序将数据导入数据存储来减少延迟。然而，即使进行持续的ETL，事件被查询处理之前总会有延迟。尽管这种延迟在过去可能是可以接受的，但现今的应用程序必须能够实时收集数据并立即对其进行操作（例如通过调整移动游戏中的变化条件或为在线零售商个性化用户体验）。
- en: Instead of waiting to be periodically triggered, a streaming analytics application
    continuously ingests streams of events and updates its result by incorporating
    the latest events with low latency. This is similar to the maintenance techniques
    database systems use to update materialized views. Typically, streaming applications
    store their result in an external data store that supports efficient updates,
    such as a database or key-value store. The live updated results of a streaming
    analytics application can be used to power dashboard applications as shown in
    [Figure 1-6](#fig-streaming-analytics).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 流式分析应用程序不等待周期性触发，而是持续摄取事件流，并通过低延迟将最新事件集成到其结果中。这类似于数据库系统用来更新物化视图的维护技术。通常，流式应用程序将其结果存储在支持高效更新的外部数据存储中，例如数据库或键值存储。流式分析应用程序的实时更新结果可用于驱动仪表盘应用程序，如[图1-6](#fig-streaming-analytics)所示。
- en: '![](assets/spaf_0106.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0106.png)'
- en: Figure 1-6\. A streaming analytics application
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-6\. 流式分析应用程序
- en: Besides the much shorter time needed for an event to be incorporated into an
    analytics result, there is another, less obvious, advantage of streaming analytics
    applications. Traditional analytics pipelines consist of several individual components
    such as an ETL process, a storage system, and in the case of a Hadoop-based environment,
    a data processor and scheduler to trigger jobs or queries. In contrast, a stream
    processor that runs a stateful streaming application takes care of all these processing
    steps, including event ingestion, continuous computation including state maintenance,
    and updating the results. Moreover, the stream processor can recover from failures
    with exactly-once state consistency guarantees and can adjust the compute resources
    of an application. Stream processors like Flink also support event-time processing
    to produce correct and deterministic results and the ability to process large
    amounts of data in little time.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 除了事件进入分析结果所需的时间大大缩短外，流分析应用程序还有另一个不太明显的优势。传统的分析流水线由多个单独的组件组成，如ETL过程、存储系统，在基于Hadoop的环境中还包括数据处理器和调度器以触发作业或查询。相比之下，运行有状态流应用程序的流处理器负责所有这些处理步骤，包括事件摄入、连续计算（包括状态维护）和更新结果。此外，流处理器可以通过确保一次性状态一致性保证从故障中恢复，并且可以调整应用程序的计算资源。像Flink这样的流处理器还支持事件时间处理以生成正确和确定性的结果，并能在短时间内处理大量数据。
- en: 'Streaming analytics applications are commonly used for:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 流分析应用程序通常用于：
- en: Monitoring the quality of cellphone networks
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手机网络质量的监测
- en: Analyzing user behavior in mobile applications
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析移动应用程序中的用户行为
- en: Ad-hoc analysis of live data in consumer technology
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在消费技术中对实时数据进行即席分析
- en: Although we don’t cover it here, Flink also provides support for analytical
    SQL queries over streams.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在这里没有涉及，但Flink还支持对流进行分析SQL查询。
- en: The Evolution of Open Source Stream Processing
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源流处理技术的演变
- en: 'Data stream processing is not a novel technology. Some of the first research
    prototypes and commercial products date back to the late 1990s. However, the growing
    adoption of stream processing technology in the recent past has been driven to
    a large extent by the availability of mature open source stream processors. Today,
    distributed open source stream processors power business-critical applications
    in many enterprises across different industries such as (online) retail, social
    media, telecommunication, gaming, and banking. Open source software is a major
    driver of this trend, mainly due to two reasons:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流处理并不是一项新技术。一些最早的研究原型和商业产品可以追溯到上世纪90年代末。然而，最近过去的流处理技术的广泛采用在很大程度上是由成熟的开源流处理器的可用性推动的。今天，分布式开源流处理器在许多企业的关键业务应用中发挥着作用，涵盖不同行业，如（在线）零售、社交媒体、电信、游戏和银行业。开源软件是这一趋势的主要推动力，主要原因有两点：
- en: Open source stream processing software is a commodity that everybody can evaluate
    and use.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开源流处理软件是每个人都可以评估和使用的商品。
- en: Scalable stream processing technology is rapidly maturing and evolving due to
    the efforts of many open source communities.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于许多开源社区的努力，可伸缩的流处理技术正在迅速成熟和发展。
- en: The Apache Software Foundation alone is the home of more than a dozen projects
    related to stream processing. New distributed stream processing projects are continuously
    entering the open source stage and are challenging the state of the art with new
    features and capabilities. Open source communities are constantly improving the
    capabilities of their projects and are pushing the technical boundaries of stream
    processing. We will take a brief look into the past to see where open source stream
    processing came from and where it is today.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅Apache软件基金会就拥有超过十几个与流处理相关的项目。新的分布式流处理项目不断进入开源阶段，并通过新的功能和能力挑战现有技术的水平。开源社区不断改进其项目的能力，并推动流处理技术的技术边界。我们将简要回顾一下过去，看看开源流处理技术的起源及其现状。
- en: A Bit of History
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一点历史
- en: The first generation of distributed open source stream processors (2011) focused
    on event processing with millisecond latencies and provided guarantees against
    loss of events in the case of failures. These systems had rather low-level APIs
    and did not provide built-in support for accurate and consistent results of streaming
    applications because the results depended on the timing and order of arriving
    events. Moreover, even though events were not lost, they could be processed more
    than once. In contrast to batch processors, the first open source stream processors
    traded result accuracy for better latency. The observation that data processing
    systems (at this point in time) could either provide fast or accurate results
    led to the design of the so-called lambda architecture, which is depicted in [Figure 1-7](#fig-lambda-arch).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第一代分布式开源流处理器（2011）专注于事件处理，具有毫秒级延迟，并在发生故障时保证不丢失事件。这些系统具有相对低级的API，并且不提供内置支持以获取流应用程序的准确和一致的结果，因为结果取决于事件到达的时间和顺序。此外，即使事件未丢失，它们也可能被处理多次。与批处理处理器相比，首批开源流处理器通过更好的延迟来交换结果的准确性。数据处理系统（在这个时间点上）只能提供快速或准确的结果的观察结果导致了所谓的Lambda架构的设计，如[图 1-7](#fig-lambda-arch)所示。
- en: '![](assets/spaf_0107.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0107.png)'
- en: Figure 1-7\. The lambda architecture
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-7\. Lambda架构
- en: The lambda architecture augments the traditional periodic batch processing architecture
    with a speed layer that is powered by a low-latency stream processor. Data arriving
    at the lambda architecture is ingested by the stream processor and also written
    to batch storage. The stream processor computes approximated results in near real
    time and writes them into a speed table. The batch processor periodically processes
    the data in batch storage, writes the exact results into a batch table, and drops
    the corresponding inaccurate results from the speed table. Applications consume
    the results by merging approximated results from the speed table and the accurate
    results from the batch table.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda架构通过一个低延迟流处理器增加了传统周期批处理架构的速度层。到达Lambda架构的数据由流处理器摄取，同时写入批处理存储。流处理器实时计算近似结果并将其写入速度表。批处理器定期处理批处理存储中的数据，将准确结果写入批处理表，并从速度表中删除相应的不准确结果。应用程序通过合并速度表中的近似结果和批处理表中的准确结果来消费这些结果。
- en: The lambda architecture is no longer state of the art, but is still used in
    many places. The original goals of this architecture were to improve the high
    result latency of the original batch analytics architecture. However, it has a
    few notable drawbacks. First of all, it requires two semantically equivalent implementations
    of the application logic for two separate processing systems with different APIs.
    Second, the results computed by the stream processor are only approximate. Third,
    the lambda architecture is hard to set up and maintain.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda架构已不再是技术的最前沿，但仍然在许多地方使用。这种架构的最初目标是改善原始批处理分析架构的高结果延迟。然而，它也有一些显著的缺点。首先，它需要两个语义等效的应用逻辑实现，分别针对具有不同API的两个独立处理系统。其次，流处理器计算的结果仅是近似的。第三，Lambda架构难以设置和维护。
- en: Improving on the first generation, the next generation of distributed open source
    stream processors (2013) provided better failure guarantees and ensured that in
    case of a failure each input record affects the result exactly once. In addition,
    programming APIs evolved from rather low-level operator interfaces to high-level
    APIs with more built-in primitives. However, some improvements such as higher
    throughput and better failure guarantees came at the cost of increasing processing
    latencies from milliseconds to seconds. Moreover, results were still dependent
    on timing and order of arriving events.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一代基础上改进，下一代分布式开源流处理器（2013）提供了更好的故障保证，并确保在故障情况下每个输入记录对结果的影响恰好一次。此外，编程API从相对低级的运算符界面发展到具有更多内置原语的高级API。然而，一些改进，如更高的吞吐量和更好的故障保证，以增加处理延迟（从毫秒到秒）为代价。此外，结果仍然依赖于到达事件的时间和顺序。
- en: The third generation of distributed open source stream processors (2015) addressed
    the dependency of results on the timing and order of arriving events. In combination
    with exactly-once failure semantics, systems of this generation are the first
    open source stream processors capable of computing consistent and accurate results.
    By only computing results based on actual data, these systems are also able to
    process historical data in the same way as “live” data. Another improvement was
    the dissolution of the latency/throughput tradeoff. While previous stream processors
    only provide either high throughput or low latency, systems of the third generation
    are able to serve both ends of the spectrum. Stream processors of this generation
    made the lambda architecture obsolete.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第三代分布式开源流处理器（2015年）解决了结果依赖于到达事件的时间和顺序的问题。结合精确一次的失败语义，这一代系统是第一个能够计算一致和准确结果的开源流处理器。通过仅基于实际数据计算结果，这些系统还能以与“实时”数据相同的方式处理历史数据。另一个改进是取消了延迟/吞吐量的折衷。第三代系统能够同时提供高吞吐量和低延迟，使λ架构过时。
- en: In addition to the system properties discussed so far, such as failure tolerance,
    performance, and result accuracy, stream processors have also continuously added
    new operational features such as highly available setups, tight integration with
    resource managers, such as YARN or Kubernetes, and the ability to dynamically
    scale streaming applications. Other features include support to upgrade application
    code or migrate a job to a different cluster or a new version of the stream processor
    without losing the current state.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 除了迄今为止讨论的系统特性（如容错性，性能和结果准确性）之外，流处理器还不断增加新的操作特性，例如高度可用的设置，与资源管理器（如YARN或Kubernetes）的紧密集成，以及动态扩展流处理应用程序的能力。其他特性包括支持升级应用程序代码或将作业迁移到不同的集群或流处理器的新版本，而不会丢失当前状态。
- en: A Quick Look at Flink
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速查看Flink
- en: 'Apache Flink is a third-generation distributed stream processor with a competitive
    feature set. It provides accurate stream processing with high throughput and low
    latency at scale. In particular, the following features make Flink stand out:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink是一个具有竞争力特性集的第三代分布式流处理器。它在大规模情况下提供精确的流处理，具有高吞吐量和低延迟。特别是以下特性使Flink脱颖而出：
- en: Event-time and processing-time semantics. Event-time semantics provide consistent
    and accurate results despite out-of-order events. Processing-time semantics can
    be used for applications with very low latency requirements.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件时间和处理时间语义。事件时间语义提供了一致和准确的结果，尽管事件顺序可能混乱。处理时间语义可用于具有非常低延迟要求的应用程序。
- en: Exactly-once state consistency guarantees.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确一次的状态一致性保证。
- en: Millisecond latencies while processing millions of events per second. Flink
    applications can be scaled to run on thousands of cores.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理每秒数百万事件的同时毫秒级延迟。Flink应用程序可以扩展到数千个核心。
- en: Layered APIs with varying tradeoffs for expressiveness and ease of use. This
    book covers the DataStream API and process functions, which provide primitives
    for common stream processing operations, such as windowing and asynchronous operations,
    and interfaces to precisely control state and time. Flink’s relational APIs, SQL
    and the LINQ-style Table API, are not discussed in this book.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层API具有不同的表达能力和易用性折衷。本书涵盖了DataStream API和处理函数，提供了常见流处理操作的原语，如窗口操作和异步操作，并提供了精确控制状态和时间的接口。本书不讨论Flink的关系型API，即SQL和LINQ风格的Table
    API。
- en: Connectors to the most commonly used storage systems such as Apache Kafka, Apache
    Cassandra, Elasticsearch, JDBC, Kinesis, and (distributed) filesystems such as
    HDFS and S3.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接器与最常用的存储系统，例如Apache Kafka，Apache Cassandra，Elasticsearch，JDBC，Kinesis，以及（分布式）文件系统，如HDFS和S3。
- en: Ability to run streaming applications 24/7 with very little downtime due to
    its highly available setup (no single point of failure), tight integration with
    Kubernetes, YARN, and Apache Mesos, fast recovery from failures, and the ability
    to dynamically scale jobs.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其高度可用的设置（无单点故障），与Kubernetes、YARN和Apache Mesos的紧密集成，故障恢复快，以及动态扩展作业的能力，能够24/7运行流处理应用程序，几乎没有停机时间。
- en: Ability to update the application code of jobs and migrate jobs to different
    Flink clusters without losing the state of the application.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够更新作业的应用程序代码，并将作业迁移到不同的Flink集群，而不会丢失应用程序的状态。
- en: Detailed and customizable collection of system and application metrics to identify
    and react to problems ahead of time.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 详细和可定制的系统和应用程序指标收集，以便及时识别和应对问题。
- en: Last but not least, Flink is also a full-fledged batch processor.^([1](ch01.html#idm45499021586280))
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，Flink 还是一个全功能的批处理处理器。^([1](ch01.html#idm45499021586280))
- en: In addition to these features, Flink is a very developer-friendly framework
    due to its easy-to-use APIs. The embedded execution mode starts an application
    and the whole Flink system in a single JVM process, which can be used to run and
    debug Flink jobs within an IDE. This feature comes in handy when developing and
    testing Flink applications.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些特性外，由于其易于使用的 API，Flink 也是一个非常友好的开发框架。嵌入式执行模式在单个 JVM 进程中启动应用程序和整个 Flink 系统，可用于在
    IDE 中运行和调试 Flink 作业。在开发和测试 Flink 应用程序时，这一功能非常实用。
- en: Running Your First Flink Application
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行您的第一个 Flink 应用程序
- en: In the following, we will guide you through the process of starting a local
    cluster and executing a streaming application to give you a first look at Flink.
    The application we are going to run converts and aggregates randomly generated
    temperature sensor readings by time. For this example, your system needs Java
    8 installed. We describe the steps for a UNIX environment, but if you are running
    Windows, we recommend setting up a virtual machine with Linux, Cygwin (a Linux
    environment for Windows), or the Windows Subsystem for Linux, introduced with
    Windows 10\. The following steps show you how to start a local Flink cluster and
    submit an application for execution.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将指导您启动本地集群并执行流应用程序，让您首次了解 Flink。我们将要运行的应用程序会按时间转换和聚合随机生成的温度传感器读数。在这个例子中，您的系统需要安装
    Java 8。我们描述了 UNIX 环境的步骤，但如果您使用 Windows，我们建议设置一个带有 Linux、Cygwin（Windows 上的 Linux
    环境）或 Windows Subsystem for Linux（Windows 10 引入的 Linux 子系统）的虚拟机。以下步骤展示了如何启动本地 Flink
    集群并提交应用程序进行执行。
- en: Go to the [Apache Flink webpage](http://flink.apache.org) and download the Hadoop-free
    binary distribution of Apache Flink 1.7.1 for Scala 2.12.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 [Apache Flink 网页](http://flink.apache.org) 并下载适用于 Scala 2.12 的 Apache Flink
    1.7.1 的无 Hadoop 二进制发行版。
- en: 'Extract the archive file:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解压存档文件：
- en: '[PRE0]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Start a local Flink cluster:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动本地 Flink 集群：
- en: '[PRE1]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Open Flink’s Web UI by entering the URL **`http://localhost:8081`** in your
    browser. As shown in [Figure 1-8](#fig-screenshot-dashboard), you will see some
    statistics about the local Flink cluster you just started. It will show that a
    single TaskManager (Flink’s worker processes) is connected and that a single task
    slot (resource units provided by a TaskManager) is available.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器中输入 **`http://localhost:8081`** 打开 Flink 的 Web UI。如 [图 1-8](#fig-screenshot-dashboard)
    所示，您将看到有关刚启动的本地 Flink 集群的一些统计信息。它将显示一个单独的 TaskManager（Flink 的工作进程）已连接，一个单独的任务槽（由
    TaskManager 提供的资源单元）可用。
- en: '![](assets/spaf_0108.png)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](assets/spaf_0108.png)'
- en: Figure 1-8\. Screenshot of Apache Flink’s web dashboard showing the overview
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-8\. 显示 Apache Flink web 仪表板概览的屏幕截图
- en: 'Download the JAR file that includes examples in this book:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载包含本书示例的 JAR 文件：
- en: '[PRE2]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can also build the JAR file yourself by following the steps in the repository’s
    README file.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以按照存储库的 README 文件中的步骤自行构建 JAR 文件。
- en: 'Run the example on your local cluster by specifying the application’s entry
    class and JAR file:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过指定应用程序的入口类和 JAR 文件在本地集群上运行示例：
- en: '[PRE3]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Inspect the web dashboard. You should see a job listed under “Running Jobs.”
    If you click on that job, you will see the dataflow and live metrics about the
    operators of the running job similar to the screenshot in [Figure 1-9](#fig-screenshot-job).
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 web 仪表板。您应该看到“正在运行的作业”下列出的一个作业。如果点击该作业，您将看到与 [图 1-9](#fig-screenshot-job)
    类似的有关正在运行作业的数据流和实时指标的信息。
- en: '![](assets/spaf_0109.png)'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](assets/spaf_0109.png)'
- en: Figure 1-9\. Screenshot of Apache Flink’s web dashboard showing a running job
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-9\. 显示 Apache Flink web 仪表板上运行作业的屏幕截图
- en: 'The output of the job is written to the standard out of Flink’s worker process,
    which is redirected into a file in the *./log* folder by default. You can monitor
    the constantly produced output using the `tail` command as follows:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作业的输出被写入 Flink 工作进程的标准输出，通常被重定向到 *./log* 文件夹中的文件。您可以使用 `tail` 命令监控持续产生的输出：
- en: '[PRE4]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You should see lines like this being written to the file:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该看到类似这样的行被写入文件：
- en: '[PRE5]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first field of the `SensorReading` is a `sensorId`, the second field is
    the timestamp in milliseconds since `1970-01-01-00:00:00.000`, and the third field
    is an average temperature computed over 5 seconds.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`SensorReading` 的第一个字段是 `sensorId`，第二个字段是自 `1970-01-01-00:00:00.000` 以来的毫秒时间戳，第三个字段是计算出的5秒内的平均温度。'
- en: Since you are running a streaming application, the application will continue
    to run until you cancel it. You can do this by selecting the job in the web dashboard
    and clicking the Cancel button at the top of the page.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于您正在运行流应用程序，该应用程序将继续运行，直到您取消它。您可以通过在 Web 仪表板中选择作业并单击页面顶部的“取消”按钮来执行此操作。
- en: 'Finally, you should stop the local Flink cluster:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您应该停止本地 Flink 集群的运行：
- en: '[PRE6]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That’s it. You just installed and started your first local Flink cluster and
    ran your first Flink DataStream API program! Of course, there is much more to
    learn about stream processing with Apache Flink and that’s what this book is about.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止。您刚刚安装并启动了第一个本地 Flink 集群，并运行了第一个 Flink DataStream API 程序！当然，关于使用 Apache
    Flink 进行流处理还有更多知识可以学习，这也是本书的主题。
- en: Summary
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced stateful stream processing, discussed its use
    cases, and had a first look at Apache Flink. We started with a recap of traditional
    data infrastructures, how business applications are commonly designed, and how
    data is collected and analyzed in most companies today. Then we introduced the
    idea of stateful stream processing and explained how it addresses a wide spectrum
    of use cases, ranging from business applications and microservices to ETL and
    data analytics. We discussed how open source stream processing systems have evolved
    since their inception in the early 2010s and how stream processing became a viable
    solution for many use cases of today’s businesses. Finally, we took a look at
    Apache Flink and the extensive features it offers and showed how to install a
    local Flink setup and run a first stream processing application.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了有状态流处理，讨论了其用例，并初步了解了 Apache Flink。我们从传统数据基础设施的概述开始，讨论了业务应用程序的常见设计方式，以及如何在今天大多数公司中收集和分析数据。然后，我们介绍了有状态流处理的概念，并解释了它如何解决从业务应用程序和微服务到
    ETL 和数据分析的广泛用例。我们讨论了自2010年代初以来开源流处理系统的发展以及流处理如何成为今天许多业务用例的可行解决方案。最后，我们深入了解了 Apache
    Flink 及其提供的广泛功能，并展示了如何安装本地 Flink 环境并运行第一个流处理应用程序。
- en: ^([1](ch01.html#idm45499021586280-marker)) Flink’s batch processing API, the
    DataSet API, and its operators are separate from their corresponding streaming
    counterparts. However, the vision of the Flink community is to treat batch processing
    as a special case of stream processing—the processing of bounded streams. An ongoing
    effort of the Flink community is to evolve Flink toward a system with a truly
    unified batch and streaming API and runtime.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.html#idm45499021586280-marker)) Flink 的批处理 API，即 DataSet API 及其运算符与其相应的流处理
    API 是分开的。然而，Flink 社区的愿景是将批处理视为流处理的一种特殊情况——有界流的处理。Flink 社区的持续努力是将 Flink 发展成一个具有真正统一的批处理和流处理
    API 及运行时的系统。
