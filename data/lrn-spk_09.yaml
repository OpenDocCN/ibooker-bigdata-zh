- en: Chapter 8\. Structured Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章\. 结构化流处理
- en: In earlier chapters, you learned how to use structured APIs to process very
    large but finite volumes of data. However, often data arrives continuously and
    needs to be processed in a real-time manner. In this chapter, we will discuss
    how the same Structured APIs can be used for processing data streams as well.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，您学习了如何使用结构化 API 处理非常大但有限的数据量。然而，数据通常是连续到达并需要实时处理。在本章中，我们将讨论如何使用相同的结构化
    API 处理数据流。
- en: Evolution of the Apache Spark Stream Processing Engine
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 流处理引擎的演变
- en: Stream processing is defined as the continuous processing of endless streams
    of data. With the advent of big data, stream processing systems transitioned from
    single-node processing engines to multiple-node, distributed processing engines.
    Traditionally, distributed stream processing has been implemented with a *record-at-a-time
    processing model*, as illustrated in [Figure 8-1](#traditional_record_at_a_time_processing).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理被定义为对无休止数据流的连续处理。随着大数据的到来，流处理系统从单节点处理引擎过渡到多节点、分布式处理引擎。传统上，分布式流处理是使用*逐记录处理模型*实现的，如图 8-1所示。
- en: '![Traditional record-at-a-time processing model](assets/lesp_0801.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![传统逐记录处理模型](assets/lesp_0801.png)'
- en: Figure 8-1\. Traditional record-at-a-time processing model
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-1\. 传统逐记录处理模型
- en: The processing pipeline is composed of a directed graph of nodes, as shown in
    [Figure 8-1](#traditional_record_at_a_time_processing); each node continuously
    receives one record at a time, processes it, and then forwards the generated record(s)
    to the next node in the graph. This processing model can achieve very low latencies—that
    is, an input record can be processed by the pipeline and the resulting output
    can be generated within milliseconds. However, this model is not very efficient
    at recovering from node failures and straggler nodes (i.e., nodes that are slower
    than others); it can either recover from a failure very fast with a lot of extra
    failover resources, or use minimal extra resources but recover slowly.^([1](ch08.html#ch01fn9))
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 处理管道由节点的有向图组成，如[图 8-1](#traditional_record_at_a_time_processing)所示；每个节点连续接收一个记录，处理它，然后将生成的记录转发给图中的下一个节点。这种处理模型可以实现非常低的延迟——即，输入记录可以在毫秒内通过管道进行处理并生成结果输出。然而，这种模型在从节点故障和落后节点（即比其他节点慢的节点）中恢复时效率不高；它可以通过大量额外的故障转移资源快速恢复故障，或者使用最少的额外资源但恢复速度较慢。^([1](ch08.html#ch01fn9))
- en: The Advent of Micro-Batch Stream Processing
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微批次流处理的出现
- en: This traditional approach was challenged by Apache Spark when it introduced
    Spark Streaming (also called DStreams). It introduced the idea of *micro-batch
    stream processing*, where the streaming computation is modeled as a continuous
    series of small, map/reduce-style batch processing jobs (hence, “micro-batches”)
    on small chunks of the stream data. This is illustrated in [Figure 8-2](#structured_streaming_uses_a_micro_batch).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Apache Spark 引入 Spark Streaming（也称为 DStreams）时，挑战了这种传统方法。它引入了*微批次流处理*的概念，其中流处理被建模为连续的小型
    map/reduce 风格的批处理作业（因此称为“微批次”），针对流数据的小块。如图 8-2所示。
- en: '![Structured Streaming uses a micro-batch processing model](assets/lesp_0802.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![结构化流使用微批处理模型](assets/lesp_0802.png)'
- en: Figure 8-2\. Structured Streaming uses a micro-batch processing model
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-2\. 结构化流使用微批处理模型
- en: 'As shown here, Spark Streaming divides the data from the input stream into,
    say, 1-second micro-batches. Each batch is processed in the Spark cluster in a
    distributed manner with small deterministic tasks that generate the output in
    micro-batches. Breaking down the streaming computation into these small tasks
    gives us two advantages over the traditional, continuous-operator model:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所示，Spark Streaming 将输入流的数据划分为，比如，1 秒的微批次。每个批次在 Spark 集群中以分布式方式处理，使用小的确定性任务生成微批次的输出。将流计算分解为这些小任务，相比传统的连续操作模型，有两个优势：
- en: Spark’s agile task scheduling can very quickly and efficiently recover from
    failures and straggler executors by rescheduling one or more copies of the tasks
    on any of the other executors.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 的灵活任务调度可以通过在其他执行者上重新调度任务的一个或多个副本，非常快速且高效地从故障和落后执行者中恢复。
- en: The deterministic nature of the tasks ensures that the output data is the same
    no matter how many times the task is reexecuted. This crucial characteristic enables
    Spark Streaming to provide end-to-end exactly-once processing guarantees, that
    is, the generated output results will be such that every input record was processed
    exactly once.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务的确定性质保证了无论任务重新执行多少次，输出数据都是相同的。这一关键特性使得 Spark Streaming 能够提供端到端的精确一次处理保证，即生成的输出结果将确保每个输入记录仅被处理一次。
- en: 'This efficient fault tolerance does come at the cost of latency—the micro-batch
    model cannot achieve millisecond-level latencies; it usually achieves latencies
    of a few seconds (as low as half a second in some cases). However, we have observed
    that for an overwhelming majority of stream processing use cases, the benefits
    of micro-batch processing outweigh the drawback of second-scale latencies. This
    is because most streaming pipelines have at least one of the following characteristics:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种高效的容错性是以延迟为代价的——微批处理模型无法达到毫秒级的延迟；通常情况下，它可以达到几秒钟的延迟（在某些情况下甚至可以低至半秒）。然而，我们观察到，对于绝大多数流处理应用场景来说，微批处理的优势远远超过秒级延迟的缺点。这是因为大多数流水线至少具备以下一种特性：
- en: The pipeline does not need latencies lower than a few seconds. For example,
    when the streaming output is only going to be read by hourly jobs, it is not useful
    to generate output with subsecond latencies.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道不需要低于几秒钟的延迟。例如，如果流式输出仅由每小时作业读取，生成具有亚秒级延迟的输出就没有用处。
- en: There are larger delays in other parts of the pipeline. For example, if the
    writes by a sensor into Apache Kafka (a system for ingesting data streams) are
    batched to achieve higher throughput, then no amount of optimization in the downstream
    processing systems can make the end-to-end latency lower than the batching delays.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道的其他部分存在较大的延迟。例如，如果传感器向 Apache Kafka（用于摄取数据流的系统）写入的操作被批处理以获得更高的吞吐量，那么在下游处理系统中进行任何优化也无法使端到端延迟低于批处理延迟。
- en: Furthermore, the DStream API was built upon Spark’s batch RDD API. Therefore,
    DStreams had the same functional semantics and fault-tolerance model as RDDs.
    Spark Streaming thus proved that it is possible for a single, unified processing
    engine to provide consistent APIs and semantics for batch, interactive, and streaming
    workloads. This fundamental paradigm shift in stream processing propelled Spark
    Streaming to become one of the most widely used open source stream processing
    engines.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DStream API 是建立在 Spark 的批处理 RDD API 之上的。因此，DStreams 具有与 RDDs 相同的功能语义和容错模型。因此，Spark
    Streaming 证明了单一统一的处理引擎可以为批处理、交互式和流处理工作负载提供一致的 API 和语义。这种流处理中的基本范式转变推动了 Spark Streaming
    成为最广泛使用的开源流处理引擎之一。
- en: Lessons Learned from Spark Streaming (DStreams)
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 Spark Streaming（DStreams）中学到的教训
- en: 'Despite all the advantages, the DStream API was not without its flaws. Here
    are a few key areas for improvement that were identified:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，DStream API 也存在一些缺陷。以下是一些需要改进的关键领域：
- en: Lack of a single API for batch and stream processing
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏批处理和流处理的单一 API
- en: Even though DStreams and RDDs have consistent APIs (i.e., same operations and
    same semantics), developers still had to explicitly rewrite their code to use
    different classes when converting their batch jobs to streaming jobs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 DStreams 和 RDDs 具有一致的 API（即相同的操作和语义），开发人员在将批处理作业转换为流处理作业时仍然需要显式重写其代码以使用不同的类。
- en: Lack of separation between logical and physical plans
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏逻辑计划和物理计划之间的分离
- en: Spark Streaming executes the DStream operations in the same sequence in which
    they were specified by the developer. Since developers effectively specify the
    exact physical plan, there is no scope for automatic optimizations, and developers
    have to hand-optimize their code to get the best performance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 以开发人员指定的顺序执行 DStream 操作。由于开发人员有效地指定了确切的物理执行计划，因此没有自动优化的余地，开发人员必须手动优化其代码以获得最佳性能。
- en: Lack of native support for event-time windows
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏对事件时间窗口的本机支持
- en: DStreams define window operations based only on the time when each record is
    received by Spark Streaming (known as *processing time*). However, many use cases
    need to calculate windowed aggregates based on the time when the records were
    generated (known as *event time*) instead of when they were received or processed.
    The lack of native support of event-time windows made it hard for developers to
    build such pipelines with Spark Streaming.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: DStreams仅基于Spark Streaming接收每条记录的时间（称为*处理时间*）定义窗口操作。然而，许多使用案例需要根据记录生成的时间（称为*事件时间*）计算窗口聚合，而不是它们接收或处理的时间。缺乏对事件时间窗口的本机支持使得开发者难以使用Spark
    Streaming构建这样的管道。
- en: These drawbacks shaped the design philosophy of Structured Streaming, which
    we will discuss next.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些缺点塑造了我们将在接下来讨论的结构化流处理的设计理念。
- en: The Philosophy of Structured Streaming
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化流处理的哲学
- en: 'Based on these lessons from DStreams, Structured Streaming was designed from
    scratch with one core philosophy—for developers, writing stream processing pipelines
    should be as easy as writing batch pipelines. In a nutshell, the guiding principles
    of Structured Streaming are:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些来自DStreams的经验教训，结构化流处理从头开始设计，核心理念是对开发者而言，编写流处理管道应该像编写批处理管道一样简单。总结来说，结构化流处理的指导原则包括：
- en: A single, unified programming model and interface for batch and stream processing
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理和流处理的统一编程模型和接口
- en: This unified model offers a simple API interface for both batch and streaming
    workloads. You can use familiar SQL or batch-like DataFrame queries (like those
    you’ve learned about in the previous chapters) on your stream as you would on
    a batch, leaving dealing with the underlying complexities of fault tolerance,
    optimizations, and tardy data to the engine. In the coming sections, we will examine
    some of the queries you might write.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种统一模型为批处理和流处理工作负载提供了简单的API接口。您可以像在批处理上一样在流上使用熟悉的SQL或类似批处理的DataFrame查询（就像您在之前章节中学到的那样），将处理故障容忍、优化和延迟数据等底层复杂性留给引擎处理。在接下来的部分中，我们将研究您可能编写的一些查询。
- en: A broader definition of stream processing
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理的更广泛定义
- en: Big data processing applications have grown complex enough that the line between
    real-time processing and batch processing has blurred significantly. The aim with
    Structured Streaming was to broaden its applicability from traditional stream
    processing to a larger class of applications; any application that periodically
    (e.g., every few hours) to continuously (like traditional streaming applications)
    processes data should be expressible using Structured Streaming.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据处理应用程序变得足够复杂，以至于实时处理和批处理之间的界限显著模糊化。结构化流处理的目标是从传统的流处理扩展其适用性到更大类别的应用程序；任何定期（例如每几小时）到连续（如传统流处理应用程序）处理数据的应用程序都应该能够使用结构化流处理表达。
- en: Next, we’ll discuss the programming model used by Structured Streaming.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论结构化流处理使用的编程模型。
- en: The Programming Model of Structured Streaming
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流处理的编程模型
- en: “Table” is a well-known concept that developers are familiar with when building
    batch applications. Structured Streaming extends this concept to streaming applications
    by treating a stream as an unbounded, continuously appended table, as illustrated
    in [Figure 8-3](#the_structured_streaming_programming_mod).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: “表”是在构建批处理应用程序时开发者熟悉的概念。结构化流处理通过将流视为无界、持续追加的表来将此概念扩展到流应用程序中，如[图 8-3](#the_structured_streaming_programming_mod)所示。
- en: '![The Structured Streaming programming model: data stream as an unbounded table](assets/lesp_0803.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![结构化流处理的编程模型：数据流作为无界表](assets/lesp_0803.png)'
- en: 'Figure 8-3\. The Structured Streaming programming model: data stream as an
    unbounded table'
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-3\. 结构化流处理的编程模型：数据流作为无界表
- en: Every new record received in the data stream is like a new row being appended
    to the unbounded input table. Structured Streaming will not actually retain all
    the input, but the output produced by Structured Streaming until time T will be
    equivalent to having all of the input until T in a static, bounded table and running
    a batch job on the table.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流中接收的每条新记录就像附加到无界输入表的新行。结构化流处理不会实际保留所有输入，但结构化流处理产生的输出直到时间T将等同于在静态有界表中具有直到T的所有输入并在表上运行批处理作业的效果。
- en: 'As shown in [Figure 8-4](#the_structured_streaming_processing_mode), the developer
    then defines a query on this conceptual input table, as if it were a static table,
    to compute the result table that will be written to an output sink. Structured
    Streaming will automatically convert this batch-like query to a streaming execution
    plan. This is called *incrementalization*: Structured Streaming figures out what
    state needs to be maintained to update the result each time a record arrives.
    Finally, developers specify triggering policies to control when to update the
    results. Each time a trigger fires, Structured Streaming checks for new data (i.e.,
    a new row in the input table) and incrementally updates the result.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [8-4](#the_structured_streaming_processing_mode) 所示，开发人员随后定义在这个概念输入表上的查询，就像它是一个静态表，以计算将写入输出汇聚的结果表。结构化流将自动将这种类似批处理的查询转换为流执行计划。这称为*增量化*：结构化流会找出每次记录到达时需要维护的状态。最后，开发人员指定触发策略来控制何时更新结果。每次触发器触发时，结构化流都会检查新数据（即输入表中的新行），并增量更新结果。
- en: '![The Structured Streaming processing model](assets/lesp_0804.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![结构化流处理模型](assets/lesp_0804.png)'
- en: Figure 8-4\. The Structured Streaming processing model
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-4\. 结构化流处理模型
- en: 'The last part of the model is the output mode. Each time the result table is
    updated, the developer will want to write the updates to an external system, such
    as a filesystem (e.g., HDFS, Amazon S3) or a database (e.g., MySQL, Cassandra).
    We usually want to write output incrementally. For this purpose, Structured Streaming
    provides three output modes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的最后部分是输出模式。每当更新结果表时，开发人员都希望将更新写入外部系统，例如文件系统（例如HDFS，Amazon S3）或数据库（例如MySQL，Cassandra）。通常我们希望增量写入输出。为此，结构化流提供了三种输出模式：
- en: Append mode
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 追加模式
- en: Only the new rows appended to the result table since the last trigger will be
    written to the external storage. This is applicable only in queries where existing
    rows in the result table cannot change (e.g., a map on an input stream).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 自上次触发以来附加到结果表的新行将被写入外部存储。这仅适用于查询中现有结果表中的行不可更改的情况（例如，输入流的映射）。
- en: Update mode
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 更新模式
- en: Only the rows that were updated in the result table since the last trigger will
    be changed in the external storage. This mode works for output sinks that can
    be updated in place, such as a MySQL table.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自上次触发以来在结果表中更新的唯一行将在外部存储中更改。这种模式适用于可以原地更新的输出汇聚，例如MySQL表。
- en: Complete mode
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 完整模式
- en: The entire updated result table will be written to external storage.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的整个结果表将被写入外部存储。
- en: Note
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Unless complete mode is specified, the result table will not be fully materialized
    by Structured Streaming. Just enough information (known as “state”) will be maintained
    to ensure that the changes in the result table can be computed and the updates
    can be output.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除非指定了完整模式，否则结构化流不会完全实现结果表。只会维护足够的信息（称为“状态”），以确保能够计算结果表中的更改并输出更新。
- en: Thinking of the data streams as tables not only makes it easier to conceptualize
    the logical computations on the data, but also makes it easier to express them
    in code. Since Spark’s DataFrame is a programmatic representation of a table,
    you can use the DataFrame API to express your computations on streaming data.
    All you need to do is define an input DataFrame (i.e., the input table) from a
    streaming data source, and then you apply operations on the DataFrame in the same
    way as you would on a DataFrame defined on a batch source.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据流视为表不仅使得在数据上进行逻辑计算更容易概念化，而且使得在代码中表达这些计算更加容易。由于Spark的DataFrame是表的编程表示，您可以使用DataFrame
    API来表达对流数据的计算。您只需从流数据源定义一个输入DataFrame（即输入表），然后以与在批处理源上定义DataFrame相同的方式对DataFrame应用操作。
- en: In the next section, you will see how easy it is to write Structured Streaming
    queries using DataFrames.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，您将看到使用DataFrame编写结构化流查询是多么简单。
- en: The Fundamentals of a Structured Streaming Query
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流查询的基础知识
- en: In this section, we are going to cover some high-level concepts that you’ll
    need to understand to develop Structured Streaming queries. We will first walk
    through the key steps to define and start a streaming query, then we will discuss
    how to monitor the active query and manage its life cycle.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些您需要理解以开发结构化流查询的高级概念。我们将首先介绍定义和启动流查询的关键步骤，然后讨论如何监视活动查询并管理其生命周期。
- en: Five Steps to Define a Streaming Query
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义流查询的五个步骤
- en: As discussed in the previous section, Structured Streaming uses the same DataFrame
    API as batch queries to express the data processing logic. However, there are
    a few key differences you need to know about for defining a Structured Streaming
    query. In this section, we will explore the steps involved in defining a streaming
    query by building a simple query that reads streams of text data over a socket
    and counts the words.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所讨论的，结构化流处理使用与批处理查询相同的 DataFrame API 来表达数据处理逻辑。但是，在定义结构化流查询时，您需要了解一些关键的不同之处。在本节中，我们将通过构建一个简单的查询来探索定义流查询的步骤，该查询从套接字上的文本数据流中读取并计算单词数。
- en: 'Step 1: Define input sources'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 1：定义输入源
- en: 'As with batch queries, the first step is to define a DataFrame from a streaming
    source. However, when reading batch data sources, we need `spark.read` to create
    a `DataFrameReader`, whereas with streaming sources we need `spark.readStream`
    to create a `DataStreamReader`. `DataStreamReader` has most of the same methods
    as `DataFrameReader`, so you can use it in a similar way. Here is an example of
    creating a DataFrame from a text data stream to be received over a socket connection:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与批处理查询一样，第一步是从流源定义一个 DataFrame。但是，当读取批处理数据源时，我们需要使用 `spark.read` 创建一个 `DataFrameReader`，而在流处理源中，我们需要使用
    `spark.readStream` 创建一个 `DataStreamReader`。`DataStreamReader` 具有与 `DataFrameReader`
    大部分相同的方法，因此可以类似地使用它。以下是从通过套接字连接接收的文本数据流创建 DataFrame 的示例：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This code generates the `lines` DataFrame as an unbounded table of newline-separated
    text data read from localhost:9999\. Note that, similar to batch sources with
    `spark.read`, this does not immediately start reading the streaming data; it only
    sets up the configurations necessary for reading the data once the streaming query
    is explicitly started.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将 `lines` DataFrame 生成为从 localhost:9999 读取的换行分隔文本数据的无界表。需要注意的是，类似于使用 `spark.read`
    读取批处理源一样，这并不会立即开始读取流数据；它只是设置了读取数据的配置，一旦显式启动流查询，数据才会被读取。
- en: Besides sockets, Apache Spark natively supports reading data streams from Apache
    Kafka and all the various file-based formats that `DataFrameReader` supports (Parquet,
    ORC, JSON, etc.). The details of these sources and their supported options are
    discussed later in this chapter. Furthermore, a streaming query can define multiple
    input sources, both streaming and batch, which can be combined using DataFrame
    operations like unions and joins (also discussed later in this chapter).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除了套接字外，Apache Spark 还原生支持从 Apache Kafka 和所有 `DataFrameReader` 支持的各种基于文件的格式（Parquet、ORC、JSON
    等）读取数据流。这些源的详细信息及其支持的选项将在本章后面讨论。此外，流查询可以定义多个输入源，包括流式和批处理，可以使用像 union 和 join 这样的
    DataFrame 操作进行组合（同样在本章后面讨论）。
- en: 'Step 2: Transform data'
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 2：转换数据
- en: 'Now we can apply the usual DataFrame operations, such as splitting the lines
    into individual words and then counting them, as shown in the following code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以应用常规的 DataFrame 操作，比如将行拆分为单词并计数它们，如下所示的代码：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`counts` is a *streaming DataFrame* (that is, a DataFrame on unbounded, streaming
    data) that represents the running word counts that will be computed once the streaming
    query is started and the streaming input data is being continuously processed.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`counts` 是一个*流处理 DataFrame*（即在无界流数据上的 DataFrame），表示在流查询启动并持续处理流输入数据时将计算的运行单词计数。'
- en: 'Note that these operations to transform the `lines` streaming DataFrame would
    work in the exact same way if `lines` were a batch DataFrame. In general, most
    DataFrame operations that can be applied on a batch DataFrame can also be applied
    on a streaming DataFrame. To understand which operations are supported in Structured
    Streaming, you have to recognize the two broad classes of data transformations:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，用于转换 `lines` 流处理 DataFrame 的这些操作如果 `lines` 是批处理 DataFrame 也会以完全相同的方式工作。通常情况下，大多数可以应用于批处理
    DataFrame 的 DataFrame 操作也可以应用于流处理 DataFrame。要了解结构化流处理支持哪些操作，您必须了解两类广泛的数据转换：
- en: Stateless transformations
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态转换
- en: Operations like `select()`, `filter()`, `map()`, etc. do not require any information
    from previous rows to process the next row; each row can be processed by itself.
    The lack of previous “state” in these operations make them stateless. Stateless
    operations can be applied to both batch and streaming DataFrames.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`select()`、`filter()`、`map()` 等操作不需要来自前一行的任何信息来处理下一行；每行可以单独处理。这些操作在处理中不涉及先前的“状态”，因此被称为无状态操作。无状态操作可以应用于批处理和流处理的
    DataFrame。'
- en: Stateful transformations
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有状态转换
- en: In contrast, an aggregation operation like `count()` requires maintaining state
    to combine data across multiple rows. More specifically, any DataFrame operations
    involving grouping, joining, or aggregating are stateful transformations. While
    many of these operations are supported in Structured Streaming, a few combinations
    of them are not supported because it is either computationally hard or infeasible
    to compute them in an incremental manner.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，像 `count()` 这样的聚合操作需要维护状态以跨多行合并数据。具体来说，涉及分组、连接或聚合的任何数据框架操作都是有状态的转换。虽然结构化流支持其中许多操作，但由于计算难度大或无法以增量方式计算，有些组合操作不受支持。
- en: The stateful operations supported by Structured Streaming and how to manage
    their state at runtime are discussed later in the chapter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流支持的有状态操作及如何在运行时管理它们的状态将在本章后面讨论。
- en: 'Step 3: Define output sink and output mode'
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三步：定义输出目的地和输出模式
- en: 'After transforming the data, we can define how to write the processed output
    data with `DataFrame.writeStream` (instead of `DataFrame.write`, used for batch
    data). This creates a `DataStreamWriter` which, similar to `DataFrameWriter`,
    has additional methods to specify the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换数据后，我们可以使用 `DataFrame.writeStream`（而不是用于批处理数据的 `DataFrame.write`）定义如何写入处理后的输出数据。这将创建一个
    `DataStreamWriter`，类似于 `DataFrameWriter`，它具有额外的方法来指定以下内容：
- en: Output writing details (where and how to write the output)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出写入详细信息（输出位置及方式）
- en: Processing details (how to process data and how to recover from failures)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理细节（如何处理数据及如何从故障中恢复）
- en: 'Let’s start with the output writing details (we will focus on the processing
    details in the next step). For example, the following snippet shows how to write
    the final `counts` to the console:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从输出写入详细信息开始（我们将在下一步关注处理细节）。例如，以下片段展示了如何将最终的`counts`写入控制台：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here we have specified `"console"` as the output streaming sink and `"complete"`
    as the output mode. The output mode of a streaming query specifies what part of
    the updated output to write out after processing new input data. In this example,
    as a chunk of new input data is processed and the word counts are updated, we
    can choose to print to the console either the counts of all the words seen until
    now (that is, *complete mode*), or only those words that were updated in the last
    chunk of input data. This is decided by the specified output mode, which can be
    one of the following (as we already saw in [“The Programming Model of Structured
    Streaming”](#the_programming_model_of_structured_stre):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定了 `"console"` 作为输出流目的地，`"complete"` 作为输出模式。流查询的输出模式指定在处理新输入数据后写出更新后输出的部分。在此示例中，当处理一块新的输入数据并更新单词计数时，我们可以选择打印到控制台所有迄今为止看到的单词计数（即*完整模式*），或者只打印最后一块输入数据中更新的单词。这由指定的输出模式决定，可以是以下之一（正如我们在[“结构化流的编程模型”](#the_programming_model_of_structured_stre)中已经看到的）：
- en: Append mode
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 追加模式
- en: This is the default mode, where only the new rows added to the result table/DataFrame
    (for example, the `counts` table) since the last trigger will be output to the
    sink. Semantically, this mode guarantees that any row that is output is never
    going to be changed or updated by the query in the future. Hence, append mode
    is supported by only those queries (e.g., stateless queries) that will never modify
    previously output data. In contrast, our word count query can update previously
    generated counts; therefore, it does not support append mode.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是默认模式，只有自上次触发以来新增加到结果表/数据框架（例如，`counts` 表）的新行会输出到目标位置。从语义上讲，此模式保证输出的任何行将不会被将来的查询修改或更新。因此，追加模式仅支持那些永远不会修改先前输出数据的查询（例如无状态查询）。相比之下，我们的词频统计查询可以更新先前生成的计数，因此不支持追加模式。
- en: Complete mode
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 完整模式
- en: In this mode, all the rows of the result table/DataFrame will be output at the
    end of every trigger. This is supported by queries where the result table is likely
    to be much smaller than the input data and therefore can feasibly be retained
    in memory. For example, our word count query supports complete mode because the
    counts data is likely to be far smaller than the input data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模式下，结果表/数据框架的所有行将在每次触发结束时输出。这种模式适用于结果表比输入数据要小得多，因此可以在内存中保留。例如，我们的词频统计查询支持完整模式，因为计数数据很可能比输入数据小得多。
- en: Update mode
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 更新模式
- en: In this mode, only the rows of the result table/DataFrame that were updated
    since the last trigger will be output at the end of every trigger. This is in
    contrast to append mode, as the output rows may be modified by the query and output
    again in the future. Most queries support update mode.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在此模式下，只有自上次触发以来更新的结果表/DataFrame行将在每次触发结束时输出。这与追加模式相反，因为输出行可能会被查询修改，并在未来再次输出。大多数查询支持更新模式。
- en: Note
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Complete details on the output modes supported by different queries can be found
    in the latest [Structured Streaming Programming Guide](https://oreil.ly/hyuKL).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 有关不同查询支持的输出模式的详细信息可以在最新的[结构化流编程指南](https://oreil.ly/hyuKL)中找到。
- en: Besides writing the output to the console, Structured Streaming natively supports
    streaming writes to files and Apache Kafka. In addition, you can write to arbitrary
    locations using the `foreachBatch()` and `foreach()` API methods. In fact, you
    can use `foreachBatch()` to write streaming outputs using existing batch data
    sources (but you will lose exactly-once guarantees). The details of these sinks
    and their supported options are discussed later in this chapter.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将输出写入控制台外，结构化流还原生支持将流式数据写入文件和Apache Kafka。此外，您还可以使用 `foreachBatch()` 和 `foreach()`
    API 方法将数据写入任意位置。事实上，您可以使用 `foreachBatch()` 来使用现有的批数据源写入流式输出（但您将失去精确一次性保证）。这些输出位置的详细信息及其支持的选项将在本章后面讨论。
- en: 'Step 4: Specify processing details'
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第四步：指定处理细节
- en: 'The final step before starting the query is to specify details of how to process
    the data. Continuing with our word count example, we are going to specify the
    processing details as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动查询之前的最后一步是指定如何处理数据的详细信息。继续使用我们的词频统计示例，我们将如下指定处理细节：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here we have specified two types of details using the `DataStreamWriter` that
    we created with `DataFrame.writeStream`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在此处，我们使用 `DataFrame.writeStream` 创建的 `DataStreamWriter` 指定了两种类型的详细信息：
- en: Triggering details
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 触发详情
- en: 'This indicates when to trigger the discovery and processing of newly available
    streaming data. There are four options:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这指示何时触发新可用流式数据的发现和处理。有四个选项：
- en: Default
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 默认
- en: When the trigger is not explicitly specified, then by default, the streaming
    query executes data in micro-batches where the next micro-batch is triggered as
    soon as the previous micro-batch has completed.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当未显式指定触发器时，默认情况下，流式查询以微批次处理数据，其中下一个微批次在前一个微批次完成后立即触发。
- en: Processing time with trigger interval
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用触发间隔的处理时间
- en: You can explicitly specify the `ProcessingTime` trigger with an interval, and
    the query will trigger micro-batches at that fixed interval.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以显式指定 `ProcessingTime` 触发器及其间隔，并且查询将在固定间隔触发微批处理。
- en: Once
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一次
- en: In this mode, the streaming query will execute exactly one micro-batch—it processes
    all the new data available in a single batch and then stops itself. This is useful
    when you want to control the triggering and processing from an external scheduler
    that will restart the query using any custom schedule (e.g., to control cost by
    only executing a query [once per day](https://oreil.ly/Y7EZy)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模式下，流式查询将精确执行一个微批处理，处理所有新的可用数据并随后停止。这在你希望通过外部调度程序控制触发和处理，并使用任意自定义时间表重新启动查询（例如，仅执行一次查询[每天一次](https://oreil.ly/Y7EZy)）时非常有用。
- en: Continuous
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 连续
- en: This is an experimental mode (as of Spark 3.0) where the streaming query will
    process data continuously instead of in micro-batches. While only a small subset
    of DataFrame operations allow this mode to be used, it can provide much lower
    latency (as low as milliseconds) than the micro-batch trigger modes. Refer to
    the latest [Structured Streaming Programming Guide](https://oreil.ly/7cERT) for
    the most up-to-date information.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个实验性模式（截至 Spark 3.0），流式查询将连续处理数据，而不是以微批次处理。虽然只有 DataFrame 操作的一个小子集允许使用此模式，但它可以提供比微批处理触发模式更低的延迟（低至毫秒）。有关最新信息，请参阅最新的[结构化流编程指南](https://oreil.ly/7cERT)。
- en: Checkpoint location
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点位置
- en: This is a directory in any HDFS-compatible filesystem where a streaming query
    saves its progress information—that is, what data has been successfully processed.
    Upon failure, this metadata is used to restart the failed query exactly where
    it left off. Therefore, setting this option is necessary for failure recovery
    with exactly-once guarantees.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这是任何与HDFS兼容的文件系统中的目录，流查询将其进度信息保存在其中——即已成功处理的数据。在失败时，此元数据用于在失败时恢复查询，确切一次性保证因此设置此选项对于故障恢复是必要的。
- en: 'Step 5: Start the query'
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第5步：启动查询
- en: 'Once everything has been specified, the final step is to start the query, which
    you can do with the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有内容都已指定，最后一步是启动查询，可以通过以下方式完成：
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The returned object of type `streamingQuery` represents an active query and
    can be used to manage the query, which we will cover later in this chapter.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 类型为`streamingQuery`的返回对象表示活动查询，并可用于管理查询，这将在本章后面介绍。
- en: Note that `start()` is a nonblocking method, so it will return as soon as the
    query has started in the background. If you want the main thread to block until
    the streaming query has terminated, you can use `streamingQuery.awaitTermination()`.
    If the query fails in the background with an error, `awaitTermination()` will
    also fail with that same exception.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`start()`是一个非阻塞方法，因此一旦查询在后台启动，它就会立即返回。如果你希望主线程阻塞，直到流查询终止，可以使用`streamingQuery.awaitTermination()`。如果查询在后台因错误而失败，`awaitTermination()`也将因同样的异常而失败。
- en: You can wait up to a timeout duration using `awaitTermination(timeoutMillis)`,
    and you can explicitly stop the query with `streamingQuery.stop()`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`awaitTermination(timeoutMillis)`等待超时时长，也可以使用`streamingQuery.stop()`显式停止查询。
- en: Putting it all together
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'To summarize, here is the complete code for reading streams of text data over
    a socket, counting the words, and printing the counts to the console:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，这里是通过套接字读取文本流数据、计算单词数并将计数打印到控制台的完整代码：
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: After the query has started, a background thread continuously reads new data
    from the streaming source, processes it, and writes it to the streaming sinks.
    Next, let’s take a quick peek under the hood at how this is executed.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 查询启动后，后台线程不断从流源读取新数据，处理它，并将其写入流接收器。接下来，让我们快速看一下这是如何执行的。
- en: Under the Hood of an Active Streaming Query
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主动流查询的内部运行机制
- en: 'Once the query starts, the following sequence of steps transpires in the engine,
    as depicted in [Figure 8-5](#incremental_execution_of_streaming_queri). The DataFrame
    operations are converted into a logical plan, which is an abstract representation
    of the computation that Spark SQL uses to plan a query:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 查询启动后，引擎中会出现以下步骤序列，如图[8-5](#incremental_execution_of_streaming_queri)所示。DataFrame操作转换为逻辑计划，这是Spark
    SQL用来计划查询的抽象表示：
- en: Spark SQL analyzes and optimizes this logical plan to ensure that it can be
    executed incrementally and efficiently on streaming data.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark SQL分析和优化此逻辑计划，以确保可以在流数据上以增量和高效的方式执行。
- en: Spark SQL starts a background thread that continuously executes the following
    loop:^([2](ch08.html#ch01fn10))
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark SQL启动了一个后台线程，不断执行以下循环：^([2](ch08.html#ch01fn10))
- en: Based on the configured trigger interval, the thread checks the streaming sources
    for the availability of new data.
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于配置的触发间隔，线程检查流源以查看是否有新数据可用。
- en: If available, the new data is executed by running a micro-batch. From the optimized
    logical plan, an optimized Spark execution plan is generated that reads the new
    data from the source, incrementally computes the updated result, and writes the
    output to the sink according to the configured output mode.
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有新数据可用，则通过运行微批处理来执行它。从优化的逻辑计划中生成优化的Spark执行计划，该计划从源读取新数据，逐步计算更新结果，并根据配置的输出模式将输出写入接收器。
- en: For every micro-batch, the exact range of data processed (e.g., the set of files
    or the range of Apache Kafka offsets) and any associated state are saved in the
    configured checkpoint location so that the query can deterministically reprocess
    the exact range if needed.
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个微批处理，处理的确切数据范围（例如文件集或Apache Kafka偏移量的范围）和任何相关状态都保存在配置的检查点位置，以便在需要时确定性地重新处理确切的范围。
- en: 'This loop continues until the query is terminated, which can occur for one
    of the following reasons:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此循环持续到查询终止，可能出现以下原因之一：
- en: A failure has occurred in the query (either a processing error or a failure
    in the cluster).
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询发生了故障（无论是处理错误还是集群中的故障）。
- en: The query is explicitly stopped using `streamingQuery.stop()`.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`streamingQuery.stop()`明确停止查询。
- en: If the trigger is set to `Once`, then the query will stop on its own after executing
    a single micro-batch containing all the available data.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果触发器设置为`Once`，则查询将在执行包含所有可用数据的单个微批次后自行停止。
- en: '![Incremental execution of streaming queries](assets/lesp_0805.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![流式查询的增量执行](assets/lesp_0805.png)'
- en: Figure 8-5\. Incremental execution of streaming queries
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-5\. 流式查询的增量执行
- en: Note
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A key point you should remember about Structured Streaming is that underneath
    it is using Spark SQL to execute the data. As such, the full power of Spark SQL’s
    hyperoptimized execution engine is utilized to maximize the stream processing
    throughput, providing key performance advantages.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 关于结构化流处理的一个关键点是，在其背后实际上使用的是 Spark SQL 来执行数据。因此，充分利用了 Spark SQL 的超优化执行引擎的全部能力，以最大化流处理吞吐量，提供关键的性能优势。
- en: Next, we will discuss how to restart a streaming query after termination and
    the life cycle of a streaming query.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何在终止后重新启动流查询以及流查询的生命周期。
- en: Recovering from Failures with Exactly-Once Guarantees
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用精确一次性保证从故障中恢复
- en: To restart a terminated query in a completely new process, you have to create
    a new `SparkSession`, redefine all the DataFrames, and start the streaming query
    on the final result using the same checkpoint location as the one used when the
    query was started the first time. For our word count example, you can simply reexecute
    the entire code snippet shown earlier, from the definition of `spark` in the first
    line to the final `start()` in the last line.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要在完全新的进程中重新启动终止的查询，您必须创建一个新的`SparkSession`，重新定义所有的DataFrame，并在最终结果上使用与第一次启动查询时相同的检查点位置开始流式查询。对于我们的词频统计示例，您可以简单地重新执行从第一行的`spark`定义到最后一行的`start()`的整个代码片段。
- en: The checkpoint location must be the same across restarts because this directory
    contains the unique identity of a streaming query and determines the life cycle
    of the query. If the checkpoint directory is deleted or the same query is started
    with a different checkpoint directory, it is like starting a new query from scratch.
    Specifically, checkpoints have record-level information (e.g., Apache Kafka offsets)
    to track the data range the last incomplete micro-batch was processing. The restarted
    query will use this information to start processing records precisely after the
    last successfully completed micro-batch. If the previous query had planned a micro-batch
    but had terminated before completion, then the restarted query will reprocess
    the same range of data before processing new data. Coupled with Spark’s deterministic
    task execution, the regenerated output will be the same as it was expected to
    be before the restart.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点位置在重启时必须保持一致，因为该目录包含流查询的唯一标识，并确定查询的生命周期。如果删除检查点目录或者使用不同的检查点目录启动相同查询，则相当于从头开始一个新的查询。具体来说，检查点具有记录级信息（例如，Apache
    Kafka 偏移量），以跟踪上一个查询正在处理的最后一个不完整微批次的数据范围。重新启动的查询将使用此信息在成功完成的最后一个微批次之后精确地开始处理记录。如果先前的查询已计划一个微批次但在完成之前终止，则重新启动的查询将重新处理相同范围的数据，然后处理新数据。结合
    Spark 的确定性任务执行，生成的输出将与重启前预期的输出相同。
- en: 'Structured Streaming can ensure *end-to-end exactly-once guarantees* (that
    is, the output is as if each input record was processed exactly once) when the
    following conditions have been satisfied:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当满足以下条件时，结构化流处理可以确保*端到端的精确一次性保证*（即，输出就像每个输入记录确实只处理了一次）：
- en: Replayable streaming sources
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 可重放的流源
- en: The data range of the last incomplete micro-batch can be reread from the source.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个不完整的微批次的数据范围可以从源头重新读取。
- en: Deterministic computations
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性计算
- en: All data transformations deterministically produce the same result when given
    the same input data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据转换在给定相同输入数据时都能确定性地产生相同的结果。
- en: Idempotent streaming sink
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 幂等的流接收器
- en: The sink can identify reexecuted micro-batches and ignore duplicate writes that
    may be caused by restarts.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接收器可以识别重新执行的微批次，并忽略由重启可能引起的重复写入。
- en: Note that our word count example does not provide exactly-once guarantees because
    the socket source is not replayable and the console sink is not idempotent.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的词频统计示例不提供精确一次性保证，因为套接字源不可重放，控制台接收器不是幂等的。
- en: 'As a final note regarding restarting queries, it is possible to make minor
    modifications to a query between restarts. Here are a few ways you can modify
    the query:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 关于重新启动查询的最后一点，可以在重新启动之间对查询进行微小修改。以下是几种可以修改查询的方式：
- en: DataFrame transformations
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame转换
- en: 'You can make minor modifications to the transformations between restarts. For
    example, in our streaming word count example, if you want to ignore lines that
    have corrupted byte sequences that can crash the query, you can add a filter in
    the transformation:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在重新启动之间对转换进行微小修改。例如，在我们的流式字数示例中，如果要忽略具有可能导致查询崩溃的损坏字节序列的行，则可以在转换中添加一个过滤器：
- en: '[PRE12]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Upon restarting with this modified `words` DataFrame, the restarted query will
    apply the filter on all data processed since the restart (including the last incomplete
    micro-batch), preventing the query from failing again.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此修改后的`words` DataFrame重新启动后，重新启动的查询将对自重新启动以来处理的所有数据应用过滤器（包括最后一个不完整的微批次），以防止再次失败。
- en: Source and sink options
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 源和接收端选项
- en: 'Whether a `readStream` or `writeStream` option can be changed between restarts
    depends on the semantics of the specific source or sink. For example, you should
    not change the `host` and `port` options for the socket source if data is going
    to be sent to that host and port. But you can add an option to the console sink
    to print up to one hundred changed counts after every trigger:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新启动之间是否可以更改`readStream`或`writeStream`选项取决于特定源或接收端的语义。例如，如果数据将发送到该主机和端口，则不应更改`host`和`port`选项的套接字源。但是，您可以向控制台接收端添加一个选项，以在每次触发后打印最多一百个变更计数：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Processing details
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 处理细节
- en: As discussed earlier, the checkpoint location must not be changed between restarts.
    However, other details like trigger interval can be changed without breaking fault-tolerance
    guarantees.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面讨论的，检查点位置在重新启动之间不能更改。但是，可以在不破坏容错保证的情况下更改其他细节，如触发间隔。
- en: For more information on the narrow set of changes that are allowed between restarts,
    see the latest [Structured Streaming Programming Guide](https://oreil.ly/am885).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在重新启动之间允许的狭窄变化集的更多信息，请参阅最新的[结构化流编程指南](https://oreil.ly/am885)。
- en: Monitoring an Active Query
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监视活动查询
- en: An important part of running a streaming pipeline in production is tracking
    its health. Structured Streaming provides several ways to track the status and
    processing metrics of an active query.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中运行流水线的重要部分是跟踪其健康状况。结构化流提供了几种方法来跟踪活动查询的状态和处理指标。
- en: Querying current status using StreamingQuery
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用StreamingQuery查询当前状态
- en: 'You can query the current health of an active query using the `StreamingQuery`
    instance. Here are two methods:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`StreamingQuery`实例查询活动查询的当前健康状况。以下是两种方法：
- en: Get current metrics using StreamingQuery
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用StreamingQuery获取当前指标
- en: 'When a query processes some data in a micro-batch, we consider it to have made
    some progress. `lastProgress()` returns information on the last completed micro-batch.
    For example, printing the returned object (`StreamingQueryProgress` in Scala/Java
    or a dictionary in Python) will produce something like this:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当查询在微批次中处理一些数据时，我们认为它已经取得了一些进展。`lastProgress()`返回上一个完成的微批次的信息。例如，打印返回的对象（在Scala/Java中为`StreamingQueryProgress`，在Python中为字典）将产生类似于以下的内容：
- en: '[PRE15]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Some of the noteworthy columns are:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一些值得注意的列包括：
- en: '`id`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`id`'
- en: Unique identifier tied to a checkpoint location. This stays the same throughout
    the lifetime of a query (i.e., across restarts).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 与检查点位置绑定的唯一标识符。这将在查询的整个生命周期内保持不变（即在重新启动之间）。
- en: '`runId`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`runId`'
- en: Unique identifier for the current (re)started instance of the query. This changes
    with every restart.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 与当前（重新）启动查询实例相关联的唯一标识符。每次重新启动时都会更改。
- en: '`numInputRows`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`numInputRows`'
- en: Number of input rows that were processed in the last micro-batch.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上一微批次中处理的输入行数。
- en: '`inputRowsPerSecond`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`inputRowsPerSecond`'
- en: Current rate at which input rows are being generated at the source (average
    over the last micro-batch duration).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 源处生成输入行的当前速率（在上一个微批次持续时间内的平均值）。
- en: '`processedRowsPerSecond`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`processedRowsPerSecond`'
- en: Current rate at which rows are being processed and written out by the sink (average
    over the last micro-batch duration). If this rate is consistently lower than the
    input rate, then the query is unable to process data as fast as it is being generated
    by the source. This is a key indicator of the health of the query.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通过接收器处理的行的当前速率（在最后一次微批处理的平均时间内）。如果此速率一直低于输入速率，则查询无法像源生成数据那样快速处理数据。这是查询健康状况的关键指标。
- en: '`sources` and `sink`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`sources` 和 `sink`'
- en: Provides source/sink-specific details of the data processed in the last batch.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了最后一个批次中处理的数据的源/接收器特定细节。
- en: Get current status using StreamingQuery.status()
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用`StreamingQuery.status()`获取当前状态
- en: 'This provides information on what the background query thread is doing at this
    moment. For example, printing the returned object will produce something like
    this:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了有关后台查询线程此刻正在执行的操作的信息。例如，打印返回的对象将产生类似以下的输出：
- en: '[PRE16]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Publishing metrics using Dropwizard Metrics
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Dropwizard Metrics发布度量
- en: Spark supports reporting metrics via a popular library called [Dropwizard Metrics](https://metrics.dropwizard.io).
    This library allows metrics to be published to many popular monitoring frameworks
    (Ganglia, Graphite, etc.). These metrics are by default not enabled for Structured
    Streaming queries due to their high volume of reported data. To enable them, apart
    from [configuring Dropwizard Metrics for Spark](https://oreil.ly/4xenP), you have
    to explicitly set the `SparkSession` configuration `spark.sql.streaming.metricsEnabled`
    to `true` before starting your query.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持通过一个称为[Dropwizard Metrics](https://metrics.dropwizard.io)的流行库报告度量。此库允许度量数据发布到许多流行的监控框架（Ganglia、Graphite等）。由于报告的数据量很大，这些度量默认情况下不会为结构化流查询启用。要启用它们，除了[为Spark配置Dropwizard
    Metrics](https://oreil.ly/4xenP)之外，您还必须在启动查询之前显式设置`SparkSession`配置`spark.sql.streaming.metricsEnabled`为`true`。
- en: Note that only a subset of the information available through `StreamingQuery.lastProgress()`
    is published through Dropwizard Metrics. If you want to continuously publish more
    progress information to arbitrary locations, you have to write custom listeners,
    as discussed next.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，仅通过`StreamingQuery.lastProgress()`公布了Dropwizard Metrics中可用的部分信息。如果您想持续将更多进度信息发布到任意位置，您必须编写自定义监听器，如下所述。
- en: Publishing metrics using custom StreamingQueryListeners
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自定义StreamingQueryListeners发布度量
- en: '`StreamingQueryListener` is an event listener interface with which you can
    inject arbitrary logic to continuously publish metrics. This developer API is
    available only in Scala/Java. There are two steps to using custom listeners:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamingQueryListener`是一个事件监听器接口，您可以使用它注入任意逻辑以持续发布度量。此开发者API仅在Scala/Java中可用。使用自定义监听器有两个步骤：'
- en: 'Define your custom listener. The `StreamingQueryListener` interface provides
    three methods that can be defined by your implementation to get three types of
    events related to a streaming query: start, progress (i.e., a trigger was executed),
    and termination. Here is an example:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义您的自定义监听器。`StreamingQueryListener`接口提供了三种方法，您可以通过自己的实现定义这三种类型的与流查询相关的事件：启动、进度（即执行了触发器）和终止。这里是一个例子：
- en: '[PRE17]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Add your listener to the `SparkSession` before starting the query:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在启动查询之前将监听器添加到`SparkSession`：
- en: '[PRE18]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: After adding the listener, all events of streaming queries running on this `SparkSession`
    will start calling the listener’s methods.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加监听器后，运行在此`SparkSession`上的所有流查询事件将开始调用监听器的方法。
- en: Streaming Data Sources and Sinks
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流数据源和接收器
- en: Now that we have covered the basic steps you need to express an end-to-end Structured
    Streaming query, let’s examine how to use the built-in streaming data sources
    and sinks. As a reminder, you can create DataFrames from streaming sources using
    `SparkSession.readStream()` and write the output from a result DataFrame using
    `DataFrame.writeStream()`. In each case, you can specify the source type using
    the method `format()`. We will see a few concrete examples later.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了表达端到端结构化流查询所需的基本步骤，让我们看看如何使用内置的流数据源和接收器。作为提醒，您可以使用`SparkSession.readStream()`从流源创建DataFrame，并使用`DataFrame.writeStream()`将结果DataFrame的输出写入。在每种情况下，您可以使用`format()`方法指定源类型。稍后我们将看到几个具体的例子。
- en: Files
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件
- en: 'Structured Streaming supports reading and writing data streams to and from
    files in the same formats as the ones supported in batch processing: plain text,
    CSV, JSON, Parquet, ORC, etc. Here we will discuss how to operate Structured Streaming
    on files.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流支持从文件中读取和写入数据流，格式与批处理中支持的格式相同：纯文本、CSV、JSON、Parquet、ORC等。在这里，我们将讨论如何在文件上操作结构化流。
- en: Reading from files
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从文件读取
- en: 'Structured Streaming can treat files written into a directory as a data stream.
    Here is an example:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流可以将写入目录的文件视为数据流。以下是一个示例：
- en: '[PRE19]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The returned streaming DataFrame will have the specified schema. Here are a
    few key points to remember when using files:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的流DataFrame将具有指定的模式。在使用文件时，有几个需要记住的关键点：
- en: All the files must be of the same format and are expected to have the same schema.
    For example, if the format is `"json"`, all the files must be in the JSON format
    with one JSON record per line. The schema of each JSON record must match the one
    specified with `readStream()`. Violation of these assumptions can lead to incorrect
    parsing (e.g., unexpected `null` values) or query failures.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有文件必须具有相同的格式，并且预计具有相同的模式。例如，如果格式是`"json"`，则所有文件必须以每行一个JSON记录的JSON格式。每个JSON记录的模式必须与`readStream()`指定的模式匹配。违反这些假设可能导致不正确的解析（例如，意外的`null`值）或查询失败。
- en: Each file must appear in the directory listing atomically—that is, the whole
    file must be available at once for reading, and once it is available, the file
    cannot be updated or modified. This is because Structured Streaming will process
    the file when the engine finds it (using directory listing) and internally mark
    it as processed. Any changes to that file will not be processed.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个文件必须在目录列表中以原子方式显示——也就是说，整个文件必须一次性可用于读取，一旦可用，文件就不能被更新或修改。这是因为结构化流处理会在引擎找到文件后（使用目录列表）处理文件，并在内部标记为已处理。对该文件的任何更改都不会被处理。
- en: When there are multiple new files to process but it can only pick some of them
    in the next micro-batch (e.g., because of rate limits), it will select the files
    with the earliest timestamps. Within the micro-batch, however, there is no predefined
    order of reading of the selected files; all of them will be read in parallel.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当有多个新文件要处理但在下一个微批处理中只能选择其中一些（例如，因为速率限制），它将选择具有最早时间戳的文件。然而，在微批处理内部，选择的文件没有预定义的读取顺序；所有这些文件将并行读取。
- en: Note
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This streaming file source supports a number of common options, including the
    file format–specific options supported by `spark.read()` (see [“Data Sources for
    DataFrames and SQL Tables”](ch04.html#data_sources_for_dataframes_and_sql_tabl)
    in [Chapter 4](ch04.html#spark_sql_and_dataframes_introduction_to)) and several
    streaming-specific options (e.g., `maxFilesPerTrigger` to limit the file processing
    rate). See the [programming guide](https://oreil.ly/VxU9U) for full details.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 此流文件源支持许多常见选项，包括由`spark.read()`支持的特定于文件格式的选项（参见[“DataFrame和SQL表的数据源”](ch04.html#data_sources_for_dataframes_and_sql_tabl)第[第四章](ch04.html#spark_sql_and_dataframes_introduction_to)）和几个流特定选项（例如，`maxFilesPerTrigger`限制文件处理速率）。请查阅[编程指南](https://oreil.ly/VxU9U)获取完整详细信息。
- en: Writing to files
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 写入文件
- en: 'Structured Streaming supports writing streaming query output to files in the
    same formats as reads. However, it only supports append mode, because while it
    is easy to write new files in the output directory (i.e., append data to a directory),
    it is hard to modify existing data files (as would be expected with update and
    complete modes). It also supports partitioning. Here is an example:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流支持将流查询输出写入与读取相同格式的文件。但是，它只支持追加模式，因为在输出目录写入新文件很容易（即向目录追加数据），但修改现有数据文件则很难（如更新和完成模式所期望的）。它还支持分区。以下是一个示例：
- en: '[PRE21]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Instead of using the `"path"` option, you can specify the output directory directly
    as `start(outputDir)`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接指定输出目录作为`start(outputDir)`，而不是使用`"path"`选项。
- en: 'A few key points to remember:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一些需要记住的关键点：
- en: Structured Streaming achieves end-to-end exactly-once guarantees when writing
    to files by maintaining a log of the data files that have been written to the
    directory. This log is maintained in the subdirectory *_spark_metadata*. Any Spark
    query on the directory (not its subdirectories) will automatically use the log
    to read the correct set of data files so that the exactly-once guarantee is maintained
    (i.e., no duplicate data or partial files are read). Note that other processing
    engines may not be aware of this log and hence may not provide the same guarantee.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Structured Streaming 在写入文件时实现端到端的精确一次性保证，通过维护已写入目录的数据文件日志。此日志保存在子目录 *_spark_metadata*
    中。对目录（而非其子目录）的任何 Spark 查询都将自动使用日志来读取正确的数据文件集，以保持精确一次性保证（即不会读取重复数据或部分文件）。请注意，其他处理引擎可能不了解此日志，因此可能无法提供相同的保证。
- en: If you change the schema of the result DataFrame between restarts, then the
    output directory will have data in multiple schemas. These schemas have to be
    reconciled when querying the directory.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在重启之间更改结果 DataFrame 的模式，则输出目录将包含多个模式的数据。在查询目录时，这些模式必须进行协调。
- en: Apache Kafka
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Kafka
- en: '[Apache Kafka](https://kafka.apache.org/) is a popular publish/subscribe system
    that is widely used for storage of data streams. Structured Streaming has built-in
    support for reading from and writing to Apache Kafka.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[Apache Kafka](https://kafka.apache.org/) 是一种流行的发布/订阅系统，广泛用于数据流的存储。Structured
    Streaming 内置支持从 Apache Kafka 读取和写入数据。'
- en: Reading from Kafka
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 Kafka 读取
- en: 'To perform distributed reads from Kafka, you have to use options to specify
    how to connect to the source. Say you want to subscribe to data from the topic
    `"events"`. Here is how you can create a streaming DataFrame:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 Kafka 执行分布式读取，您必须使用选项来指定如何连接到源。假设您想订阅来自主题 `"events"` 的数据。以下是如何创建流式 DataFrame
    的方法：
- en: '[PRE23]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The returned DataFrame will have the schema described in [Table 8-1](#schema_of_the_dataframe_generated_by_the).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的 DataFrame 将具有 [表 8-1](#schema_of_the_dataframe_generated_by_the) 中描述的模式。
- en: Table 8-1\. Schema of the DataFrame generated by the Kafka source
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-1\. 由 Kafka 源生成的 DataFrame 的模式
- en: '| Column name | Column type | Description |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 列类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `key` | `binary` | Key data of the record as bytes. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| `key` | `binary` | 记录键的字节数据。 |'
- en: '| `value` | `binary` | Value data of the record as bytes. |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| `value` | `binary` | 记录值的字节数据。 |'
- en: '| `topic` | `string` | Kafka topic the record was in. This is useful when subscribed
    to multiple topics. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| `topic` | `string` | 订阅多个主题时，记录所在的 Kafka 主题。 |'
- en: '| `partition` | `int` | Partition of the Kafka topic the record was in. |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| `partition` | `int` | 记录所在 Kafka 主题的分区。 |'
- en: '| `offset` | `long` | Offset value of the record. |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| `offset` | `long` | 记录的偏移值。 |'
- en: '| `timestamp` | `long` | Timestamp associated with the record. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| `timestamp` | `long` | 记录相关联的时间戳。 |'
- en: '| `timestampType` | `int` | Enumeration for the type of the timestamp associated
    with the record. |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| `timestampType` | `int` | 与记录相关联的时间戳类型的枚举。 |'
- en: You can also choose to subscribe to multiple topics, a pattern of topics, or
    even a specific partition of a topic. Furthermore, you can choose whether to read
    only new data in the subscribed-to topics or process all the available data in
    those topics. You can even read Kafka data from batch queries—that is, treat Kafka
    topics like tables. See the [Kafka Integration Guide](https://oreil.ly/FVP0l)
    for more details.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以选择订阅多个主题、主题模式，甚至特定主题的分区。此外，您可以选择是否只读取订阅主题中的新数据或处理这些主题中的所有可用数据。您甚至可以从批处理查询中读取
    Kafka 数据，即将 Kafka 主题视为表格。有关更多详情，请参阅 [Kafka 集成指南](https://oreil.ly/FVP0l)。
- en: Writing to Kafka
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 写入 Kafka
- en: For writing to Kafka, Structured Streaming expects the result DataFrame to have
    a few columns of specific names and types, as outlined in [Table 8-2](#schema_of_dataframe_that_can_be_written).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于写入 Kafka，Structured Streaming 期望结果 DataFrame 具有特定名称和类型的几列，如 [表 8-2](#schema_of_dataframe_that_can_be_written)
    所述。
- en: Table 8-2\. Schema of DataFrame that can be written to the Kafka sink
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-2\. 可写入 Kafka sink 的 DataFrame 模式
- en: '| Column name | Column type | Description |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 列类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `key` (optional) | `string` or `binary` | If present, the bytes will be written
    as the Kafka record key; otherwise, the key will be empty. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `key`（可选） | `string` 或 `binary` | 如果存在，则将以 Kafka 记录键的形式写入的字节；否则，键将为空。 |'
- en: '| `value` (required) | `string` or `binary` | The bytes will be written as
    the Kafka record value. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `value`（必需） | `string` 或 `binary` | 将以 Kafka 记录值的形式写入的字节。 |'
- en: '| `topic` (required only if `"topic"` is not specified as option) | `string`
    | If `"topic"` is not specified as an option, this determines the topic to write
    the key/value to. This is useful for fanning out the writes to multiple topics.
    If the `"topic"` option has been specified, this value is ignored. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| `topic`（仅在未将 `"topic"` 指定为选项时需要） | `string` | 如果未将 `"topic"` 指定为选项，则确定要将键/值写入的主题。这对于将写入扇出到多个主题非常有用。如果已指定
    `"topic"` 选项，则忽略此值。 |'
- en: 'You can write to Kafka in all three output modes, though complete mode is not
    recommended as it will repeatedly output the same records. Here is a concrete
    example of writing the output of our earlier word count query into Kafka in update
    mode:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在所有三种输出模式下将数据写入 Kafka，尽管不推荐使用完整模式，因为它会重复输出相同的记录。以下是将前面的单词计数查询输出写入 Kafka 的具体示例，使用更新模式：
- en: '[PRE25]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: See the [Kafka Integration Guide](https://oreil.ly/tFo-N) for more details.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细信息，请参阅 [Kafka 集成指南](https://oreil.ly/tFo-N)。
- en: Custom Streaming Sources and Sinks
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义流源和接收器
- en: In this section, we will discuss how to read and write to storage systems that
    do not have built-in support in Structured Streaming. In particular, you’ll see
    how to use the `foreachBatch()` and `foreach()` methods to implement custom logic
    to write to your storage.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何读取和写入不具备结构化流内置支持的存储系统。特别是，您将看到如何使用 `foreachBatch()` 和 `foreach()`
    方法来实现自定义逻辑以写入您的存储。
- en: Writing to any storage system
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 写入任何存储系统
- en: 'There are two operations that allow you to write the output of a streaming
    query to arbitrary storage systems: `foreachBatch()` and `foreach()`. They have
    slightly different use cases: while `foreach()` allows custom write logic on every
    row, `foreachBatch()` allows arbitrary operations and custom logic on the output
    of each micro-batch. Let’s explore their usage in more detail.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种操作允许您将流查询的输出写入任意存储系统：`foreachBatch()` 和 `foreach()`。它们有略微不同的用例：`foreach()`
    允许在每一行上进行自定义写入逻辑，而 `foreachBatch()` 则允许在每个微批次的输出上执行任意操作和自定义逻辑。让我们更详细地探讨它们的用法。
- en: Using foreachBatch()
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 foreachBatch()
- en: '`foreachBatch()` allows you to specify a function that is executed on the output
    of every micro-batch of a streaming query. It takes two parameters: a DataFrame
    or Dataset that has the output of a micro-batch, and the unique identifier of
    the micro-batch. As an example, say we want to write the output of our earlier
    word count query to [Apache Cassandra](http://cassandra.apache.org/). As of [Spark
    Cassandra Connector 2.4.2](https://oreil.ly/I7Mof), there is no support for writing
    streaming DataFames. But you can use the connector’s batch DataFrame support to
    write the output of each batch (i.e., updated word counts) to Cassandra, as shown
    here:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`foreachBatch()` 允许您指定一个函数，在流查询的每个微批次的输出上执行该函数。它接受两个参数：一个 DataFrame 或 Dataset，它包含微批次的输出，以及微批次的唯一标识符。例如，假设我们想要将前面的单词计数查询的输出写入
    [Apache Cassandra](http://cassandra.apache.org/)。截至 [Spark Cassandra Connector
    2.4.2](https://oreil.ly/I7Mof)，尚不支持写入流数据集。但是，您可以使用连接器的批处理 DataFrame 支持，将每个批次的输出（即更新的单词计数）写入
    Cassandra，如下所示：'
- en: '[PRE27]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'With `foreachBatch()`, you can do the following:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `foreachBatch()`，您可以执行以下操作：
- en: Reuse existing batch data sources
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 重用现有的批处理数据源
- en: As shown in the previous example, with `foreachBatch()` you can use existing
    batch data sources (i.e., sources that support writing batch DataFrames) to write
    the output of streaming queries.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的示例所示，通过 `foreachBatch()` 您可以使用现有的批处理数据源（即支持写入批处理 DataFrame 的数据源）来写入流查询的输出。
- en: Write to multiple locations
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 写入多个位置
- en: 'If you want to write the output of a streaming query to multiple locations
    (e.g., an OLAP data warehouse and an OLTP database), then you can simply write
    the output DataFrame/Dataset multiple times. However, each attempt to write can
    cause the output data to be recomputed (including possible rereading of the input
    data). To avoid recomputations, you should cache the `batchOutputDataFrame`, write
    it to multiple locations, and then uncache it:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要将流查询的输出写入多个位置（例如 OLAP 数据仓库和 OLTP 数据库），则可以简单地多次写入输出 DataFrame/Dataset。然而，每次写入尝试可能导致输出数据重新计算（包括可能重新读取输入数据）。为了避免重新计算，您应该将
    `batchOutputDataFrame` 缓存起来，将其写入多个位置，然后取消缓存：
- en: '[PRE29]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Apply additional DataFrame operations
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 应用额外的 DataFrame 操作
- en: Many DataFrame API operations are not supported^([3](ch08.html#ch01fn11)) on
    streaming DataFrames because Structured Streaming does not support generating
    incremental plans in those cases. Using `foreachBatch()`, you can apply some of
    these operations on each micro-batch output. However, you will have to reason
    about the end-to-end semantics of doing the operation yourself.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 DataFrame API 操作在流式 DataFrame 上不受支持^([3](ch08.html#ch01fn11))，因为结构化流在这些情况下不支持生成增量计划。使用
    `foreachBatch()`，您可以在每个微批次输出上应用其中一些操作。但是，您必须自行推理执行操作的端到端语义。
- en: Note
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '`foreachBatch()` only provides at-least-once write guarantees. You can get
    exactly-once guarantees by using the `batchId` to deduplicate multiple writes
    from reexecuted micro-batches.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`foreachBatch()` 仅提供至少一次写入保证。通过使用 `batchId` 来消除重新执行的微批次中的多次写入，可以获得精确一次的保证。'
- en: Using foreach()
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 foreach()
- en: 'If `foreachBatch()` is not an option (for example, if a corresponding batch
    data writer does not exist), then you can express your custom writer logic using
    `foreach()`. Specifically, you can express the data-writing logic by dividing
    it into three methods: `open()`, `process()`, and `close()`. Structured Streaming
    will use these methods to write each partition of the output records. Here is
    an abstract example:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `foreachBatch()` 不是一个选项（例如，不存在相应的批数据写入器），那么可以使用 `foreach()` 表达您自定义的写入逻辑。具体而言，您可以通过将其分为
    `open()`、`process()` 和 `close()` 三种方法来表达数据写入逻辑。结构化流将使用这些方法来写入输出记录的每个分区。以下是一个抽象示例：
- en: '[PRE31]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The detailed semantics of these methods as executed are discussed in the [Structured
    Streaming Programming Guide](https://oreil.ly/dL7mc).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些方法的详细语义如何执行，请参阅[结构化流编程指南](https://oreil.ly/dL7mc)。
- en: Reading from any storage system
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从任何存储系统读取
- en: Unfortunately, as of Spark 3.0, the APIs to build custom streaming sources and
    sinks are still experimental. The DataSourceV2 initiative in Spark 3.0 introduces
    the streaming APIs but they are yet to be declared as stable. Hence, there is
    no official way to read from arbitrary storage systems.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，截至 Spark 3.0，构建自定义流源和接收器的 API 仍处于实验阶段。Spark 3.0 中的 DataSourceV2 倡议引入了流式
    API，但尚未宣布稳定。因此，目前没有官方方法可以从任意存储系统读取。
- en: Data Transformations
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换
- en: In this section, we are going to dig deeper into the data transformations supported
    in Structured Streaming. As briefly discussed earlier, only the DataFrame operations
    that can be executed incrementally are supported in Structured Streaming. These
    operations are broadly classified into *stateless* and *stateful* operations.
    We will define each type of operation and explain how to identify which operations
    are stateful.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨结构化流支持的数据转换。正如之前简要讨论的那样，在结构化流中，只支持可以增量执行的 DataFrame 操作。这些操作广泛分为*无状态*和*有状态*操作。我们将定义每种操作类型，并解释如何识别哪些操作是有状态的。
- en: Incremental Execution and Streaming State
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增量执行和流式状态
- en: As we discussed in [“Under the Hood of an Active Streaming Query”](#under_the_hood_of_an_active_streaming_qu),
    the Catalyst optimizer in Spark SQL converts all the DataFrame operations to an
    optimized logical plan. The Spark SQL planner, which decides how to execute a
    logical plan, recognizes that this is a streaming logical plan that needs to operate
    on continuous data streams. Accordingly, instead of converting the logical plan
    to a one-time physical execution plan, the planner generates a continuous sequence
    of execution plans. Each execution plan updates the final result DataFrame incrementally—that
    is, the plan processes only a chunk of new data from the input streams and possibly
    some intermediate, partial result computed by the previous execution plan.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[“一个活跃流式查询的内部工作原理”](https://oreil.ly/dL7mc)中讨论的那样，在 Spark SQL 的 Catalyst
    优化器中，将所有 DataFrame 操作转换为优化的逻辑计划。Spark SQL 计划器决定如何执行逻辑计划时，会识别这是一个需要在连续数据流上操作的流式逻辑计划。因此，计划器不会将逻辑计划转换为一次性物理执行计划，而是生成连续的执行计划序列。每个执行计划都会增量更新最终的结果
    DataFrame —— 也就是说，该计划仅处理来自输入流的新数据块，可能还会处理上一个执行计划计算的一些中间部分结果。
- en: Each execution is considered as a micro-batch, and the partial intermediate
    result that is communicated between the executions is called the streaming “state.”
    DataFrame operations can be broadly classified into stateless and stateful operations
    based on whether executing the operation incrementally requires maintaining a
    state. In the rest of this section, we are going to explore the distinction between
    stateless and stateful operations and how their presence in a streaming query
    requires different runtime configuration and resource management.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 每次执行被视为一个微批处理，并且在执行之间传递的部分中间结果被称为流“状态”。DataFrame操作可以根据执行操作增量需要维护状态来广泛分类为无状态和有状态操作。在本节的其余部分，我们将探讨无状态和有状态操作之间的区别，以及它们在流查询中的存在如何需要不同的运行时配置和资源管理。
- en: Note
  id: totrans-275
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Some logical operations are fundamentally either impractical or very expensive
    to compute incrementally, and hence they are not supported in Structured Streaming.
    For example, any attempt to start a streaming query with an operation like `cube()`
    or `rollup()` will throw an `UnsupportedOperationException`.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 一些逻辑操作基本上不可能或者计算增量非常昂贵，因此不支持结构化流。例如，任何尝试以`cube()`或`rollup()`等操作启动流查询的尝试将引发`UnsupportedOperationException`。
- en: Stateless Transformations
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无状态转换
- en: All projection operations (e.g., `select()`, `explode()`, `map()`, `flatMap()`)
    and selection operations (e.g., `filter()`, `where()`) process each input record
    individually without needing any information from previous rows. This lack of
    dependence on prior input data makes them stateless operations.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 所有投影操作（例如`select()`、`explode()`、`map()`、`flatMap()`）和选择操作（例如`filter()`、`where()`）都会逐个处理每个输入记录，无需任何先前行的信息。这种不依赖于先前输入数据的特性使它们成为无状态操作。
- en: 'A streaming query having only stateless operations supports the append and
    update output modes, but not complete mode. This makes sense: since any processed
    output row of such a query cannot be modified by any future data, it can be written
    out to all streaming sinks in append mode (including append-only ones, like files
    of any format). On the other hand, such queries naturally do not combine information
    across input records, and therefore may not reduce the volume of the data in the
    result. Complete mode is not supported because storing the ever-growing result
    data is usually costly. This is in sharp contrast with stateful transformations,
    as we will discuss next.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 仅具有无状态操作的流查询支持追加和更新输出模式，但不支持完整模式。这是有道理的：由于此类查询的任何处理输出行都不能被未来的数据修改，因此可以将其写入所有流式接收器的追加模式中（包括仅追加的接收器，如任何格式的文件）。另一方面，这样的查询自然不会跨输入记录组合信息，因此可能不会减少结果中数据的量。通常不支持完整模式，因为存储不断增长的结果数据通常代价高昂。这与有状态的转换形成鲜明对比，接下来我们将讨论这一点。
- en: Stateful Transformations
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有状态的转换
- en: The simplest example of a stateful transformation is `DataFrame.groupBy().count()`,
    which generates a running count of the number of records received since the beginning
    of the query. In every micro-batch, the incremental plan adds the count of new
    records to the previous count generated by the previous micro-batch. This partial
    count communicated between plans is the state. This state is maintained in the
    memory of the Spark executors and is checkpointed to the configured location in
    order to tolerate failures. While Spark SQL automatically manages the life cycle
    of this state to ensure correct results, you typically have to tweak a few knobs
    to control the resource usage for maintaining state. In this section, we are going
    to explore how different stateful operators manage their state under the hood.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 有状态转换的最简单示例是`DataFrame.groupBy().count()`，它生成自查询开始以来接收到的记录数量的累积计数。在每个微批处理中，增量计划将新记录的计数添加到前一个微批处理生成的先前计数中。计划之间传递的部分计数即为状态。这种状态保存在Spark执行器的内存中，并且以检查点的方式保存到配置的位置，以容忍故障。尽管Spark
    SQL会自动管理此状态的生命周期以确保正确的结果，但通常需要调整一些参数来控制维护状态的资源使用情况。在本节中，我们将探讨不同有状态操作符在底层管理其状态的方式。
- en: Distributed and fault-tolerant state management
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式和容错的状态管理
- en: Recall from Chapters [1](ch01.html#introduction_to_apache_spark_a_unified_a)
    and [2](ch02.html#downloading_apache_spark_and_getting_sta) that a Spark application
    running in a cluster has a driver and one or more executors. Spark’s scheduler
    running in the driver breaks down your high-level operations into smaller tasks
    and puts them in task queues, and as resources become available, the executors
    pull the tasks from the queues to execute them. Each micro-batch in a streaming
    query essentially performs one such set of tasks that read new data from streaming
    sources and write updated output to streaming sinks. For stateful stream processing
    queries, besides writing to sinks, each micro-batch of tasks generates intermediate
    state data which will be consumed by the next micro-batch. This state data generation
    is completely partitioned and distributed (as all reading, writing, and processing
    is in Spark), and it is cached in the executor memory for efficient consumption.
    This is illustrated in [Figure 8-6](#distributed_state_management_in_structur),
    which shows how the state is managed in our original streaming word count query.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 从第 [1](ch01.html#introduction_to_apache_spark_a_unified_a) 章和第 [2](ch02.html#downloading_apache_spark_and_getting_sta)
    章回忆起，运行在集群中的 Spark 应用程序具有一个 driver 和一个或多个 executor。驱动器中运行的 Spark 调度程序将您的高级操作分解为更小的任务，并将它们放入任务队列中，当资源可用时，执行者从队列中拉取任务以执行。流查询中的每个微批次基本上执行一组从流源读取新数据并将更新后的输出写入流接收器的任务集。对于有状态的流处理查询，除了写入接收器，每个微批次的任务还会生成中间状态数据，这些数据将由下一个微批次消耗。这些状态数据生成完全分区和分布（与
    Spark 中的所有读取、写入和处理一样），并且被缓存在执行者内存中以便高效消费。这在 [图 8-6](#distributed_state_management_in_structur)
    中有所体现，展示了我们原始的流式单词计数查询中的状态管理方式。
- en: '![Distributed state management in Structured Streaming](assets/lesp_0806.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![结构化流式处理中的分布式状态管理](assets/lesp_0806.png)'
- en: Figure 8-6\. Distributed state management in Structured Streaming
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-6\. 结构化流式处理中的分布式状态管理
- en: Each micro-batch reads a new set of words, shuffles them within the executors
    to group them, computes the counts within the micro-batch, and finally adds them
    to the running counts to produce the new counts. These new counts are both the
    output and the state for the next micro-batch, and hence they are cached in the
    memory of the executors. The next micro-batch of data is grouped between executors
    in exactly the same way as before, so that each word is always processed by the
    same executor, and can therefore locally read and update its running count.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 每个微批次读取一组新的单词，将它们在执行者内部进行分组以计算每个微批次中的计数，最后将它们添加到运行计数中以生成新的计数。这些新计数既是输出也是下一个微批次的状态，并因此缓存在执行者的内存中。下一个数据微批次在执行者之间的分组方式与之前完全相同，因此每个单词始终由相同的执行者处理，可以在本地读取和更新其运行计数。
- en: However, it is not sufficient to just keep this state in memory, as any failure
    (either of an executor or of the entire application) will cause the in-memory
    state to be lost. To avoid loss, we synchronously save the key/value state update
    as change logs in the checkpoint location provided by the user. These changes
    are co-versioned with the offset ranges processed in each batch, and the required
    version of the state can be automatically reconstructed by reading the checkpointed
    logs. In case of any failure, Structured Streaming is able to re-execute the failed
    micro-batch by reprocessing the same input data along with the same state that
    it had before that micro-batch, thus producing the same output data as it would
    have if there had been no failure. This is critical for ensuring end-to-end exactly-once
    guarantees.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅将这些状态保存在内存中是不够的，因为任何失败（无论是执行者的失败还是整个应用程序的失败）都会导致内存中的状态丢失。为了避免丢失，我们将键/值状态更新同步保存为用户提供的检查点位置中的变更日志。这些更改与每个批次处理的偏移范围共同版本化，通过读取检查点日志可以自动重建所需版本的状态。在任何故障情况下，结构化流处理能够通过重新处理相同的输入数据以及之前微批次中的相同状态来重新执行失败的微批次，从而生成与没有失败时相同的输出数据。这对于确保端到端的精确一次性保证至关重要。
- en: To summarize, for all stateful operations, Structured Streaming ensures the
    correctness of the operation by automatically saving and restoring the state in
    a distributed manner. Depending on the stateful operation, all you may have to
    do is tune the state cleanup policy such that old keys and values can be automatically
    dropped from the cached state. This is what we will discuss next.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，对于所有有状态操作，结构化流处理通过在分布式方式下自动保存和恢复状态来确保操作的正确性。根据有状态操作的不同，您可能只需调整状态清理策略，以便可以自动从缓存状态中丢弃旧键和值。接下来我们将讨论这一点。
- en: Types of stateful operations
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有状态操作类型
- en: 'The essence of streaming state is to retain summaries of past data. Sometimes
    old summaries need to be cleaned up from the state to make room for new summaries.
    Based on how this is done, we can distinguish two types of stateful operations:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 流式状态的核心是保留过去数据的摘要。有时需要清理状态中的旧摘要，以为新摘要腾出空间。根据操作方式的不同，我们可以区分两种类型的有状态操作：
- en: Managed stateful operations
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 托管的有状态操作
- en: 'These automatically identify and clean up old state, based on an operation-specific
    definition of “old.” You can tune what is defined as old in order to control the
    resource usage (e.g., executor memory used to store state). The operations that
    fall into this category are those for:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作根据特定于操作的“旧”定义自动识别和清理旧状态。您可以调整“旧”定义以控制资源使用情况（例如，用于存储状态的执行器内存）。属于此类别的操作包括：
- en: Streaming aggregations
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式聚合
- en: Stream–stream joins
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流-流连接
- en: Streaming deduplication
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式去重
- en: Unmanaged stateful operations
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 非托管的有状态操作
- en: 'These operations let you define your own custom state cleanup logic. The operations
    in this category are:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作允许您定义自己的自定义状态清理逻辑。本类别中的操作包括：
- en: '`MapGroupsWithState`'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MapGroupsWithState`'
- en: '`FlatMapGroupsWithState`'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FlatMapGroupsWithState`'
- en: These operations allow you to define arbitrary stateful operations (sessionization,
    etc.).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作允许您定义任意的有状态操作（如会话化等）。
- en: Each of these operations are discussed in detail in the following sections.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各操作在以下各节中详细讨论。
- en: Stateful Streaming Aggregations
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有状态流聚合
- en: Structured Streaming can incrementally execute most DataFrame aggregation operations.
    You can aggregate data by keys (e.g., streaming word count) and/or by time (e.g.,
    count records received every hour). In this section, we are going to discuss the
    semantics and operational details of tuning these different types of streaming
    aggregations. We’ll also briefly discuss the few types of aggregations that are
    not supported in streaming. Let’s begin with aggregations not involving time.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理可以逐步执行大多数 DataFrame 聚合操作。您可以按键（例如，流式词频统计）和/或时间（例如，每小时接收到的记录计数）聚合数据。在本节中，我们将讨论调整这些不同类型流式聚合的语义和操作细节。我们还将简要讨论一些不支持流式操作的聚合类型。让我们从不涉及时间的聚合开始。
- en: Aggregations Not Based on Time
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不基于时间的聚合
- en: 'Aggregations not involving time can be broadly classified into two categories:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 不涉及时间的聚合可以大致分为两类：
- en: Global aggregations
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 全局聚合
- en: 'Aggregations across all the data in the stream. For example, say you have a
    stream of sensor readings as a streaming DataFrame named `sensorReadings`. You
    can calculate the running count of the total number of readings received with
    the following query:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 跨流中所有数据的聚合。例如，假设您有一个名为 `sensorReadings` 的流式 DataFrame，表示传感器读数流。您可以使用以下查询计算接收到的读数总数的运行计数：
- en: '[PRE33]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note
  id: totrans-310
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You cannot use direct aggregation operations like `DataFrame.count()` and `Dataset.reduce()`
    on streaming DataFrames. This is because, for static DataFrames, these operations
    immediately return the final computed aggregates, whereas for streaming DataFrames
    the aggregates have to be continuously updated. Therefore, you have to always
    use `DataFrame.groupBy()` or `Dataset.groupByKey()` for aggregations on streaming
    DataFrames.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 您不能在流式 DataFrame 上直接使用聚合操作，例如 `DataFrame.count()` 和 `Dataset.reduce()`。这是因为对于静态
    DataFrame，这些操作会立即返回最终计算的聚合结果，而对于流式 DataFrame，聚合结果必须持续更新。因此，您必须始终使用 `DataFrame.groupBy()`
    或 `Dataset.groupByKey()` 进行流式 DataFrame 的聚合。
- en: Grouped aggregations
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 分组聚合
- en: 'Aggregations within each group or key present in the data stream. For example,
    if `sensorReadings` contains data from multiple sensors, you can calculate the
    running average reading of each sensor (say, for setting up a baseline value for
    each sensor) with the following:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据流中的每个组或键中进行聚合。例如，如果 `sensorReadings` 包含来自多个传感器的数据，则可以计算每个传感器的运行平均读数（例如，为每个传感器设置基准值），如下所示：
- en: '[PRE35]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Besides counts and averages, streaming DataFrames support the following types
    of aggregations (similar to batch DataFrames):'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 除计数和平均值之外，流式 DataFrame 还支持以下类型的聚合（类似于批处理 DataFrame）：
- en: All built-in aggregation functions
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 所有内置的聚合函数
- en: '`sum()`, `mean()`, `stddev()`, `countDistinct()`, `collect_set()`, `approx_count_distinct()`,
    etc. Refer to the API documentation ([Python](https://oreil.ly/olWT0) and [Scala](https://oreil.ly/gvoeK))
    for more details.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '`sum()`, `mean()`, `stddev()`, `countDistinct()`, `collect_set()`, `approx_count_distinct()`
    等。更多详细信息请参阅 API 文档（[Python](https://oreil.ly/olWT0) 和 [Scala](https://oreil.ly/gvoeK)）。'
- en: Multiple aggregations computed together
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 一起计算多个聚合
- en: 'You can apply multiple aggregation functions to be computed together in the
    following manner:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将多个聚合函数应用于以下方式一起计算：
- en: '[PRE37]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: User-defined aggregation functions
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 用户定义的聚合函数
- en: All user-defined aggregation functions are supported. See the [Spark SQL programming
    guide](https://oreil.ly/8nvJ2) for more details on untyped and typed user-defined
    aggregation functions.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 支持所有用户定义的聚合函数。有关无类型和有类型用户定义聚合函数的更多详细信息，请参阅 [Spark SQL 编程指南](https://oreil.ly/8nvJ2)。
- en: 'Regarding the execution of such streaming aggregations, we have already illustrated
    in previous sections how the running aggregates are maintained as a distributed
    state. In addition to this, there are two very important points to remember for
    aggregations not based on time: the output mode to use for such queries and planning
    the resource usage by state. These are discussed toward the end of this section.
    Next, we are going to discuss aggregations that combine data within time windows.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 关于执行此类流式聚合，我们已经在前面的章节中说明了如何将运行聚合保持为分布式状态。除此之外，还有两个非常重要的注意事项适用于非时间依赖的聚合：用于此类查询的输出模式和通过状态规划资源使用。这些将在本节末尾讨论。接下来，我们将讨论在时间窗口内组合数据的聚合。
- en: Aggregations with Event-Time Windows
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事件时间窗口内的聚合
- en: 'In many cases, rather than running aggregations over the whole stream, you
    want aggregations over data bucketed by time windows. Continuing with our sensor
    example, say each sensor is expected to send at most one reading per minute and
    we want to detect if any sensor is reporting an unusually high number of times.
    To find such anomalies, we can count the number of readings received from each
    sensor in five-minute intervals. In addition, for robustness, we should be computing
    the time interval based on when the data was generated at the sensor and not based
    on when the data was received, as any transit delay would skew the results. In
    other words, we want to use the *event time*—that is, the timestamp in the record
    representing when the reading was generated. Say the `sensorReadings` DataFrame
    has the generation timestamp as a column named `eventTime`. We can express this
    five-minute count as follows:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，不是对整个流执行聚合，而是希望对按时间窗口分桶的数据执行聚合。继续使用传感器示例，假设每个传感器每分钟最多发送一次读数，并且我们希望检测任何传感器报告异常高次数。为了找到这样的异常，我们可以计算每个传感器在五分钟间隔内接收到的读数数量。此外，为了保证稳健性，应基于数据在传感器生成时生成的时间间隔计算时间间隔，而不是基于接收数据时的时间，因为任何传输延迟都会使结果偏斜。换句话说，我们要使用*事件时间*——即记录中的时间戳，表示生成读数的时间。假设
    `sensorReadings` DataFrame 具有命名为 `eventTime` 的生成时间戳列。我们可以将这个五分钟计数表示如下：
- en: '[PRE39]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The key thing to note here is the `window()` function, which allows us to express
    the five-minute windows as a dynamically computed grouping column. When started,
    this query will effectively do the following for each sensor reading:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的关键是 `window()` 函数，它允许我们将五分钟窗口表达为动态计算的分组列。启动时，此查询将为每个传感器读数执行以下操作：
- en: Use the `eventTime` value to compute the five-minute time window the sensor
    reading falls into.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `eventTime` 值计算传感器读数所在的五分钟时间窗口。
- en: Group the reading based on the composite group `(*<computed window>*, SensorId)`.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于组合组 `(*<computed window>*, SensorId)` 对读数进行分组。
- en: Update the count of the composite group.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新组合组的计数。
- en: Let’s understand this with an illustrative example. [Figure 8-7](#mapping_of_event_time_to_tumbling_window)
    shows how a few sensor readings are mapped to groups of five-minute tumbling (i.e.,
    nonoverlapping) windows based on their event time. The two timelines show when
    each received event will be processed by Structured Streaming, and the timestamp
    in the event data (usually, the time when the event was generated at the sensor).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例来理解这个过程。[图 8-7](#mapping_of_event_time_to_tumbling_window)展示了如何将一些传感器读数映射到基于事件时间的五分钟滚动（即非重叠）窗口组。两个时间线展示了每个接收事件将被结构化流处理的时间，以及事件数据中的时间戳（通常是传感器生成事件时的时间）。
- en: '![Mapping of event time to tumbling windows](assets/lesp_0807.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![事件时间映射到滚动窗口](assets/lesp_0807.png)'
- en: Figure 8-7\. Mapping of event time to tumbling windows
  id: totrans-336
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-7\. 事件时间映射到滚动窗口
- en: 'Each five-minute window over event time is considered for the grouping based
    on which the counts will be calculated. Note that events may come late and out
    of order in terms of event time. As shown in the figure, the event with event
    time 12:07 was received and processed after the event with time 12:11\. However,
    irrespective of when they arrive, each event is assigned to the appropriate group
    based on its event time. In fact, depending on the window specification, each
    event can be assigned to multiple groups. For example, if you want to compute
    counts corresponding to 10-minute windows sliding every 5 minutes, then you can
    do the following:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 基于事件时间，每个五分钟窗口都用于基于组的计数。请注意，事件可能会以晚到或乱序的方式到达。如图所示，事件时间为 12:07 的事件在事件时间为 12:11
    的事件之后接收和处理。然而，无论它们何时到达，每个事件都会根据其事件时间分配到适当的组中。实际上，根据窗口规范，每个事件可能会分配到多个组。例如，如果要计算每隔
    5 分钟滑动的 10 分钟窗口对应的计数，则可以执行以下操作：
- en: '[PRE41]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In this query, every event will be assigned to two overlapping windows as illustrated
    in [Figure 8-8](#mapping_of_event_time_to_multiple_overla).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在此查询中，每个事件都将被分配到两个重叠的窗口，如[图 8-8](#mapping_of_event_time_to_multiple_overla)所示。
- en: '![Mapping of event time to multiple overlapping windows](assets/lesp_0808.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![事件时间映射到多个重叠窗口](assets/lesp_0808.png)'
- en: Figure 8-8\. Mapping of event time to multiple overlapping windows
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-8\. 事件时间映射到多个重叠窗口
- en: Each unique tuple of `(*<assigned time window>*, sensorId)` is considered a
    dynamically generated group for which counts will be computed. For example, the
    event `[eventTime = 12:07, sensorId = id1]` gets mapped to two time windows and
    therefore two groups, `(12:00-12:10, id1)` and `(12:05-12:15, id1)`. The counts
    for these two windows are each incremented by 1\. [Figure 8-9](#updated_counts_in_the_result_table_after)
    illustrates this for the previously shown events.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 每个唯一的元组`(*<分配的时间窗口>*, sensorId)`被视为动态生成的组，将对其进行计数。例如，事件`[eventTime = 12:07,
    sensorId = id1]`映射到两个时间窗口，因此生成两个组，`(12:00-12:10, id1)`和`(12:05-12:15, id1)`。这两个窗口的计数分别增加了
    1\. [图 8-9](#updated_counts_in_the_result_table_after) 对之前展示的事件进行了说明。
- en: Assuming that the input records were processed with a trigger interval of five
    minutes, the tables at the bottom of [Figure 8-9](#updated_counts_in_the_result_table_after)
    show the state of the result table (i.e., the counts) at each of the micro-batches.
    As the event time moves forward, new groups are automatically created and their
    aggregates are automatically updated. Late and out-of-order events get handled
    automatically, as they simply update older groups.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 假设输入记录以五分钟的触发间隔进行处理，底部的表在[图 8-9](#updated_counts_in_the_result_table_after)显示了结果表（即计数）在每个微批处理时的状态。随着事件时间的推移，新的组会自动创建并且它们的聚合会自动更新。晚到和乱序的事件会被自动处理，因为它们只是更新旧的组。
- en: '![Updated counts in the result table after each 5-minute trigger](assets/lesp_0809.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![每隔 5 分钟触发后结果表中的更新计数](assets/lesp_0809.png)'
- en: Figure 8-9\. Updated counts in the result table after each five-minute trigger
  id: totrans-346
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-9\. 每隔五分钟触发后结果表中的更新计数
- en: However, from the point of view of resource usage, this poses a different problem—indefinitely
    growing state size. As new groups are created corresponding to the latest time
    windows, the older groups continue to occupy the state memory, waiting for any
    late data to update them. Even if in practice there is a bound on how late the
    input data can be (e.g., data cannot be more than seven days late), the query
    does not know that information. Hence, it does not know when to consider a window
    as “too old to receive updates” and drop it from the state. To provide a lateness
    bound to a query (and prevent unbounded state), you can specify *watermarks*,
    as we discuss next.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从资源使用的角度来看，这带来了另一个问题——状态大小无限增长。随着创建新组对应于最新时间窗口，旧组继续占用状态内存，等待任何延迟数据更新它们。即使在实践中，输入数据的延迟可能有一个界限（例如，数据不能迟于七天），但查询不知道这些信息。因此，它不知道何时将窗口视为“太旧以接收更新”并将其从状态中删除。为了向查询提供延迟边界（并防止无界状态），您可以指定*水印*，接下来我们将讨论它。
- en: Handling late data with watermarks
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理延迟数据与水印
- en: A *watermark* is defined as a moving threshold in event time that trails behind
    the maximum event time seen by the query in the processed data. The trailing gap,
    known as the *watermark delay*, defines how long the engine will wait for late
    data to arrive. By knowing the point at which no more data will arrive for a given
    group, the engine can automatically finalize the aggregates of certain groups
    and drop them from the state. This limits the total amount of state that the engine
    has to maintain to compute the results of the query.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '*水印*定义为事件时间中的移动阈值，落后于查询在处理数据中观察到的最大事件时间。这种落后的间隙被称为*水印延迟*，它定义了引擎等待延迟数据到达的时间。通过了解对于给定组不会再有更多数据到达的时间点，引擎可以自动完成某些组的聚合并将其从状态中删除。这限制了引擎必须维护的状态总量，以计算查询结果。'
- en: 'For example, suppose you know that your sensor data will not be late by more
    than 10 minutes. Then you can set the watermark as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您知道您的传感器数据不会迟到超过10分钟。那么，您可以设置水印如下：
- en: '[PRE43]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note that you must call `withWatermark()` before the `groupBy()` and on the
    same timestamp column as that used to define windows. When this query is executed,
    Structured Streaming will continuously track the maximum observed value of the
    `eventTime` column and accordingly update the watermark, filter the “too late”
    data, and clear old state. That is, any data late by more than 10 minutes will
    be ignored, and all time windows that are more than 10 minutes older than the
    latest (by event time) input data will be cleaned up from the state. To clarify
    how this query will be executed, consider the timeline in [Figure 8-10](#illustration_of_how_the_engine_tracks_th)
    showing how a selection of input records were processed.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在`groupBy()`之前必须调用`withWatermark()`，并且在与用于定义窗口的时间戳列相同的时间戳列上。当执行此查询时，结构化流将持续跟踪`eventTime`列的观察到的最大值，并相应更新水印，过滤“太迟”的数据，并清除旧状态。也就是说，任何迟到超过10分钟的数据都将被忽略，并且所有比最新（按事件时间）输入数据早10分钟以上的时间窗口将从状态中清除。为了澄清此查询的执行方式，请考虑图[8-10](#illustration_of_how_the_engine_tracks_th)中显示的时间线，显示了如何处理输入记录的选择。
- en: '![Illustration of how the engine tracks the maximum event time across events,
    updates the watermark, and accordingly handles late data](assets/lesp_0810.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![展示引擎如何跟踪事件的最大时间戳，更新水印，并相应处理延迟数据的插图](assets/lesp_0810.png)'
- en: Figure 8-10\. Illustration of how the engine tracks the maximum event time across
    events, updates the watermark, and accordingly handles late data
  id: totrans-355
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-10\. 展示引擎如何跟踪事件的最大时间戳，更新水印，并相应处理延迟数据
- en: This figure shows a two-dimensional plot of records processed in terms of their
    processing times (x-axis) and their event times (y-axis). The records are processed
    in micro-batches of five minutes and marked with circles. The tables at the bottom
    show the state of the result table after each micro-batch completes.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示了记录按其处理时间（x轴）和事件时间（y轴）处理的二维图。记录以五分钟的微批次处理，并标记为圆圈。底部的表显示了每个微批次完成后结果表的状态。
- en: Each record was received and processed after all the records to its left. Consider
    the two records `[12:15, id1]` (processed around 12:17) and `[12:13, id3]` (processed
    around 12:18). The record for `id3` was considered late (and therefore marked
    in solid red) because it was generated by the sensor before the record for `id1`
    but it was processed after the latter. However, in the micro-batch for processing-time
    range 12:15–12:20, the watermark used was 12:04 which was calculated based on
    the maximum event time seen till the previous micro-batch (that is, 12:14 minus
    the 10-minute watermark delay). Therefore, the late record `[12:13, id3]` was
    not considered to be too late and was successfully counted. In contrast, in the
    next micro-batch, the record `[12:04, id1]` was considered to be too late compared
    to the new watermark of 12:11 and was discarded.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 每条记录在所有左侧记录接收并处理后收到。考虑两条记录 `[12:15, id1]`（大约在12:17处理）和 `[12:13, id3]`（大约在12:18处理）。记录
    `id3` 因为在传感器生成 `id1` 记录之前生成，但在后者之后处理，因此被认为是迟到的（并因此标记为红色实线）。然而，在处理时间范围为12:15–12:20的微批次中，水印使用的是12:04，该时间基于前一微批次中看到的最大事件时间（即12:14减去10分钟水印延迟）。因此，迟到的记录
    `[12:13, id3]` 不被认为太迟并成功计数。相反，在下一个微批次中，记录 `[12:04, id1]` 因与新水印12:11比较被认为太迟，并被丢弃。
- en: You can set the watermark delay based on the requirements of your application—larger
    values for this parameter allow data to arrive later, but at the cost of increased
    state size (i.e., memory usage), and vice versa.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以根据应用程序的要求设置水印延迟——此参数的较大值允许数据稍晚到达，但代价是增加状态大小（即内存使用），反之亦然。
- en: Semantic guarantees with watermarks
  id: totrans-359
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 水印的语义保证
- en: Before we conclude this section about watermarks, let’s consider the precise
    semantic guarantee that watermarking provides. A watermark of 10 minutes guarantees
    that the engine will *never drop any data* that is delayed by less than 10 minutes
    compared to the latest event time seen in the input data. However, the guarantee
    is strict only in one direction. Data delayed by more than 10 minutes is not guaranteed
    to be dropped—that is, it may get aggregated. Whether an input record more than
    10 minutes late will actually be aggregated or not depends on the exact timing
    of when the record was received and when the micro-batch processing it was triggered.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束关于水印的本节之前，让我们考虑水印提供的精确语义保证。10分钟的水印保证引擎 *永远不会丢弃* 与输入数据中的最新事件时间相比延迟不到10分钟的任何数据。然而，该保证仅在一个方向上严格。延迟超过10分钟的数据不保证会被丢弃——也就是说，它可能会被聚合。输入记录是否会聚合取决于记录接收和触发处理它的微批次的确切时间。
- en: Supported output modes
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持的输出模式
- en: 'Unlike streaming aggregations not involving time, aggregations with time windows
    can use all three output modes. However, there are other implications regarding
    state cleanup that you need to be aware of, depending on the mode:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 与不涉及时间的流聚合不同，使用时间窗口的聚合可以使用所有三种输出模式。然而，根据模式，您需要注意与状态清理相关的其他影响：
- en: Update mode
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 更新模式
- en: In this mode, every micro-batch will output only the rows where the aggregate
    got updated. This mode can be used with all types of aggregations. Specifically
    for time window aggregations, watermarking will ensure that the state will get
    cleaned up regularly. This is the most useful and efficient mode to run queries
    with streaming aggregations. However, you cannot use this mode to write aggregates
    to append-only streaming sinks, such as any file-based formats like Parquet and
    ORC (unless you use Delta Lake, which we will discuss in the next chapter).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在此模式下，每个微批次仅输出聚合得到更新的行。此模式可用于所有类型的聚合。特别是对于时间窗口聚合，水印将确保状态定期清理。这是运行带有流聚合查询的最有用和高效的模式。然而，您不能使用此模式将聚合写入仅附加流目标，如任何基于文件的格式，如Parquet和ORC（除非您使用Delta
    Lake，我们将在下一章中讨论）。
- en: Complete mode
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 完全模式
- en: In this mode, every micro-batch will output all the updated aggregates, irrespective
    of their age or whether they contain changes. While this mode can be used on all
    types of aggregations, for time window aggregations, using complete mode means
    state will not be cleaned up even if a watermark is specified. Outputting all
    aggregates requires all past state, and hence aggregation data must be preserved
    even if a watermark has been defined. Use this mode on time window aggregations
    with caution, as this can lead to an indefinite increase in state size and memory
    usage.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在此模式下，每个微批次都会输出所有更新的聚合结果，无论它们的年龄或是否包含更改。虽然这种模式可用于所有类型的聚合，但对于时间窗口聚合来说，使用完整模式意味着即使指定了水印，状态也不会被清除。输出所有聚合需要所有过去的状态，因此即使已定义水印，聚合数据也必须保留。在使用时间窗口聚合时要谨慎使用此模式，因为这可能导致状态大小和内存使用无限增加。
- en: Append mode
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 追加模式
- en: '*This mode can be used only with aggregations on event-time windows and with
    watermarking enabled*. Recall that append mode does not allow previously output
    results to change. For any aggregation without watermarks, every aggregate may
    be updated with any future data, and hence these cannot be output in append mode.
    Only when watermarking is enabled on aggregations on event-time windows does the
    query know when an aggregate is not going to update any further. Hence, instead
    of outputting the updated rows, append mode outputs each key and its final aggregate
    value only when the watermark ensures that the aggregate is not going to be updated
    again. The advantage of this mode is that it allows you to write aggregates to
    append-only streaming sinks (e.g., files). The disadvantage is that the output
    will be delayed by the watermark duration—the query has to wait for the trailing
    watermark to exceed the time window of a key before its aggregate can be finalized.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '*此模式仅适用于事件时间窗口上的聚合，并且启用了水印*。请注意，追加模式不允许先前输出的结果更改。对于没有水印的聚合，每个聚合都可能随着未来数据的变化而更新，因此不能在追加模式下输出。只有在事件时间窗口上启用了水印时，查询才知道聚合不会再次更新。因此，追加模式仅在水印确保聚合不会再次更新时输出每个键及其最终聚合值。此模式的优势在于允许你将聚合结果写入追加模式的流数据接收器（例如文件）。缺点是输出会延迟水印持续时间——查询必须等待尾随水印超过键的时间窗口，然后才能完成聚合输出。'
- en: Streaming Joins
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流连接
- en: Structured Streaming supports joining a streaming Dataset with another static
    or streaming Dataset. In this section we will explore what types of joins (inner,
    outer, etc.) are supported, and how to use watermarks to limit the state stored
    for stateful joins. We will start with the simple case of joining a data stream
    and a static Dataset.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流支持将流数据集与另一个静态或流数据集进行连接。在本节中，我们将探讨支持的不同类型的连接（内连接、外连接等），以及如何使用水印来限制用于有状态连接的存储。我们将从连接数据流和静态数据集的简单情况开始。
- en: Stream–Static Joins
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流-静态连接
- en: 'Many use cases require joining a data stream with a static Dataset. For example,
    let’s consider the case of ad monetization. Suppose you are an advertisement company
    that shows ads on websites and you make money when users click on them. Let’s
    assume that you have a static Dataset of all the ads to be shown (known as impressions),
    and another stream of events for each time users click on the displayed ads. To
    calculate the click revenue, you have to match each click in the event stream
    to the corresponding ad impression in the table. Let’s first represent the data
    as two DataFrames, a static one and a streaming one, as shown here:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 许多使用情况需要将数据流与静态数据集连接。例如，让我们考虑广告变现的情况。假设你是一家在网站上展示广告并且当用户点击时赚钱的广告公司。假设你有一个静态数据集，其中包含所有要展示的广告（称为展示次数），以及另一个流事件流，记录用户每次点击展示的广告。为了计算点击收入，你需要将事件流中的每次点击与表中对应的广告展示次数进行匹配。首先，我们将数据表示为两个DataFrame，一个是静态的，一个是流式的，如下所示：
- en: '[PRE45]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'To match the clicks with the impressions, you can simply apply an inner equi-join
    between them using the common `adId` column:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 要将点击与广告展示次数进行匹配，可以简单地使用它们之间共同的`adId`列进行内连接等值连接：
- en: '[PRE47]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This is the same code as you would have written if both impressions and clicks
    were static DataFrames—the only difference is that you use `spark.read()` for
    batch processing and `spark.readStream()` for a stream. When this code is executed,
    every micro-batch of clicks is inner-joined against the static impression table
    to generate the output stream of matched events.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这与如果印象和点击都是静态数据框架所写的代码相同 - 唯一的区别是您用于批处理的`spark.read()`和用于流的`spark.readStream()`。
    执行此代码时，每个点击的微批次都会与静态印象表进行内连接，以生成匹配事件的输出流。
- en: 'Besides inner joins, Structured Streaming also supports two types of stream–static
    outer joins:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 除了内连接外，结构化流还支持两种流 - 静态外连接：
- en: Left outer join when the left side is a streaming DataFrame
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在左侧是流数据框架时的左外连接
- en: Right outer join when the right side is a streaming DataFrame
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在右侧是流数据框架时的右外连接
- en: 'The other kinds of outer joins (e.g., full outer and left outer with a streaming
    DataFrame on the right) are not supported because they are not easy to run incrementally.
    In both supported cases, the code is exactly as it would be for a left/right outer
    join between two static DataFrames:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 不支持其他类型的外连接（例如，在右侧是流数据框架的完全外连接和左外连接），因为它们不容易进行增量运行。 在这两种支持的情况下，代码与在两个静态数据框架之间进行左/右外连接时完全相同：
- en: '[PRE49]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'There are a few key points to note about stream–static joins:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 关于流-静态连接，有几个关键点需要注意：
- en: Stream–static joins are stateless operations, and therefore do not require any
    kind of watermarking.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流-静态连接是无状态操作，因此不需要任何水印。
- en: The static DataFrame is read repeatedly while joining with the streaming data
    of every micro-batch, so you can cache the static DataFrame to speed up the reads.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态数据框架在与每个微批次的流数据进行连接时被重复读取，因此可以缓存静态数据框架以加快读取速度。
- en: If the underlying data in the data source on which the static DataFrame was
    defined changes, whether those changes are seen by the streaming query depends
    on the specific behavior of the data source. For example, if the static DataFrame
    was defined on files, then changes to those files (e.g., appends) will not be
    picked up until the streaming query is restarted.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在定义静态数据框架的数据源中的底层数据发生更改，则流查询是否看到这些更改取决于数据源的具体行为。 例如，如果静态数据框架是在文件上定义的，则对这些文件的更改（例如，附加）将在重新启动流查询之前不会被检测到。
- en: 'In this stream–static example, we made a significant assumption: that the impression
    table is a static table. In reality, there will be a stream of new impressions
    generated as new ads are displayed. While stream–static joins are good for enriching
    data in one stream with additional static (or slowly changing) information, this
    approach is insufficient when both sources of data are changing rapidly. For that
    you need stream–stream joins, which we will discuss next.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个流-静态示例中，我们做了一个重要的假设：印象表是一个静态表。 实际上，随着新广告的显示，将生成新的印象流。 虽然流-静态连接适用于在一个流中使用额外的静态（或缓慢变化）信息来丰富数据，但当数据源都在快速变化时，这种方法是不足够的。
    为此，您需要流-流连接，我们将在下面讨论。
- en: Stream–Stream Joins
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流-流连接
- en: The challenge of generating joins between two data streams is that, at any point
    in time, the view of either Dataset is incomplete, making it much harder to find
    matches between inputs. The matching events from the two streams may arrive in
    any order and may be arbitrarily delayed. For example, in our advertising use
    case an impression event and its corresponding click event may arrive out of order,
    with arbitrary delays between them. Structured Streaming accounts for such delays
    by buffering the input data from both sides as the streaming state, and continuously
    checking for matches as new data is received. The conceptual idea is sketched
    out in [Figure 8-11](#ad_monetization_using_a_streamen_dashstr).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个数据流之间生成连接的挑战在于，在任何时间点，任一数据集的视图都是不完整的，这使得在输入之间查找匹配事件变得更加困难。 两个流的匹配事件可能以任何顺序到达，并可能被任意延迟。
    例如，在我们的广告使用案例中，可能会以任意顺序到达印象事件及其对应的点击事件，并可能存在任意延迟。 结构化流通过缓冲来自两侧的输入数据作为流状态，并在接收到新数据时持续检查匹配来解决这些延迟。
    [图8-11](#ad_monetization_using_a_streamen_dashstr)中概述了概念想法。
- en: '![Ad monetization using a stream–stream join](assets/lesp_0811.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![使用流-流连接进行广告货币化](assets/lesp_0811.png)'
- en: Figure 8-11\. Ad monetization using a stream–stream join
  id: totrans-393
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-11。 使用流-流连接进行广告货币化
- en: Let’s consider this in more detail, first with inner joins and then with outer
    joins.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地考虑这一点，首先是内连接，然后是外连接。
- en: Inner joins with optional watermarking
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有可选水印的内连接
- en: 'Say we have redefined our `impressions` DataFrame to be a streaming DataFrame.
    To get the stream of matching impressions and their corresponding clicks, we can
    use the same code we used earlier for static joins and stream–static joins:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已重新定义我们的`impressions` DataFrame 为流DataFrame。为了获取匹配的展示和它们对应的点击的流，我们可以使用我们之前用于静态连接和流-静态连接的相同代码：
- en: '[PRE51]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Even though the code is the same, the execution is completely different. When
    this query is executed, the processing engine will recognize it to be a stream–stream
    join instead of a stream–static join. The engine will buffer all clicks and impressions
    as state, and will generate a matching impression-and-click as soon as a received
    click matches a buffered impression (or vice versa, depending on which was received
    first). Let’s visualize how this inner join works using the example timeline of
    events in [Figure 8-12](#illustrative_timeline_of_clickscomma_imp).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代码相同，执行方式完全不同。当执行此查询时，处理引擎将识别它为流-流连接而不是流-静态连接。引擎将所有点击和展示作为状态进行缓冲，并在接收到的点击与缓冲的展示（或相反，取决于哪个先收到）匹配时生成匹配的展示和点击。让我们通过示例事件时间轴图
    [图 8-12](#illustrative_timeline_of_clickscomma_imp) 来可视化这个内连接是如何工作的。
- en: '![Illustrative timeline of clicks, impressions, and their joined output](assets/lesp_0812.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![点击、展示和它们的连接输出的示意时间轴](assets/lesp_0812.png)'
- en: Figure 8-12\. Illustrative timeline of clicks, impressions, and their joined
    output
  id: totrans-401
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-12\. 点击、展示和它们的连接输出的示意时间轴
- en: In [Figure 8-12](#illustrative_timeline_of_clickscomma_imp), the blue dots represent
    the event times of impression and click events that were received across different
    micro-batches (separated by the dashed grey lines). For the purposes of this illustration,
    assume that each event was actually received at the same wall clock time as the
    event time. Note the different scenarios under which the related events are being
    joined. Both events with `adId` = **⧮** were received in the same micro-batch,
    so their joined output was generated by that micro-batch. However, for `adId`
    = **⧉** the impression was received at 12:04, much earlier than its corresponding
    click at 12:13\. Structured Streaming will first receive the impression at 12:04
    and buffer it in the state. For each received click, the engine will try to join
    it with all buffered impressions (and vice versa). Eventually, in a later micro-batch
    running around 12:13, the engine receives the click for `adId` = **⧉** and generates
    the joined output.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 8-12](#illustrative_timeline_of_clickscomma_imp)中，蓝色点表示在不同微批次（用虚线灰色线分隔）中接收到的展示和点击事件的事件时间。为了本示例，假设每个事件实际上在相同的挂钟时间收到。注意相关事件被连接的不同场景。`adId`
    = **⧮** 的两个事件在同一微批次中接收，因此它们的连接输出由该微批次生成。然而，对于`adId` = **⧉**，展示事件在12:04收到，比其对应的点击事件12:13要早得多。结构化流处理将首先在12:04收到展示并将其缓冲在状态中。对于每个收到的点击，引擎将尝试将其与所有已缓冲的展示事件（反之亦然）进行连接。最终，在稍后大约12:13左右运行的微批次中，引擎收到`adId`
    = **⧉** 的点击事件并生成了连接的输出。
- en: 'However, in this query, we have not given any indication of how long the engine
    should buffer an event to find a match. Therefore, the engine may buffer an event
    forever and accumulate an unbounded amount of streaming state. To limit the streaming
    state maintained by stream–stream joins, you need to know the following information
    about your use case:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这个查询中，我们没有给出引擎缓冲事件以找到匹配的时间长度的任何指示。因此，引擎可能会无限期地缓冲事件，并积累无界限的流状态。为了限制流-流连接维护的流状态，您需要了解关于您的用例的以下信息：
- en: '*What is the maximum time range between the generation of the two events at
    their respective sources?* In the context of our use case, let’s assume that a
    click can occur within zero seconds to one hour after the corresponding impression.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*两个事件在它们各自源生成之间的最大时间范围是多少？* 在我们的用例背景下，假设一个点击可以在相应展示之后的零秒到一小时内发生。'
- en: '*What is the maximum duration an event can be delayed in transit between the
    source and the processing engine?* For example, ad clicks from a browser may get
    delayed due to intermittent connectivity and arrive much later than expected,
    and out of order. Let’s say that impressions and clicks can be delayed by at most
    two and three hours, respectively.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*事件在源和处理引擎之间最长可以延迟多久？* 例如，来自浏览器的广告点击可能由于间歇性连接问题而延迟到达，并且顺序可能错乱。假设展示和点击最多可以分别延迟两个小时和三个小时到达。'
- en: 'These delay limits and event-time constraints can be encoded in the DataFrame
    operations using watermarks and time range conditions. In other words, you will
    have to do the following additional steps in the join to ensure state cleanup:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 这些延迟限制和事件时间约束可以通过数据框操作使用水印和时间范围条件进行编码。换句话说，您将需要在连接中执行以下附加步骤，以确保状态清理：
- en: Define watermark delays on both inputs, such that the engine knows how delayed
    the input can be (similar to with streaming aggregations).
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在两个输入上定义水印延迟，使引擎知道输入可以延迟多久（类似于流聚合）。
- en: 'Define a constraint on event time across the two inputs, such that the engine
    can figure out when old rows of one input are not going to be required (i.e.,
    will not satisfy the time constraint) for matches with the other input. This constraint
    can be defined in one of the following ways:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在两个输入之间定义事件时间约束，以便引擎可以确定不需要旧行数据（即不满足时间约束的数据）与另一个输入匹配。可以通过以下一种方式定义此约束：
- en: Time range join conditions (e.g., join condition = `"leftTime BETWEEN rightTime
    AND rightTime + INTERVAL 1 HOUR"`)
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 时间范围连接条件（例如，连接条件 = `"leftTime BETWEEN rightTime AND rightTime + INTERVAL 1 HOUR"`）
- en: Join on event-time windows (e.g., join condition = `"leftTimeWindow = rightTimeWindow"`)
  id: totrans-410
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在事件时间窗口上进行连接（例如，连接条件 = `"leftTimeWindow = rightTimeWindow"`）
- en: 'In our advertisement use case, our inner join code will get a little bit more
    complicated:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的广告使用案例中，我们的内连接代码会变得稍微复杂一些：
- en: '[PRE53]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'With these time constraints for each event, the processing engine can automatically
    calculate how long events need to be buffered to generate correct results, and
    when the events can be dropped from the state. For example, it will evaluate the
    following (illustrated in [Figure 8-13](#structured_streaming_automatically_calcu)):'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 有了每个事件的时间约束，处理引擎可以自动计算事件需要缓冲多长时间以生成正确的结果，并确定何时可以从状态中删除事件。例如，它将评估以下内容（如图8-13中所示）
- en: Impressions need to be buffered for at most four hours (in event time), as a
    three-hour-late click may match with an impression made four hours ago (i.e.,
    three hours late + up to one-hour delay between the impression and click).
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示需要最多缓冲四个小时（按事件时间计算），因为三小时延迟的点击事件可能与四小时前的展示匹配（即三小时延迟 + 展示和点击事件之间最多一小时的延迟）。
- en: Conversely, clicks need to be buffered for at most two hours (in event time),
    as a two-hour-late impression may match with a click received two hours ago.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，点击事件需要最多缓冲两个小时（按事件时间计算），因为两小时延迟的展示可能与两小时前收到的点击事件匹配。
- en: '![Structured Streaming automatically calculates thresholds for state cleanup
    using watermark delays and time range conditions](assets/lesp_0813.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![结构化流处理使用水印延迟和时间范围条件自动计算状态清理的阈值](assets/lesp_0813.png)'
- en: Figure 8-13\. Structured Streaming automatically calculates thresholds for state
    cleanup using watermark delays and time range conditions
  id: totrans-418
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-13。结构化流处理会使用水印延迟和时间范围条件自动计算状态清理的阈值
- en: 'There are a few key points to remember about inner joins:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 关于内连接有几个关键点需要记住：
- en: For inner joins, specifying watermarking and event-time constraints are both
    optional. In other words, at the risk of potentially unbounded state, you may
    choose not to specify them. Only when both are specified will you get state cleanup.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于内连接，指定水印和事件时间约束都是可选的。换句话说，在可能存在无界状态的风险下，您可以选择不指定它们。只有当两者都指定时，才会进行状态清理。
- en: Similar to the guarantees provided by watermarking on aggregations, a watermark
    delay of two hours guarantees that the engine will never drop or not match any
    data that is less than two hours delayed, but data delayed by more than two hours
    may or may not get processed.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于水印在聚合中提供的保证，两小时的水印延迟保证引擎不会丢弃或不匹配少于两小时延迟的任何数据，但延迟超过两小时的数据可能会被处理或不会被处理。
- en: Outer joins with watermarking
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用水印的外连接
- en: 'The previous inner join will output only those ads for which both events have
    been received. In other words, ads that received no clicks will not be reported
    at all. Instead, you may want all ad impressions to be reported, with or without
    the associated click data, to enable additional analysis later (e.g., click-through
    rates). This brings us to *stream–stream outer joins*. All you need to do to implement
    this is specify the outer join type:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的内连接只会输出那些已收到两个事件的广告。换句话说，未收到任何点击的广告将不被报告。相反，您可能希望报告所有广告展示，无论是否有相关点击数据，以便稍后进行额外分析（例如点击率）。这带我们来到
    *流-流外连接*。要实现这一点，您只需指定外连接类型：
- en: '[PRE55]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'As expected of outer joins, this query will start generating output for every
    impression, with or without (i.e., using `NULL`s) the click data. However, there
    are a few additional points to note about outer joins:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 如外连接所预期的那样，此查询将开始为每个展示生成输出，无论是否有（即使用`NULL`）点击数据。然而，关于外连接还有一些额外的要点需要注意：
- en: Unlike with inner joins, the watermark delay and event-time constraints are
    not optional for outer joins. This is because for generating the `NULL` results,
    the engine must know when an event is not going to match with anything else in
    the future. For correct outer join results and state cleanup, the watermarking
    and event-time constraints must be specified.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与内连接不同，外连接不可选的水印延迟和事件时间约束。这是因为为了生成`NULL`结果，引擎必须知道事件何时不会与将来的任何其他事件匹配。为了获得正确的外连接结果和状态清理，必须指定水印和事件时间约束。
- en: Consequently, the outer `NULL` results will be generated with a delay as the
    engine has to wait for a while to ensure that there neither were nor would be
    any matches. This delay is the maximum buffering time (with respect to event time)
    calculated by the engine for each event as discussed in the previous section (i.e.,
    four hours for impressions and two hours for clicks).
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，外部`NULL`结果将会有延迟生成，因为引擎必须等待一段时间以确保没有任何匹配。这个延迟是引擎为每个事件计算的最大缓冲时间（关于事件时间）如前一节所讨论的（即展示四小时，点击两小时）。
- en: Arbitrary Stateful Computations
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任意有状态的计算
- en: Many use cases require more complicated logic than the SQL operations we have
    discussed up to now. For example, say you want to track the statuses (e.g., signed
    in, busy, idle) of users by tracking their activities (e.g., clicks) in real time.
    To build this stream processing pipeline, you will have to track each user’s activity
    history as a state with arbitrary data structure, and continuously apply arbitrarily
    complex changes on the data structure based on the user’s actions. The operation
    `mapGroupsWithState()` and its more flexible counterpart `flatMapGroupsWithState()`
    are designed for such complex analytical use cases.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 许多用例需要比我们目前讨论的 SQL 操作更复杂的逻辑。例如，假设您想通过实时跟踪用户的活动（例如点击）来跟踪用户的状态（例如登录、忙碌、空闲）。为了构建这个流处理管道，您将必须跟踪每个用户的活动历史作为带有任意数据结构的状态，并基于用户的操作在数据结构上连续应用任意复杂的更改。`mapGroupsWithState()`
    操作及其更灵活的对应变体 `flatMapGroupsWithState()` 专为这种复杂的分析用例设计。
- en: Note
  id: totrans-431
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As of Spark 3.0, these two operations are only available in Scala and Java.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 截至 Spark 3.0，在 Scala 和 Java 中才能使用这两种操作。
- en: In this section, we will start with a simple example with `mapGroupsWithState()`
    to illustrate the four key steps to modeling custom state data and defining custom
    operations on it. Then we will discuss the concept of timeouts and how you can
    use them to expire state that has not been updated for a while. We will end with
    `flatMapGroupsWithState()`, which gives you even more flexibility.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从一个简单的示例开始，使用`mapGroupsWithState()`来说明建模自定义状态数据和定义自定义操作的四个关键步骤。然后我们将讨论超时的概念及如何使用它们来清除长时间未更新的状态。最后我们将介绍`flatMapGroupsWithState()`，它能给予你更多的灵活性。
- en: Modeling Arbitrary Stateful Operations with mapGroupsWithState()
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`mapGroupsWithState()`建模任意有状态的操作
- en: 'State with an arbitrary schema and arbitrary transformations on the state is
    modeled as a user-defined function that takes the previous version of the state
    value and new data as inputs, and generates the updated state and computed result
    as outputs. Programmatically in Scala, you will have to define a function with
    the following signature (`K`, `V`, `S`, and `U` are data types, as explained shortly):'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 具有任意模式和任意状态转换的状态被建模为一个用户定义函数，该函数将先前版本的状态值和新数据作为输入，并生成更新后的状态和计算结果作为输出。在 Scala
    中编程时，您需要定义以下签名的函数（`K`、`V`、`S` 和 `U` 是稍后将要解释的数据类型）：
- en: '[PRE57]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This function is provided to a streaming query using the operations `groupByKey()`
    and `mapGroupsWithState()`, as follows:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数通过 `groupByKey()` 和 `mapGroupsWithState()` 操作提供给流式查询，如下所示：
- en: '[PRE58]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'When this streaming query is started, in each micro-batch Spark will call this
    `arbitraryStateUpdateFunction()` for each unique key in the micro-batch’s data.
    Let’s take a closer look at what the parameters are and what parameter values
    Spark will call the function with:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 当启动这个流式查询时，Spark 将在每个微批次中的数据中为每个唯一键调用 `arbitraryStateUpdateFunction()`。让我们更详细地看看参数是什么以及
    Spark 将使用哪些参数值调用函数：
- en: '`key: K`'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '`key: K`'
- en: '`K` is the data type of the common keys defined in the state and the input.
    Spark will call this function for each unique key in the data.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '`K` 是状态和输入中定义的公共键的数据类型。Spark 将为数据中的每个唯一键调用此函数。'
- en: '`newDataForKey: Iterator[V]`'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '`newDataForKey: Iterator[V]`'
- en: '`V` is the data type of the input Dataset. When Spark calls this function for
    a key, this parameter will have all the new input data corresponding to that key.
    Note that the order in which the input data objects will be present in the iterator
    is not defined.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '`V` 是输入数据集的数据类型。当 Spark 调用此函数来处理一个键时，这个参数将包含对应于该键的所有新输入数据。注意，迭代器中输入数据对象的顺序是不确定的。'
- en: '`previousStateForKey: GroupState[S]`'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '`previousStateForKey: GroupState[S]`'
- en: '`S` is the data type of the arbitrary state you are going to maintain, and
    `GroupState[S]` is a typed wrapper object that provides methods to access and
    manage the state value. When Spark calls this function for a key, this object
    will provide the state value set the previous time Spark called this function
    for that key (i.e., for one of the previous micro-batches).'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '`S` 是您要维护的任意状态的数据类型，而 `GroupState[S]` 是一个类型化的包装对象，提供访问和管理状态值的方法。当 Spark 为一个键调用此函数时，这个对象将提供先前微批次中
    Spark 上次为该键调用此函数时设置的状态值。'
- en: '`U`'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '`U`'
- en: '`U` is the data type of the output of the function.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '`U` 是函数输出的数据类型。'
- en: Note
  id: totrans-448
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There are a couple of additional parameters that you have to provide. All the
    types (`K`, `V`, `S`, `U`) must be encodable by Spark SQL’s encoders. Accordingly,
    in `mapGroupsWithState()`, you have to provide the typed encoders for `S` and
    `U` either implicitly in Scala or explicitly in Java. See [“Dataset Encoders”](ch06.html#dataset_encoders)
    in [Chapter 6](ch06.html#spark_sql_and_datasets) for more details.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些额外的参数需要提供。所有类型（`K`、`V`、`S`、`U`）必须能够由 Spark SQL 的编码器进行编码。因此，在 `mapGroupsWithState()`
    中，您必须在 Scala 中隐式提供或在 Java 中显式提供 `S` 和 `U` 的类型化编码器。有关更多详细信息，请参阅 [“Dataset Encoders”](ch06.html#dataset_encoders)
    第 6 章 [Chapter 6](ch06.html#spark_sql_and_datasets)。
- en: 'Let’s examine how to express the desired state update function in this format
    with an example. Say we want to understand user behavior based on their actions.
    Conceptually, it’s quite simple: in every micro-batch, for each active user, we
    will use the new actions taken by the user and update the user’s “status.” Programmatically,
    we can define the state update function with the following steps:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来看看如何以这种格式表达所需的状态更新函数。假设我们想根据用户的操作理解他们的行为。从概念上讲，这很简单：在每个微批次中，对于每个活跃用户，我们将使用用户执行的新操作并更新用户的“状态”。在程序上，我们可以定义以下步骤的状态更新函数：
- en: 'Define the data types. We need to define the exact types of `K`, `V`, `S`,
    and `U`. In this case, we’ll use the following:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据类型。我们需要定义 `K`、`V`、`S` 和 `U` 的确切类型。在这种情况下，我们将使用以下内容：
- en: 'Input data (`V`) = `case class UserAction(userId: String, action: String)`'
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '输入数据 (`V`) = `case class UserAction(userId: String, action: String)`'
- en: Keys (`K`) = `String` (that is, the `userId`)
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 键 (`K`) = `String`（即 `userId`）
- en: 'State (`S`) = `case class UserStatus(userId: String, active: Boolean)`'
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '状态 (`S`) = `case class UserStatus(userId: String, active: Boolean)`'
- en: Output (`U`) = `UserStatus`, as we want to output the latest user status
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出 (`U`) = `UserStatus`，因为我们希望输出最新的用户状态。
- en: Note that all these data types are supported in encoders.
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，所有这些数据类型都受编码器支持。
- en: 'Define the function. Based on the chosen types, let’s translate the conceptual
    idea into code. When this function is called with new user actions, there are
    two main situations we need to handle: whether a previous state (i.e., previous
    user status) exists for that key (i.e., `userId`) or not. Accordingly, we will
    initialize the user’s status, or update the existing status with the new actions.
    We will explicitly update the state with the new running count, and finally return
    the updated `userId`-`userStatus` pair:'
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义函数。基于所选类型，让我们将概念理念翻译成代码。当此函数与新用户行动一起调用时，我们需要处理两种主要情况：是否存在该键（即 `userId`）的先前状态（即先前用户状态）。因此，我们将初始化用户状态，或者使用新行动更新现有状态。我们将明确使用新的运行计数更新状态，并最终返回更新的
    `userId`-`userStatus` 对：
- en: '[PRE59]'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Apply the function on the actions. We will group the input actions Dataset
    using `groupByKey()` and then apply the `updateUserStatus` function using `mapGroupsWithState()`:'
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对行动应用函数。我们将使用 `groupByKey()` 对输入的行动数据集进行分组，然后使用 `mapGroupsWithState()` 应用 `updateUserStatus`
    函数：
- en: '[PRE60]'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Once we start this streaming query with console output, we will see the updated
    user statuses being printed.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们启动带有控制台输出的流式查询，我们将看到打印出更新的用户状态。
- en: 'Before we move on to more advanced topics, there are a few notable points to
    remember:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入更高级话题之前，有几个值得注意的点要记住：
- en: When the function is called, there is no well-defined order for the input records
    in the new data iterator (e.g., `newActions`). If you need to update the state
    with the input records in a specific order (e.g., in the order the actions were
    performed), then you have to explicitly reorder them (e.g., based on the event
    timestamp or some other ordering ID). In fact, if there is a possibility that
    actions may be read out of order from the source, then you have to consider the
    possibility that a future micro-batch may receive data that should be processed
    before the data in the current batch. In that case, you have to buffer the records
    as part of the state.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当调用函数时，新数据迭代器（例如 `newActions`）中的输入记录没有明确定义的顺序。如果需要按特定顺序（例如按操作执行的顺序）更新状态，则必须显式重新排序它们（例如基于事件时间戳或某种其他排序
    ID）。实际上，如果存在从源读取的操作可能无序的情况，则必须考虑未来微批处理可能接收到应在当前批处理数据之前处理的数据的可能性。在这种情况下，必须将记录作为状态的一部分进行缓冲。
- en: In a micro-batch, the function is called on a key once only if the micro-batch
    has data for that key. For example, if a user becomes inactive and provides no
    new actions for a long time, then by default, the function will not be called
    for a long time. If you want to update or remove state based on a user’s inactivity
    over an extended period you have to use timeouts, which we will discuss in the
    next section.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在微批处理中，如果微批处理为特定键提供了数据，则仅对该键调用函数一次。例如，如果用户变得不活跃并且长时间没有提供新的行动，那么默认情况下函数将长时间不被调用。如果你想根据用户在长时间内的不活动来更新或移除状态，那么你必须使用超时，我们将在下一节讨论这一点。
- en: The output of `mapGroupsWithState()` is assumed by the incremental processing
    engine to be continuously updated key/value records, similar to the output of
    aggregations. This limits what operations are supported in the query after `mapGroupsWithState()`,
    and what sinks are supported. For example, appending the output into files is
    not supported. If you want to apply arbitrary stateful operations with greater
    flexibility, then you have to use `flatMapGroupsWithState()`. We will discuss
    that after timeouts.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mapGroupsWithState()` 的输出被增量处理引擎假定为持续更新的键/值记录，类似于聚合的输出。这限制了在 `mapGroupsWithState()`
    之后查询支持的操作以及支持的输出目的地。例如，不支持将输出追加到文件中。如果你希望以更大的灵活性应用任意有状态的操作，那么你必须使用 `flatMapGroupsWithState()`。我们将在超时后讨论这一点。'
- en: Using Timeouts to Manage Inactive Groups
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用超时来管理非活动组
- en: In the preceding example of tracking active user sessions, as more users become
    active, the number of keys in the state will keep increasing, and so will the
    memory used by the state. Now, in a real-world scenario, users are likely not
    going to stay active all the time. It may not be very useful to keep the status
    of inactive users in the state, as it is not going to change again until those
    users become active again. Hence, we may want to explicitly drop all information
    for inactive users. However, a user may not explicitly take any action to become
    inactive (e.g., explicitly logging off), and we may have to define inactivity
    as lack of any action for a threshold duration. This becomes tricky to encode
    in the function, as the function is not called for a user until there are new
    actions from that user.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在跟踪活跃用户会话的上述示例中，随着更多用户变得活跃，状态中的键的数量将不断增加，状态使用的内存也会增加。然而，在实际情况中，用户可能不会一直保持活跃状态。保持不活动用户状态可能并不是非常有用，因为在这些用户再次变得活跃之前，状态不会再次改变。因此，我们可能希望明确地删除所有不活跃用户的信息。但是，用户可能不会明确采取任何行动来变得不活跃（例如，明确注销），我们可能需要定义不活动为一段时间没有任何操作。这在函数中编码变得棘手，因为在没有来自用户的新动作之前，函数不会为用户调用。
- en: 'To encode time-based inactivity, `mapGroupsWithState()` supports timeouts that
    are defined as follows:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编码基于时间的不活动，`mapGroupsWithState()`支持以下定义的超时：
- en: Each time the function is called on a key, a timeout can be set on the key based
    on a duration or a threshold timestamp.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次在键上调用函数时，可以根据持续时间或阈值时间戳设置超时。
- en: If that key does not receive any data, such that the timeout condition is met,
    the key is marked as “timed out.” The next micro-batch will call the function
    on this timed-out key even if there is no data for that key in that micro-batch.
    In this special function call, the new input data iterator will be empty (since
    there is no new data) and `GroupState.hasTimedOut()` will return `true`. This
    is the best way to identify inside the function whether the call was due to new
    data or a timeout.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果某个键没有接收到任何数据，使得超时条件得到满足，该键将被标记为“超时”。下一个微批次将即使对于该键没有新数据，也会调用该键上的函数。在这个特殊的函数调用中，新的输入数据迭代器将为空（因为没有新数据），并且`GroupState.hasTimedOut()`将返回`true`。这是函数内部最佳的识别调用原因是由于新数据还是超时的方式。
- en: 'There are two types of timeouts, based on our two notions of time: processing
    time and event time. The processing-time timeout is the simpler of the two to
    use, so we’ll start with that.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的超时，基于我们的两种时间概念：处理时间和事件时间。处理时间超时是其中更简单的一种，因此我们将从这里开始。
- en: Processing-time timeouts
  id: totrans-472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理时间超时
- en: 'Processing-time timeouts are based on the system time (also known as the wall
    clock time) of the machine running the streaming query and are defined as follows:
    if a key last received data at system timestamp `T`, and the current timestamp
    is more than `(T + *<timeout duration>*)`, then the function will be called again
    with a new empty data iterator.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 处理时间超时基于运行流查询的机器的系统时间（也称为挂钟时间），定义如下：如果一个键在系统时间戳`T`时最后接收到数据，而当前时间戳超过`(T + *<超时时长>*)`，则函数将再次被调用，但是使用一个新的空数据迭代器。
- en: 'Let’s investigate how to use timeouts by updating our user example to remove
    a user’s state based on one hour of inactivity. We will make three changes:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过更新我们的用户示例来调查如何使用超时，以便根据一小时的不活动时间删除用户的状态。我们将进行三处更改：
- en: In `mapGroupsWithState()`, we will specify the timeout as `GroupStateTimeout.ProcessingTimeTimeout`.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`mapGroupsWithState()`中，我们将超时指定为`GroupStateTimeout.ProcessingTimeTimeout`。
- en: In the state update function, before updating the state with new data, we have
    to check whether the state has timed out or not. Accordingly, we will update or
    remove the state.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在状态更新函数中，在使用新数据更新状态之前，我们必须检查状态是否已经超时。相应地，我们将更新或删除状态。
- en: In addition, every time we update the state with new data, we will set the timeout
    duration.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另外，每次使用新数据更新状态时，我们将设置超时时长。
- en: 'Here’s the updated code:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是更新后的代码：
- en: '[PRE61]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This query will automatically clean up the state of users for whom the query
    has not processed any data for more than an hour. However, there are a few points
    to note about timeouts:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询会自动清理那些超过一个小时没有处理任何数据的用户的状态。但是，关于超时需要注意几点：
- en: The timeout set by the last call to the function is automatically cancelled
    when the function is called again, either for the new received data or for the
    timeout. Hence, whenever the function is called, the timeout duration or timestamp
    needs to be explicitly set to enable the timeout.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当再次调用函数时，最后一次调用函数设置的超时将自动取消，无论是为了新接收的数据还是为了超时。因此，每次调用函数时，都需要显式设置超时持续时间或时间戳以启用超时。
- en: Since the timeouts are processed during the micro-batches, the timing of their
    execution is imprecise and depends heavily on the trigger interval and micro-batch
    processing times. Therefore, it is not advised to use timeouts for precise timing
    control.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于超时是在微批处理期间处理的，它们的执行时间是不精确的，并且严重依赖触发间隔和微批处理时间。因此，不建议使用超时来进行精确的时间控制。
- en: While processing-time timeouts are simple to reason about, they are not robust
    to slowdowns and downtimes. If the streaming query suffers a downtime of more
    than one hour, then after restart, all the keys in the state will be timed out
    because more than one hour has passed since each key received data. Similar wide-scale
    timeouts can occur if the query processes data slower than it is arriving at the
    source (e.g., if data is arriving and getting buffered in Kafka). For example,
    if the timeout is five minutes, then a sudden drop in processing rate (or spike
    in data arrival rate) that causes a five-minute lag could produce spurious timeouts.
    To avoid such issues we can use an event-time timeout, which we will discuss next.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然处理时间超时易于理解，但对于减速和停机不够健壮。如果流查询经历超过一小时的停机时间，那么重新启动后，状态中的所有键都将超时，因为每个键自接收数据以来已经过去了一小时以上。如果查询处理数据的速度比从源头到达的速度慢（例如，如果数据到达并在Kafka中缓冲），则可能发生类似的大规模超时。例如，如果超时为五分钟，那么处理速率突然下降（或数据到达速率激增），导致五分钟延迟，可能会产生偶发超时。为了避免这种问题，我们可以使用事件时间超时，接下来将讨论。
- en: Event-time timeouts
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 事件时间超时
- en: Instead of the system clock time, an event-time timeout is based on the event
    time in the data (similar to time-based aggregations) and a watermark defined
    on the event time. If a key is configured with a specific timeout timestamp of
    `T` (i.e., not a duration), then that key will time out when the watermark exceeds
    `T` if no new data was received for that key since the last time the function
    was called. Recall that the watermark is a moving threshold that lags behind the
    maximum event time seen while processing the data. Hence, unlike system time,
    the watermark moves forward in time at the same rate as the data is processed.
    This means (unlike with processing-time timeouts) any slowdown or downtime in
    query processing will not cause spurious timeouts.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 事件时间超时不是基于系统时钟时间，而是基于数据中的事件时间（类似于基于时间的聚合）和在事件时间上定义的水印。如果一个键配置了特定的超时时间戳`T`（即不是一个持续时间），那么如果自上次调用函数以来该键未接收到新数据，则当水印超过`T`时，该键将超时。请注意，水印是一个移动的阈值，在处理数据时会落后于所见的最大事件时间。因此，与系统时间不同，水印以数据处理的速度向前移动。这意味着（与处理时间超时不同）查询处理中的任何减速或停机都不会导致偶发超时。
- en: 'Let’s modify our example to use an event-time timeout. In addition to the changes
    we already made for using the processing-time timeout, we will make the following
    changes:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改我们的示例以使用事件时间超时。除了我们已经为使用处理时间超时所做的更改外，我们还将进行以下更改：
- en: Define watermarks on the input Dataset (assume that the class `UserAction` has
    an `eventTimestamp` field). Recall that the watermark threshold represents the
    acceptable amount of time by which input data can be late and out of order.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在输入数据集上定义水印（假设类`UserAction`有一个`eventTimestamp`字段）。请注意，水印阈值表示输入数据可以延迟和无序的可接受时间量。
- en: Update `mapGroupsWithState()` to use `EventTimeTimeout`.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新`mapGroupsWithState()`以使用`EventTimeTimeout`。
- en: Update the function to set the threshold timestamp at which the timeout will
    occur. Note that event-time timeouts do not allow setting a timeout duration,
    like processing-time timeouts. We will discuss the reason for this later. In this
    example, we will calculate this timeout as the current watermark plus one hour.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新函数以设置将发生超时的阈值时间戳。请注意，事件时间超时不允许设置超时持续时间，如处理时间超时。稍后我们将讨论这一点的原因。在本例中，我们将计算此超时为当前水印加一小时。
- en: 'Here is the updated example:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是更新后的示例：
- en: '[PRE62]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: This query will be much more robust to spurious timeouts caused by restarts
    and processing delays.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询将更加健壮，能够抵御由于重新启动和处理延迟引起的偶发超时。
- en: 'Here are a few points to note about event-time timeouts:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 关于事件时间超时，有几点需要注意：
- en: Unlike in the previous example with processing-time timeouts, we have used `GroupState.setTimeoutTimestamp()`
    instead of `GroupState.setTimeoutDuration()`. This is because with processing-time
    timeouts the duration is sufficient to calculate the exact future timestamp (i.e.,
    current system time + specified duration) when the timeout would occur, but this
    is not the case for event-time timeouts. Different applications may want to use
    different strategies to calculate the threshold timestamp. In this example we
    simply calculate it based on the current watermark, but a different application
    may instead choose to calculate a key’s timeout timestamp based on the maximum
    event-time timestamp seen for that key (tracked and saved as part of the state).
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与处理时间超时的前一个示例不同，我们使用了`GroupState.setTimeoutTimestamp()`而不是`GroupState.setTimeoutDuration()`。这是因为对于处理时间超时，持续时间足以计算未来精确的时间戳（即，当前系统时间
    + 指定持续时间），超时会发生，但对于事件时间超时则不是这样。不同的应用可能希望使用不同的策略来计算阈值时间戳。在本例中，我们简单地基于当前水印计算它，但是不同的应用可能选择基于该键的最大事件时间戳来计算超时时间戳（作为状态的一部分进行跟踪和保存）。
- en: The timeout timestamp must be set to a value larger than the current watermark.
    This is because the timeout is expected to happen when the timestamp crosses the
    watermark, so it’s illogical to set the timestamp to a value already larger than
    the current watermark.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超时时间戳必须设置为大于当前水印的值。这是因为超时预期会在时间戳穿过水印时发生，因此将时间戳设置为已经大于当前水印的值是不合逻辑的。
- en: 'Before we move on from timeouts, one last thing to remember is that you can
    use these timeout mechanisms for more creative processing than fixed-duration
    timeouts. For example, you can implement an approximately periodic task (say,
    every hour) on the state by saving the last task execution timestamp in the state
    and using that to set the processing-time timeout duration, as shown in this code
    snippet:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论超时机制的更多创造性处理方式之前，有一件事情需要记住，那就是你可以使用这些超时机制来进行比固定持续时间超时更有创意的处理。例如，你可以通过在状态中保存上次任务执行的时间戳，并使用它来设置处理时间超时持续时间来实现大约周期性的任务（比如，每小时一次），如本代码片段所示：
- en: '[PRE63]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Generalization with flatMapGroupsWithState()
  id: totrans-498
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`flatMapGroupsWithState()`进行泛化
- en: 'There are two key limitations with `mapGroupsWithState()` that may limit the
    flexibility that we want to implement more complex use cases (e.g., chained sessionizations):'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapGroupsWithState()`存在两个关键限制，可能限制我们想要实现更复杂用例（例如，链式会话化）的灵活性：'
- en: Every time `mapGroupsWithState()` is called, you have to return one and only
    one record. For some applications, in some triggers, you may not want to output
    anything at all.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次调用`mapGroupsWithState()`时，你必须返回一个且仅返回一个记录。对于某些应用程序，在某些触发器中，你可能根本不想输出任何内容。
- en: With `mapGroupsWithState()`, due to the lack of more information about the opaque
    state update function, the engine assumes that generated records are updated key/value
    data pairs. Accordingly, it reasons about downstream operations and allows or
    disallows some of them. For example, the DataFrame generated using `mapGroupsWithState()`
    cannot be written out in append mode to files. However, some applications may
    want to generate records that can be considered as appends.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`mapGroupsWithState()`时，由于对不透明状态更新函数的更多信息缺乏，引擎假定生成的记录是更新的键/值数据对。因此，它推理关于下游操作并允许或禁止其中的一些。例如，使用`mapGroupsWithState()`生成的DataFrame不能以追加模式写入文件。然而，某些应用程序可能希望生成可以视为追加的记录。
- en: '`flatMapGroupsWithState()` overcomes these limitations, at the cost of slightly
    more complex syntax. It has two differences from `mapGroupsWithState()`:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatMapGroupsWithState()`克服了这些限制，代价是稍微复杂的语法。它与`mapGroupsWithState()`有两个不同之处：'
- en: The return type is an iterator, instead of a single object. This allows the
    function to return any number of records, or, if needed, no records at all.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回类型是一个迭代器，而不是单个对象。这允许函数返回任意数量的记录，或者如果需要的话，根本不返回记录。
- en: It takes another parameter, called the *operator output mode* (not to be confused
    with the query output modes we discussed earlier in the chapter), that defines
    whether the output records are new records that can be appended (`OutputMode.Append`)
    or updated key/value records (`OutputMode.Update`).
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还带有一个称为*操作员输出模式*的参数（不要与本章前面讨论的查询输出模式混淆），该参数定义了输出记录是可以追加的新记录（`OutputMode.Append`）还是更新键/值记录（`OutputMode.Update`）。
- en: 'To illustrate the use of this function, let’s extend our user tracking example
    (we have removed timeouts to keep the code simple). For example, if we want to
    generate alerts only for certain user changes and we want to write the output
    alerts to files, we can do the following:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明此函数的使用，让我们扩展我们的用户跟踪示例（我们已经删除了超时以保持代码简单）。例如，如果我们只想为某些用户更改生成警报，并且我们希望将输出警报写入文件，我们可以执行以下操作：
- en: '[PRE64]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Performance Tuning
  id: totrans-507
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能调整
- en: 'Structured Streaming uses the Spark SQL engine and therefore can be tuned with
    the same parameters as those discussed for Spark SQL in Chapters [5](ch05.html#spark_sql_and_dataframes_interacting_wit)
    and [7](ch07.html#optimizing_and_tuning_spark_applications). However, unlike batch
    jobs that may process gigabytes to terabytes of data, micro-batch jobs usually
    process much smaller volumes of data. Hence, a Spark cluster running streaming
    queries usually needs to be tuned slightly differently. Here are a few considerations
    to keep in mind:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流使用Spark SQL引擎，因此可以使用与讨论Spark SQL章节中相同的参数进行调优[5](ch05.html#spark_sql_and_dataframes_interacting_wit)和[7](ch07.html#optimizing_and_tuning_spark_applications)。然而，与可能处理几十亿到几百万亿字节数据的批处理作业不同，微批处理作业通常处理的数据量要小得多。因此，运行流处理查询的Spark集群通常需要略微不同的调优。以下是一些需要记住的考虑因素：
- en: Cluster resource provisioning
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 集群资源配置
- en: 'Since Spark clusters running streaming queries are going to run 24/7, it is
    important to provision resources appropriately. Underprovisoning the resources
    can cause the streaming queries to fall behind (with micro-batches taking longer
    and longer), while overprovisioning (e.g., allocated but unused cores) can cause
    unnecessary costs. Furthermore, allocation should be done based on the nature
    of the streaming queries: stateless queries usually need more cores, and stateful
    queries usually need more memory.'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 由于运行流处理查询的Spark集群将会全天候运行，因此适当地配置资源非常重要。资源配置不足可能会导致流处理查询落后（微批处理需要的时间越来越长），而过度配置（例如分配但未使用的核心）可能会导致不必要的成本。此外，分配应根据流处理查询的性质进行：无状态查询通常需要更多核心，而有状态查询通常需要更多内存。
- en: Number of partitions for shuffles
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 洗牌分区的数量
- en: For Structured Streaming queries, the number of shuffle partitions usually needs
    to be set much lower than for most batch queries—dividing the computation too
    much increases overheads and reduces throughput. Furthermore, shuffles due to
    stateful operations have significantly higher task overheads due to checkpointing.
    Hence, for streaming queries with stateful operations and trigger intervals of
    a few seconds to minutes, it is recommended to tune the number of shuffle partitions
    from the default value of 200 to at most two to three times the number of allocated
    cores.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 对于结构化流查询，洗牌分区的数量通常需要设置比大多数批处理查询低得多——过度划分计算会增加开销并降低吞吐量。此外，由于检查点，由有状态操作引起的洗牌具有显着更高的任务开销。因此，对于具有几秒到几分钟触发间隔的流处理查询，建议将洗牌分区的数量从默认值200调整为至多分配核心数的两到三倍。
- en: Setting source rate limits for stability
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 为稳定性设置源速率限制
- en: 'After the allocated resources and configurations have been optimized for a
    query’s expected input data rates, it’s possible that sudden surges in data rates
    can generate unexpectedly large jobs and subsequent instability. Besides the costly
    approach of overprovisioning, you can safeguard against instability using source
    rate limits. Setting limits in supported sources (e.g., Kafka and files) prevents
    a query from consuming too much data in a single micro-batch. The surge data will
    stay buffered in the source, and the query will eventually catch up. However,
    note the following:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在为查询的预期输入数据速率优化了分配的资源和配置之后，突然的数据速率激增可能会生成意外的大型作业和随后的不稳定性。除了过度配置的昂贵方法之外，您可以使用源速率限制来防止不稳定性。在支持的源（例如Kafka和文件）中设置限制可以防止查询在单个微批处理中消耗过多的数据。激增的数据将保留在源中，并且查询最终会赶上。但是，请注意以下事项：
- en: Setting the limit too low can cause the query to underutilize allocated resources
    and fall behind the input rate.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将限制设置得太低可能导致查询未充分利用分配的资源并落后于输入速率。
- en: Limits do not effectively guard against sustained increases in input rate. While
    stability is maintained, the volume of buffered, unprocessed data will grow indefinitely
    at the source and so will the end-to-end latencies.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制不能有效防范输入速率持续增加。尽管保持稳定性，但在源头处未处理的缓冲数据量将无限增长，端到端延迟也会增加。
- en: Multiple streaming queries in the same Spark application
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一 Spark 应用程序中运行多个流查询
- en: 'Running multiple streaming queries in the same `SparkContext` or `SparkSession`
    can lead to fine-grained resource sharing. However:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同的`SparkContext`或`SparkSession`中运行多个流查询可以实现细粒度的资源共享。然而：
- en: Executing each query continuously uses resources in the Spark driver (i.e.,
    the JVM where it is running). This limits the number of queries that the driver
    can execute simultaneously. Hitting those limits can either bottleneck the task
    scheduling (i.e., underutilizing the executors) or exceed memory limits.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个查询持续执行会消耗 Spark 驱动程序（即运行它的 JVM）中的资源。这限制了驱动程序可以同时执行的查询数量。达到这些限制可能会成为任务调度的瓶颈（即未充分利用执行者），或超出内存限制。
- en: 'You can ensure fairer resource allocation between queries in the same context
    by setting them to run in separate scheduler pools. Set the `SparkContext`’s thread-local
    property `spark.scheduler.pool` to a different string value for each stream:'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过设置流到不同调度程序池的方式，在同一上下文中确保更公平的资源分配。为每个流设置`SparkContext`的线程本地属性`spark.scheduler.pool`为不同的字符串值：
- en: '[PRE65]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Summary
  id: totrans-523
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'This chapter explored writing Structured Streaming queries using the DataFrame
    API. Specifically, we discussed:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了使用 DataFrame API 编写结构化流查询的方法。具体而言，我们讨论了：
- en: The central philosophy of Structured Streaming and the processing model of treating
    input data streams as unbounded tables
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流的核心理念以及将输入数据流视为无界表的处理模型
- en: The key steps to define, start, restart, and monitor streaming queries
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义、启动、重启和监视流查询的关键步骤
- en: How to use various built-in streaming sources and sinks and write custom streaming
    sinks
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用各种内置流源和汇以及编写自定义流汇
- en: How to use and tune managed stateful operations like streaming aggregations
    and stream–stream joins
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用和调整管理的有状态操作，例如流聚合和流-流连接
- en: Techniques for expressing custom stateful computations
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表达自定义有状态计算的技术
- en: By working through the code snippets in the chapter and the notebooks in the
    book’s [GitHub repo](https://github.com/databricks/LearningSparkV2), you will
    get a feel for how to use Structured Streaming effectively. In the next chapter,
    we explore how you can manage structured data read and written simultaneously
    from batch and streaming workloads.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析本章的代码片段和书籍的 [GitHub 存储库](https://github.com/databricks/LearningSparkV2)
    中的笔记本，您将深入了解如何有效使用结构化流。在下一章中，我们将探讨如何管理从批处理和流处理工作负载中同时读取和写入的结构化数据。
- en: '^([1](ch08.html#ch01fn9-marker)) For a more detailed explanation, see the original
    research paper [“Discretized Streams: Fault-Tolerant Streaming Computation at
    Scale”](https://oreil.ly/Lz8mM) by Matei Zaharia et al. (2013).'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch08.html#ch01fn9-marker)) 更详细的解释，请参阅 Matei Zaharia 等人（2013）的原始研究论文 [“Discretized
    Streams: Fault-Tolerant Streaming Computation at Scale”](https://oreil.ly/Lz8mM)。'
- en: ^([2](ch08.html#ch01fn10-marker)) This execution loop runs for micro-batch-based
    trigger modes (i.e., `ProcessingTime` and `Once`), but not for the `Continuous`
    trigger mode.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch08.html#ch01fn10-marker)) 这个执行循环适用于基于微批次触发模式（即`ProcessingTime`和`Once`），但不适用于`Continuous`触发模式。
- en: ^([3](ch08.html#ch01fn11-marker)) For the full list of unsupported operations,
    see the [Structured Streaming Programming Guide](https://oreil.ly/wa60L).
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch08.html#ch01fn11-marker)) 有关不支持操作的完整列表，请参阅[结构化流编程指南](https://oreil.ly/wa60L)。
