- en: 16  Cross Validation and Regularization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16  交叉验证和正则化
- en: 原文：[https://ds100.org/course-notes/cv_regularization/cv_reg.html](https://ds100.org/course-notes/cv_regularization/cv_reg.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://ds100.org/course-notes/cv_regularization/cv_reg.html](https://ds100.org/course-notes/cv_regularization/cv_reg.html)'
- en: '*Learning Outcomes* ***   Recognize the need for validation and test sets to
    preview model performance on unseen data'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习成果* ***   认识到需要验证和测试集来预览模型在未知数据上的表现'
- en: Apply cross-validation to select model hyperparameters
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用交叉验证来选择模型超参数
- en: 'Understand the conceptual basis for L1 and L2 regularization**  **At the end
    of the Feature Engineering lecture (Lecture 14), we arrived at the issue of fine-tuning
    model complexity. We identified that a model that’s too complex can lead to overfitting,
    while a model that’s too simple can lead to underfitting. This brings us to a
    natural question: how do we control model complexity to avoid under- and overfitting?'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解L1和L2正则化的概念基础**  **在特征工程讲座结束时（第14讲），我们提出了调整模型复杂度的问题。我们发现一个过于复杂的模型会导致过拟合，而一个过于简单的模型会导致欠拟合。这带来了一个自然的问题：我们如何控制模型复杂度以避免欠拟合和过拟合？
- en: 'To answer this question, we will need to address two things: first, we need
    to understand *when* our model begins to overfit by assessing its performance
    on unseen data. We can achieve this through **cross-validation**. Secondly, we
    need to introduce a technique to adjust the complexity of our models ourselves
    – to do so, we will apply **regularization**.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们需要解决两件事：首先，我们需要通过评估模型在未知数据上的表现来了解*何时*我们的模型开始过拟合。我们可以通过**交叉验证**来实现这一点。其次，我们需要引入一种调整模型复杂度的技术
    - 为此，我们将应用**正则化**。
- en: 16.1 Training, Test, and Validation Sets
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.1 训练、测试和验证集
- en: 'From the last lecture, we learned that *increasing* model complexity *decreased*
    our model’s training error but *increased* its variance. This makes intuitive
    sense: adding more features causes our model to fit more closely to data it encountered
    during training, but generalize worse to new data it hasn’t seen before. For this
    reason, a low training error is not always representative of our model’s underlying
    performance - we need to also assess how well it performs on unseen data to ensure
    that it is not overfitting.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一讲中，我们了解到*增加*模型复杂度*减少*了模型的训练误差，但*增加*了它的方差。这是很直观的：添加更多的特征使我们的模型更紧密地拟合了训练过程中遇到的数据，但对新数据的泛化能力更差。因此，低训练误差并不总是代表我们模型的基本性能
    - 我们还需要评估它在未知数据上的表现，以确保它没有过拟合。
- en: Truly, the only way to know when our model overfits is by evaluating it on unseen
    data. Unfortunately, that means we need to wait for more data. This may be very
    expensive and time-consuming.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，唯一知道我们的模型何时过拟合的方法是在未知数据上评估它。不幸的是，这意味着我们需要等待更多的数据。这可能非常昂贵和耗时。
- en: How should we proceed? In this section, we will build up a viable solution to
    this problem.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何进行？在本节中，我们将建立一个可行的解决方案来解决这个问题。
- en: 16.1.1 Test Sets
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.1.1 测试集
- en: 'The simplest approach to avoid overfitting is to keep some of our data “secret”
    from ourselves. We can set aside a random portion of our full dataset to use only
    for testing purposes. The datapoints in this **test set** will *not* be used in
    the model fitting process. Instead, we will:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 避免过拟合的最简单方法是将一些数据“保密”不让自己知道。我们可以将完整数据集的随机部分保留下来，仅用于测试目的。这个**测试集**中的数据点*不*会用于模型拟合过程。相反，我们将：
- en: Use the remaining portion of our dataset – now called the **training set** –
    to run ordinary least squares, gradient descent, or some other technique to fit
    model parameters
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我们数据集的剩余部分 - 现在称为**训练集** - 运行普通最小二乘法、梯度下降或其他一些技术来拟合模型参数
- en: Take the fitted model and use it to make predictions on datapoints in the test
    set. The model’s performance on the test set (expressed as the MSE, RMSE, etc.)
    is now indicative of how well it can make predictions on unseen data
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拿到拟合的模型并用它对测试集中的数据点进行预测。模型在测试集上的表现（以MSE、RMSE等表示）现在表明了它在未知数据上的预测能力有多好
- en: Importantly, the optimal model parameters were found by *only* considering the
    data in the training set. After the model has been fitted to the training data,
    we do not change any parameters before making predictions on the test set. Importantly,
    we only ever make predictions on the test set **once** after all model design
    has been completely finalized. We treat the test set performance as the final
    test of how well a model does.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，最佳模型参数是通过*仅*考虑训练集中的数据找到的。在模型拟合到训练数据之后，我们在进行测试集上的预测之前不改变任何参数。重要的是，我们在最终确定所有模型设计后，只对测试集进行**一次**预测。我们将测试集的表现视为模型表现的最终测试。
- en: The process of sub-dividing our dataset into training and test sets is known
    as a **train-test split**. Typically, between 10% and 20% of the data is allocated
    to the test set.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的数据集分成训练集和测试集的过程被称为**训练-测试拆分**。通常，10%到20%的数据被分配给测试集。
- en: '![train-test-split](../Images/1f8de73467b65b8c61ea5cf5a73b593e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![train-test-split](../Images/1f8de73467b65b8c61ea5cf5a73b593e.png)'
- en: In `sklearn`, the `train_test_split` function of the `model_selection` module
    allows us to automatically generate train-test splits.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在`sklearn`中，`model_selection`模块的`train_test_split`函数允许我们自动生成训练-测试拆分。
- en: Throughout today’s work, we will work with the `vehicles` dataset from previous
    lectures. As before, we will attempt to predict the `mpg` of a vehicle from transformations
    of its `hp`. In the cell below, we allocate 20% of the full dataset to testing,
    and the remaining 80% to training.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的工作中，我们将继续使用之前讲座中的`vehicles`数据集。与以往一样，我们将尝试从`hp`的转换中预测车辆的`mpg`。在下面的单元格中，我们将完整数据集的20%分配给测试，剩下的80%分配给训练。
- en: <details><summary>Code</summary>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE0]</details>'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE0]</details>'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After performing our train-test split, we fit a model to the training set and
    assess its performance on the test set.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行训练-测试拆分后，我们将模型拟合到训练集，并评估其在测试集上的表现。
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 16.1.2 Validation Sets
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.1.2 验证集
- en: Now, what if we were dissatisfied with our test set performance? With our current
    framework, we’d be stuck. As outlined previously, assessing model performance
    on the test set is the *final* stage of the model design process. We can’t go
    back and adjust our model based on the new discovery that it is overfitting –
    if we did, then we would be *factoring in information from the test set* to design
    our model. The test error would no longer be a true representation of the model’s
    performance on unseen data!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们对测试集的性能不满意怎么办？按照我们目前的框架，我们就会陷入困境。如前所述，评估模型在测试集上的性能是模型设计过程的*最终*阶段。我们不能回头根据新发现的过拟合来调整模型
    - 如果这样做，我们就会*考虑测试集的信息*来设计我们的模型。测试误差将不再是模型在未见数据上性能的真实代表！
- en: 'Our solution is to introduce a **validation set**. A validation set is a random
    portion of the *training set* that is set aside for assessing model performance
    while the model is *still being developed*. The process for using a validation
    set is:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解决方案是引入一个**验证集**。验证集是训练集的一个随机部分，用于在模型*仍在开发中*时评估模型性能。使用验证集的过程是：
- en: Perform a train-test split. Set the test set aside; we will not touch it until
    the very end of the model design process.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行训练-测试分割。将测试集放在一边；直到模型设计过程的最后才会使用。
- en: Set aside a portion of the training set to be used for validation.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一部分训练集用于验证。
- en: Fit the model parameters to the datapoints contained in the remaining portion
    of the training set.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型参数拟合到剩余训练集中包含的数据点。
- en: Assess the model’s performance on the validation set. Adjust the model as needed,
    re-fit it to the remaining portion of the training set, then re-evaluate it on
    the validation set. Repeat as necessary until you are satisfied.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型在验证集上的性能。根据需要调整模型，重新拟合剩余部分的训练集，然后在验证集上重新评估。如有必要，重复此过程直到满意为止。
- en: After *all* model development is complete, assess the model’s performance on
    the test set. This is the final test of how well the model performs on unseen
    data. No further modifications should be made to the model.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*所有*模型开发完成后，评估模型在测试集上的性能。这是模型在未见数据上表现如何的最终测试。不应对模型进行进一步修改。
- en: The process of creating a validation set is called a **validation split**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 创建验证集的过程称为**验证分割**。
- en: '![validation-split](../Images/26a9890469f7a69a07d2355d23eeb38e.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![validation-split](../Images/26a9890469f7a69a07d2355d23eeb38e.png)'
- en: Note that the validation error behaves quite differently from the training error
    explored previously. Recall that the training error decreased monotonically with
    increasing model degree – as the model became more complex, it made better and
    better predictions on the training data. The validation error, in contrast, decreases
    *then increases* as we increase model complexity. This reflects the transition
    from under- to overfitting. At low model complexity, the model underfits because
    it is not complex enough to capture the main trends in the data. At high model
    complexity, the model overfits because it “memorizes” the training data too closely.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，验证误差与之前探讨的训练误差行为有很大不同。回想一下，训练误差随着模型度数的增加而单调递减 - 随着模型变得更复杂，它在训练数据上做出了更好的预测。相反，验证误差在增加模型复杂度时先减少*然后增加*。这反映了从欠拟合到过拟合的转变。在低模型复杂度时，模型欠拟合，因为它不够复杂以捕捉数据的主要趋势。在高模型复杂度时，模型过拟合，因为它对训练数据进行了过于紧密的“记忆”。
- en: 'We can update our understanding of the relationships between error, complexity,
    and model variance:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更新我们对误差、复杂度和模型方差之间关系的理解：
- en: '![training_validation_curve](../Images/b7dd80dc3ac60957c1705068929dd428.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![training_validation_curve](../Images/b7dd80dc3ac60957c1705068929dd428.png)'
- en: Our goal is to train a model with complexity near the orange dotted line – this
    is where our model achieves minimum error on the validation set. Note that this
    relationship is a simplification of the real-world. But for the purposes of Data
    100, this is good enough.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是训练一个复杂度接近橙色虚线的模型 - 这是我们的模型在验证集上达到最小误差的地方。请注意，这种关系是对现实世界的简化。但对于Data 100来说，这已经足够了。
- en: 16.2 K-Fold Cross-Validation
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.2 K-Fold Cross-Validation
- en: Introducing a validation set gave us an “extra” chance to assess model performance
    on another set of unseen data. We are able to finetune the model design based
    on its performance on this one set of validation data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 引入验证集给了我们一个“额外”的机会，评估模型在另一组未见数据上的性能。我们能够根据模型在这一组验证数据上的性能来微调模型设计。
- en: But what if, by random chance, our validation set just happened to contain many
    outliers? It is possible that the validation datapoints we set aside do not actually
    represent other unseen data that the model might encounter. Ideally, we would
    like to validate our model’s performance on several different unseen datasets.
    This would give us greater confidence in our understanding of how the model behaves
    on new data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果碰巧我们的验证集包含了很多异常值怎么办？可能我们设置的验证数据点实际上并不代表模型可能遇到的其他未见数据。理想情况下，我们希望在几个不同的未见数据集上验证模型的性能。这将让我们更加自信地了解模型在新数据上的行为。
- en: Let’s think back to our validation framework. Earlier, we set aside x% of our
    training data (say, 20%) to use for validation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们的验证框架。之前，我们设置了训练数据的x%（比如20%）用于验证。
- en: '![validation_set](../Images/61e87652dbc45db97f49da437456e26d.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![validation_set](../Images/61e87652dbc45db97f49da437456e26d.png)'
- en: In the example above, we set aside the first 20% of training datapoints for
    the validation set. This was an arbitrary choice. We could have set aside *any*
    20% portion of the training data for validation. In fact, there are 5 non-overlapping
    “chunks” of training points that we could have designated as the validation set.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们设置了前20%的训练数据点作为验证集。这是一个任意的选择。我们可以将*任何*20%的训练数据用于验证。事实上，有5个不重叠的“块”训练数据点，我们可以指定为验证集。
- en: '![possible_validation_sets](../Images/3ee0580b5969ecab9856086c1b5527d4.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![possible_validation_sets](../Images/3ee0580b5969ecab9856086c1b5527d4.png)'
- en: 'The common term for one of these chunks is a **fold**. In the example above,
    we had 5 folds, each containing 20% of the training data. This gives us a new
    perspective: we really have *5* validation sets “hidden” in our training set.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个这样的块的常见术语是**折叠**。在上面的示例中，我们有5个折叠，每个折叠包含20%的训练数据。这给了我们一个新的视角：我们在我们的训练集中实际上有*5*个“隐藏”的验证集。
- en: 'In **cross-validation**, we perform validation splits for each fold in the
    training set. For a dataset with \(K\) folds, we:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在**交叉验证**中，我们为训练集中的每个折叠执行验证拆分。对于具有\(K\)个折叠的数据集，我们：
- en: Pick one fold to be the validation fold
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个折叠作为验证折叠
- en: Fit the model to training data from every fold *other* than the validation fold
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型拟合到除验证折叠之外的每个折叠的训练数据
- en: Compute the model’s error on the validation fold and record it
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算验证折叠上的模型误差并记录它
- en: Repeat for all \(K\) folds
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对所有\(K\)个折叠重复
- en: The **cross-validation error** is then the *average* error across all \(K\)
    validation folds.![cross_validation](../Images/a13b75875fb0a3edee189cc3cd7e4677.png)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证误差然后是所有\(K\)个验证折叠的*平均*误差。![cross_validation](../Images/a13b75875fb0a3edee189cc3cd7e4677.png)
- en: 16.2.1 Model Selection Workflow
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.1 模型选择工作流程
- en: At this stage, we have refined our model selection workflow. We begin by performing
    a train-test split to set aside a test set for the final evaluation of model performance.
    Then, we alternate between adjusting our design matrix and computing the cross-validation
    error to finetune the model’s design. In the example below, we illustrate the
    use of 4-fold cross-validation to help inform model design.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经完善了我们的模型选择工作流程。我们首先执行训练-测试拆分，以设置一个测试集，用于最终评估模型性能。然后，我们在调整设计矩阵和计算交叉验证误差之间交替，以微调模型的设计。在下面的示例中，我们说明了使用4折交叉验证来帮助确定模型设计。
- en: '![model_selection](../Images/0ac6e08d6caf58de3314b3f8ef3414bb.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![model_selection](../Images/0ac6e08d6caf58de3314b3f8ef3414bb.png)'
- en: 16.2.2 Hyperparameters
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.2 超参数
- en: An important use of cross-validation is for **hyperparameter** selection. A
    hyperparameter is some value in a model that is chosen *before* the model is fit
    to any data. This means that it is distinct from the model *parameters* \(\theta_i\)
    because its value is selected before the training process begins. We cannot use
    our usual techniques – calculus, ordinary least squares, or gradient descent –
    to choose its value. Instead, we must decide it ourselves.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证的一个重要用途是进行超参数选择。超参数是模型中在模型拟合到任何数据之前选择的一些值。这意味着它与模型参数\(\theta_i\)不同，因为它的值是在训练过程开始之前选择的。我们不能使用我们通常的技术
    - 微积分、普通最小二乘法或梯度下降 - 来选择它的值。相反，我们必须自己决定。
- en: 'Some examples of hyperparameters in Data 100 are:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Data 100中一些超参数的例子是：
- en: The degree of our polynomial model (recall that we selected the degree before
    creating our design matrix and calling `.fit`)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的多项式模型的程度（回想一下，在创建设计矩阵和调用`.fit`之前我们选择了程度）
- en: The learning rate, \(\alpha\), in gradient descent
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降中的学习率\(\alpha\)
- en: The regularization penalty, \(\lambda\) (to be introduced later this lecture)
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化惩罚\(\lambda\)（稍后将介绍）
- en: To select a hyperparameter value via cross-validation, we first list out several
    “guesses” for what the best hyperparameter may be. For each guess, we then run
    cross-validation to compute the cross-validation error incurred by the model when
    using that choice of hyperparameter value. We then select the value of the hyperparameter
    that resulted in the lowest cross-validation error.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过交叉验证选择超参数值，我们首先列出了几个关于最佳超参数可能是什么的“猜测”。对于每个猜测，我们然后运行交叉验证，计算模型在使用该超参数值时产生的交叉验证误差。然后我们选择导致最低交叉验证误差的超参数值。
- en: 'For example, we may wish to use cross-validation to decide what value we should
    use for \(\alpha\), which controls the step size of each gradient descent update.
    To do so, we list out some possible guesses for the best \(\alpha\): 0.1, 1, and
    10\. For each possible value, we perform cross-validation to see what error the
    model has *when we use that value of \(\alpha\) to train it*.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能希望使用交叉验证来决定我们应该使用什么值作为\(\alpha\)，它控制每次梯度下降更新的步长。为此，我们列出了最佳\(\alpha\)的一些可能猜测：0.1、1和10。对于每个可能的值，我们执行交叉验证，看当我们使用该\(\alpha\)值来训练模型时，模型产生了什么错误。
- en: '![hyperparameter_tuning](../Images/09d620b79ba3e75fa760e7a051df2550.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![hyperparameter_tuning](../Images/09d620b79ba3e75fa760e7a051df2550.png)'
- en: 16.3 Regularization
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.3 正则化
- en: 'We’ve now addressed the first of our two goals for today: creating a framework
    to assess model performance on unseen data. Now, we’ll discuss our second objective:
    developing a technique to adjust model complexity. This will allow us to directly
    tackle the issues of under- and overfitting.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经解决了今天的两个目标中的第一个：创建一个框架来评估模型在未见数据上的性能。现在，我们将讨论我们的第二个目标：开发一种调整模型复杂性的技术。这将使我们能够直接解决欠拟合和过拟合的问题。
- en: Earlier, we adjusted the complexity of our polynomial model by tuning a hyperparameter
    – the degree of the polynomial. We trialed several different polynomial degrees,
    computed the validation error for each, and selected the value that minimized
    the validation error. Tweaking the “complexity” was simple; it was only a matter
    of adjusting the polynomial degree.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们通过调整超参数（多项式的程度）来调整多项式模型的复杂性。我们尝试了几个不同的多项式程度，计算了每个的验证误差，并选择了最小化验证误差的值。调整“复杂性”很简单；只需要调整多项式程度。
- en: 'In most machine learning problems, complexity is defined differently from what
    we have seen so far. Today, we’ll explore two different definitions of complexity:
    the *squared* and *absolute* magnitude of \(\theta_i\) coefficients.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数机器学习问题中，复杂性的定义与我们迄今为止所见的不同。今天，我们将探讨复杂性的两种不同定义：\(\theta_i\)系数的*平方*和*绝对*大小。
- en: 16.3.1 Constraining Model Parameters
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.1 约束模型参数
- en: Think back to our work using gradient descent to descend down a loss surface.
    You may find it helpful to refer back to the Gradient Descent note to refresh
    your memory. Our aim was to find the combination of model parameters that led
    to the model having minimum loss. We visualized this using a contour map by plotting
    possible parameter values on the horizontal and vertical axes, which allows us
    to take a bird’s eye view above the loss surface. We want to find the model parameters
    corresponding to the lowest point on the loss surface.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们使用梯度下降来下降损失曲面的工作。您可能会发现参考梯度下降笔记来提醒自己会很有帮助。我们的目标是找到导致模型损失最小的模型参数组合。我们通过在水平和垂直轴上绘制可能的参数值来使用等高线图来可视化这一点，这使我们可以从上方鸟瞰损失曲面。我们希望找到对应于损失曲面上最低点的模型参数。
- en: '![unconstrained](../Images/2d687d89e789d73b2c5e05c5957f4037.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![unconstrained](../Images/2d687d89e789d73b2c5e05c5957f4037.png)'
- en: Let’s review our current modeling framework.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们当前的建模框架。
- en: \[\hat{\mathbb{Y}} = \theta_0 + \theta_1 \phi_1 + \theta_2 \phi_2 + \ldots +
    \theta_p \phi_p\]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat{\mathbb{Y}} = \theta_0 + \theta_1 \phi_1 + \theta_2 \phi_2 + \ldots +
    \theta_p \phi_p\]
- en: Recall that we represent our features with \(\phi_i\) to reflect the fact that
    we have performed feature engineering.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们用\(\phi_i\)表示我们的特征，以反映我们进行了特征工程的事实。
- en: Previously, we restricted model complexity by limiting the total number of features
    present in the model. We only included a limited number of polynomial features
    at a time; all other polynomials were excluded from the model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们通过限制模型中存在的特征的总数来限制模型的复杂性。我们一次只包括有限数量的多项式特征；所有其他多项式都被排除在模型之外。
- en: What if, instead of fully removing particular features, we kept all features
    and used each one only a “little bit”? If we put a limit on how *much* each feature
    can contribute to the predictions, we can still control the model’s complexity
    without the need to manually determine how many features should be removed.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不是完全删除特定特征，而是保留所有特征，并且每个特征只使用“一点点”，会怎么样？如果我们限制每个特征对预测的贡献*量*，我们仍然可以控制模型的复杂性，而无需手动确定应该删除多少个特征。
- en: What do we mean by a “little bit”? Consider the case where some parameter \(\theta_i\)
    is close to or equal to 0\. Then, feature \(\phi_i\) barely impacts the prediction
    – the feature is weighted by such a small value that its presence doesn’t significantly
    change the value of \(\hat{\mathbb{Y}}\). If we restrict how large each parameter
    \(\theta_i\) can be, we restrict how much feature \(\phi_i\) contributes to the
    model. This has the effect of *reducing* model complexity.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的“一点点”是什么意思？考虑某个参数\(\theta_i\)接近或等于0的情况。那么，特征\(\phi_i\)几乎不会影响预测 - 特征的权重值如此之小，以至于它的存在并不会显著改变\(\hat{\mathbb{Y}\)的值。如果我们限制每个参数\(\theta_i\)的大小，我们就限制了特征\(\phi_i\)对模型的贡献。这会*减少*模型的复杂性。
- en: In **regularization**, we restrict model complexity by *putting a limit* on
    the magnitudes of the model parameters \(\theta_i\).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在**正则化**中，我们通过对模型参数\(\theta_i\)的大小*设置限制*来限制模型的复杂性。
- en: 'What do these limits look like? Suppose we specify that the sum of all absolute
    parameter values can be no greater than some number \(Q\). In other words:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些限制看起来是什么样子？假设我们规定所有绝对参数值的总和不能大于某个数字\(Q\)。换句话说：
- en: \[\sum_{i=1}^p |\theta_i| \leq Q\]
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: \[\sum_{i=1}^p |\theta_i| \leq Q\]
- en: where \(p\) is the total number of parameters in the model. You can think of
    this as us giving our model a “budget” for how it distributes the magnitudes of
    each parameter. If the model assigns a large value to some \(\theta_i\), it may
    have to assign a small value to some other \(\theta_j\). This has the effect of
    increasing feature \(\phi_i\)’s influence on the predictions while decreasing
    the influence of feature \(\phi_j\). The model will need to be strategic about
    how the parameter weights are distributed – ideally, more “important” features
    will receive greater weighting.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中\(p\)是模型中参数的总数。您可以将这看作是我们为模型分配每个参数的大小的“预算”。如果模型为某些\(\theta_i\)分配了一个较大的值，它可能必须为其他一些\(\theta_j\)分配一个较小的值。这会增加特征\(\phi_i\)对预测的影响，同时减少特征\(\phi_j\)的影响。模型需要战略地分配参数权重
    - 理想情况下，更“重要”的特征将获得更大的权重。
- en: Notice that the intercept term, \(\theta_0\), is excluded from this constraint.
    **We typically do not regularize the intercept term**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，截距项\(\theta_0\)不受此约束的影响。**我们通常不对截距项进行正则化**。
- en: Now, let’s think back to gradient descent and visualize the loss surface as
    a contour map. As a refresher, a loss surface means that each point represents
    the model’s loss for a particular combination of \(\theta_1\), \(\theta_2\). Let’s
    say our goal is to find the combination of parameters that gives us the lowest
    loss.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回想一下梯度下降，并将损失曲面可视化为等高线图。损失曲面表示每个点代表模型对\(\theta_1\)、\(\theta_2\)的特定组合的损失。假设我们的目标是找到使我们获得最低损失的参数组合。
- en: '![constrained_gd](../Images/82c96a93889ebeee1c8383ef75e21f41.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![constrained_gd](../Images/82c96a93889ebeee1c8383ef75e21f41.png)'
- en: With no constraint, the optimal \(\hat{\theta}\) is in the center.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有约束的情况下，最优的\(\hat{\theta}\)位于中心。
- en: Applying this constraint limits what combinations of model parameters are valid.
    We can now only consider parameter combinations with a total absolute sum less
    than or equal to our number \(Q\). This means that we can only assign our *regularized*
    parameter vector \(\hat{\theta}_{\text{Reg}}\) to positions in the green diamond
    below.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这个约束限制了模型参数的有效组合。现在我们只能考虑总绝对和小于或等于我们的数字\(Q\)的参数组合。这意味着我们只能将我们的*正则化*参数向量\(\hat{\theta}_{\text{Reg}}\)分配到下面绿色菱形中的位置。
- en: '![diamondreg](../Images/ab67ae6c7c282924117ca96b785c04a5.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![diamondreg](../Images/ab67ae6c7c282924117ca96b785c04a5.png)'
- en: We can no longer select the parameter vector that *truly* minimizes the loss
    surface, \(\hat{\theta}_{\text{No Reg}}\), because this combination of parameters
    does not lie within our allowed region. Instead, we select whatever allowable
    combination brings us *closest* to the true minimum loss.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能再选择*真正*最小化损失曲面的参数向量\(\hat{\theta}_{\text{No Reg}}\)，因为这组参数不在我们允许的区域内。相反，我们选择任何允许的组合，使我们尽可能接近真正的最小损失。
- en: '![diamond](../Images/1c2a72e33395d0ae4b0a86903a1c3cad.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![diamond](../Images/1c2a72e33395d0ae4b0a86903a1c3cad.png)'
- en: Notice that, under regularization, our optimized \(\theta_1\) and \(\theta_2\)
    values are much smaller than they were without regularization (indeed, \(\theta_1\)
    has decreased to 0). The model has *decreased in complexity* because we have limited
    how much our features contribute to the model. In fact, by setting its parameter
    to 0, we have effectively removed the influence of feature \(\phi_1\) from the
    model altogether.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在正则化下，我们优化的\(\theta_1\)和\(\theta_2\)的值要比没有正则化时小得多（确实，\(\theta_1\)已经减少到0）。模型的*复杂度降低*，因为我们限制了特征对模型的贡献。事实上，通过将其参数设置为0，我们有效地从模型中完全删除了特征\(\phi_1\)的影响。
- en: If we change the value of \(Q\), we change the region of allowed parameter combinations.
    The model will still choose the combination of parameters that produces the lowest
    loss – the closest point in the constrained region to the true minimizer, \(\hat{\theta}_{\text{No
    Reg}}\).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们改变\(Q\)的值，我们就改变了允许的参数组合区域。模型仍然会选择产生最低损失的参数组合——最接近受约束区域中真正的最小化器\(\hat{\theta}_{\text{No
    Reg}}\)的点。
- en: If we make \(Q\) smaller:![diamondpoint](../Images/eea4721e637bab95042477ed55f1a31b.png)If
    we make \(Q\) larger:![largerq](../Images/dc3c685569d9d60f72ee5146c1a66576.png)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使\(Q\)更小：![diamondpoint](../Images/eea4721e637bab95042477ed55f1a31b.png)如果我们使\(Q\)更大：![largerq](../Images/dc3c685569d9d60f72ee5146c1a66576.png)
- en: When \(Q\) is small, we severely restrict the size of our parameters. \(\theta_i\)s
    are small in value, and features \(\phi_i\) only contribute a little to the model.
    The allowed region of model parameters contracts, and the model becomes much simpler.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当\(Q\)很小时，我们严重限制参数的大小。\(\theta_i\)的值很小，特征\(\phi_i\)对模型的贡献很小。模型参数的允许区域收缩，模型变得简单得多。
- en: When \(Q\) is large, we do not restrict our parameter sizes by much. \(\theta_i\)s
    are large in value, and features \(\phi_i\) contribute more to the model. The
    allowed region of model parameters expands, and the model becomes more complex.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当\(Q\)很大时，我们并不严重限制参数的大小。\(\theta_i\)的值很大，特征\(\phi_i\)对模型的贡献更大。模型参数的允许区域扩大，模型变得更复杂。
- en: Consider the extreme case of when \(Q\) is extremely large. In this situation,
    our restriction has essentially no effect, and the allowed region includes the
    OLS solution!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑当\(Q\)极大时的极端情况。在这种情况下，我们的限制基本上没有效果，允许的区域包括OLS解！
- en: '![verylarge](../Images/e3dd2d3dfc3f3978e78a8a9094c3f8da.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![verylarge](../Images/e3dd2d3dfc3f3978e78a8a9094c3f8da.png)'
- en: 'Now what if \(Q\) were very small? Our parameters are then set to (essentially
    0). If the model has no intercept term: \(\hat{\mathbb{Y}} = (0)\phi_1 + (0)\phi_2
    + \ldots = 0\). And if the model has an intercept term: \(\hat{\mathbb{Y}} = (0)\phi_1
    + (0)\phi_2 + \ldots = \theta_0\). Remember that the intercept term is excluded
    from the constraint - this is so we avoid the situation where we always predict
    0.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果\(Q\)非常小会怎么样？我们的参数将被设置为（基本上是0）。如果模型没有截距项：\(\hat{\mathbb{Y}} = (0)\phi_1
    + (0)\phi_2 + \ldots = 0\)。如果模型有一个截距项：\(\hat{\mathbb{Y}} = (0)\phi_1 + (0)\phi_2
    + \ldots = \theta_0\)。请记住，截距项被排除在约束之外——这样我们就避免了总是预测0的情况。
- en: Let’s summarize what we have seen.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下我们所看到的。
- en: '![summary](../Images/2eb6619a981b9cae768bc0a578afdbe4.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![summary](../Images/2eb6619a981b9cae768bc0a578afdbe4.png)'
- en: 16.4 L1 (LASSO) Regularization
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4 L1（LASSO）正则化
- en: How do we actually apply our constraint \(\sum_{i=1}^p |\theta_i| \leq Q\)?
    We will do so by modifying the *objective function* that we seek to minimize when
    fitting a model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何实际应用我们的约束\(\sum_{i=1}^p |\theta_i| \leq Q\)？我们将通过修改我们在拟合模型时寻求最小化的*目标函数*来实现。
- en: 'Recall our ordinary least squares objective function: our goal was to find
    parameters that minimize the model’s mean squared error.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们的普通最小二乘目标函数：我们的目标是找到最小化模型均方误差的参数。
- en: \[\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^n (y_i
    - (\theta_0 + \theta_1 \phi_{i, 1} + \theta_2 \phi_{i, 2} + \ldots + \theta_p
    \phi_{i, p}))^2\]
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: \[\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^n (y_i
    - (\theta_0 + \theta_1 \phi_{i, 1} + \theta_2 \phi_{i, 2} + \ldots + \theta_p
    \phi_{i, p}))^2\]
- en: To apply our constraint, we need to rephrase our minimization goal.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用我们的约束，我们需要重新表述我们的最小化目标。
- en: \[\frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0 + \theta_1 \phi_{i, 1} + \theta_2
    \phi_{i, 2} + \ldots + \theta_p \phi_{i, p}))^2\:\text{such that} \sum_{i=1}^p
    |\theta_i| \leq Q\]
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: \[\frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0 + \theta_1 \phi_{i, 1} + \theta_2
    \phi_{i, 2} + \ldots + \theta_p \phi_{i, p}))^2\:\text{such that} \sum_{i=1}^p
    |\theta_i| \leq Q\]
- en: Unfortunately, we can’t directly use this formulation as our objective function
    – it’s not easy to mathematically optimize over a constraint. Instead, we will
    apply the magic of the [Lagrangian Duality](https://en.wikipedia.org/wiki/Duality_(optimization)).
    The details of this are out of scope (take EECS 127 if you’re interested in learning
    more), but the end result is very useful. It turns out that minimizing the following
    *augmented* objective function is *equivalent* to our minimization goal above.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们不能直接使用这个公式作为我们的目标函数——在约束上进行数学优化并不容易。相反，我们将应用[拉格朗日对偶](https://en.wikipedia.org/wiki/Duality_(optimization))的魔力。这方面的细节超出了范围（如果你有兴趣了解更多，请参加EECS
    127课程），但最终结果非常有用。事实证明，最小化以下*增广*目标函数等同于我们上面的最小化目标。
- en: \[\frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0 + \theta_1 \phi_{i, 1} + \theta_2
    \phi_{i, 2} + \ldots + \theta_p \phi_{i, p}))^2 + \lambda \sum_{i=1}^p \vert \theta_i
    \vert = ||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum_{i=1}^p |\theta_i|\]
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: \[\frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0 + \theta_1 \phi_{i, 1} + \theta_2
    \phi_{i, 2} + \ldots + \theta_p \phi_{i, p}))^2 + \lambda \sum_{i=1}^p \vert \theta_i
    \vert = ||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum_{i=1}^p |\theta_i|\]
- en: The second of these two expressions includes the MSE expressed using vector
    notation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个表达式中的第二个包括使用向量表示的MSE。
- en: 'Notice that we’ve replaced the constraint with a second term in our objective
    function. We’re now minimizing a function with an additional regularization term
    that *penalizes large coefficients*. In order to minimize this new objective function,
    we’ll end up balancing two components:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已经用目标函数中的第二项替换了约束。我们现在正在最小化一个带有额外正则化项的函数，该项*惩罚大的系数*。为了最小化这个新的目标函数，我们最终会平衡两个组成部分：
- en: Keep the model’s error on the training data low, represented by the term \(\frac{1}{n}
    \sum_{i=1}^n (y_i - (\theta_0 + \theta_1 x_{i, 1} + \theta_2 x_{i, 2} + \ldots
    + \theta_p x_{i, p}))^2\)
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持模型在训练数据上的误差低，表示为术语\(\frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0 + \theta_1 x_{i,
    1} + \theta_2 x_{i, 2} + \ldots + \theta_p x_{i, p}))^2\)
- en: At the same time, keep the magnitudes of model parameters low, represented by
    the term \(\lambda \sum_{i=1}^p |\theta_i|\)
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时，保持模型参数的幅度低，表示为术语\(\lambda \sum_{i=1}^p |\theta_i|\)
- en: The \(\lambda\) factor controls the degree of regularization. Roughly speaking,
    \(\lambda\) is related to our \(Q\) constraint from before by the rule \(\lambda
    \approx \frac{1}{Q}\).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lambda\)因子控制正则化的程度。粗略地说，\(\lambda\)与之前的\(Q\)约束相关，规则为\(\lambda \approx \frac{1}{Q}\)。
- en: 'To understand why, let’s consider two extreme examples:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解原因，让我们考虑两个极端的例子：
- en: Assume \(\lambda \rightarrow \infty\). Then, \(\lambda \sum_{j=1}^{d} \vert
    \theta_j \vert\) dominates the cost function. To minimize this term, we set \(\theta_j
    = 0\) for all \(j \ge 1\). This is a very constrained model that is mathematically
    equivalent to the constant model. Earlier, we explained the constant model also
    arises when the L2 norm ball radius \(Q \rightarrow 0\).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设\(\lambda \rightarrow \infty\)。那么，\(\lambda \sum_{j=1}^{d} \vert \theta_j
    \vert\) 主导成本函数。为了最小化这个项，我们对所有\(j \ge 1\)设置\(\theta_j = 0\)。这是一个非常受限的模型，从数学上讲等同于常数模型。早些时候，我们解释了当L2范数球半径\(Q
    \rightarrow 0\)时，常数模型也会出现。
- en: Assume \(\lambda \rightarrow 0\). Then, \(\lambda \sum_{j=1}^{d} \vert \theta_j
    \vert\) is 0\. Minimizing the cost function is equivalent to \(\min_{\theta} \frac{1}{n}
    || Y - X\theta ||_2^2\), our usual MSE loss function. The act of minimizing MSE
    loss is just our familiar OLS, and the optimal solution is the global minimum
    \(\hat{\theta} = \hat\theta_{No Reg.}\). We showed that the global optimum is
    achieved when the L2 norm ball radius \(Q \rightarrow \infty\).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设\(\lambda \rightarrow 0\)。那么，\(\lambda \sum_{j=1}^{d} \vert \theta_j \vert\)为0。最小化成本函数等价于\(\min_{\theta}
    \frac{1}{n} || Y - X\theta ||_2^2\)，我们通常的MSE损失函数。最小化MSE损失的行为就是我们熟悉的OLS，最优解是全局最小值\(\hat{\theta}
    = \hat\theta_{No Reg.}\)。我们表明当L2范数球半径\(Q \rightarrow \infty\)时，全局最优解被实现。
- en: We call \(\lambda\) the **regularization penalty hyperparameter** and select
    its value via cross-validation.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称\(\lambda\)为**正则化惩罚超参数**，并通过交叉验证选择其值。
- en: The process of finding the optimal \(\hat{\theta}\) to minimize our new objective
    function is called **L1 regularization**. It is also sometimes known by the acronym
    “LASSO”, which stands for “Least Absolute Shrinkage and Selection Operator.”
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最优\(\hat{\theta}\)以最小化我们的新目标函数的过程称为**L1正则化**。它有时也被称为首字母缩写“LASSO”，代表“最小绝对收缩和选择算子”。
- en: Unlike ordinary least squares, which can be solved via the closed-form solution
    \(\hat{\theta}_{OLS} = (\mathbb{X}^{\top}\mathbb{X})^{-1}\mathbb{X}^{\top}\mathbb{Y}\),
    there is no closed-form solution for the optimal parameter vector under L1 regularization.
    Instead, we use the `Lasso` model class of `sklearn`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通最小二乘法不同，可以通过封闭形式解\(\hat{\theta}_{OLS} = (\mathbb{X}^{\top}\mathbb{X})^{-1}\mathbb{X}^{\top}\mathbb{Y}\)来解决，L1正则化下的最优参数向量没有封闭形式解。相反，我们使用`sklearn`的`Lasso`模型类。
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice that all model coefficients are very small in magnitude. In fact, some
    of them are so small that they are essentially 0\. An important characteristic
    of L1 regularization is that many model parameters are set to 0\. In other words,
    LASSO effectively **selects only a subset** of the features. The reason for this
    comes back to our loss surface and allowed “diamond” regions from earlier – we
    can often get closer to the lowest loss contour at a corner of the diamond than
    along an edge.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意所有模型系数的幅度都非常小。实际上，其中一些系数非常小，基本上为0。L1正则化的一个重要特征是许多模型参数被设置为0。换句话说，LASSO有效地**只选择了一部分**特征。这一原因可以追溯到我们先前的损失曲面和允许的“菱形”区域
    - 我们通常可以在菱形的一个角附近更接近最低损失轮廓，而不是沿着边缘。
- en: When a model parameter is set to 0 or close to 0, its corresponding feature
    is essentially removed from the model. We say that L1 regularization performs
    **feature selection** because, by setting the parameters of unimportant features
    to 0, LASSO “selects” which features are more useful for modeling.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型参数设置为0或接近0时，其对应的特征基本上从模型中移除了。我们说L1正则化执行**特征选择**，因为通过将不重要特征的参数设置为0，LASSO“选择”了哪些特征对建模更有用。
- en: 16.5 Scaling Features for Regularization
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.5 特征缩放用于正则化
- en: The regularization procedure we just performed had one subtle issue. To see
    what it is, let’s take a look at the design matrix for our `lasso_model`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚执行的正则化过程有一个微妙的问题。为了看清楚，让我们来看看我们的`lasso_model`的设计矩阵。
- en: '[PRE6]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|  | hp | hp^2 | hp^3 | hp^4 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | hp | hp^2 | hp^3 | hp^4 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 259 | 85.0 | 7225.0 | 614125.0 | 52200625.0 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 259 | 85.0 | 7225.0 | 614125.0 | 52200625.0 |'
- en: '| 129 | 67.0 | 4489.0 | 300763.0 | 20151121.0 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 129 | 67.0 | 4489.0 | 300763.0 | 20151121.0 |'
- en: '| 207 | 102.0 | 10404.0 | 1061208.0 | 108243216.0 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 207 | 102.0 | 10404.0 | 1061208.0 | 108243216.0 |'
- en: '| 302 | 70.0 | 4900.0 | 343000.0 | 24010000.0 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 302 | 70.0 | 4900.0 | 343000.0 | 24010000.0 |'
- en: '| 71 | 97.0 | 9409.0 | 912673.0 | 88529281.0 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 71 | 97.0 | 9409.0 | 912673.0 | 88529281.0 |'
- en: Our features – `hp`, `hp^2`, `hp^3`, and `hp^4` – are on drastically different
    numeric scales! The values contained in `hp^4` are orders of magnitude larger
    than those contained in `hp`. This can be a problem because the value of `hp^4`
    will naturally contribute more to each predicted \(\hat{y}\) because it is so
    much greater than the values of the other features. For `hp` to have much of an
    impact at all on the prediction, it must be scaled by a large model parameter.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的特征——`hp`、`hp^2`、`hp^3`和`hp^4`——在数值尺度上有着截然不同的差异！`hp^4`中的值比`hp`中的值大几个数量级！这可能是一个问题，因为`hp^4`的值自然上会对每个预测的\(\hat{y}\)贡献更多，因为它比其他特征的值大得多。对于`hp`对每个预测产生影响，它必须被一个大的模型参数所缩放。
- en: By inspecting the fitted parameters of our model, we see that this is the case
    – the parameter for `hp` is much larger in magnitude than the parameter for `hp^4`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查我们模型的拟合参数，我们发现这种情况确实存在——`hp`的参数的数量级远大于`hp^4`的参数。
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|  | Feature | Parameter |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | 特征 | 参数 |'
- en: '| --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | hp | -2.549321e-01 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 0 | hp | -2.549321e-01 |'
- en: '| 1 | hp^2 | -9.485972e-04 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 1 | hp^2 | -9.485972e-04 |'
- en: '| 2 | hp^3 | 8.919763e-06 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 2 | hp^3 | 8.919763e-06 |'
- en: '| 3 | hp^4 | -1.228723e-08 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 3 | hp^4 | -1.228723e-08 |'
- en: Recall that by applying regularization, we give our a model a “budget” for how
    it can allocate the values of model parameters. For `hp` to have much of an impact
    on each prediction, LASSO is forced to “spend” more of this budget on the parameter
    for `hp`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用正则化，我们给我们的模型一个“预算”，来分配模型参数的值。为了让`hp`对每个预测产生影响，LASSO被迫在`hp`的参数上“花费”更多的预算。
- en: We can avoid this issue by **scaling** the data before regularizing. This is
    a process where we convert all features to the same numeric scale. A common way
    to scale data is to perform **standardization** such that all features have mean
    0 and standard deviation 1; essentially, we replace everything with its Z-score.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在正则化之前对数据进行**缩放**来避免这个问题。这是一个过程，我们将所有特征转换为相同的数值尺度。一个常见的缩放数据的方法是进行**标准化**，使得所有特征的均值为0，标准差为1；基本上，我们用Z分数替换所有内容。
- en: \[z_k = \frac{x_k - \mu_k}{\sigma_k}\]
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: \[z_k = \frac{x_k - \mu_k}{\sigma_k}\]
- en: 16.6 L2 (Ridge) Regularization
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.6 L2（岭）正则化
- en: In all of our work above, we considered the constraint \(\sum_{i=1}^p |\theta_i|
    \leq Q\) to limit the complexity of the model. What if we had applied a different
    constraint?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上面的所有工作中，我们考虑了约束\(\sum_{i=1}^p |\theta_i| \leq Q\)来限制模型的复杂性。如果我们应用了不同的约束会怎样呢？
- en: 'In **L2 regularization**, also known as **ridge regression**, we constrain
    the model such that the sum of the *squared* parameters must be less than some
    number \(Q\). This constraint takes the form:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在L2正则化中，也被称为岭回归，我们约束模型，使得*平方*参数的总和必须小于某个数\(Q\)。这个约束的形式如下：
- en: \[\sum_{i=1}^p \theta_i^2 \leq Q\]
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: \[\sum_{i=1}^p \theta_i^2 \leq Q\]
- en: As before, we typically do not regularize the intercept term.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，我们通常不对截距项进行正则化。
- en: The allowed region of parameters for a given value of \(Q\) is now shaped like
    a ball.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的\(Q\)值，参数的允许区域现在呈球状。
- en: '![green_constrained_gd_sol](../Images/70eda8c2e4672aa02d59766876e9689d.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![green_constrained_gd_sol](../Images/70eda8c2e4672aa02d59766876e9689d.png)'
- en: 'If we modify our objective function like before, we find that our new goal
    is to minimize the function: \[\frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0 + \theta_1
    \phi_{i, 1} + \theta_2 \phi_{i, 2} + \ldots + \theta_p \phi_{i, p}))^2\:\text{such
    that} \sum_{i=1}^p \theta_i^2 \leq Q\]'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们像之前一样修改我们的目标函数，我们发现我们的新目标是最小化函数：\[\frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0
    + \theta_1 \phi_{i, 1} + \theta_2 \phi_{i, 2} + \ldots + \theta_p \phi_{i, p}))^2\:\text{such
    that} \sum_{i=1}^p \theta_i^2 \leq Q\]
- en: Notice that all we have done is change the constraint on the model parameters.
    The first term in the expression, the MSE, has not changed.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们所做的只是改变了模型参数的约束。表达式中的第一项，均方误差，没有改变。
- en: 'Using Lagrangian Duality, we can re-express our objective function as: \[\frac{1}{n}
    \sum_{i=1}^n (y_i - (\theta_0 + \theta_1 \phi_{i, 1} + \theta_2 \phi_{i, 2} +
    \ldots + \theta_p \phi_{i, p}))^2 + \lambda \sum_{i=1}^p \theta_i^2 = ||\mathbb{Y}
    - \mathbb{X}\theta||_2^2 + \lambda \sum_{i=1}^p \theta_i^2\]'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用拉格朗日对偶性，我们可以重新表达我们的目标函数为：\[\frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0 + \theta_1
    \phi_{i, 1} + \theta_2 \phi_{i, 2} + \ldots + \theta_p \phi_{i, p}))^2 + \lambda
    \sum_{i=1}^p \theta_i^2 = ||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum_{i=1}^p
    \theta_i^2\]
- en: When applying L2 regularization, our goal is to minimize this updated objective
    function.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 应用L2正则化时，我们的目标是最小化这个更新的目标函数。
- en: 'Unlike L1 regularization, L2 regularization *does* have a closed-form solution
    for the best parameter vector when regularization is applied:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 与L1正则化不同，L2正则化在应用正则化时确实有一个最佳参数向量的封闭形式解：
- en: \[\hat\theta_{\text{ridge}} = (\mathbb{X}^{\top}\mathbb{X} + n\lambda I)^{-1}\mathbb{X}^{\top}\mathbb{Y}\]
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat\theta_{\text{ridge}} = (\mathbb{X}^{\top}\mathbb{X} + n\lambda I)^{-1}\mathbb{X}^{\top}\mathbb{Y}\]
- en: This solution exists **even if \(\mathbb{X}\) is not full column rank**. This
    is a major reason why L2 regularization is often used – it can produce a solution
    even when there is colinearity in the features. We will discuss the concept of
    colinearity in a future lecture. We will not derive this result in Data 100, as
    it involves a fair bit of matrix calculus.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 即使\(\mathbb{X}\)不是完全列秩，这个解仍然存在。这是L2正则化经常被使用的一个主要原因——即使特征中存在共线性，它也可以产生一个解。我们将在未来的讲座中讨论共线性的概念。我们不会在Data
    100中推导这个结果，因为它涉及相当多的矩阵微积分。
- en: In `sklearn`, we perform L2 regularization using the `Ridge` class. Notice that
    we scale the data before regularizing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在`sklearn`中，我们使用`Ridge`类来执行L2正则化。请注意，在正则化之前我们会对数据进行缩放。
- en: '[PRE8]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 16.7 Regression Summary
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.7 回归总结
- en: Our regression models are summarized below. Note the objective function is what
    the gradient descent optimizer minimizes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的回归模型总结如下。请注意，目标函数是梯度下降优化器最小化的内容。
- en: '| Type | Model | Loss | Regularization | Objective Function | Solution |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 模型 | 损失 | 正则化 | 目标函数 | 解决方案 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| OLS | \(\hat{\mathbb{Y}} = \mathbb{X}\theta\) | MSE | None | \(\frac{1}{n}
    \&#124;\mathbb{Y}-\mathbb{X} \theta\&#124;^2_2\) | \(\hat{\theta}_{OLS} = (\mathbb{X}^{\top}\mathbb{X})^{-1}\mathbb{X}^{\top}\mathbb{Y}\)
    if \(\mathbb{X}\) is full column rank |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 普通最小二乘法（OLS） | \(\hat{\mathbb{Y}} = \mathbb{X}\theta\) | 均方误差 | 无 | \(\frac{1}{n}
    \&#124;\mathbb{Y}-\mathbb{X} \theta\&#124;^2_2\) | \(\hat{\theta}_{OLS} = (\mathbb{X}^{\top}\mathbb{X})^{-1}\mathbb{X}^{\top}\mathbb{Y}\)
    如果 \(\mathbb{X}\) 是满秩的 |'
- en: '| Ridge | \(\hat{\mathbb{Y}} = \mathbb{X} \theta\) | MSE | L2 | \(\frac{1}{n}
    \&#124;\mathbb{Y}-\mathbb{X}\theta\&#124;^2_2 + \lambda \sum_{i=1}^p \theta_i^2\)
    | \(\hat{\theta}_{ridge} = (\mathbb{X}^{\top}\mathbb{X} + n \lambda I)^{-1}\mathbb{X}^{\top}\mathbb{Y}\)
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 岭回归 | \(\hat{\mathbb{Y}} = \mathbb{X} \theta\) | 均方误差 | L2 | \(\frac{1}{n}
    \&#124;\mathbb{Y}-\mathbb{X}\theta\&#124;^2_2 + \lambda \sum_{i=1}^p \theta_i^2\)
    | \(\hat{\theta}_{ridge} = (\mathbb{X}^{\top}\mathbb{X} + n \lambda I)^{-1}\mathbb{X}^{\top}\mathbb{Y}\)
    |'
- en: '| LASSO | \(\hat{\mathbb{Y}} = \mathbb{X} \theta\) | MSE | L1 | \(\frac{1}{n}
    \&#124;\mathbb{Y}-\mathbb{X}\theta\&#124;^2_2 + \lambda \sum_{i=1}^p \vert \theta_i
    \vert\) | No closed form |**'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '| LASSO | \(\hat{\mathbb{Y}} = \mathbb{X} \theta\) | 均方误差 | L1 | \(\frac{1}{n}
    \&#124;\mathbb{Y}-\mathbb{X}\theta\&#124;^2_2 + \lambda \sum_{i=1}^p \vert \theta_i
    \vert\) | 无闭式解 |'
