- en: Chapter 9\. Migrating to Amazon Redshift
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。迁移到Amazon Redshift
- en: 'Organizations have been running on-premises data warehouses for years, and
    these have served them well for the workloads of yesterday. But today’s volume,
    variety, and velocity of data requires customers to modernize their data warehouses
    to ensure optimal performance. Here are some of the major limitations or shortcomings
    of the traditional data warehouse:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 组织多年来一直在本地数据仓库上运行，并且这些数据仓库对昨天的工作负载非常有用。但今天的数据量、种类和速度要求客户现代化他们的数据仓库，以确保最佳性能。以下是传统数据仓库的一些主要限制或缺陷：
- en: Slow to obtain
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 获取速度慢
- en: Procuring your own servers and sizing them takes much longer compared to provisioning
    infrastructure in the cloud.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自己采购服务器并调整它们的大小比在云中配置基础设施要花费更长的时间。
- en: Costly to maintain
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 维护成本高昂
- en: They are so rigid in the structure that any modifications means a drastic increase
    in costs and project timelines.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的结构如此严格，以至于任何修改都意味着成本和项目时间表的急剧增加。
- en: Resiliency
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性
- en: Hardware components are bound to fail sooner or later. Designing redundancy
    around failures and having multiple data centers with standby servers gets expensive
    really fast.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件组件迟早会出现故障。围绕故障设计冗余，并拥有多个数据中心和待命服务器会非常快速地增加成本。
- en: Inflexible architecture
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 不灵活的架构
- en: The foremost requirement of every business is agility and scalability. The inflexible
    architecture of the traditional data warehouses makes it next to impossible to
    bring in changes rapidly.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 每个企业的首要需求是敏捷性和可扩展性。传统数据仓库的不灵活架构几乎无法快速进行变更。
- en: Technology advances
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 技术进步
- en: Advancements in technology are made every day. The traditional data warehouse
    you set up for your business was probably done a couple of years back. So, you
    are already behind.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 每天都有技术进步。您为业务建立的传统数据仓库可能已经是几年前的事情了。因此，您已经落后了。
- en: To address these limitations, one option is to adopt Amazon Redshift for your
    analytical needs because it is a fully managed, fast, scalable, and cost-effective
    service that enables you to derive insights from all your data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这些限制，一个选择是为您的分析需求采用Amazon Redshift，因为它是一个完全托管的、快速、可扩展且成本效益高的服务，使您能够从所有数据中获取洞察。
- en: However, data warehouse migration projects can be complex and challenging. It’s
    easy to underestimate the complexity of the migration process, resulting in a
    lack of clarity about what needs to be migrated, how long it will take, and what
    resources will be required.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据仓库迁移项目可能非常复杂和具有挑战性。很容易低估迁移过程的复杂性，导致对需要迁移的内容、所需时间以及所需资源缺乏清晰的认识。
- en: This chapter will cover [“Migration Considerations”](#migration_considerations),
    then look at [“Migration Strategies”](#migration_strategies) and AWS native [“Migration
    Tools and Services”](#migration_tools_services). These topics will assist in clearly
    outlining the migration complexity and providing you clarity on challenges.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖[“迁移考虑”](#migration_considerations)，然后讨论[“迁移策略”](#migration_strategies)以及AWS原生的[“迁移工具和服务”](#migration_tools_services)。这些主题将帮助您清楚地概述迁移的复杂性，并为您提供挑战的明确认识。
- en: Then we get into details of the actual [“Database Migration Process”](#migration_process)
    and finally, discuss how to [“Accelerate Your Migration to Amazon Redshift”](#accelerate_redshift_migration).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将详细讨论实际的[“数据库迁移过程”](#migration_process)，最后讨论如何[“加速您的Amazon Redshift迁移”](#accelerate_redshift_migration)。
- en: Migration Considerations
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移考虑
- en: Data warehouse migration projects can be challenging in terms of project complexity
    and can expose risk related to resources, time, and cost. Before you begin your
    data warehouse migration, consider applying the principles covered in [“Modern
    Data Architecture”](ch01.html#modern_data_architecture) and reevaluate your future
    state data warehouse architecture. Just because you have all those tables in your
    current database does not imply that you must move all of them to Amazon Redshift.
    Evaluate how you can capitalize on this opportunity and modernize your overall
    data warehouse strategy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库迁移项目在项目复杂性方面可能具有挑战性，并可能暴露与资源、时间和成本相关的风险。在开始数据仓库迁移之前，请考虑应用[“现代数据架构”](ch01.html#modern_data_architecture)中涵盖的原则，并重新评估您未来状态的数据仓库架构。仅仅因为您当前数据库中有所有这些表并不意味着您必须将它们全部迁移到Amazon
    Redshift。评估如何利用此机会并现代化您的整体数据仓库战略。
- en: Retire Versus Retain
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 废弃与保留
- en: Migration to any new platform is an opportunity to take stock of your data footprint,
    and eliminate redundant or unused data and reports. You can start with analyzing
    the usage of reports and identify if there are any unused ones. Review the reports
    that might have accumulated over time, and eliminate the ones that are not used
    anymore. Unless your organization has a periodic recertification process, it is
    very likely that reports are simply being generated but are not really used.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移到任何新平台都是审视您的数据占用情况的机会，消除冗余或未使用的数据和报告。您可以从分析报告的使用情况开始，并识别是否存在未使用的报告。审查随时间积累的报告，并消除不再使用的报告。除非您的组织有定期的重新认证过程，否则很可能只是生成了报告而实际上并未使用。
- en: One of the most common reasons for this is that *business processes evolve over
    time*, and once that happens, the older report is no longer providing the value
    it was providing before. The urgency and drive to get a new report that will satisfy
    the new process takes precedence, and the older report is often left behind.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况最常见的原因之一是*业务流程随时间的演变*，一旦发生这种情况，较旧的报告就不再提供它以前提供的价值。满足新流程的紧迫性和推动力更重要，而较旧的报告通常会被搁置。
- en: 'Another common reason is *report aging*: the report was built as requested
    and it was very useful, but the data behind it has grown, and now the report runs
    into too many pages. So a new higher-level or summary report was commissioned
    and this original report was still used, albeit infrequently, and eventually not
    at all. If all reports associated to a dataset are determined unnecessary, you
    may be able to remove that dataset from the ETL processes completely.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见原因是*报告老化*：报告按照要求构建，非常有用，但其背后的数据已经增长，现在报告占据了太多页面。因此，委托建立了一个新的高级或摘要报告，原始报告仍然被使用，尽管不频繁，最终根本不再使用。如果确定与数据集关联的所有报告都不再需要，则可以彻底从ETL过程中删除该数据集。
- en: Review the data accumulated in your current data warehouse and classify data
    that needs high performance query execution compared to queries that do not have
    such strict execution SLA requirements. Consider a modern data architecture, as
    covered previously in [“Reference Architecture for Modern Data Architecture”](ch01.html#ref_arch_modern_data_architecture).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 审查当前数据仓库中积累的数据，并分类需要高性能查询执行与不需要严格执行SLA要求的查询数据。考虑现代数据架构，如之前介绍的[“现代数据架构参考架构”](ch01.html#ref_arch_modern_data_architecture)。
- en: Clean up any existing backup schemas or tables that are no longer required and
    if possible, drop those objects. To retain any required backup tables, you can
    use the `unload` command as shown in [Example 7-27](ch07.html#unload_example)
    and offload these tables to your Amazon S3 bucket. Amazon S3 offers a range of
    storage classes for the objects you store. You choose an S3 storage class depending
    on your use case scenario and performance access requirements; review the various
    [Amazon S3 storage classes](https://oreil.ly/tzKAn). After the `unload`, you can
    apply the [S3 lifecycle configuration](https://oreil.ly/tuk9u) policy and move
    the S3 backup files to less-expensive storage classes. Also review the [considerations
    for transitioning objects to different S3 storage classes](https://oreil.ly/PGlzY)
    to plan out these rules.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 清理掉任何不再需要的现有备份方案或表格，并在可能的情况下删除这些对象。要保留任何必需的备份表格，您可以使用`unload`命令，如[示例 7-27](ch07.html#unload_example)所示，并将这些表格卸载到您的Amazon
    S3存储桶中。Amazon S3为您存储的对象提供多种存储类别。根据您的使用情况和性能访问需求选择S3存储类别；请查看各种[Amazon S3存储类别](https://oreil.ly/tzKAn)。在`unload`之后，您可以应用[S3生命周期配置](https://oreil.ly/tuk9u)策略，并将S3备份文件移动到更便宜的存储类别。还需审查[将对象过渡到不同S3存储类别的考虑因素](https://oreil.ly/PGlzY)以制定这些规则。
- en: You can use the Amazon S3 Glacier Instant Retrieval storage class, which is
    queryable by Amazon Athena. But to use the objects under S3 Glacier Flexible Retrieval
    or S3 Glacier Deep Archive storage classes, you need to copy the restored objects
    back into Amazon S3 to change their storage class.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Amazon S3 Glacier Instant Retrieval存储类别，Amazon Athena可以查询。但要使用S3 Glacier
    Flexible Retrieval或S3 Glacier Deep Archive存储类别下的对象，您需要将恢复的对象复制回Amazon S3以更改其存储类别。
- en: Once all necessary cleanups are done the next thing is choosing the right migration
    strategy. This is based on your source data warehouse landscape and the amount
    of transformation required for the migration to Amazon Redshift. This will reduce
    the complexity and risk to your data warehouse migration project.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成所有必要的清理工作，下一步是选择合适的迁移策略。这基于您的源数据仓库景观以及迁移到Amazon Redshift所需的转换量。这将减少数据仓库迁移项目的复杂性和风险。
- en: 'Key factors that can influence your migration strategy decisions are:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可影响您迁移策略决策的关键因素包括：
- en: Migration data volume
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移数据量
- en: Transformations required
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要的转换
- en: Data volatility and availability
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据波动性和可用性
- en: Migration and ETL tools
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移和ETL工具
- en: Data movement options
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据移动选项
- en: Usage of Domain Name System
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用域名系统
- en: Migration Data Size
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移数据大小
- en: The total size of the source data warehouse to be migrated is determined by
    the number of databases, the number of schemas in those databases, and the number
    of objects in those schemas that are in scope for the migration. Having a good
    understanding of the data sources and data domains required for moving to Amazon
    Redshift will lead to optimal sizing of the migration project.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要迁移的源数据仓库的总大小取决于数据库的数量、这些数据库中模式的数量以及迁移范围内的这些模式中的对象数量。了解移至Amazon Redshift所需的数据源和数据域将有助于优化迁移项目的大小。
- en: For example, because you have five schemas on your source data warehouse, it
    does not imply that you must have five schemas on a single Amazon Redshift data
    warehouse. Rather, you should try to isolate the schemas and their workloads,
    evaluate if you could stand up multiple Amazon Redshift data warehouses, and leverage
    data sharing to provide workload isolation, as we have covered in-depth in [“Data
    Sharing Use Cases”](ch07.html#Data-sharing-use-cases).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您的源数据仓库有五个模式，并不意味着您在单个Amazon Redshift数据仓库上必须有五个模式。相反，您应该尝试隔离模式及其工作负载，评估是否可以启动多个Amazon
    Redshift数据仓库，并利用数据共享来提供工作负载隔离，正如我们在[“数据共享用例”](ch07.html#Data-sharing-use-cases)中详细介绍的那样。
- en: Platform-Specific Transformations Required
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需要平台特定的转换
- en: Your existing data warehouse might be having some proprietary components specific
    to your current vendor. Migrating to Amazon Redshift can involve transformation
    such as data mapping and schema change. The complexity of the data transformation
    needed to be applied will determine the preprocessing time required for the migration.
    Also consider if you have your transformation logic coded as Stored Procedures
    and stored on your schema itself.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您现有的数据仓库可能具有某些专有组件，特定于当前供应商。迁移到Amazon Redshift可能涉及数据映射和模式更改等转换。需要应用的数据转换复杂性将确定迁移所需的预处理时间。还要考虑您是否已将转换逻辑编码为存储过程并存储在您的模式本身。
- en: Take this opportunity to modernize and have the ETL logic outside your data
    warehouse. This future-proofs your overall data warehouse design and, if required,
    enables you to swap out your ETL tool or your data warehouse technology stack
    independently. Modern ETL tools will take advantage of push-down capabilities
    to deliver optimal performance based on where it is connecting to. We have previously
    covered data transformation strategies in [Chapter 4](ch04.html#AR_TGD_CH4) that
    you can leverage to modernize the transformation process through the use of federation,
    and we cover [“AWS Schema Conversion Tool”](#aws_sct).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这个机会现代化，并将ETL逻辑放在数据仓库之外。这将未来保护您的整体数据仓库设计，并且如果需要，使您能够独立地更换ETL工具或数据仓库技术堆栈。现代ETL工具将利用推送功能以基于其连接位置提供最佳性能。我们先前在[第4章](ch04.html#AR_TGD_CH4)中讨论过数据转换策略，您可以利用联邦化来现代化转换过程，并且我们介绍了[“AWS模式转换工具”](#aws_sct)。
- en: Data Volatility and Availability Requirements
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据波动性和可用性要求
- en: Be cognizant of your existing data warehouse’s up-time and availability requirements.
    These requirements may be dictated by ingestion rates, update intervals, and your
    end user’s data consumption patterns. These requirements will influence the options
    you have for your data warehouse migration project. A source data warehouse with
    a high data change rate might require a stringent cutover window. If the migration
    requires an extended period of service downtime, it could lead to higher complexity.
    You can test migration to ensure you can meet the stringent cutover window.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意您现有数据仓库的上线时间和可用性要求。这些要求可能由摄入速率、更新间隔以及最终用户的数据消费模式所决定。这些要求将影响您在数据仓库迁移项目中的选择。高数据变更率的源数据仓库可能需要严格的过渡窗口。如果迁移需要延长的服务停机时间，可能会导致更高的复杂性。您可以进行迁移测试以确保能够满足严格的过渡窗口。
- en: If you have multiple logical workloads on a single-source data warehouse, you
    might find that they all have same up-time requirements just because they are
    sharing the same hardware. Double-check with those individual business stakeholders,
    discuss workload isolation with multiple target Amazon Redshift data warehouses,
    and each business stakeholder gets the opportunity to [establish their own RTO
    and RPO objectives](https://oreil.ly/VqPHN) individually. We dive deeper into
    different cutover options in [“Migration Strategies”](#migration_strategies).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在单源数据仓库上有多个逻辑工作负载，您可能会发现它们都具有相同的上线时间要求，因为它们共享相同的硬件。与各个业务利益相关者确认，讨论在多个目标Amazon
    Redshift数据仓库中的工作负载隔离，并且每个业务利益相关者有机会单独确定其[自己的RTO和RPO目标](https://oreil.ly/VqPHN)。我们在[“Migration
    Strategies”](#migration_strategies)中深入探讨不同的过渡选项。
- en: Selection of Migration and ETL Tools
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择迁移和ETL工具
- en: The selection of migration tools to ETL can impact the migration project. When
    migrating to Amazon Redshift, you can choose to migrate your ETL workflows to
    a new AWS native service like [AWS Glue ETL](https://oreil.ly/egGZ8) and leverage
    [AWS Glue Studio visual editor](https://oreil.ly/Zv8Z5) or instead just retain
    your existing ETL tool. You can weight the benefits and plan the ETL migration
    based on the timeline and budget you have for the project. You can take an iterative
    approach, where you migrate the Data Warehouse first, retaining the existing ETL
    workflows in your legacy tool, and eventually migrate the ETL workflows as well.
    When planning the timelines for the migration project, accounting for additional
    time required for deployment and setup of these tools can ease the execution cycles.
    We will cover AWS tools and services in [“Migration Tools and Services”](#migration_tools_services).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 选择迁移工具至ETL可能会影响迁移项目。当迁移到Amazon Redshift时，您可以选择将ETL工作流迁移到新的AWS本地服务，如[AWS Glue
    ETL](https://oreil.ly/egGZ8)，并利用[AWS Glue Studio visual editor](https://oreil.ly/Zv8Z5)，或者保留现有的ETL工具。您可以根据项目的时间表和预算权衡利弊，并计划ETL迁移。您可以采取迭代方法，首先迁移数据仓库，保留现有ETL工作流工具，最终迁移ETL工作流。在规划迁移项目的时间表时，考虑这些工具的部署和设置所需的额外时间，可以简化执行周期。我们将在[“Migration
    Tools and Services”](#migration_tools_services)中介绍AWS工具和服务。
- en: Data Movement Considerations
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据移动考虑事项
- en: Data warehouse migration involves data transfer between the source data warehouse
    servers and your AWS infrastructure. Depending on your current network capacity
    and its existing utilization, you can transfer data over your network connection
    via Direct Connect as covered in [“Private/Public VPC and Secure Access”](ch02.html#network_configuration),
    or choose to transfer data offline via services such as the [AWS Snow Family](https://aws.amazon.com/snow).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库迁移涉及源数据仓库服务器与您的AWS基础设施之间的数据传输。根据当前网络容量及其现有利用率，您可以通过Direct Connect进行网络连接传输，详见[“Private/Public
    VPC and Secure Access”](ch02.html#network_configuration)，或选择通过离线服务如[AWS Snow Family](https://aws.amazon.com/snow)进行数据传输。
- en: Data warehouses ranging under 10 TB can be considered for network transfer,
    but higher data volumes are typically migrated with the AWS Snowball Edge device.
    We cover the AWS Snow family of devices, including AWS Snowball Edge, as well
    as an estimated time to transfer 10 TB data over different network links, in [“AWS
    Snow Family”](#snow_family).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库大小在10 TB以下时可以考虑通过网络传输进行迁移，但通常情况下，更高的数据量会使用AWS Snowball Edge设备进行迁移。我们介绍AWS
    Snow系列设备，包括AWS Snowball Edge，以及在不同网络连接下传输10 TB数据的预估时间，详见[“AWS Snow Family”](#snow_family)。
- en: Domain Name System (DNS)
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 域名系统（DNS）
- en: The consumers of your data warehouse will be using the current data warehouse
    name or IP address to establish connections to the data warehouse. Migrating to
    a new data warehouse will require applications to make changes to their connection
    settings, pointing to the new data warehouse. If you are already using a DNS service,
    then this migration project will minimize the impact to consumer applications.
    If not, then this is a good time to introduce a DNS layer in your architecture.
    This DNS layer can also help during disaster scenarios by transparently failing
    over to a secondary region; refer the [Route 53 failover types](https://oreil.ly/G_hrv)
    for details.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据仓库的使用者将使用当前数据仓库名称或IP地址来建立到数据仓库的连接。迁移到新的数据仓库将需要应用程序修改其连接设置，指向新的数据仓库。如果您已经使用DNS服务，则此迁移项目将最小化对消费者应用程序的影响。如果没有，那么现在是在您的架构中引入DNS层的好时机。此DNS层还可以在灾难场景中通过自动切换到次要区域来帮助；参考[Route
    53故障转移类型](https://oreil.ly/G_hrv)了解详情。
- en: Migration Strategies
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移策略
- en: 'Depending on the source data warehouse’s data velocity and availability requirements,
    there are three main migration strategies to choose from:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 根据源数据仓库的数据速率和可用性要求，有三种主要的迁移策略可供选择：
- en: One-step migration
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单步迁移
- en: Two-step migration
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两步迁移
- en: Iterative migration
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代迁移
- en: One-Step Migration
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单步迁移
- en: One-step migration requires that you freeze your current database and disallow
    changes by stopping all ingestion jobs. Then you extract a point-in-time database
    snapshot to CSV files, or columnar formats like Parquet. Then, depending on your
    connectivity options, you use an existing network or AWS Snow Family services
    such as AWS Snowball to deliver datasets to Amazon S3 for loading into Amazon
    Redshift. You then test the destination Amazon Redshift data warehouse for data
    consistency with the frozen snapshot of your source. After all validations have
    passed, you switch over the consumers of your existing database to Amazon Redshift.
    This is a good option for databases that don’t require continuous availability
    and you have an acceptable time window like a weekend or a couple days to perform
    the migration.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 单步迁移要求您冻结当前数据库并通过停止所有摄入作业来禁止更改。然后，提取时点数据库快照到CSV文件或列格式，如Parquet。然后，根据您的连接选项，您可以使用现有网络或AWS
    Snow Family服务（如AWS Snowball）将数据集传送到Amazon S3，以加载到Amazon Redshift中。然后，使用源的冻结快照测试目标Amazon
    Redshift数据仓库以确保数据一致性。在所有验证通过后，您将切换现有数据库的使用者到Amazon Redshift。这对于不需要持续可用性并且具有可接受的时间窗口（如周末或几天）来执行迁移的数据库是一个不错的选择。
- en: If your existing data warehouse is predominantly batch-oriented, then depending
    on the batch intervals, one-step migration can be a good fit.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现有的数据仓库主要是批处理导向的，那么根据批处理间隔，单步迁移可能是一个很好的选择。
- en: Two-Step Migration
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两步迁移
- en: This is commonly used for databases that require continuous operation, such
    as the continuous replication. During the migration, the source databases allow
    ongoing data changes, and you will need a continuous replication process to keep
    data changes in sync between the source data warehouse and Amazon Redshift. The
    breakdown of the two-step migration strategy is as follows.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常用于需要连续运行的数据库，如连续复制。在迁移过程中，源数据库允许持续的数据变更，并且您需要一个持续的复制过程来保持源数据仓库和Amazon Redshift之间数据变更的同步。两步迁移策略的详细步骤如下：
- en: Initial data migration
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始数据迁移
- en: The initial data is migrated to Amazon Redshift by following the one-step migration
    approach previously described. This data snapshot is extracted from the source
    database during minimal usage periods to minimize the impact to ongoing activity.
    To capture changes after the initial snapshot, you can turn on change logs at
    the same time you take the snapshot. If you have date-timestamps in all your tables
    indicating the update time, you can also use the date-timestamp to capture changes
    after the initial snapshot. You will conduct testing to ensure data consistency
    by running validation scripts and/or business users testing reports.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 初始数据通过之前描述的单步迁移方法迁移到Amazon Redshift。在最小使用期间从源数据库提取此数据快照，以减少对持续活动的影响。为了捕获初始快照后的变更，您可以同时启用变更日志。如果您的所有表中都有日期时间戳表示的更新时间，您还可以使用日期时间戳来捕获初始快照后的变更。您将通过运行验证脚本和/或业务用户测试报告来进行测试，以确保数据一致性。
- en: Changed data migration
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变更数据迁移
- en: Data that changed in the source database after the initial data migration is
    subsequently propagated to the destination before switchover. Your migration tool
    can facilitate this via ongoing replication jobs, or you can use the date-timestamp
    as we previously mentioned to identify changed data. This second step synchronizes
    the source and destination databases. After all the changed data is migrated,
    you can validate the data in the destination database by running validation scripts
    and/or business users testing reports. After all validations have passed, the
    consumers of your existing database are switched over to Amazon Redshift.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始数据迁移后，在切换之前，将源数据库中发生变化的数据传播到目标数据库。您的迁移工具可以通过持续的复制作业来实现这一点，或者您可以像之前提到的那样使用日期时间戳来识别变更的数据。第二步是通过同步源和目标数据库来验证目的地数据库中的所有已更改数据。在所有验证通过后，可以将现有数据库的使用者切换到Amazon
    Redshift。
- en: Iterative Migration
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代迁移
- en: This migration approach is suitable for large-scale data warehouse migration
    projects. The principle of iterative migration is to cautiously divide a complex
    migration project into multiple systematic iterations. This strategy is used to
    significantly reduce the complexity and the overall risk for the migration project
    by breaking the total risk into multiple smaller pieces.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此迁移方法适用于大规模数据仓库迁移项目。迭代迁移的原则是将复杂的迁移项目谨慎地划分为多个系统化的迭代。此策略通过将总体风险分解为多个较小的部分，显著降低了迁移项目的复杂性和整体风险。
- en: You start from a workload that covers a good number of data sources and subject
    areas, typically with a medium complexity area, then add more data sources and
    subject areas in each subsequent iteration. The challenge in this approach is
    to be able to synthetically break down the overall migration project into multiple
    logical iterations. See the blog [“Develop an Application Migration Methodology
    to Modernize Your Data Warehouse with Amazon Redshift”](https://oreil.ly/fzDAA)
    for more details on how to identify and group data sources and analytics applications
    to migrate from the source data warehouse to Amazon Redshift using the iterations-based
    migration approach.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您从覆盖多个数据源和主题领域的工作负载开始，通常涵盖中等复杂性区域，然后在每个后续迭代中添加更多数据源和主题领域。这种方法的挑战在于能够将整体迁移项目分解为多个逻辑迭代。有关如何使用基于迭代的迁移方法识别和分组从源数据仓库迁移到Amazon
    Redshift的数据源和分析应用程序的更多详细信息，请参阅博客[“开发应用迁移方法论，以现代化Amazon Redshift数据仓库”](https://oreil.ly/fzDAA)。
- en: With this strategy, you run both the source data warehouse and Amazon Redshift
    production environments in parallel for a certain amount of time before you can
    fully retire the specific workloads that have been successfully migrated in this
    iteration. Also as you move into the next iteration, if feasible, you can downsize
    your source system to accommodate the reduction of workload.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种策略，您在完全淘汰成功迁移的特定工作负载之前，在一定时间内同时运行源数据仓库和Amazon Redshift生产环境。此外，在进入下一迭代时，如果可行，您可以缩小源系统以适应工作负载的减少。
- en: Refer to [Figure 9-1](#one_two_iterative) to visualize the three migration strategies
    we discussed.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 参考[图 9-1](#one_two_iterative)，以可视化我们讨论的三种迁移策略。
- en: '![Migration strategies](assets/ardg_0901.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![迁移策略](assets/ardg_0901.png)'
- en: Figure 9-1\. Migration strategies
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1\. 迁移策略
- en: And furthermore, to guide your migration strategy decision, refer to [Table 9-1](#migration_strategy_decision)
    to map the consideration factors with a preferred migration strategy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，为了指导您的迁移策略决策，请参考[表 9-1](#migration_strategy_decision)，将考虑因素与首选迁移策略进行映射。
- en: Table 9-1\. Migration strategy decision
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9-1\. 迁移策略决策
- en: '| Migration strategy | One-step migration | Two-step migration | Iterative
    migration |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 迁移策略 | 单步迁移 | 两步迁移 | 迭代迁移 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| The number of subject areas in migration scope | Small | Medium to Large
    | Large |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 迁移范围中的主题区域数量 | 小 | 中等到大 | 大 |'
- en: '| Data transfer volume | Small | Small to Large | Large |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 数据传输量 | 小 | 小到大 | 大 |'
- en: '| Data change rate during migration | None | Minimal | Minimal to Frequent
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 迁移过程中的数据变更率 | 无 | 最小 | 最小到频繁 |'
- en: '| Data transformation complexity | Small | Medium | Any |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 数据转换复杂性 | 小 | 中等 | 任意 |'
- en: '| Migration change window for switching from source to target | Hours to days
    | Days | Minutes |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 从源到目标切换的迁移更改窗口 | 几小时到几天 | 几天 | 几分钟 |'
- en: '| Migration project duration | Weeks | Weeks to few months | Multiple months
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 迁移项目持续时间 | 周 | 几周到几个月 | 多个月 |'
- en: Migration Tools and Services
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移工具和服务
- en: Your data warehouse migration to Amazon Redshift will entail migration of schema
    objects and then migration of data. The objects on your source will include schemas,
    tables, views, materialized views, and also code objects like functions, stored
    procedures, and packages. Certain objects that are not supported in Amazon Redshift
    like sequences, triggers, and indexes will not be migrated.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据仓库迁移到 Amazon Redshift 将涉及模式对象和数据的迁移。源端的对象包括模式、表、视图、物化视图以及代码对象，如函数、存储过程和包。Amazon
    Redshift 不支持的某些对象，如序列、触发器和索引，将不会被迁移。
- en: While you can find hands-on assistance through a combination of [Amazon Redshift
    Partners](https://oreil.ly/GLUTP) as well as [AWS Professional Services](https://oreil.ly/xys8u),
    this section focuses on AWS native tools and services. These tools and services
    can be leveraged to migrate from numerous source data warehouse engines to Amazon
    Redshift, as covered in [Table 9-2](#sct_src_tgt).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以通过 [Amazon Redshift 合作伙伴](https://oreil.ly/GLUTP) 和 [AWS 专业服务](https://oreil.ly/xys8u)
    的组合找到实际帮助，但本节重点介绍 AWS 原生工具和服务。这些工具和服务可以用来从多种源数据仓库引擎迁移到 Amazon Redshift，如 [表 9-2](#sct_src_tgt)
    所述。
- en: AWS Schema Conversion Tool
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Schema Conversion Tool
- en: '[AWS SCT](https://oreil.ly/fluxt) is a desktop application that provides a
    project-based UI to automatically convert the database schema of your source database
    into a format compatible with your target AWS native database. It supports multiple
    types of source and target databases. Use the AWS SCT to convert your existing
    database schema from one database engine to another. AWS SCT supports several
    industry standards, including Federal Information Processing Standards (FIPS),
    for connections to an Amazon S3 bucket or another AWS resource. AWS SCT is also
    compliant with the Federal Risk and Authorization Management Program (FedRAMP).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[AWS SCT](https://oreil.ly/fluxt) 是一个桌面应用程序，提供基于项目的用户界面，自动将源数据库的数据库架构转换为与目标
    AWS 本地数据库兼容的格式。它支持多种类型的源和目标数据库。使用 AWS SCT 将现有数据库架构从一个数据库引擎转换为另一个。AWS SCT 支持多个行业标准，包括联邦信息处理标准
    (FIPS)，用于连接到 Amazon S3 存储桶或其他 AWS 资源。AWS SCT 还符合联邦风险和授权管理计划 (FedRAMP)。'
- en: AWS SCT supports the following data warehouse schema conversions, and [Table 9-2](#sct_src_tgt)
    provides the specific source privileges needed to be granted, details on how to
    establish a secure connection, any known limitations for that source, and also
    how to target Amazon Redshift–specific conversion settings and conversion optimization
    settings.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SCT 支持以下数据仓库架构转换，并提供了必须授予的特定源权限、建立安全连接的详细信息、该源的任何已知限制，以及如何定向 Amazon Redshift
    的转换设置和优化设置。
- en: Table 9-2\. SCT sources supported for Amazon Redshift target
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9-2\. SCT 源支持的 Amazon Redshift 目标
- en: '| Source data warehouse | Version | Setups required |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 源数据仓库 | 版本 | 所需设置 |'
- en: '| --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Amazon Redshift | Any | [Amazon Redshift as a source](https://oreil.ly/8Ul5v)
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Amazon Redshift | 任意 | [Amazon Redshift 作为源](https://oreil.ly/8Ul5v) |'
- en: '| Azure Synapse | Any | [Azure Synapse Analytics as a source](https://oreil.ly/IPnVK)
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Azure Synapse | 任意 | [Azure Synapse Analytics 作为源](https://oreil.ly/IPnVK)
    |'
- en: '| BigQuery | Any | [BigQuery as a source](https://oreil.ly/Pi_Z8) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| BigQuery | 任意 | [BigQuery 作为源](https://oreil.ly/Pi_Z8) |'
- en: '| Greenplum | 4.3 and 6.21 | [Greenplum as a source](https://oreil.ly/81HBB)
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Greenplum | 4.3 和 6.21 | [Greenplum 作为源](https://oreil.ly/81HBB) |'
- en: '| MS SQL Server | 2008 or later | [SQL Server Data Warehouse as a source](https://oreil.ly/RYHD5)
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| MS SQL Server | 2008 或更高版本 | [SQL Server 数据仓库作为源](https://oreil.ly/RYHD5)
    |'
- en: '| Netezza | 7.0.3 or later | [Netezza as a source](https://oreil.ly/nbWfU)
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Netezza | 7.0.3 或更高版本 | [Netezza 作为源](https://oreil.ly/nbWfU) |'
- en: '| Oracle | 10.1 or later | [Oracle Data Warehouse as a source](https://oreil.ly/TL7a2)
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Oracle | 10.1 或更高版本 | [Oracle 数据仓库作为源](https://oreil.ly/TL7a2) |'
- en: '| Snowflake | 3 or later | [Snowflake as a source](https://oreil.ly/FSfR9)
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Snowflake | 3 或更高版本 | [Snowflake 作为源](https://oreil.ly/FSfR9) |'
- en: '| Vertica | 7.2.2 or later | [Vertica as a source](https://oreil.ly/LcJUG)
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Vertica | 7.2.2 或更高版本 | [Vertica 作为源](https://oreil.ly/LcJUG) |'
- en: '| Teradata | 13 or later | [Teradata as a source](https://oreil.ly/_65vV) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Teradata | 13 或更高版本 | [Teradata 作为源](https://oreil.ly/_65vV) |'
- en: '[Table 9-2](#sct_src_tgt) is up-to-date at the time of writing, but refer to
    the [latest data warehouse sources](https://oreil.ly/wWjxS).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[Table 9-2](#sct_src_tgt) 在撰写时是最新的，但请参阅[最新的数据仓库源](https://oreil.ly/wWjxS)。'
- en: Additionally, AWS SCT also supports conversions of the following ETL processes
    to target AWS services; refer to [Table 9-3](#sct_etl).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，AWS SCT 还支持将以下 ETL 过程转换为目标 AWS 服务；参考 [Table 9-3](#sct_etl)。
- en: Table 9-3\. SCT-supported ETL conversions
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9-3\. SCT 支持的 ETL 转换
- en: '| Source | Target |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 源 | 目标 |'
- en: '| --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Microsoft SQL Server Integration Services (SSIS) ETL packages | AWS Glue
    or AWS Glue Studio |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Microsoft SQL Server Integration Services (SSIS) ETL 包 | AWS Glue 或 AWS Glue
    Studio |'
- en: '| Shell scripts with embedded commands from Teradata Basic Teradata Query (BTEQ)
    | Amazon Redshift RSQL |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 包含 Teradata Basic Teradata Query (BTEQ) 嵌入命令的 Shell 脚本 | Amazon Redshift
    RSQL |'
- en: '| Teradata BTEQ ETL scripts | AWS Glue or Amazon Redshift RSQL |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Teradata BTEQ ETL 脚本 | AWS Glue 或 Amazon Redshift RSQL |'
- en: '| Teradata FastExport job scripts | Amazon Redshift RSQL |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Teradata FastExport 作业脚本 | Amazon Redshift RSQL |'
- en: '| Teradata FastLoad job scripts | Amazon Redshift RSQL |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Teradata FastLoad 作业脚本 | Amazon Redshift RSQL |'
- en: '| Teradata MultiLoad job scripts | Amazon Redshift RSQL |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Teradata MultiLoad 作业脚本 | Amazon Redshift RSQL |'
- en: SCT overview
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SCT 概述
- en: AWS SCT performs most of the schema object conversions automatically. But because
    the source database engines can have many different features and capabilities,
    AWS SCT attempts to create an equivalent schema in Amazon Redshift wherever possible.
    AWS SCT allows you to provide source data warehouse statistics so that it can
    optimize how your data warehouse is converted. You can either collect statistics
    directly from the database or upload an existing statistics file.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SCT 自动执行大部分模式对象转换。但由于源数据库引擎可能具有多种不同的功能和能力，AWS SCT 尽可能在 Amazon Redshift 中创建等效模式。AWS
    SCT 允许您提供源数据仓库统计信息，以便优化数据仓库的转换方式。您可以直接从数据库收集统计信息，或者上传现有的统计文件。
- en: AWS SCT automatically assigns the distribution style and sort keys for Redshift
    tables based off the primary key and foreign keys from the source tables. Source
    tables with single-column primary keys are assigned the key distribution style,
    and the primary key column is set as the distribution key as well as the sort
    key. Source tables with multi-column primary keys are assigned the key distribution
    style with the first primary key column being set as the distribution key, and
    all the source primary key columns added as a composite sort key.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SCT 根据源表的主键和外键自动为 Redshift 表分配分发样式和排序键。具有单列主键的源表被分配为键分发样式，并将主键列设置为分发键以及排序键。具有多列主键的源表被分配为键分发样式，第一个主键列被设置为分发键，并将所有源主键列添加为复合排序键。
- en: SCT migration assessment report
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SCT 迁移评估报告
- en: AWS SCT provides an database [migration assessment report](https://oreil.ly/-1Vky)
    with a listing of database objects and their conversion complexity. This report
    includes an executive summary, license evaluation, cloud readiness (indicating
    any features in the source database not available on the target), and recommendations
    for conversion of server objects, backup suggestions, and linked server changes.
    And most importantly, this report includes estimates of the complexity of effort
    that it will take to write the equivalent code for your target DB instance that
    can’t be converted automatically.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SCT 提供了一个数据库 [迁移评估报告](https://oreil.ly/-1Vky)，其中列出了数据库对象及其转换复杂度。该报告包括执行摘要、许可评估、云就绪性（指示源数据库中目标不可用的任何特性）以及有关转换服务器对象、备份建议和链接服务器更改的建议。最重要的是，该报告包括为目标
    DB 实例编写等效代码所需的工作复杂度估计，这些代码无法自动转换。
- en: 'The report categorizes the estimated time to convert these schema items as:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 报告将这些模式项转换所需的估计时间分类如下：
- en: Simple
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 简单
- en: Actions that can be completed in less than two hours.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可在两小时以内完成的操作。
- en: Medium
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 中等
- en: Actions that are more complex and can be completed in two to six hours.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂且需要两至六小时完成的操作。
- en: Significant
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 显著
- en: Actions that are very complex and take more than six hours to complete.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 非常复杂且需要超过六小时完成的操作。
- en: Using AWS SCT, you can manage target Amazon Redshift sort keys and distribution
    keys, map data types and objects, and also create manual conversions. If certain
    objects cannot be automatically converted, then SCT provides a listing of possible
    actions for you to take manually. AWS SCT creates a local version of the converted
    schema for you to review. You can either update the source schema and try again,
    or perform a manual conversion. When you are ready, you can apply the converted
    schema to the Amazon Redshift target.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AWS SCT，您可以管理目标Amazon Redshift的排序键和分布键，映射数据类型和对象，并创建手动转换。 如果某些对象无法自动转换，则SCT会为您提供可能的手动操作列表。
    AWS SCT为您创建转换后模式的本地版本供您审核。 您可以更新源模式并重试，或执行手动转换。 当准备就绪时，您可以将转换后的模式应用于Amazon Redshift目标。
- en: SCT data extraction agents
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SCT数据提取代理
- en: In some migration scenarios, the source and target databases are very different
    from one another and require additional data transformation. AWS SCT is extensible,
    so you can address these scenarios using an agent. An agent is an external program
    that’s integrated with AWS SCT, but performs data transformation elsewhere (such
    as on an Amazon EC2 instance). In addition, an AWS SCT agent can interact with
    other AWS services on your behalf, such as creating and managing AWS Database
    Migration Service tasks for you. It also helps you in increasing parallelism of
    tasks to load into Redshift. Use AWS [SCT data extraction agents](https://oreil.ly/BkWre)
    to extract data from your on-premises data warehouse and migrate it to Amazon
    Redshift. The SCT agent extracts your data and uploads the data to either a Snowball
    Edge device or over your network directly to Amazon S3\. The Snowball Edge device
    is shipped to AWS, and once received, the data is unloaded to your designated
    S3\. For large-scale migrations, the Snowball Edge device, which we cover in [“AWS
    Snow Family”](#snow_family), is preferred, as it does not put overhead on your
    network. You can then use AWS SCT to copy the data to Amazon Redshift, using the
    SCT copy agent, as shown in [Figure 9-2](#sct_agents).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些迁移场景中，源数据库和目标数据库彼此非常不同，需要额外的数据转换。 AWS SCT是可扩展的，因此您可以使用代理来处理这些情况。 代理是与AWS
    SCT集成的外部程序，但在其他地方执行数据转换（例如在Amazon EC2实例上）。 另外，AWS SCT代理可以代表您与其他AWS服务交互，例如为您创建和管理AWS数据库迁移服务任务。
    它还帮助您增加加载到Redshift的任务并行性。 使用AWS [SCT数据提取代理](https://oreil.ly/BkWre) 从您的本地数据仓库中提取数据并迁移到Amazon
    Redshift。 SCT代理提取您的数据并将数据上传到Snowball Edge设备或通过网络直接上传到Amazon S3。 Snowball Edge设备被运送到AWS，一旦接收到，数据将卸载到您指定的S3上。
    对于大规模迁移，我们推荐使用Snowball Edge设备，它不会给您的网络带来额外负担，我们在[“AWS Snow Family”](#snow_family)中进行了介绍。
    然后，您可以使用AWS SCT将数据复制到Amazon Redshift，使用SCT复制代理，如[图9-2](#sct_agents)所示。
- en: '![AWS SCT Agents flow](assets/ardg_0902.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![AWS SCT代理流程](assets/ardg_0902.png)'
- en: Figure 9-2\. AWS SCT agents flow
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2. AWS SCT代理流程
- en: This configuration can be useful when your source database server supports up
    to 120 connections and your network has ample storage attached. This methodology
    is also useful when you have partitioned tables on the source data warehouse and
    you can extract huge datasets in parallel, especially for the initial load stage
    of your data warehouse migration.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的源数据库服务器支持最多120个连接，并且您的网络附加了足够的存储时，此配置非常有用。 当您在源数据仓库上有分区表并且可以并行提取大量数据集时，尤其是在数据仓库迁移的初始加载阶段，这种方法也非常有用。
- en: Install AWS SCT data extractor agents as close as possible to the data source
    to improve data migration performance and reliability. And to increase the speed
    of data migration, use several AWS SCT agents in parallel.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 将AWS SCT数据提取代理安装在尽可能靠近数据源的位置，以提高数据迁移性能和可靠性。 为了增加数据迁移速度，请并行使用多个AWS SCT代理。
- en: Alternatively, you can use AWS DMS, which we cover in [“Data Warehouse Migration
    Service”](#aws_dms), to migrate data to Amazon Redshift. The advantage of AWS
    DMS is the ability to execute ongoing replication (change data capture) tasks.
    You can also use an approach where you use a combination of AWS SCT and AWS DMS.
    Use AWS SCT for initial load and AWS DMS for the ongoing replication tasks.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用AWS DMS，我们在[“数据仓库迁移服务”](#aws_dms)中介绍，将数据迁移到Amazon Redshift。 AWS DMS的优点在于能够执行持续的复制（变更数据捕获）任务。
    您还可以使用AWS SCT和AWS DMS的组合方法。 使用AWS SCT进行初始加载，使用AWS DMS进行持续复制任务。
- en: Migrating BLOBs to Amazon Redshift
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将BLOB迁移到Amazon Redshift
- en: Amazon Redshift doesn’t support storing binary large objects (BLOBs). However,
    if you need to migrate one or more BLOBs to Amazon Redshift, AWS SCT can perform
    the migration. AWS SCT uses an Amazon S3 bucket to store the BLOBs and writes
    the URL of the objects in Amazon S3 to the table column of the target database.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift不支持存储二进制大对象（BLOB）。但是，如果您需要将一个或多个BLOB迁移到Amazon Redshift，AWS SCT可以执行迁移。AWS
    SCT使用Amazon S3存储桶来存储BLOB，并将对象在Amazon S3中的URL写入目标数据库的表列中。
- en: We recommend using AWS SCT for very large data warehouse migrations and AWS
    DMS for small to medium data warehouse migrations. AWS SCT agents migrate data
    faster than AWS DMS by 15% to 35%, due to data compression, support of parallel
    migration of table partitions, and different configuration settings.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议对非常大型数据仓库迁移使用AWS SCT，对小到中型数据仓库迁移使用AWS DMS。AWS SCT代理通过数据压缩、支持表分区的并行迁移以及不同的配置设置，比AWS
    DMS快15%到35%。
- en: Data Warehouse Migration Service
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据仓库迁移服务
- en: AWS DMS is a managed migration and replication service that helps move your
    database workloads to AWS. AWS DMS supports migration between 20-plus database
    engines and analytics engines. With AWS DMS, you can discover your source data
    stores, convert your source schemas, and migrate your data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: AWS DMS是一个托管的迁移和复制服务，帮助您将数据库工作负载迁移到AWS。AWS DMS支持20多种数据库引擎和分析引擎之间的迁移。通过AWS DMS，您可以发现源数据存储，转换源模式并迁移数据。
- en: Use the [DMS Fleet Advisor](https://oreil.ly/P3tmR) to discover your source
    data infrastructure. This service collects data from your on-premises database
    and analytic servers and builds an inventory of servers, databases, and schemas
    that you can migrate to the AWS Cloud.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[DMS Fleet Advisor](https://oreil.ly/P3tmR)来发现您的源数据基础设施。该服务从您的本地数据库和分析服务器收集数据，并构建一个服务器、数据库和模式的清单，您可以将其迁移到AWS云上。
- en: Use [DMS Schema Conversion](https://oreil.ly/wp-O5) to migrate from your source
    database engine to AWS database engines. This service automatically assesses and
    converts your source schemas to a new target engine. Alternatively, you can download
    the AWS SCT to your local machine to convert your source schemas, as described
    in [“AWS Schema Conversion Tool”](#aws_sct).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[DMS模式转换](https://oreil.ly/wp-O5)从源数据库引擎迁移到AWS数据库引擎。该服务自动评估并转换您的源模式到新的目标引擎。或者，您可以将AWS
    SCT下载到本地机器上，按照[“AWS模式转换工具”](#aws_sct)中描述的方式转换源模式。
- en: After you convert your source schemas and apply the converted code to your Amazon
    Redshift, you can use AWS DMS to migrate your data. You can perform one-time migrations
    or replicate ongoing changes to keep sources and targets in sync. Because AWS
    DMS is a part of the AWS Cloud, you get the cost efficiency, speed to market,
    security, and flexibility that AWS services offer.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换源模式并将转换后的代码应用到您的Amazon Redshift后，您可以使用AWS DMS迁移数据。您可以执行一次性迁移或复制持续变更以保持源和目标同步。由于AWS
    DMS是AWS云的一部分，您可以享受AWS服务提供的成本效益、市场速度、安全性和灵活性。
- en: '![AWS DMS replication process](assets/ardg_0903.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![AWS DMS复制过程](assets/ardg_0903.png)'
- en: Figure 9-3\. AWS DMS replication process
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3\. AWS DMS复制过程
- en: How AWS DMS works
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS DMS的工作原理
- en: AWS DMS is a server in the AWS Cloud that runs replication software (see [Figure 9-3](#dms_rplcn_prcs)).
    You create a source and target connection to tell AWS DMS where to extract from
    and where to load to. Then you schedule a task that runs on this server to move
    your data. AWS DMS creates the tables and associated primary keys if they don’t
    exist on the target. You can create the target tables yourself if you prefer,
    or you can use AWS SCT to create some or all of the target database objects.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: AWS DMS是AWS云中运行复制软件的服务器（见[图9-3](#dms_rplcn_prcs)）。您需要创建源和目标连接来告诉AWS DMS从哪里提取数据和加载到哪里。然后，您可以调度在此服务器上运行的任务来移动您的数据。AWS
    DMS会在目标上创建表和相关的主键（如果它们在目标上不存在）。如果您愿意，您可以自己创建目标表，或者您可以使用AWS SCT创建部分或全部目标数据库对象。
- en: AWS DMS supports initial load tasks as well as ongoing-replication or change-data-capture
    (CDC) tasks to migrate new data as it comes in to your source data warehouse.
    It is worth noting that larger data warehouse migrations can include many terabytes
    of data. Executing the replication process over your existing network can be cumbersome
    due to network bandwidth limits. AWS DMS can use AWS Snowball Edge, part of AWS
    Snow Family (covered in [“AWS Snow Family”](#snow_family)), and Amazon S3 to migrate
    large databases. While AWS DMS allows replication for source tables without a
    primary or unique key, the CDC latency might be high, resulting in an unacceptable
    level of performance. So, it is a best practice to always have a primary key defined
    for every source table.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: AWS DMS 支持初始加载任务以及持续复制或变更数据捕获（CDC）任务，以迁移新进入源数据仓库的数据。值得注意的是，较大的数据仓库迁移可能包括多个TB的数据。在现有网络上执行复制过程可能会因网络带宽限制而变得繁琐。AWS
    DMS 可使用 AWS Snow Family 的一部分 AWS Snowball Edge 和 Amazon S3 来迁移大型数据库。虽然 AWS DMS
    允许对没有主键或唯一键的源表进行复制，但 CDC 延迟可能较高，导致性能水平不可接受。因此，始终为每个源表定义主键是一种最佳实践。
- en: If there is no primary key defined on the source table and you do not want to
    alter the source, then you can use a [DMS transformation rule to define a surrogate
    primary key by concatenating multiple source columns](https://oreil.ly/CrVOg)
    and then telling DMS that it’s the primary key for the table. However, this approach
    requires enhanced logging on the source database where all columns of the table
    are captured in the logs even if only few columns actually changed.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果源表上未定义主键且您不希望更改源表，则可以使用 [DMS 转换规则来定义合成主键，通过连接多个源列](https://oreil.ly/CrVOg)，然后告诉
    DMS 它是该表的主键。但是，这种方法要求在源数据库上增强日志记录，即使只有少数列实际发生变化，也需捕获表的所有列。
- en: During ongoing replication, it is critical to identify the network bandwidth
    between your source database system and your AWS DMS replication instance. Make
    sure that the network doesn’t cause any bottlenecks during ongoing replication.
    It is also important to identify the rate of change and archive log generation
    per hour on your source database system. Doing this can help you understand the
    throughput that you might get during ongoing replication.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在持续复制期间，识别源数据库系统与 AWS DMS 复制实例之间的网络带宽至关重要。确保网络在持续复制过程中不会造成任何瓶颈。还要识别源数据库系统每小时的变更率和归档日志生成率。这样做可以帮助您了解持续复制过程中可能获得的吞吐量。
- en: AWS DMS uses a pay-as-you-go model. You pay for AWS DMS resources only while
    you use them, as opposed to traditional licensing models that have up-front purchase
    costs and ongoing maintenance charges. AWS DMS automatically manages the deployment,
    management, and monitoring of all hardware and software needed for your migration.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: AWS DMS 使用按使用量付费的模式。您只在使用 AWS DMS 资源时付费，而不是传统许可模型的前期购买成本和持续维护费用。AWS DMS 自动管理迁移所需的所有硬件和软件的部署、管理和监控。
- en: AWS DMS automatically manages all of the infrastructure that supports your migration
    server, including hardware and software, software patching, and error reporting.
    AWS DMS provides automatic failover, too; if your primary replication server fails
    for any reason, a backup replication server can take over with little or no interruption
    of service.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: AWS DMS 自动管理支持迁移服务器的所有基础设施，包括硬件和软件、软件打补丁和错误报告。AWS DMS 还提供自动故障转移功能；如果您的主复制服务器由于任何原因发生故障，备用复制服务器可以接管服务而几乎不会中断服务。
- en: Install the AWS SCT and the AWS DMS agent on separate machines. Make sure that
    the AWS DMS agent is installed on the same machine as the ODBC drivers and the
    AWS Snowball Edge client for efficient performance, as covered in [“AWS Snowball
    Edge Client”](#snowball_edge_client).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 将 AWS SCT 和 AWS DMS 代理安装在不同的机器上。确保 AWS DMS 代理安装在与 ODBC 驱动程序和 AWS Snowball Edge
    客户端相同的机器上，以实现高效性能，详细信息请参阅 [“AWS Snowball Edge 客户端”](#snowball_edge_client)。
- en: 'With AWS DMS you create one of three migration tasks:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AWS DMS 可创建三种迁移任务之一：
- en: Migrate existing data (full load only)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移现有数据（仅全量加载）
- en: Perform a one-time migration from the source endpoint to the target endpoint.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 执行从源端点到目标端点的一次性迁移。
- en: Migrate existing data and replicate ongoing changes (full load and CDC)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移现有数据并复制持续变更（全量加载和CDC）
- en: Perform a one-time migration from the source to the target, and then continue
    replicating data changes from the source to the target.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 执行一次性从源到目标的迁移，然后继续从源到目标复制数据更改。
- en: Replicate data changes only (CDC only)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 仅复制数据更改（仅CDC）
- en: Don’t perform a one-time migration, but continue to replicate data changes from
    the source to the target.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 不执行一次性迁移，而是继续从源到目标复制数据更改。
- en: 'Here are the steps AWS DMS takes to load data into the Amazon Redshift target:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是AWS DMS加载数据到Amazon Redshift目标的步骤：
- en: AWS DMS writes data from the source to CSV files on the replication server.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS DMS将数据从源写入CSV文件到复制服务器。
- en: AWS DMS uses the AWS SDK to upload the CSV files into an S3 bucket you specify
    from your account.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS DMS使用AWS SDK将CSV文件上传到您在您的帐户中指定的S3存储桶。
- en: AWS DMS then issues the `COPY` command in Amazon Redshift to copy data from
    the CSV files to the target Amazon Redshift table.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后AWS DMS在Amazon Redshift中发出`COPY`命令，将数据从CSV文件复制到目标Amazon Redshift表中。
- en: 'For ongoing replication, AWS DMS first loads data to a staging table and then
    runs the DML statements as follows:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于持续复制，AWS DMS首先将数据加载到临时表，然后按以下方式运行DML语句：
- en: With enhanced logging
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用增强日志记录
- en: Inserted source rows → Insert on target
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入的源行 → 插入到目标
- en: Deleted source rows → Delete on target
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除的源行 → 删除到目标
- en: Updated source rows → Delete and Insert to target
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新的源行 → 删除并插入到目标
- en: With partial logging
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用部分日志记录
- en: Inserted source rows → Insert on target
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入的源行 → 插入到目标
- en: Deleted source rows → Delete on target
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除的源行 → 删除到目标
- en: Updated source rows → Update on target
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新的源行 → 更新到目标
- en: If you use [enhanced VPC routing](https://oreil.ly/EuQXq) with your Amazon Redshift
    target, all `COPY` traffic between your Amazon Redshift cluster and your data
    repositories goes through your VPC. If enhanced VPC routing is not enabled, Amazon
    Redshift routes traffic through the internet, including traffic to other services
    within the AWS network. If the feature is not enabled, you do not have to configure
    the network path. However, if the feature is enabled, you must specifically create
    a network path between your cluster’s VPC and your data resources. You can configure
    either [VPC endpoints](https://oreil.ly/Bv2jc) or a [Network Address Translation
    (NAT) gateway](https://oreil.ly/b1Ztm) in your VPC.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Amazon Redshift目标中使用[增强VPC路由](https://oreil.ly/EuQXq)，则所有Amazon Redshift集群与数据存储库之间的`COPY`流量都通过您的VPC。如果未启用增强VPC路由，Amazon
    Redshift将通过Internet路由流量，包括AWS网络内其他服务的流量。如果未启用此功能，则无需配置网络路径。但是，如果启用了此功能，则必须明确创建集群VPC与数据资源之间的网络路径。您可以配置[VPC终端节点](https://oreil.ly/Bv2jc)或[VPC中的网络地址转换（NAT）网关](https://oreil.ly/b1Ztm)。
- en: You can also use AWS KMS keys to encrypt data pushed to Amazon S3 and then load
    to the Amazon Redshift target. You just need the appropriate IAM role with an
    AWS-managed policy, and KMS key ARN with a permissive key policy, to be specified
    in AWS DMS settings.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用AWS KMS密钥对推送到Amazon S3的数据进行加密，然后加载到Amazon Redshift目标。您只需具备适当的IAM角色与AWS托管策略，并指定带有宽容密钥策略的KMS密钥ARN，即可在AWS
    DMS设置中使用。
- en: AWS DMS also has resource quotas and constraints, listed at [Resource Quotas
    for AWS Database Migration Service](https://oreil.ly/baRxg).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: AWS DMS还具有列出的资源配额和约束，请参阅[AWS数据库迁移服务的资源配额](https://oreil.ly/baRxg)。
- en: Additionally, you can reference the [“Optimizing AWS Database Migration Service
    Performance with Amazon Redshift as Target”](https://oreil.ly/PaF-I) whitepaper.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以参考[“使用亚马逊Redshift优化AWS数据库迁移服务性能”](https://oreil.ly/PaF-I)白皮书。
- en: DMS replication instances
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DMS复制实例
- en: Right sizing the replication server is crucial for performance. Some of the
    smaller Amazon EC2 instance classes are sufficient for testing the service or
    for small migrations. But if your migration involves a large number of tables,
    or if you intend to run multiple concurrent replication tasks, consider using
    one of the larger EC2 instances with a fair amount of memory and CPU.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于性能而言，适当大小的复制服务器至关重要。一些较小的亚马逊EC2实例类别已足以用于测试服务或小规模迁移。但是，如果您的迁移涉及大量表，或者打算同时运行多个复制任务，请考虑使用具有相对较多内存和CPU的较大EC2实例之一。
- en: The C5 instance classes are designed to deliver the highest level of processor
    performance for computer-intensive workloads. AWS DMS can be CPU-intensive, especially
    when performing large-scale migrations to Amazon Redshift.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: C5实例类别专为处理计算密集型工作负载而设计。AWS DMS可能会消耗大量CPU资源，特别是在大规模迁移到Amazon Redshift时。
- en: The R5 instance classes are memory optimized for memory-intensive workloads.
    The ongoing replication tasks migrations or replications of high-throughput transaction
    systems using AWS DMS can consume large amounts of CPU and memory. The R5 instances
    are recommended as they include more memory per vCPU.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: R5 实例类别为内存优化，适用于内存密集型工作负载。使用 AWS DMS 进行高吞吐量事务系统的持续复制任务迁移或复制可能会消耗大量 CPU 和内存。建议使用
    R5 实例，因为每个 vCPU 包含更多内存。
- en: 'Since an AWS DMS server is a compute resource in the AWS Cloud, the performance
    of your AWS DMS migration tasks will depend on:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 AWS DMS 服务器是 AWS 云中的计算资源，因此您的 AWS DMS 迁移任务的性能取决于：
- en: Resource availability on the source
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源端资源可用性
- en: The available network throughput
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用网络吞吐量
- en: The resource capacity of the replication server
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制服务器的资源容量
- en: The ability of the target to ingest changes
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标端接收更改的能力
- en: The type and distribution of source data
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源数据类型和分布
- en: The number of objects to be migrated
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要迁移的对象数量
- en: You can improve performance of full-load and CDC tasks for an Amazon Redshift
    target endpoint by using multithreaded task settings. They enable you to specify
    the number of concurrent threads and the number of records to store in a buffer.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用多线程任务设置来提高 Amazon Redshift 目标端点的全量加载和 CDC 任务的性能。它们允许您指定并发线程数和要存储在缓冲区中的记录数。
- en: Refer to [best practices for AWS DMS](https://oreil.ly/6OXH_) migrations, which
    covers a broad range of recommendations.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 参考 [AWS DMS 最佳实践](https://oreil.ly/6OXH_)，其中涵盖了广泛的建议。
- en: DMS replication validation
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DMS 复制验证
- en: AWS DMS provides support for data validation to ensure that your data was migrated
    accurately from your source to Amazon Redshift. If enabled, validation begins
    immediately after a full load is performed for a table. Validation compares the
    incremental changes for the ongoing replication task as they occur. When validation
    is enabled for a replication-only task, then all pre-existing data in a table
    is validated before starting validation of new data.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: AWS DMS 支持数据验证，以确保您的数据从源到 Amazon Redshift 的准确迁移。如果启用验证，则在表的全量加载后立即开始验证。验证会比较正在进行的复制任务的增量变化。当为仅复制任务启用验证时，然后在开始新数据验证之前验证表中所有现有数据。
- en: During the data validation phase, AWS DMS compares each row in the source with
    its corresponding row at the target, verifies that the rows contain the same data,
    and reports any mismatches. To accomplish this, AWS DMS issues appropriate queries
    to retrieve the data.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据验证阶段，AWS DMS 将每行源数据与目标数据进行比较，验证它们是否包含相同的数据，并报告任何不匹配。为实现此目的，AWS DMS 发出适当的查询以检索数据。
- en: DMS validation queries will consume additional resources at the source and the
    target as well as additional network resources, so be sure to size the DMS instance
    accordingly.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: DMS 验证查询会在源和目标端消耗额外资源，以及额外的网络资源，因此请确保适当调整 DMS 实例的大小。
- en: Data validation requires additional time, beyond the amount required for the
    migration itself. The extra time required depends on how much data was migrated.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 数据验证需要额外的时间，超出迁移本身所需的时间。额外的时间取决于迁移了多少数据。
- en: Split up the data validation portion of a migration or replication task into
    a separate validation-only task. This allows you to control exactly when validation
    occurs and reduce the load on the main replication instance by having a separate
    DMS instance for the validation task. Also, having a separate validation-only
    task allows you to quickly ascertain how many rows don’t match at the point in
    time when you run this task.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 将迁移或复制任务的数据验证部分拆分为单独的仅验证任务。这样可以控制验证发生的确切时间，并通过为验证任务使用单独的 DMS 实例减少主复制实例的负载。同时，单独的验证任务允许您在运行该任务时快速确定有多少行不匹配。
- en: The primary key value from the source is used for tracking ongoing replication
    changes. Running `update` and `delete` operations on source tables that alter
    or drop the primary key value will need AWS DMS to run a full validation scan.
    This is an expensive and time-consuming task, unless it is used for small reference
    data source tables.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 源端主键值用于跟踪持续复制变更。在源表上运行更改或删除操作，这些操作会更改或删除主键值，将需要 AWS DMS 运行完整验证扫描。除非用于小型参考数据源表，否则这是一项昂贵且耗时的任务。
- en: '[Table 9-4](#sct_vs_dms) summarizes the AWS migration tools we have discussed
    so far.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 9-4](#sct_vs_dms) 总结了我们迄今讨论过的 AWS 迁移工具。'
- en: Table 9-4\. AWS DMS versus AWS SCT
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9-4\. AWS DMS与AWS SCT对比
- en: '| AWS Database Migration Service | AWS Schema Conversion Tool |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| AWS 数据库迁移服务 | AWS 模式转换工具 |'
- en: '| --- | --- |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Paid service (has a free tier) | Free download software |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 付费服务（有免费套餐） | 免费下载软件 |'
- en: '| Servers run on AWS Cloud | Installed on-prem machine |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 服务器在AWS云上运行 | 安装在本地机器上 |'
- en: '| Supports multiple Availability Zones for high availability | One machine
    at a time |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 支持多个可用区以实现高可用性 | 一次只能处理一个机器 |'
- en: '| Lesser than 10 TB migrations | More than 10 TB migrations |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 小于10 TB 迁移 | 大于10 TB 迁移 |'
- en: '| Either source or target database must be on AWS | Supports on-prem to on-prem
    conversions |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 源数据库或目标数据库必须在AWS上 | 支持本地到本地转换 |'
- en: '| Migrates data from source database tables to target | Converts schema from
    one database engine to another |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 从源数据库表迁移数据到目标 | 将架构从一个数据库引擎转换为另一个 |'
- en: '| Supports change data capture (CDC) | Main use for initial data load |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 支持变更数据捕获（CDC） | 主要用于初始数据加载 |'
- en: '| Can work directly with target database | Can work with AWS Snowball Edge
    |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 可直接与目标数据库工作 | 可与AWS Snowball Edge工作 |'
- en: '| Can read/write from/to encrypted databases | Limited encryption support,
    Amazon RDS or Amazon Aurora |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 能够读写加密数据库 | 有限的加密支持，Amazon RDS 或 Amazon Aurora |'
- en: There are additional data integration partner tools, such as Informatica, Matillion,
    SnapLogic, Talend, and BryteFlow Ingest, that can be considered, especially if
    you already have build expertise with them.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他数据集成合作伙伴工具，如Informatica、Matillion、SnapLogic、Talend和BryteFlow Ingest，特别是如果你已经对它们有所了解的话，可以考虑使用它们。
- en: AWS Snow Family
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Snow Family
- en: The [AWS Snow Family](https://aws.amazon.com/snow) of devices is used for moving
    data from your data center to AWS infrastructure without depending on your existing
    networks that might be in use for your day-to-day activities. [AWS Storage Gateway](https://aws.amazon.com/storagegateway)
    and [AWS Direct Connect](https://aws.amazon.com/directconnect) services are good
    choices when network bandwidth limitations do not exist. You use AWS Snow Family
    services for offline data transfer.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[AWS Snow Family](https://aws.amazon.com/snow) 设备用于将数据从您的数据中心移动到AWS基础设施，而无需依赖可能正在使用的现有网络，用于日常活动。当网络带宽限制不存在时，[AWS
    Storage Gateway](https://aws.amazon.com/storagegateway) 和 [AWS Direct Connect](https://aws.amazon.com/directconnect)
    服务是很好的选择。您可以使用AWS Snow Family服务进行脱机数据传输。'
- en: Let’s look at the estimated time to transfer 10 TB of data over different network
    links in [Table 9-5](#transfer_estimate_10tb). The time it takes is expressed
    in day/hour/min/sec format, and assumes that you are getting the entire bandwidth
    or rated speed. On a shared line, you typically get anywhere from 1/10 to 1/25
    of rated speeds.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在不同网络链接上传输10 TB 数据的预估时间（见[表 9-5](#transfer_estimate_10tb)）。所需时间以天/小时/分钟/秒的格式表示，并假设你能获得整个带宽或额定速度。在共享线路上，通常可以获得额定速度的1/10至1/25。
- en: Table 9-5\. Estimated time to transfer 10 TB
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9-5\. 传输10 TB 数据的预估时间
- en: '| Network type | Rated speed | Estimated time | Shared (1/10) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 网络类型 | 额定速度 | 预估时间 | 共享（1/10） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| T3/DS3 line | 45 Mbps | 20:13:49:38 | 205:18:16:18 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| T3/DS3 线路 | 45 Mbps | 20天13小时49分钟38秒 | 205天18小时16分钟18秒 |'
- en: '| Fast Ethernet | 100 Mbps | 9:06:13:20 | 92:14:13:20 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 快速以太网 | 100 Mbps | 9天06小时13分钟20秒 | 92天14小时13分钟20秒 |'
- en: '| T4/DS4 line | 275 Mbps | 3:08:48:29 | 33:16:04:51 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| T4/DS4 线路 | 275 Mbps | 3天08小时48分钟29秒 | 33天16小时04分钟51秒 |'
- en: '| Gigabit Ethernet | 1000 Mbps | 0:22:13:20 | 9:06:13:20 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 吉比特以太网 | 1000 Mbps | 0天22小时13分钟20秒 | 9天06小时13分钟20秒 |'
- en: '| 10 Gigabit Ethernet | 10 Gbps | 0:02:13:20 | 0:22:13:20 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 10 吉比特以太网 | 10 Gbps | 0天02小时13分钟20秒 | 0天22小时13分钟20秒 |'
- en: Take this into consideration when choosing between using your existing network
    for data transfer and AWS Snow devices.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择使用现有网络进行数据传输还是AWS Snow设备时，请考虑以下几点。
- en: AWS Snow Family key features
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS Snow Family关键特性
- en: Snow Family devices have computing resources to collect and process data at
    the edge. This provides ability for running transformations like preprocessing
    data on your site as you write data to the AWS Snow device.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Snow Family设备具有计算资源，可在边缘收集和处理数据。这提供了在写入数据到AWS Snow设备时，在您的站点上运行预处理数据等转换的能力。
- en: '[AWS OpsHub](https://oreil.ly/08UmM) is a complimentary GUI available to make
    it easy to set up and manage Snow devices and rapidly deploy edge computing workloads
    and migrate data to the cloud. Your on-premise applications can work with Snow
    Family devices as a Network File System (NFS) mount point. NFS v3 and v4.1 are
    supported, so you can easily use Snow devices with your existing on-premises servers
    and file-based applications.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[AWS OpsHub](https://oreil.ly/08UmM) 是一个免费的图形界面工具，可帮助您轻松设置和管理Snow设备，并快速部署边缘计算工作负载，并将数据迁移到云端。您的本地应用程序可以将Snow
    Family设备作为网络文件系统（NFS）挂载点使用。支持NFS v3和v4.1，因此您可以轻松地将Snow设备与现有的本地服务器和基于文件的应用程序配合使用。'
- en: Each Snow device uses an [E Ink](https://oreil.ly/AUIdX) shipping label for
    tamper-proof tracking and automatic label updates for return shipping using Amazon
    Simple Notification Service (SNS), text messages, and via the AWS Console.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Snow设备使用[E Ink](https://oreil.ly/AUIdX)航运标签进行防篡改跟踪，并通过Amazon简单通知服务（SNS）、短信和AWS控制台进行返回运输的自动标签更新。
- en: All data moved to AWS Snow Family devices is automatically encrypted with 256-bit
    encryption keys that are managed by the AWS Key Management Service (KMS). Encryption
    keys are never stored on the AWS Snow device, so your data stays secure during
    transit.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 所有迁移到AWS Snow Family设备的数据都会使用256位加密密钥自动加密，这些密钥由AWS密钥管理服务（KMS）管理。加密密钥永远不会存储在AWS
    Snow设备上，因此您的数据在传输过程中保持安全。
- en: AWS Snow devices feature a Trusted Platform Module (TPM) that provides a hardware
    root of trust. Each device is inspected after each use to ensure the integrity
    of the device and helps preserve the confidentiality of your data.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Snow设备具有可信平台模块（TPM），提供硬件信任根。每个设备在使用后都会进行检查，以确保设备的完整性，并帮助保护您数据的机密性。
- en: AWS Snow Family devices
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS Snow Family devices
- en: AWS Snowcone is the smallest member of the AWS Snow Family of edge computing
    and data transfer devices. It offers 22 TB of combined storage, 4 vCPUs, 4 GB
    memory, and weighs under 5 pounds. You can use Snowcone to collect, process, and
    move data to AWS, either offline by shipping the device or online with [AWS DataSync](https://aws.amazon.com/datasync).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Snowcone 是AWS Snow Family边缘计算和数据传输设备中最小的成员。它提供22 TB的存储空间，4个vCPU，4 GB内存，重量不到5磅。您可以使用Snowcone收集、处理并将数据移动到AWS，可以通过邮寄设备离线使用，也可以通过[AWS
    DataSync](https://aws.amazon.com/datasync)在线使用。
- en: 'AWS Snowball is a suitcase-sized, 50-pound, data migration and edge computing
    device that comes in two device options—a Compute Optimized device or Storage
    Optimized device:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Snowball 是一个手提箱大小的，重50磅的数据迁移和边缘计算设备，提供两种设备选项——计算优化设备或存储优化设备：
- en: Snowball Edge Storage Optimized devices
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Snowball Edge存储优化设备
- en: Provide 80 TB of hard disk drive (HDD) storage, 1 TB of SSD storage, 40 vCPU,
    and 80 GB memory for local processing. It is well suited for large local storage
    for large-scale data transfer.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为本地处理提供80 TB硬盘驱动器（HDD）存储空间，1 TB SSD存储空间，40 vCPU和80 GB内存。非常适合大规模数据传输的大容量本地存储。
- en: Snowball Edge Compute Optimized devices
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Snowball Edge计算优化设备
- en: Provide 28 TB of SSD storage, 104 vCPU, and 416 GB memory for local processing,
    and an optional GPU for use cases such as advanced machine learning and full-motion
    video analysis in disconnected environments.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为本地处理提供28 TB SSD存储空间，104 vCPU和416 GB内存，并提供可选的GPU，适用于断开连接环境中的高级机器学习和全运动视频分析等用例。
- en: AWS Snowmobile is an exabyte-scale data migration device used to move extremely
    large amounts of data to AWS. Migrate up to 100 PB in a 45-foot long ruggedized
    shipping container, pulled by a semi-trailer truck.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Snowmobile 是一个用于将大规模数据迁移到AWS的百亿字节级数据迁移设备。您可以在一个45英尺长的强化运输集装箱中迁移高达100 PB的数据，由半挂卡车牵引。
- en: When you’re using an AWS Snow device, the data migration process can be visualized
    as shown in [Figure 9-4](#snowball_mgrtn_flow).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用AWS Snow设备时，数据迁移过程可以如[图9-4](#snowball_mgrtn_flow)所示进行可视化。
- en: You use the AWS SCT to extract the data locally and move it to an AWS Snowball
    Edge device.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用AWS SCT在本地提取数据并将其移动到AWS Snowball Edge设备中。
- en: You ship the Edge device(s) back to AWS.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将边缘设备（们）运回AWS。
- en: After AWS receives your shipment, the Edge device automatically loads its data
    into an Amazon S3 bucket.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在AWS收到您的运输后，边缘设备会自动将其数据加载到Amazon S3存储桶中。
- en: AWS DMS takes the S3 files and applies the data to the target data store.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS DMS 将S3文件应用到目标数据存储中。
- en: '![AWS Snowball migration process](assets/ardg_0904.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![AWS Snowball迁移过程](assets/ardg_0904.png)'
- en: Figure 9-4\. AWS Snowball migration process
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4。AWS Snowball迁移过程
- en: In AWS DMS, you can specify a particular timestamp or system change number (SCN)
    to start the CDC. And based off that timestamp or SCN, the CDC files will be generated.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS DMS中，您可以指定特定的时间戳或系统更改号（SCN）来启动CDC。基于该时间戳或SCN，将生成CDC文件。
- en: You can’t use an AWS Snowcone device to migrate data with AWS DMS. You can use
    AWS Snowcone devices only with AWS SCT.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 您不能使用AWS DMS来使用AWS Snowcone设备迁移数据。您只能使用AWS SCT与AWS Snowcone设备一起使用。
- en: AWS Snowball Edge Client
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Snowball Edge 客户端
- en: The Snowball Edge Client is a standalone terminal application that you run on
    your local server to unlock the AWS Snow device and get credentials, logs, and
    status information. You can also cluster multiple AWS Snow devices to form a Snowball
    Edge cluster. You use this Snowball Edge Client for all setups including setting
    up networking, tags, starting and stopping services, and setting up the cluster.
    For a full listing on Snowball Edge client commands, including examples of use
    and sample outputs, refer to [Commands for the Snowball Edge Client](https://oreil.ly/5m33-).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Snowball Edge 客户端是一个独立的终端应用程序，您可以在本地服务器上运行它，以解锁AWS Snow设备并获取凭据、日志和状态信息。您还可以将多个AWS
    Snow设备集群成Snowball Edge集群。您可以使用此Snowball Edge客户端进行所有设置，包括设置网络、标签、启动和停止服务以及设置集群。有关Snowball
    Edge客户端命令的完整列表，包括使用示例和样本输出，请参阅[Snowball Edge 客户端命令](https://oreil.ly/5m33-)。
- en: Database Migration Process
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据库迁移过程
- en: At a high level, the migration process comprises three steps. The two-step migration
    strategy and iterative migration strategy involve all three migration steps. However,
    note that the iterative migration strategy runs over a number of iterations, and
    each iteration has to go through these migration process steps every iteration.
    Since only databases that don’t require continuous operations are good fits for
    one-step migration, only Steps 1 and 2 from the migration process outlined in
    the following sections are required for the one-step migration strategy.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，迁移过程包括三个步骤。两步迁移策略和迭代迁移策略涵盖了所有三个迁移步骤。但请注意，迭代迁移策略会在多个迭代中运行，每个迭代都必须按照这些迁移过程步骤进行。由于只有不需要连续运行的数据库才适合单步迁移，因此在以下各节中概述的迁移过程中只需要步骤1和步骤2，这是单步迁移策略所需的。
- en: 'Step 1: Convert Schema and Subject Area'
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：转换模式和主题区域
- en: In this step, you need to convert source data warehouse schema to make it compatible
    with Amazon Redshift (see [Figure 9-5](#schema_convert)). The complexity of this
    conversion needs to be assessed before undertaking the actual conversion. You
    leverage schema conversion tools such as AWS SCT, other tools from AWS partners,
    or any third-party providers that you already might have expertise in. Remember
    that in some situations, you may also be required to use custom code to conduct
    complex schema conversions as we explored in [“AWS Schema Conversion Tool”](#aws_sct).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，您需要转换源数据仓库的模式，使其与Amazon Redshift兼容（参见[Figure 9-5](#schema_convert)）。在进行实际转换之前，需要评估此转换的复杂性。您可以利用AWS
    SCT等模式转换工具，AWS合作伙伴的其他工具，或者您可能已经具有专业知识的第三方提供商的工具。请记住，在某些情况下，您可能还需要使用自定义代码来进行复杂的模式转换，正如我们在[“AWS模式转换工具”](#aws_sct)中所探讨的那样。
- en: '![AWS SCT schema conversion](assets/ardg_0905.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![AWS SCT模式转换](assets/ardg_0905.png)'
- en: Figure 9-5\. AWS SCT schema conversion
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5\. AWS SCT模式转换
- en: 'Step 2: Initial Data Extraction and Load'
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：初始数据提取和加载
- en: In this step, you complete the initial data extraction and load the source data
    into Amazon Redshift for the first time (see [Figure 9-6](#sct_initial_load)).
    You can either create AWS DMS load tasks or use AWS SCT data extractors to extract
    data from the source data warehouse, as covered in [“SCT data extraction agents”](#sct_data_extrct),
    and load data to Amazon S3 if your data size and data transfer requirements allow
    you to transfer data over the interconnected network. Alternatively, if there
    are limitations such as network capacity limit, you can load data to Snowball
    for transfer to Amazon S3\. When the data in the source data warehouse is available
    on Amazon S3, it is loaded to Amazon Redshift, as covered in [“AWS Snow Family”](#snow_family).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，您完成了初始数据提取，并首次将源数据加载到Amazon Redshift中（参见[Figure 9-6](#sct_initial_load)）。您可以创建AWS
    DMS加载任务或使用AWS SCT数据提取器从源数据仓库提取数据，如[“SCT数据提取代理”](#sct_data_extrct)所述，并在数据大小和数据传输要求允许在互联网络上传输数据时将数据加载到Amazon
    S3。或者，如果存在网络容量限制等限制，则可以将数据加载到Snowball以便转移到Amazon S3。当源数据仓库中的数据在Amazon S3上可用时，将其加载到Amazon
    Redshift，如[“AWS Snow Family”](#snow_family)中所述。
- en: '![AWS SCT initial load](assets/ardg_0906.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![AWS SCT 初始加载](assets/ardg_0906.png)'
- en: Figure 9-6\. AWS SCT initial load
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6. AWS SCT 初始加载
- en: 'Step 3: Incremental Load Through Data Capture'
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步：通过数据捕获进行增量加载
- en: This is often referred to as the CDC phase, incremental load phase, or ongoing
    replication phase. CDC is a process that captures changes made in a database and
    ensures that those changes are replicated to a destination such as a data warehouse.
    In this step, you use AWS DMS or AWS SCT, and sometimes even source data warehouse
    native tools to capture and load these incremental changes from sources to Amazon
    Redshift.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被称为CDC阶段、增量加载阶段或持续复制阶段。CDC是一个捕获数据库中更改的过程，并确保这些更改复制到数据仓库等目标。在这一步骤中，您使用AWS
    DMS或AWS SCT，有时甚至使用源数据仓库的本地工具来捕获和加载这些增量变更，从源到Amazon Redshift。
- en: '[Figure 9-7](#sct_incr_load) shows the AWS services that can be used for the
    different aspects of your data migration.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-7](#sct_incr_load)显示了可用于数据迁移不同方面的AWS服务。'
- en: '![AWS data migration services](assets/ardg_0907.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![AWS数据迁移服务](assets/ardg_0907.png)'
- en: Figure 9-7\. AWS data migration services
  id: totrans-257
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7. AWS数据迁移服务
- en: You should now have enough information to start developing a migration plan
    for your data warehouse. As in the previous sections, we took a deep dive into
    the AWS services that can help you migrate your data warehouse to Amazon Redshift,
    and the best practices of using these services to accelerate a successful delivery
    of your data warehouse migration project. [Table 9-6](#migration_summary) summarizes
    what we have covered so far.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您应该有足够的信息来开始为数据仓库制定迁移计划。与之前的部分一样，我们深入探讨了可以帮助您将数据仓库迁移到Amazon Redshift的AWS服务，并使用这些服务的最佳实践来加速数据仓库迁移项目的成功交付。[表9-6](#migration_summary)总结了我们到目前为止所涵盖的内容。
- en: Table 9-6\. Migration actions summary
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-6. 迁移操作摘要
- en: '| Assess | Migrate | Validate |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 评估 | 迁移 | 验证 |'
- en: '| --- | --- | --- |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Retire versus retain | Initial data migration | Schema validation |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 淘汰与保留 | 初始数据迁移 | 模式验证 |'
- en: '| Migration scope | Ongoing changes replication | Data validation tasks |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 迁移范围 | 持续变更复制 | 数据验证任务 |'
- en: '| Data availability | Iterative migration | Ongoing changes validation |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 数据可用性 | 迭代迁移 | 持续变更验证 |'
- en: '| ETL tools | Schema conversion | Business validation |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| ETL工具 | 模式转换 | 业务验证 |'
- en: '| Data movement options | Database migration service | Switchover |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 数据移动选项 | 数据库迁移服务 | 切换 |'
- en: '| Migration assessment report | AWS Snow devices |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 迁移评估报告 | AWS Snow设备 |  |'
- en: Amazon Redshift Migration Tools Considerations
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Amazon Redshift迁移工具考虑事项
- en: 'To improve and accelerate data warehouse migration to Amazon Redshift, consider
    the following tips and best practices:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善和加速数据仓库向Amazon Redshift的迁移，请考虑以下提示和最佳实践：
- en: Use AWS SCT to create a migration assessment report and scope migration effort.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS SCT创建迁移评估报告并确定迁移工作的范围。
- en: Automate migration with AWS SCT where possible. AWS SCT can automatically create
    the majority of DDL and SQL scripts.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可能的情况下使用AWS SCT自动化迁移。AWS SCT可以自动创建大部分DDL和SQL脚本。
- en: When automated schema conversion is not possible, use custom scripting for the
    code conversion.
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当无法进行自动化模式转换时，使用自定义脚本进行代码转换。
- en: Install AWS SCT data extractor agents as close as possible to the data source
    to improve data migration performance and reliability.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能将AWS SCT数据抽取代理程序安装在靠近数据源的位置，以提高数据迁移性能和可靠性。
- en: To improve data migration performance, properly size your Amazon EC2 instance
    and its equivalent virtual machines that the data extractor agents are installed
    on.
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了提高数据迁移性能，正确调整您的Amazon EC2实例及其等效的虚拟机，安装数据抽取代理程序。
- en: Configure multiple data extractor agents to run multiple tasks in parallel to
    improve data migration performance by maximizing the usage of the allocated network
    bandwidth.
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置多个数据抽取代理程序以并行运行多个任务，通过最大化分配的网络带宽使用来提高数据迁移性能。
- en: Adjust AWS SCT memory configuration to improve schema conversion performance.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整AWS SCT内存配置以提高模式转换性能。
- en: 'Edit the `JavaOptions` section to set the `minimum` and `maximum` memory available.
    The following example sets the minimum to `4 GB` and the maximum to `40 GB`:'
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编辑`JavaOptions`部分以设置可用内存的最小和最大值。以下示例将最小值设置为`4 GB`，最大值设置为`40 GB`：
- en: '[PRE0]'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Use Amazon S3 to store large objects such as images, PDFs, and other binary
    data from your existing data warehouse.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Amazon S3存储大对象，例如来自现有数据仓库的图像、PDF和其他二进制数据。
- en: To migrate large tables, use virtual partitioning and create subtasks to improve
    data migration performance.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了迁移大表，使用虚拟分区并创建子任务以提高数据迁移性能。
- en: Understand the use cases of AWS services such as Direct Connect, the AWS Transfer
    Family, and the AWS Snow Family.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解AWS服务（例如Direct Connect、AWS Transfer Family和AWS Snow Family）的用例。
- en: Select the right service or tool to meet your data migration requirements.
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择合适的服务或工具以满足您的数据迁移需求。
- en: Understand AWS service quotas and make informed migration design decisions.
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解AWS服务配额并做出明智的迁移设计决策。
- en: Accelerate Your Migration to Amazon Redshift
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速您的迁移到亚马逊Redshift
- en: There are several new capabilities to automate your schema conversion, preserve
    your investment in existing scripts, reports and applications, accelerate query
    performance, and reduce your overall cost to migrate to Amazon Redshift.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个新功能可自动化您的架构转换，保留您对现有脚本、报告和应用程序的投资，加快查询性能，并降低迁移到亚马逊Redshift的总体成本。
- en: 'AWS SCT converts proprietary SQL statements including:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SCT转换专有SQL语句，包括：
- en: '`TO_DATE()` function'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TO_DATE()`函数'
- en: '`CURSOR` result sets'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CURSOR`结果集'
- en: '`IDENTITY` columns'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IDENTITY`列'
- en: '`ANY` and `SOME` filters with inequality predicates'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有不等谓词的`ANY`和`SOME`过滤器
- en: Analytic functions with `RESET WHEN`
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有`RESET WHEN`的分析函数
- en: '`TD_NORMALIZE_OVERLAP()` function'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TD_NORMALIZE_OVERLAP()`函数'
- en: '`TD_UNPIVOT()` function'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TD_UNPIVOT()`函数'
- en: '`QUANTILE` function'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QUANTILE`函数'
- en: '`QUALIFY` filter'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QUALIFY`过滤器'
- en: Refer to this [automation for proprietary SQL statements blog](https://oreil.ly/ettSl)
    for further details.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅这篇[自动化专有SQL语句的博客](https://oreil.ly/ettSl)获取更多详细信息。
- en: 'In the following sections, we discuss some of the commonly faced migration
    challenges, including:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们讨论了一些常见的迁移挑战，包括：
- en: Macro conversion
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 宏转换
- en: Macros are a proprietary SQL extension. Essentially, macros are SQL statements
    that accept parameters and can be called from multiple entry points in your application
    code. You can think of macros as a stored procedure that does not return any output
    values.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 宏是一种专有的SQL扩展。本质上，宏是SQL语句，可以接受参数，并可以从应用程序代码的多个入口点调用。您可以将宏视为不返回任何输出值的存储过程。
- en: Case-insensitive string comparison
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 大小写不敏感的字符串比较
- en: ANSI-compliant string comparison is case-sensitive, and Amazon Redshift being
    ANSI-compliant, an uppercase “A” is different from a lowercase “a”. Some databases
    support case-insensitive string comparison. Here, “A” = “a” is `TRUE`, as if both
    operands are converted to lowercase (or uppercase) for the purposes of the comparison.
    For example, in a Teradata database, case-insensitive collation is the default
    semantics for sessions running in `BEGIN TRANSACTION` and `END TRANSACTION` semantics
    (BTET mode), which is the default session mode for the Teradata engine.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ANSI兼容的字符串比较是区分大小写的，而且亚马逊Redshift是符合ANSI标准的，大写的“A”和小写的“a”是不同的。一些数据库支持大小写不敏感的字符串比较。在这里，“A”
    = “a”为`TRUE`，就好像两个操作数都转换为小写（或大写）进行比较。例如，在Teradata数据库中，大小写不敏感的排序是运行在`BEGIN TRANSACTION`和`END
    TRANSACTION`语义（BTET模式）的默认会话模式，这是Teradata引擎的默认会话模式。
- en: Recursive common table expressions
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 递归通用表达式
- en: Common table expressions (CTEs) are a convenient way to encapsulate query logic
    in large, complex SQL statements. CTEs are defined using the `WITH` clause, and
    the main query uses the CTE by referencing it in a `FROM` clause.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 通用表达式（CTEs）是在大型复杂的SQL语句中封装查询逻辑的便捷方式。使用`WITH`子句定义CTEs，主查询通过在`FROM`子句中引用它来使用CTEs。
- en: Proprietary data types
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 专有数据类型
- en: Amazon Redshift does not natively support `INTERVAL`, `PERIOD`, or `BLOB` data
    types as of this writing. AWS SCT includes automation for `INTERVAL` and `PERIOD`
    data types, automatic type casting, binary data support, and several other data
    type enhancements.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文撰写时，亚马逊Redshift不原生支持`INTERVAL`、`PERIOD`或`BLOB`数据类型。AWS SCT包括`INTERVAL`和`PERIOD`数据类型的自动化、自动类型转换、二进制数据支持以及其他多个数据类型增强功能。
- en: Macro Conversion
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 宏转换
- en: Although Amazon Redshift doesn’t natively support macros, AWS SCT can automate
    this conversion for you. AWS SCT will convert a macro into an Amazon Redshift
    stored procedure.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管亚马逊Redshift本身不支持宏，AWS SCT可以为您自动执行此转换。AWS SCT将宏转换为亚马逊Redshift存储过程。
- en: '[Example 9-1](#macro_example) illustrates a macro that gives an employee a
    raise.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 9-1](#macro_example)展示了一个为员工加薪的宏示例。'
- en: Example 9-1\. Example of a Macro
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-1。宏的示例
- en: '[PRE1]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This macro will be converted to the stored procedure shown in [Example 9-2](#macro_stored_procedure)
    and executed in Amazon Redshift data warehouse.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 此宏将转换为显示在[示例 9-2](#macro_stored_procedure)中的存储过程，并在Amazon Redshift数据仓库中执行。
- en: Example 9-2\. Macro converted to stored procedure
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-2\. 宏转换为存储过程
- en: '[PRE2]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: AWS SCT will also convert any corresponding macro invocations into calls to
    the corresponding stored procedure, to minimize manual conversions.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SCT还将任何相应的宏调用转换为对应存储过程的调用，以减少手动转换。
- en: Case-Insensitive String Comparison
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不区分大小写的字符串比较
- en: Case-sensitive comparison is the default semantics in Amazon Redshift. Amazon
    Redshift uses the normal ANSI-compliant semantics by default. Amazon Redshift
    now performs case-insensitive comparison natively as a feature of the database
    engine. With this new feature, you can enable case-insensitive collation when
    you define a new database, a new column, or use a column in an expression, as
    shown in [Example 9-3](#case_insensitive).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在Amazon Redshift中，区分大小写的比较是默认的语义。Amazon Redshift默认使用正常的ANSI兼容语义。Amazon Redshift现在作为数据库引擎的特性本地执行不区分大小写的比较。通过这一新特性，您可以在定义新数据库、新列或在表达式中使用列时启用不区分大小写的排序规则，如[示例 9-3](#case_insensitive)所示。
- en: Example 9-3\. Case insensitive sample
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-3\. 不区分大小写的样例
- en: '[PRE3]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let’s compare the default column and the explicitly declared case-insensitive
    column with the query in [Example 9-4](#case_insensitive_test). Because the database
    default for `new_db` is `C⁠A⁠S⁠E⁠_​I⁠N⁠S⁠E⁠N⁠S⁠I⁠T⁠I⁠V⁠E`, the comparison is case-insensitive
    and the strings match.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较默认列和显式声明的不区分大小写列与[示例 9-4](#case_insensitive_test)中的查询。因为 `new_db` 的数据库默认值是
    `C⁠A⁠S⁠E⁠_​I⁠N⁠S⁠E⁠N⁠S⁠I⁠T⁠I⁠V⁠E`，所以比较是不区分大小写的，字符串匹配。
- en: Example 9-4\. Case insensitive test
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-4\. 不区分大小写的测试
- en: '[PRE4]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Similarly, you can override the case sensitivity of a column. In [Example 9-5](#case_insensitive_test2),
    we override the case-sensitive column to be `CASE_INSENSITIVE`, and observe that
    the comparison matches again.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，您可以覆盖列的大小写敏感性。在[示例 9-5](#case_insensitive_test2)中，我们将大小写敏感的列覆盖为 `CASE_INSENSITIVE`，并观察到比较再次匹配。
- en: Example 9-5\. Case insensitive `test2`
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-5\. 不区分大小写的 `test2`
- en: '[PRE5]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that Amazon Redshift won’t let you directly compare a `CASE_SENSITIVE`
    column to a `CASE_INSENSITIVE` column (as in [Example 9-6](#case_insensitive_test3)).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Amazon Redshift不会允许您直接比较 `CASE_SENSITIVE` 列与 `CASE_INSENSITIVE` 列（如[示例 9-6](#case_insensitive_test3)）。
- en: Example 9-6\. Case insensitive test3
  id: totrans-326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-6\. 不区分大小写的测试3
- en: '[PRE6]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To avoid this, make sure you explicitly override the collation of one or both
    operands appropriately. This is a best practice for your SQL code, and it will
    be easier to understand when collation is explicitly applied.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，请确保适当地显式覆盖一个或两个操作数的排序规则。这是SQL代码的最佳实践，当显式应用排序规则时，将更容易理解。
- en: Recursive Common Table Expressions
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 递归公共表达式
- en: Amazon Redshift supports recursive common table expressions (CTEs). A recursive
    CTE is useful in querying hierarchical data, such as organization charts that
    show reporting relationships between employees and managers. Also note that AWS
    SCT automatically converts queries with recursive CTEs. If you create a view with
    query `johns_org`, as shown in [Example 9-7](#rec_cte_ex), SCT will convert to
    the equivalent view in Amazon Redshift.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift支持递归公共表达式（CTE）。递归CTE在查询层次数据（例如显示员工与经理之间报告关系的组织结构图）时非常有用。还请注意，AWS
    SCT会自动转换具有递归CTE的查询。如果您创建了一个名为 `johns_org` 的视图，如[示例 9-7](#rec_cte_ex)所示，SCT将转换为Amazon
    Redshift中等效的视图。
- en: Example 9-7\. Recursive CTE example
  id: totrans-331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-7\. 递归CTE示例
- en: '[PRE7]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Proprietary Data Types
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 私有数据类型
- en: AWS SCT automatically converts `INTERVAL` data types for you. AWS SCT converts
    an `INTERVAL` column into a `CHARACTER VARYING` column in Amazon Redshift. Then
    AWS SCT converts your application code that uses the column to emulate the `INTERVAL`
    semantics.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SCT会自动为您转换 `INTERVAL` 数据类型。AWS SCT将 `INTERVAL` 列转换为Amazon Redshift中的 `CHARACTER
    VARYING` 列。然后AWS SCT将使用该列的应用程序代码转换为模拟 `INTERVAL` 语义。
- en: Consider the following table, which has a `MONTH` interval column. This table
    stores different types of *leaves of absences* and the allowable *duration* for
    each. Your application code uses the `leave_duration` column, as shown in [Example 9-8](#interval-sql).
    Here, the `INTERVAL MONTH` field is added to the current date to compute when
    a leave of absence ends as if it starts today.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下表格，其中有一个`MONTH`区间列。该表存储了不同类型的*缺席请假*以及每种请假类型的*允许持续时间*。您的应用程序代码使用`leave_duration`列，如[示例 9-8](#interval-sql)所示。在这里，`INTERVAL
    MONTH`字段被添加到当前日期上，以计算请假从今天开始时的结束日期。
- en: Example 9-8\. Interval example
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-8\. 区间示例
- en: '[PRE8]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When AWS SCT converts the table to Amazon Redshift, AWS SCT replaces `INTERVAL`
    with a `VARCHAR` data type, as shown in [Example 9-9](#converted_to_date) for
    column `leave_duration`. And now since the data is stored as `VARCHAR`, AWS SCT
    adds the proper type `CAST` into the Amazon Redshift code to interpret the string
    values as a `MONTH` interval. It then converts the view arithmetic logic using
    Amazon Redshift date function for `dateadd MONTH`.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 当AWS SCT将表转换为Amazon Redshift时，AWS SCT会将`INTERVAL`替换为`VARCHAR`数据类型，如[示例 9-9](#converted_to_date)中`leave_duration`列所示。现在由于数据存储为`VARCHAR`，AWS
    SCT会将适当的`CAST`类型添加到Amazon Redshift代码中，以将字符串值解释为`MONTH`区间。然后，它会转换视图的算术逻辑，使用Amazon
    Redshift日期函数进行`dateadd MONTH`。
- en: Example 9-9\. Interval converted to date
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-9\. 区间转换为日期
- en: '[PRE9]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: By using a `VARCHAR` data type for the `leave_duration` column, AWS SCT has
    reduced the chances of table conversion failure, thus increasing the probability
    of data migration success. If some manual rewrite is required, then it will most
    likely be the SQL code in the view.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`VARCHAR`数据类型来存储`leave_duration`列，AWS SCT减少了表转换失败的机会，从而提高了数据迁移成功的概率。如果需要进行一些手动重写，则很可能是视图中的SQL代码。
- en: AWS SCT automatically converts `PERIOD` data types into two `DATE` (or `TIME`
    or `TIMESTAMP`) columns as appropriate on Amazon Redshift. Then AWS SCT converts
    your application code that uses the column to emulate the source engine semantics
    (see [Example 9-10](#sql_period)).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SCT会自动将`PERIOD`数据类型转换为两个相应的`DATE`（或`TIME`或`TIMESTAMP`）列在Amazon Redshift上。然后AWS
    SCT会转换使用该列的应用程序代码，以模拟源引擎的语义（见[示例 9-10](#sql_period)）。
- en: Example 9-10\. Period example
  id: totrans-343
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-10\. 期间示例
- en: '[PRE10]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'AWS SCT converts the `PERIOD(TIMESTAMP)` column in the `CREATE SET TABLE` statement
    in [Example 9-10](#sql_period) into two `TIMESTAMP` columns, as shown in [Example 9-11](#period_timestamp)’s
    `CREATE TABLE IF NOT EXISTS` command. Then, it converts the application code for
    the view `` period_view_begin_end` `` to use the two new `TIMESTAMP` columns:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SCT将[示例 9-10](#sql_period)中`CREATE SET TABLE`语句中的`PERIOD(TIMESTAMP)`列转换为两个`TIMESTAMP`列，如[示例 9-11](#period_timestamp)的`CREATE
    TABLE IF NOT EXISTS`命令所示。然后，它会转换视图`` period_view_begin_end` ``的应用程序代码，以使用这两个新的`TIMESTAMP`列：
- en: Example 9-11\. Period converted to timestamp
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-11\. 期间转换为时间戳
- en: '[PRE11]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Summary
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: Data volume is growing faster than ever. Traditionally only a fraction of this
    invaluable asset is available for analysis. On-premises data warehouses have rigid
    architectures that don’t scale for modern big data analytics use cases. These
    traditional data warehouses are expensive to set up and operate, and require large
    up-front investments both in terms of software and hardware investments.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 数据量正在以前所未有的速度增长。传统上，这些宝贵的资产仅供分析的一小部分。本地数据仓库具有不适合现代大数据分析用例的严格架构。这些传统数据仓库在软件和硬件方面都需要大量的前期投资。
- en: We covered how Amazon Redshift can help you analyze all your data and achieve
    performance at any scale with low and predictable cost. To migrate your data warehouse
    to Amazon Redshift, you need to consider a range of factors, such as the total
    size of the data warehouse, data change rate, and data transformation complexity,
    before picking a suitable migration strategy and process to reduce the complexity
    and cost of your data warehouse migration project.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了Amazon Redshift如何帮助您分析所有数据，并在任何规模下实现低成本和可预测的性能。要将您的数据仓库迁移到Amazon Redshift，您需要考虑一系列因素，例如数据仓库的总大小、数据变更速率和数据转换复杂性，在选择适当的迁移策略和过程之前，以减少数据仓库迁移项目的复杂性和成本。
- en: With AWS services such AWS Schema Conversion Tool (SCT) and AWS Database Migration
    Service (DMS), and by adopting the tips and best practices of these services,
    you can automate migration tasks, scale migration, and accelerate the delivery
    of your data warehouse migration projects.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AWS 服务，如 AWS Schema Conversion Tool (SCT) 和 AWS Database Migration Service
    (DMS)，并采纳这些服务的建议和最佳实践，您可以自动化迁移任务，扩展迁移规模，并加快数据仓库迁移项目的交付。
- en: In the next chapter, we will cover the aspects of monitoring and administration
    for your Amazon Redshift data warehouse.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论监控和管理 Amazon Redshift 数据仓库的各个方面。
