- en: Chapter 1\. Introducing Stream Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章。引入流处理
- en: In 2011, Marc Andreessen famously said that “software is eating the world,”
    referring to the booming digital economy, at a time when many enterprises were
    facing the challenges of a digital transformation. Successful online businesses,
    using “online” and “mobile” operation modes, were taking over their traditional
    “brick-and-mortar” counterparts.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 2011年，马克·安德森（Marc Andreessen）著名地说过，“软件正在吞噬世界”，指的是在许多企业面临数字转型挑战的时代，数字经济蓬勃发展。成功的在线企业采用“在线”和“移动”运营模式，正在取代传统的“实体店”对手。
- en: 'For example, imagine the traditional experience of buying a new camera in a
    photography shop: we would visit the shop, browse around, maybe ask a few questions
    of the clerk, make up our mind, and finally buy a model that fulfilled our desires
    and expectations. After finishing our purchase, the shop would have registered
    a credit card transaction—or only a cash balance change in case of a cash payment—and
    the shop manager would that know they have one less inventory item of that particular
    camera model.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一下在摄影店购买新相机的传统体验：我们会去店里转转，可能向店员询问几个问题，最终决定购买一个满足我们需求和期望的型号。完成购买后，店铺会记录信用卡交易，或者在现金支付时只会记录现金余额的变化，店长会知道他们库存中的特定相机型号减少了一个。
- en: 'Now, let’s take that experience online: first, we begin searching the web.
    We visit a couple of online stores, leaving digital traces as we pass from one
    to another. Advertisements on websites suddenly begin showing us promotions for
    the camera we were looking at as well as for competing alternatives. We finally
    find an online shop offering us the best deal and purchase the camera. We create
    an account. Our personal data is registered and linked to the purchase. While
    we complete our purchase, we are offered additional options that are allegedly
    popular with other people who bought the same camera. Each of our digital interactions,
    like searching for keywords on the web, clicking some link, or spending time reading
    a particular page generates a series of events that are collected and transformed
    into business value, like personalized advertisement or upsale recommendations.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这种体验带到在线环境中：首先，我们开始在网上搜索。我们访问了几家在线商店，我们在一个又一个商店之间留下了数字足迹。网站上的广告突然开始向我们展示我们查看的相机及其竞争对手的促销活动。最终，我们找到了一家在线商店提供给我们最好的交易并购买了相机。我们创建了一个账户。我们的个人数据被注册并与购买相关联。在我们完成购买时，我们被提供了其他人购买相同相机时可能喜欢的额外选项。我们的每一次数字交互，比如在网上搜索关键词，点击某些链接或花时间阅读特定页面，都会生成一系列事件，这些事件被收集并转化为业务价值，比如个性化广告或升级推荐。
- en: Commenting on Andreessen’s quote, in 2015, Dries Buytaert said “no, actually,
    *data* is eating the world.” What he meant is that the disruptive companies of
    today are no longer disruptive because of their software, but because of the unique
    data they collect and their ability to transform that data into value.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 评论安德森的话，2015年，德里斯·布伊塔特（Dries Buytaert）说，“不，实际上，*数据*正在吞噬世界。”他的意思是，今天的颠覆性公司不再仅仅因为他们的软件而颠覆，而是因为他们收集的独特数据以及将这些数据转化为价值的能力。
- en: The adoption of stream-processing technologies is driven by the increasing need
    of businesses to improve the time required to react and adapt to changes in their
    operational environment. This way of processing data as it comes in provides a
    technical and strategical advantage. Examples of this ongoing adoption include
    sectors such as internet commerce, continuously running data pipelines created
    by businesses that interact with customers on a 24/7 basis, or credit card companies,
    analyzing transactions as they happen in order to detect and stop fraudulent activities
    as they happen.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理技术的采用是由企业日益增长的需求驱动的，以改善其在运营环境中对变化做出反应和适应的时间。这种随数据到来即时处理的方式提供了技术和战略上的优势。这种持续采用的示例包括互联网商务等行业，这些行业通过与客户的24/7互动创建了持续运行的数据管道，或者信用卡公司，分析交易在发生时以便检测和阻止欺诈活动。
- en: 'Another driver of stream processing is that our ability to generate data far
    surpasses our ability to make sense of it. We are constantly increasing the number
    of computing-capable devices in our personal and professional environments<m-dash>televisions,
    connected cars, smartphones, bike computers, smart watches, surveillance cameras,
    thermostats, and so on. We are surrounding ourselves with devices meant to produce
    event logs: streams of messages representing the actions and incidents that form
    part of the history of the device in its context. As we interconnect those devices
    more and more, we create an ability for us to access and therefore analyze those
    event logs. This phenomenon opens the door to an incredible burst of creativity
    and innovation in the domain of near real-time data analytics, on the condition
    that we find a way to make this analysis tractable. In this world of aggregated
    event logs, stream processing offers the most resource-friendly way to facilitate
    the analysis of streams of data.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个推动流处理的因素是，我们生成数据的能力远远超过我们理解数据的能力。我们不断增加个人和专业环境中计算能力设备的数量<破折号>电视机、连接汽车、智能手机、自行车电脑、智能手表、监控摄像机、恒温器等等。我们正在用设备围绕自己产生事件日志：代表设备历史上的行动和事件流的消息流。随着我们将这些设备越来越多地互联起来，我们创造了一种能够访问和因此分析这些事件日志的能力。这种现象为我们在近乎实时数据分析领域开启了令人难以置信的创意和创新的大爆发，前提是我们找到一种使分析成为可能的方法。在这个聚合事件日志的世界中，流处理提供了最节约资源的方式来促进数据流的分析。
- en: It is not a surprise that not only is data eating the world, but so is *streaming
    data*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪的是，数据正在吞噬这个世界，*流数据*也是如此。
- en: 'In this chapter, we start our journey in stream processing using Apache Spark.
    To prepare us to discuss the capabilities of Spark in the stream-processing area,
    we need to establish a common understanding of what stream processing is, its
    applications, and its challenges. After we build that common language, we introduce
    Apache Spark as a generic data-processing framework able to handle the requirements
    of batch and streaming workloads using a unified model. Finally, we zoom in on
    the streaming capabilities of Spark, where we present the two available APIs:
    Spark Streaming and Structured Streaming. We briefly discuss their salient characteristics
    to provide a sneak peek into what you will discover in the rest of this book.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Apache Spark开始我们的流处理之旅。为了让我们能够讨论Spark在流处理领域的能力，我们需要建立对流处理的共同理解，其应用及其挑战。在我们建立这种共同语言之后，我们将Apache
    Spark作为一个通用的数据处理框架引入，能够使用统一模型处理批处理和流处理工作负载的要求。最后，我们将重点介绍Spark的流处理能力，其中我们介绍了两个可用的API：Spark
    Streaming和Structured Streaming。我们简要讨论它们的显著特点，以提供一个关于本书其余部分内容的预览。
- en: What Is Stream Processing?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是流处理？
- en: Stream processing is the discipline and related set of techniques used to extract
    information from *unbounded data*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理是一种从*无界数据*中提取信息的学科和相关技术集合。
- en: 'In his book *Streaming Systems*, Tyler Akidau defines unbounded data as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的书籍*流式系统*中，Tyler Akidau如下定义了无界数据：
- en: A type of dataset that is infinite in size (at least theoretically).
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一种数据集类型，大小是无限的（至少在理论上是这样）。
- en: Given that our information systems are built on hardware with finite resources
    such as memory and storage capacity, they cannot possibly hold unbounded datasets.
    Instead, we observe the data as it is received at the processing system in the
    form of a flow of events over time. We call this a *stream* of data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们的信息系统建立在诸如内存和存储容量等有限资源的硬件上，它们不可能容纳无界数据集。相反，我们观察数据在处理系统中的接收形式，即随时间流逝事件的流。我们称之为数据*流*。
- en: In contrast, we consider *bounded data* as a dataset of known size. We can count
    the number of elements in a bounded dataset.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们将*有界数据*定义为已知大小的数据集。我们可以计算有界数据集中元素的数量。
- en: Batch Versus Stream Processing
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理与流处理的比较
- en: How do we process both types of datasets? With *batch processing*, we refer
    to the computational analysis of bounded datasets. In practical terms, this means
    that those datasets are available and retrievable as a whole from some form of
    storage. We know the size of the dataset at the start of the computational process,
    and the duration of that process is limited in time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何处理这两种类型的数据集？通过*批处理*，我们指的是对有界数据集进行的计算分析。在实际应用中，这意味着可以从某种形式的存储中作为整体获取和检索这些数据集。我们在计算过程开始时知道数据集的大小，并且该过程的持续时间是有限的。
- en: In contrast, with *stream processing* we are concerned with the processing of
    data as it arrives to the system. Given the unbounded nature of data streams,
    the stream processors need to run constantly for as long as the stream is delivering
    new data. That, as we learned, might be—theoretically—forever.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，*流处理*关注数据到达系统时的处理。考虑到数据流的无界特性，流处理器需要持续运行，直至数据流不再提供新数据。理论上，这可能是永久的。
- en: Stream-processing systems apply programming and operational techniques to make
    possible the processing of potentially infinite data streams with a limited amount
    of computing resources.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理系统应用编程和操作技术，以利用有限的计算资源处理潜在无限的数据流。
- en: The Notion of Time in Stream Processing
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流处理中的时间概念
- en: 'Data can be encountered in two forms:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以以两种形式出现：
- en: At rest, in the form of a file, the contents of a database, or some other kind
    of record
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态状态，以文件形式、数据库内容或其他某种记录形式存在
- en: In motion, as continuously generated sequences of signals, like the measurement
    of a sensor or GPS signals from moving vehicles
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运动中，作为连续生成的信号序列，如传感器的测量或移动车辆的GPS信号
- en: We discussed already that a stream-processing program is a program that assumes
    its input is potentially infinite in size. More specifically, a stream-processing
    program assumes that its input is a sequence of signals of indefinite length,
    *observed over time*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过，流处理程序是一种假设其输入可能无限大的程序。更具体地说，流处理程序假设其输入是一系列信号，长度不确定，*随时间观察*。
- en: 'From the point of view of a timeline, *data at rest* is data from the past:
    arguably, all bounded datasets, whether stored in files or contained in databases,
    were initially a stream of data collected over time into some storage. The user’s
    database, all the orders from the last quarter, the GPS coordinates of taxi trips
    in a city, and so on all started as individual events collected in a repository.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从时间线的角度来看，*静态数据*是过去的数据：可以说，所有有界数据集，无论是存储在文件中还是包含在数据库中，最初都是随时间收集到某种存储中的数据流。用户数据库，上个季度所有订单，城市中出租车行程的GPS坐标等，都始于存储库中收集的各个事件。
- en: Trying to reason about *data in motion* is more challenging. There is a time
    difference between the moment data is originally generated and when it becomes
    available for processing. That time delta might be very short, like web log events
    generated and processed within the same datacenter, or much longer, like GPS data
    of a car traveling through a tunnel that is dispatched only when the vehicle reestablishes
    its wireless connectivity after it leaves the tunnel.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试推理*数据在运动*中更具挑战性。原始生成数据和其可用于处理之间存在时间差。该时间差可能非常短，如在同一数据中心生成和处理的网络日志事件，或者更长，如汽车通过隧道时的GPS数据，该数据仅在车辆离开隧道后重新建立无线连接时才可用于处理。
- en: 'We can observe that there’s a timeline when the events were produced and another
    for when the events are handled by the stream-processing system. These timelines
    are so significant that we give them specific names:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，事件生成和流处理系统处理时存在不同的时间线。这些时间线如此重要，以至于我们给它们特定的名称：
- en: Event time
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 事件时间
- en: The time when the event was created. The time information is provided by the
    local clock of the device generating the event.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 事件创建的时间。时间信息由生成事件的设备的本地时钟提供。
- en: Processing time
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 处理时间
- en: The time when the event is handled by the stream-processing system. This is
    the clock of the server running the processing logic. It’s usually relevant for
    technical reasons like computing the processing lag or as criteria to determine
    duplicated output.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 事件由流处理系统处理的时间。这是运行处理逻辑的服务器的时钟。通常与计算处理延迟或作为确定重复输出的标准相关。
- en: The differentiation among these timelines becomes very important when we need
    to correlate, order, or aggregate the events with respect to one another.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要相互关联、排序或聚合事件时，这些时间线之间的区别变得非常重要。
- en: The Factor of Uncertainty
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不确定因素
- en: In a timeline, data at rest relates to the past, and data in motion can be seen
    as the present. But what about the future? One of the most subtle aspects of this
    discussion is that it makes no assumptions on the throughput at which the system
    receives events.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间线上，静态数据关联于过去，而运动数据可以视为现在。但未来呢？这次讨论中最微妙的一点是，它对系统接收事件的吞吐量没有任何假设。
- en: 'In general, streaming systems do not require the input to be produced at regular
    intervals, all at once, or following a certain rhythm. This means that, because
    computation usually has a cost, it’s a challenge to predict peak load: matching
    the sudden arrival of input elements with the computing resources necessary to
    process them.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，流处理系统不要求输入定期产生、一次性产生或遵循特定的节奏。这意味着，由于计算通常有成本，预测峰值负载是一个挑战：将突然到达的输入元素与处理它们所需的计算资源匹配起来。
- en: If we have the computing capacity needed to match a sudden influx of input elements,
    our system will produce results as expected, but if we have not planned for such
    a burst of input data, some streaming systems might face delays, resource constriction,
    of failure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有足够的计算能力来应对突如其来的输入元素增加，我们的系统将如期产生结果，但如果我们没有为这种输入数据的突发增加做好计划，一些流处理系统可能会面临延迟、资源限制或故障。
- en: Dealing with uncertainty is an important aspect of stream processing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不确定性是流处理的重要方面。
- en: In summary, stream processing lets us extract information from infinite data
    streams observed as events delivered over time. Nevertheless, as we receive and
    process data, we need to deal with the additional complexity of event-time and
    the uncertainty introduced by an unbounded input.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，流处理让我们从随时间交付的事件中提取信息。然而，当我们接收和处理数据时，我们需要处理事件时间的复杂性以及无界输入带来的不确定性。
- en: Why would we want to deal with the additional trouble? In the next section,
    we glance over a number of use cases that illustrate the value added by stream
    processing and how it delivers on the promise of providing faster, actionable
    insights, and hence business value, on data streams.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要面对额外的麻烦？在接下来的章节中，我们将概述一些使用案例，这些案例说明了流处理所增加的价值，并展示了它如何在数据流上提供更快速、可操作的洞察力，从而带来商业价值。
- en: Some Examples of Stream Processing
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些流处理的例子
- en: 'The use of stream processing goes as wild as our capacity to imagine new real-time,
    innovative applications of data. The following use cases, in which the authors
    have been involved in one way or another, are only a small sample that we use
    to illustrate the wide spectrum of application of stream processing:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理的应用范围与我们想象新的实时创新应用数据的能力一样广泛。以下使用案例是我们以一种或另一种方式参与的一个小样本，用来说明流处理的广泛应用领域：
- en: Device monitoring
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 设备监控
- en: A small startup rolled out a cloud-based Internet of Things (IoT) device monitor
    able to collect, process, and store data from up to 10 million devices. Multiple
    stream processors were deployed to power different parts of the application, from
    real-time dashboard updates using in-memory stores, to continuous data aggregates,
    like unique counts and minimum/maximum measurements.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一家小型创业公司推出了一个基于云的物联网设备监视器，能够收集、处理和存储来自多达1000万台设备的数据。多个流处理器被部署用于支持应用程序的不同部分，从使用内存存储的实时仪表板更新，到连续数据聚合，如唯一计数和最小/最大测量。
- en: Fault detection
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 故障检测
- en: A large hardware manufacturer applies a complex stream-processing pipeline to
    receive device metrics. Using time-series analysis, potential failures are detected
    and corrective measures are automatically sent back to the device.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一家大型硬件制造商应用了复杂的流处理管道来接收设备指标。使用时间序列分析，可以检测潜在的故障，并自动发送纠正措施回到设备。
- en: Billing modernization
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 计费现代化
- en: A well-established insurance company moved its billing system to a streaming
    pipeline. Batch exports from its existing mainframe infrastructure are streamed
    through this system to meet the existing billing processes while allowing new
    real-time flows from insurance agents to be served by the same logic.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一家知名的保险公司将其计费系统迁移到了流处理管道。通过这个系统，现有的主机基础设施的批量导出数据被流式传输，以满足现有的计费流程，同时允许保险代理人的新实时流通过同样的逻辑进行服务。
- en: Fleet management
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 车队管理
- en: A fleet management company installed devices able to report real-time data from
    the managed vehicles, such as location, motor parameters, and fuel levels, allowing
    it to enforce rules like geographical limits and analyze driver behavior regarding
    speed limits.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一家车队管理公司安装了能够实时报告受管控车辆的数据的设备，例如位置、电机参数和燃油水平，使其能够执行诸如地理限制的规则，并分析驾驶员在速度限制方面的行为。
- en: Media recommendations
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 媒体推荐
- en: A national media company deployed a streaming pipeline to ingest new videos,
    such as news reports, into its recommendation system, making the videos available
    to its users’ personalized suggestions almost as soon as they are ingested into
    the company’s media repository. The company’s previous system would take hours
    to do the same.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一家国家级媒体公司部署了一个流式处理管道，将新视频（如新闻报道）引入其推荐系统，使得视频几乎在被加入公司媒体库后立即对用户进行个性化推荐。之前的系统需要数小时才能完成同样的任务。
- en: Faster loans
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 更快速的贷款
- en: A bank active in loan services was able to reduce loan approval from hours to
    seconds by combining several data streams into a streaming application.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一家从事贷款服务的银行通过将多个数据流合并到一个流式应用程序中，将贷款批准时间从数小时减少到数秒。
- en: 'A common thread among those use cases is the need of the business to process
    the data and create actionable insights in a short period of time from when the
    data was received. This time is relative to the use case: although *minutes* is
    a very fast turn-around for a loan approval, a milliseconds response is probably
    necessary to detect a device failure and issue a corrective action within a given
    service-level threshold.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些用例的共同特点是企业需要在收到数据后的短时间内处理数据并生成可操作的见解。这个时间相对于用例来说不同：虽然对于贷款批准来说*几分钟*已经非常快速，但是要在设备故障时检测并在给定的服务级别阈值内采取纠正措施，可能需要毫秒级响应。
- en: In all cases, we can argue that *data* is better when consumed as fresh as possible.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有情况下，我们可以认为*数据*尽可能新鲜时效果更好。
- en: Now that we have an understanding of what stream processing is and some examples
    of how it is being used today, it’s time to delve into the concepts that underpin
    its implementation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了流处理是什么，以及它如何被今天使用的一些示例所应用，是时候深入探讨支持其实现的概念了。
- en: Scaling Up Data Processing
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展数据处理
- en: Before we discuss the implications of distributed computation in stream processing,
    let’s take a quick tour through *MapReduce*, a computing model that laid the foundations
    for scalable and reliable data processing.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论分布式计算在流处理中的影响之前，让我们快速浏览一下*MapReduce*，这个为可扩展和可靠数据处理奠定基础的计算模型。
- en: MapReduce
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MapReduce
- en: The history of programming for distributed systems experienced a notable event
    in February 2003. Jeff Dean and Sanjay Gemawhat, after going through a couple
    of iterations of rewriting Google’s crawling and indexing systems, began noticing
    some operations that they could expose through a common interface. This led them
    to develop *MapReduce*, a system for distributed processing on large clusters
    at Google.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 编程分布式系统的历史在2003年2月经历了一个显著事件。杰夫·迪恩和桑杰·吉玛瓦特在多次重写谷歌的爬网和索引系统后，开始注意到一些可以通过一个共同接口公开的操作。这使他们开发出了*MapReduce*，一个在谷歌大型集群上进行分布式处理的系统。
- en: 'Part of the reason we didn’t develop MapReduce earlier was probably because
    when we were operating at a smaller scale, then our computations were using fewer
    machines, and therefore robustness wasn’t quite such a big deal: it was fine to
    periodically checkpoint some computations and just restart the whole computation
    from a checkpoint if a machine died. Once you reach a certain scale, though, that
    becomes fairly untenable since you’d always be restarting things and never make
    any forward progress.'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 之所以我们没有早些开发MapReduce的一部分原因可能是，当我们的规模较小时，我们的计算使用的机器较少，因此鲁棒性并不是一个很大的问题：周期性地检查一些计算并从检查点重新启动整个计算是可以接受的。但是一旦达到一定规模，这种方式就变得不可行了，因为你总是在重新启动事务而无法取得任何进展。
- en: ''
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jeff Dean, email to Bradford F. Lyon, August 2013
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 杰夫·迪恩，2013年8月，给布拉德福德·F·里昂的电子邮件
- en: MapReduce is a programming API first, and a set of components second, that make
    programming for a distributed system a relatively easier task than all of its
    predecessors.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce首先是一种编程API，其次是一组组件，使得编写分布式系统相对于其前身变得更加容易。
- en: 'Its core tenets are two functions:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 它的核心原则是两个函数：
- en: Map
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Map
- en: The map operation takes as an argument a function to be applied to every element
    of the collection. The collection’s elements are read in a distributed manner,
    through the distributed filesystem, one chunk per executor machine. Then, all
    of the elements of the collection that reside in the local chunk see the function
    applied to them, and the executor emits the result of that application, if any.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: map操作将一个要应用于集合每个元素的函数作为参数。通过分布式文件系统，集合的元素以分布方式读取，每个执行程序机器每次读取一个块。然后，所有驻留在本地块中的集合元素将该函数应用于它们，并且如果适用，执行程序将发出该应用的结果。
- en: Reduce
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 减少
- en: 'The reduce operation takes two arguments: one is a neutral element, which is
    what the *reduce* operation would return if passed an empty collection. The other
    is an aggregation operation, that takes the current value of an aggregate, a new
    element of the collection, and lumps them into a new aggregate.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 减少操作接受两个参数：一个是中性元素，即如果传递一个空集合给*reduce*操作，它将返回的值。另一个是聚合操作，它接受聚合的当前值、集合的新元素，并将它们合并成一个新的聚合。
- en: Combinations of these two higher-order functions are powerful enough to express
    every operation that we would want to do on a dataset.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个高阶函数的组合足以表达我们在数据集上想要执行的每个操作。
- en: 'The Lesson Learned: Scalability and Fault Tolerance'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从中汲取的教训：可扩展性和容错性
- en: 'From the programmer’s perspective, here are the main advantages of MapReduce:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从程序员的角度来看，MapReduce的主要优势包括：
- en: It has a simple API.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有一个简单的API。
- en: It offers very high expressivity.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了非常高的表现力。
- en: It significantly offloads the difficulty of distributing a program from the
    shoulders of the programmer to those of the library designer. In particular, resilience
    is built into the model.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它显著减轻了将程序分布化的困难，从程序员的肩膀上转移到库设计者的肩膀上。特别是，弹性已内建于该模型中。
- en: 'Although these characteristics make the model attractive, the main success
    of MapReduce is its ability to sustain growth. As data volumes increase and growing
    business requirements lead to more information-extraction jobs, the MapReduce
    model demonstrates two crucial properties:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些特性使得模型变得吸引人，但MapReduce的主要成功在于其能够支持增长。随着数据量的增加和不断增长的业务需求导致更多的信息提取作业，MapReduce模型展示了两个关键属性：
- en: Scalability
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性
- en: As datasets grow, it is possible to add more resources to the cluster of machines
    in order to preserve a stable processing performance.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据集的增长，可以向机器集群添加更多资源，以保持稳定的处理性能。
- en: Fault tolerance
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 容错
- en: The system can sustain and recover from partial failures. All data is replicated.
    If a data-carrying executor crashes, it is enough to relaunch the task that was
    running on the crashed executor. Because the master keeps track of that task,
    that does not pose any particular problem other than rescheduling.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 系统能够持续运行并从部分故障中恢复。所有数据都是复制的。如果一个携带数据的执行程序崩溃，只需重新启动在崩溃执行程序上运行的任务即可。因为主节点跟踪该任务，这不会带来任何特别的问题，除了重新调度。
- en: These two characteristics combined result in a system able to constantly sustain
    workloads in an environment fundamentally unreliable, *properties that we also
    require for stream processing*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个特征的结合使得系统能够在根本不可靠的环境中持续支持工作负载，*这些特性也是我们对流处理的要求*。
- en: Distributed Stream Processing
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式流处理
- en: One fundamental difference of stream processing with the MapReduce model, and
    with batch processing in general, is that although batch processing has access
    to the complete dataset, with streams, we see only a small portion of the dataset
    at any time.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与批处理一般情况下相比，使用MapReduce模型进行流处理的一个基本区别是，虽然批处理可以访问完整的数据集，但在流处理中，我们每次只能看到数据集的一小部分。
- en: This situation becomes aggravated in a distributed system; that is, in an effort
    to distribute the processing load among a series of executors, we further split
    up the input stream into partitions. Each executor gets to see only a partial
    view of the complete stream.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况在分布式系统中变得更加严重；也就是说，为了将处理负载分布到一系列执行程序中，我们进一步将输入流分割成分区。每个执行程序只能看到完整流的部分视图。
- en: The challenge for a distributed stream-processing framework is to provide an
    abstraction that hides this complexity from the user and lets us reason about
    the stream as a whole.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式流处理框架的挑战在于提供一个抽象，隐藏用户不需要关心的复杂性，并允许我们将流作为一个整体来推理。
- en: Stateful Stream Processing in a Distributed System
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式系统中的有状态流处理
- en: Let’s imagine that we are counting the votes during a presidential election.
    The classic batch approach would be to wait until all votes have been cast and
    then proceed to count them. Even though this approach produces a correct end result,
    it would make for very boring news over the day because no (intermediate) results
    are known until the end of the electoral process.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下，在总统选举期间我们正在统计选票。经典的批处理方法是等待所有选票都被投出，然后开始计数。尽管这种方法可以产生正确的最终结果，但因为在选举过程结束之前不知道（中间）结果，这将导致新闻过程非常乏味。
- en: A more exciting scenario is when we can count the votes per candidate as each
    vote is cast. At any moment, we have a partial count by participant that lets
    us see the current standing as well as the voting trend. We can probably anticipate
    a result.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 更令人兴奋的场景是，我们可以在每个选票投出时计算每位候选人的票数。在任何时刻，我们都可以通过参与者的部分计数看到当前的排名以及投票趋势。我们可以预测结果。
- en: To accomplish this scenario, the stream processor needs to keep an internal
    register of the votes seen so far. To ensure a consistent count, this register
    must recover from any partial failure. Indeed, we can’t ask the citizens to issue
    their vote again due to a technical failure.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这种情景，流处理器需要保持一个内部注册表，记录到目前为止看到的选票。为了保证计数的一致性，这个注册表必须能够从任何部分故障中恢复。确实，由于技术故障，我们不能要求公民们再次发出他们的选票。
- en: Also, any eventual failure recovery cannot affect the final result. We can’t
    risk declaring the wrong winning candidate as a side effect of an ill-recovered
    system.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，任何可能的故障恢复都不能影响最终结果。我们不能因为系统恢复不良的副作用而冒险宣布错误的获胜候选人。
- en: 'This scenario illustrates the challenges of stateful stream processing running
    in a distributed environment. Stateful processing poses additional burdens on
    the system:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个场景说明了在分布式环境中运行的有状态流处理的挑战。有状态处理对系统提出了额外的负担：
- en: We need to ensure that the state is preserved over time.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要确保状态随时间得以保留。
- en: We require data consistency guarantees, even in the event of partial system
    failures.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们要求数据在部分系统故障的情况下，仍能保证一致性。
- en: As you will see throughout the course of this book, addressing these concerns
    is an important aspect of stream processing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在本书中看到的，解决这些问题是流处理的重要方面。
- en: Now that we have a better sense of the drivers behind the popularity of stream
    processing and the challenging aspects of this discipline, we can introduce Apache
    Spark. As a unified data analytics engine, Spark offers data-processing capabilities
    for both batch and streaming, making it an excellent choice to satisfy the demands
    of the data-intensive applications, as we see next.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们更清楚了推动流处理流行以及这一学科挑战性因素的驱动力，我们可以介绍Apache Spark。作为统一的数据分析引擎，Spark 提供了对批处理和流处理的数据处理能力，使其成为满足数据密集型应用需求的极佳选择，接下来我们将详细讨论。
- en: Introducing Apache Spark
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Apache Spark
- en: Apache Spark is a fast, reliable, and fault-tolerant distributed computing framework
    for large-scale data processing.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个快速、可靠和容错的大规模数据处理分布式计算框架。
- en: 'The First Wave: Functional APIs'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一波：功能API
- en: In its early days, Spark’s breakthrough was driven by its novel use of memory
    and expressive functional API. The Spark memory model uses RAM to cache data as
    it is being processed, resulting in up to 100 times faster processing than Hadoop
    MapReduce, the open source implementation of Google’s MapReduce for batch workloads.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期，Spark的突破在于其对内存的新颖使用和表达功能API。Spark内存模型使用RAM在处理数据时缓存数据，处理速度比Hadoop MapReduce快100倍，后者是Google
    MapReduce的开源实现，用于批量工作负载。
- en: Its core abstraction, the *Resilient Distributed Dataset* (RDD), brought a rich
    functional programming model that abstracted out the complexities of distributed
    computing on a cluster. It introduced the concepts of *transformations* and *actions*
    that offered a more expressive programming model than the map and reduce stages
    that we discussed in the MapReduce overview. In that model, many available *transformations*
    like `map`, `flatmap`, `join`, and `filter` express the lazy conversion of the
    data from one internal representation to another, whereas eager operations called
    *actions* materialize the computation on the distributed system to produce a result.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其核心抽象——*弹性分布式数据集*（RDD），引入了一个丰富的函数式编程模型，将集群上的分布式计算复杂性抽象化。它引入了*转换*和*动作*的概念，提供了比我们在MapReduce概述中讨论的map和reduce阶段更具表达力的编程模型。在这个模型中，许多可用的*转换*，如`map`、`flatmap`、`join`和`filter`，表达了数据从一种内部表示到另一种的惰性转换，而急切操作称为*动作*，在分布式系统上材料化计算以生成结果。
- en: 'The Second Wave: SQL'
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二波：SQL
- en: The second game-changer in the history of the Spark project was the introduction
    of Spark SQL and *DataFrames* (and later, *Dataset*, a strongly typed DataFrame).
    From a high-level perspective, Spark SQL adds SQL support to any dataset that
    has a schema. It makes it possible to query a comma-separated values (CSV), Parquet,
    or JSON dataset in the same way that we used to query a SQL database.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Spark项目历史上的第二个变革者是引入Spark SQL和*数据框*（后来是*数据集*，一个强类型的数据框）。从高层次来看，Spark SQL为具有模式的任何数据集添加了SQL支持。它使我们可以像查询SQL数据库一样查询逗号分隔值（CSV）、Parquet或JSON数据集。
- en: This evolution also lowered the threshold of adoption for users. Advanced distributed
    data analytics were no longer the exclusive realm of software engineers; it was
    now accessible to data scientists, business analysts, and other professionals
    familiar with SQL. From a performance point of view, SparkSQL brought a query
    optimizer and a physical execution engine to Spark, making it even faster while
    using fewer resources.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这一演进还降低了用户采用的门槛。高级分布式数据分析不再是软件工程师的专属领域；现在数据科学家、业务分析师以及熟悉SQL的其他专业人员也可以轻松使用。从性能角度来看，SparkSQL为Spark引入了查询优化器和物理执行引擎，使其在使用更少资源的同时运行速度更快。
- en: A Unified Engine
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统一引擎
- en: Nowadays, Spark is a unified analytics engine offering batch and streaming capabilities
    that is compatible with a polyglot approach to data analytics, offering APIs in
    Scala, Java, Python, and the R language.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，Spark是一个统一的分析引擎，提供批处理和流处理功能，并支持多语言数据分析方法，在Scala、Java、Python和R语言中提供API。
- en: While in the context of this book we are going to focus our interest on the
    streaming capabilities of Apache Spark, its batch functionality is equally advanced
    and is highly complementary to streaming applications. Spark’s unified programming
    model means that developers need to learn only one new paradigm to address both
    batch and streaming workloads.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在本书的上下文中，我们将关注Apache Spark的流处理功能，其批处理功能同样先进，并且与流处理应用程序高度互补。Spark的统一编程模型意味着开发人员只需学习一种新的范式来处理批处理和流处理工作负载。
- en: Note
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In the course of the book, we use *Apache Spark* and *Spark* interchangeably.
    We use *Apache Spark* when we want to make emphasis on the project or open source
    aspect of it, whereas we use *Spark* to refer to the technology in general.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的过程中，我们将*Apache Spark*和*Spark*互换使用。当我们希望强调项目或开源方面时，我们使用*Apache Spark*，而当我们指代技术总体时，我们使用*Spark*。
- en: Spark Components
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark组件
- en: '[Figure 1-1](#spark-layers) illustrates how Spark consists of a core engine,
    a set of abstractions built on top of it (represented as horizontal layers), and
    libraries that use those abstractions to address a particular area (vertical boxes).
    We have highlighted the areas that are within the scope of this book and grayed
    out those that are not covered. To learn more about these other areas of Apache
    Spark, we recommend *Spark, The Definitive Guide* by Bill Chambers and Matei Zaharia
    (O’Reilly), and *High Performance Spark* by Holden Karau and Rachel Warren (O’Reilly).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-1](#spark-layers)展示了Spark如何由一个核心引擎、构建在其之上的一组抽象（表示为水平层）、以及使用这些抽象来处理特定领域的库（垂直框）组成。我们已经突出显示了本书涵盖的领域，并将未涵盖的部分置灰。要了解更多关于Apache
    Spark其他领域的信息，我们推荐阅读Bill Chambers和Matei Zaharia（O’Reilly）的*Spark权威指南*，以及Holden
    Karau和Rachel Warren（O’Reilly）的*高性能Spark*。'
- en: '![spas 0101](Images/spas_0101.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0101](Images/spas_0101.png)'
- en: Figure 1-1\. Abstraction layers (horizontal) and libraries (vertical) offered
    by Spark
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-1\. Spark 提供的抽象层（水平）和库（垂直）
- en: 'As abstraction layers in Spark, we have the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 Spark 中的抽象层，我们有以下内容：
- en: Spark Core
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Core
- en: Contains the Spark core execution engine and a set of low-level functional APIs
    used to distribute computations to a cluster of computing resources, called *executors*
    in Spark lingo. Its cluster abstraction allows it to submit workloads to YARN,
    Mesos, and Kubernetes, as well as use its own standalone cluster mode, in which
    Spark runs as a dedicated service in a cluster of machines. Its datasource abstraction
    enables the integration of many different data providers, such as files, block
    stores, databases, and event brokers.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 包含 Spark 核心执行引擎和一组低级函数 API，用于将计算分发到一组计算资源的集群，Spark 的术语中称为 *执行器*。其集群抽象允许将工作负载提交到
    YARN、Mesos 和 Kubernetes，也可以使用自己的独立集群模式，在此模式下，Spark 作为一项专用服务在一组机器的集群中运行。其数据源抽象使其能够集成许多不同的数据提供者，例如文件、块存储、数据库和事件代理。
- en: Spark SQL
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Implements the higher-level `Dataset` and `DataFrame` APIs of Spark and adds
    SQL support on top of arbitrary data sources. It also introduces a series of performance
    improvements through the Catalyst query engine, and code generation and memory
    management from project Tungsten.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 实现了 Spark 的高级 `Dataset` 和 `DataFrame` API，并在任意数据源之上增加了 SQL 支持。它还通过 Catalyst
    查询引擎、项目 Tungsten 中的代码生成和内存管理引入了一系列性能改进。
- en: 'The libraries built on top of these abstractions address different areas of
    large-scale data analytics: *MLLib* for machine learning, *GraphFrames* for graph
    analysis, and the two APIs for stream processing that are the focus of this book:
    Spark Streaming and Structured Streaming.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些抽象构建的库解决了大规模数据分析的不同领域：*MLLib* 用于机器学习，*GraphFrames* 用于图分析，以及本书关注的两个流处理 API：Spark
    Streaming 和 Structured Streaming。
- en: Spark Streaming
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: Spark Streaming was the first stream-processing framework built on top of the
    distributed processing capabilities of the core Spark engine. It was introduced
    in the Spark 0.7.0 release in February of 2013 as an alpha release that evolved
    over time to become today a mature API that’s widely adopted in the industry to
    process large-scale data streams.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 是建立在核心 Spark 引擎的分布式处理能力之上的第一个流处理框架。它是在 2013 年 2 月的 Spark 0.7.0
    版本中作为 alpha 发布，随着时间的推移逐步发展成为今天业界广泛采用的成熟 API，用于处理大规模数据流。
- en: 'Spark Streaming is conceptually built on a simple yet powerful premise: apply
    Spark’s distributed computing capabilities to stream processing by transforming
    continuous streams of data into discrete data collections on which Spark could
    operate. This approach to stream processing is called the *microbatch* model;
    this is in contrast with the *element-at-time* model that dominates in most other
    stream-processing implementations.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 的概念基于一个简单而强大的前提：通过将连续的数据流转换为离散的数据集合，应用 Spark 的分布式计算能力进行流处理。这种流处理方法被称为
    *微批处理* 模型；与大多数其他流处理实现中占主导地位的 *逐元素处理* 模型形成对比。
- en: Spark Streaming uses the same functional programming paradigm as the Spark core,
    but it introduces a new abstraction, the *Discretized Stream* or *DStream*, which
    exposes a programming model to operate on the underlying data in the stream.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 使用与 Spark 核心相同的函数式编程范式，但引入了一个新的抽象，*离散流* 或 *DStream*，它暴露了一个编程模型，用于操作流中的底层数据。
- en: Structured Streaming
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化流处理
- en: Structured Streaming is a stream processor built on top of the Spark SQL abstraction.
    It extends the `Dataset` and `DataFrame` APIs with streaming capabilities. As
    such, it adopts the schema-oriented transformation model, which confers the *structured*
    part of its name, and inherits all the optimizations implemented in Spark SQL.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理是建立在 Spark SQL 抽象之上的流处理器。它通过扩展 `Dataset` 和 `DataFrame` API 添加了流处理能力。因此，它采用了基于模式的转换模型，这是其名称中
    *结构化* 部分的来源，并继承了 Spark SQL 中实现的所有优化。
- en: Structured Streaming was introduced as an experimental API with Spark 2.0 in
    July of 2016. A year later, it reached *general availability* with the Spark 2.2
    release becoming eligible for production deployments. As a relatively new development,
    Structured Streaming is still evolving fast with each new version of Spark.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理作为 Spark 2.0 中的实验性 API 在 2016 年 7 月引入。一年后，它在 Spark 2.2 版本中达到了 *一般可用性*，适合用于生产部署。作为一个相对新的开发，结构化流处理每个
    Spark 的新版本都在快速演进。
- en: Structured Streaming uses a declarative model to acquire data from a stream
    or set of streams. To use the API to its full extent, it requires the specification
    of a schema for the data in the stream. In addition to supporting the general
    transformation model provided by the `Dataset` and `DataFrame` APIs, it introduces
    stream-specific features such as support for event-time, streaming joins, and
    separation from the underlying runtime. That last feature opens the door for the
    implementation of runtimes with different execution models. The default implementation
    uses the classical microbatch approach, whereas a more recent *continuous processing*
    backend brings experimental support for near-real-time continuous execution mode.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理使用声明性模型从流或一组流获取数据。要充分利用该API，需要为流中的数据指定模式。除了支持`Dataset`和`DataFrame` API提供的一般转换模型外，它还引入了流特定功能，如支持事件时间、流连接以及与底层运行时的分离。最后一个特性为不同执行模型的运行时实现打开了大门。默认实现使用经典的微批处理方法，而较新的*连续处理*后端则为接近实时的连续执行模式提供了实验性支持。
- en: Structured Streaming delivers a unified model that brings stream processing
    to the same level of batch-oriented applications, removing a lot of the cognitive
    burden of reasoning about stream processing.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理（Structured Streaming）提供了一个统一的模型，将流处理带到与面向批处理应用程序相同的水平，消除了大量关于流处理推理的认知负担。
- en: Where Next?
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Where Next?
- en: If you are feeling the urge to learn either of these two APIs right away, you
    could directly jump to Structured Streaming in [Part II](part02.xhtml#str-str)
    or Spark Streaming in [Part III](part03.xhtml#spark-streaming).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您立刻想要学习这两个API中的任何一个，您可以直接跳转到[第二部分](part02.xhtml#str-str)中的结构化流处理或者[第三部分](part03.xhtml#spark-streaming)中的Spark流处理。
- en: If you are not familiar with stream processing, we recommend that you continue
    through this initial part of the book because we build the vocabulary and common
    concepts that we use in the discussion of the specific frameworks.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对流处理不熟悉，我们建议您继续阅读本书的初始部分，因为我们会构建在讨论特定框架时使用的词汇和常见概念。
