- en: Chapter 3\. Streaming Architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章\. 流处理架构
- en: The implementation of a distributed data analytics system has to deal with the
    management of a pool of computational resources, as in-house clusters of machines
    or reserved cloud-based capacity, to satisfy the computational needs of a division
    or even an entire company. Since teams and projects rarely have the same needs
    over time, clusters of computers are best amortized if they are a shared resource
    among a few teams, which requires dealing with the problem of multitenancy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式数据分析系统的实现必须处理计算资源池的管理，例如内部机器群集或预留的基于云的容量，以满足部门甚至整个公司的计算需求。由于团队和项目很少在时间上有相同的需求，如果几个团队共享计算资源，计算机群集最好是摊销的共享资源，这需要处理多租户问题。
- en: When the needs of two teams differ, it becomes important to give each a fair
    and secure access to the resources for the cluster, while making sure the computing
    resources are best utilized over time.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个团队的需求不同时，给每个团队提供公平和安全的访问集群资源变得很重要，同时确保随着时间的推移最佳利用计算资源。
- en: This need has forced people using large clusters to address this heterogeneity
    with modularity, making several functional blocks emerge as interchangeable pieces
    of a data platform. For example, when we refer to database storage as the functional
    block, the most common component that delivers that functionality is a relational
    database such as PostgreSQL or MySQL, but when the streaming application needs
    to write data at a very high throughput, a scalable column-oriented database like
    Apache Cassandra would be a much better choice.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这一需求迫使使用大型集群的人们通过模块化来应对这种异构性，使得几个功能块成为数据平台中可互换的组成部分。例如，当我们将数据库存储作为功能块时，提供此功能的最常见组件是关系型数据库，如PostgreSQL或MySQL，但是当流应用程序需要以非常高的吞吐量写入数据时，像Apache
    Cassandra这样的可扩展的列式数据库会是更好的选择。
- en: 'In this chapter, we briefly explore the different parts that comprise the architecture
    of a streaming data platform and see the position of a processing engine relative
    to the other components needed for a complete solution. After we have a good view
    of the different elements in a streaming architecture, we explore two architectural
    styles used to approach streaming applications: the Lambda and the Kappa architectures.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要探讨了构成流数据平台架构的不同部分，并查看了处理引擎相对于其他组件在完整解决方案中的位置。在我们深入了解流架构的各个元素后，我们探索了两种用于处理流应用程序的架构风格：Lambda架构和Kappa架构。
- en: Components of a Data Platform
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据平台的组件
- en: We can see a data platform as a composition of standard components that are
    expected to be useful to most stakeholders and specialized systems that serve
    a purpose specific to the challenges that the business wants to address.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将数据平台视为标准组件的组合，这些组件预计对大多数利益相关者有用，并且专门的系统则为解决业务挑战而服务。
- en: '[Figure 3-1](#big_data_parts) illustrates the pieces of this puzzle.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-1](#big_data_parts)说明了这一难题的各个部分。'
- en: '![spas 0301](Images/spas_0301.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0301](Images/spas_0301.png)'
- en: Figure 3-1\. The parts of a data platform
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1\. 数据平台的组成部分
- en: 'Going from the bare-metal level at the bottom of the schema to the actual data
    processing demanded by a business requirement, you could find the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构底部的裸金属级别到业务需求所需的实际数据处理，你可以找到以下内容：
- en: The hardware level
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件层级
- en: On-premises hardware installations, datacenters, or potentially virtualized
    in homogeneous cloud solutions (such as the T-shirt size offerings of Amazon,
    Google, or Microsoft), with a base operating system installed.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地硬件安装、数据中心或潜在的虚拟化在同质云解决方案中（如Amazon、Google或Microsoft的T恤大小套餐），安装基本操作系统。
- en: The persistence level
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 持久性层级
- en: On top of that baseline infrastructure, it is often expected that machines offer
    a shared interface to a persistence solution to store the results of their computation
    as well as perhaps its input. At this level, you would find distributed storage
    solutions like the Hadoop Distributed File System (HDFS)—among many other distributed
    storage systems. On the cloud, this persistence layer is provided by a dedicated
    service such as Amazon Simple Storage Service (Amazon S3) or Google Cloud Storage.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个基线基础设施之上，通常期望机器提供一个共享接口来存储其计算结果以及可能的输入。在这一层级上，你会找到诸如Hadoop分布式文件系统（HDFS）之类的分布式存储解决方案——还有许多其他分布式存储系统。在云上，这种持久性层由专门的服务提供，比如Amazon
    Simple Storage Service（Amazon S3）或Google Cloud Storage。
- en: The resource manager
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 资源管理器
- en: After persistence, most cluster architectures offer a single point of negotiation
    to submit jobs to be executed on the cluster. This is the task of the resource
    manager, like YARN and Mesos, and the more evolved *schedulers* of the *cloud-native*
    era, like Kubernetes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在持久性之后，大多数集群架构提供了一个单一的协商点，用于提交要在集群上执行的作业。这是资源管理器的任务，如YARN和Mesos，以及更进化的云原生时代的调度程序，如Kubernetes。
- en: The execution engine
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 执行引擎
- en: At an even higher level, there is the execution engine, which is tasked with
    executing the actual computation. Its defining characteristic is that it holds
    the interface with the programmer’s input and describes the data manipulation.
    Apache Spark, Apache Flink, or MapReduce would be examples of this.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 更高级别的是执行引擎，它负责执行实际的计算任务。其定义特征是它与程序员输入的接口并描述数据操作。Apache Spark、Apache Flink或MapReduce就是此类的例子。
- en: A data ingestion component
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄入组件
- en: Besides the execution engine, you could find a data ingestion server that could
    be plugged directly into that engine. Indeed, the old practice of reading data
    from a distributed filesystem is often supplemented or even replaced by another
    data source that can be queried in real time. The realm of messaging systems or
    log processing engines such as Apache Kafka is set at this level.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了执行引擎，您还可以找到一个数据摄入服务器，可以直接插入该引擎。事实上，从分布式文件系统读取数据的旧做法经常被另一种可以实时查询的数据源所补充或甚至替代。消息系统或日志处理引擎如Apache
    Kafka在这个层次上被设置。
- en: A processed data sink
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的数据接收器
- en: On the output side of an execution engine, you will frequently find a high-level
    data sink, which might be either another analytics system (in the case of an execution
    engine tasked with an *Extract, Transform and Load* [ETL] job), a NoSQL database,
    or some other service.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行引擎的输出端，您经常会找到一个高级数据接收器，它可能是另一个分析系统（在执行提取、转换和加载[ETL]作业的情况下），NoSQL数据库或其他服务。
- en: A visualization layer
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化层
- en: We should note that because the results of data-processing are useful only if
    they are integrated in a larger framework, those results are often plugged into
    a visualization. Nowadays, since the data being analyzed evolves quickly, that
    visualization has moved away from the old static report toward more real-time
    visual interfaces, often using some web-based technology.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，因为数据处理的结果只有在集成到更大的框架中才有用，所以这些结果通常被插入到可视化中。如今，由于被分析的数据迅速演变，这种可视化已经从旧的静态报告转向更实时的视觉界面，通常使用一些基于Web的技术。
- en: In this architecture, Spark, as a computing engine, focuses on providing data
    processing capabilities and relies on having functional interfaces with the other
    blocks of the picture. In particular, it implements a cluster abstraction layer
    that lets it interface with YARN, Mesos, and Kubernetes as resource managers,
    provides connectors to many data sources while new ones are easily added through
    an easy-to-extend API, and integrates with output data sinks to present results
    to upstream systems.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，作为计算引擎的Spark专注于提供数据处理能力，并依赖于与图片的其他块具有功能接口。特别是，它实现了一个集群抽象层，允许它与YARN、Mesos和Kubernetes作为资源管理器进行接口，通过易于扩展的API提供连接到许多数据源，而新的数据源可以轻松添加，并集成了输出数据接收器，以向上游系统展示结果。
- en: Architectural Models
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构模型
- en: Now we turn our attention to the link between stream processing and batch processing
    in a concrete architecture. In particular, we’re going to ask ourselves the question
    of whether batch processing is still relevant if we have a system that can do
    stream processing, and if so, why?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将注意力转向具体架构中流处理和批处理之间的关联。特别是，我们将问自己一个问题，即如果我们有一个能够进行流处理的系统，那么批处理是否仍然相关，如果是，为什么呢？
- en: 'In this chapter, we contrast two conceptions of streaming application architecture:
    the *Lambda architecture*, which suggests duplicating a streaming application
    with a batch counterpart running in parallel to obtain complementary results,
    and the *Kappa architecture*, which purports that if two versions of an application
    need to be compared, those should both be streaming applications. We are going
    to see in detail what those architectures intend to achieve, and we examine that
    although the Kappa architecture is easier and lighter to implement in general,
    there might be cases for which a Lambda architecture is still needed, and why.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们对比了两种流应用程序架构的概念：*Lambda 架构* 建议在并行运行的批处理对应应用程序中复制一个流应用程序，以获得互补的结果，而 *Kappa
    架构* 则主张如果需要比较应用程序的两个版本，这两个版本都应该是流应用程序。我们将详细看看这些架构的意图，以及我们将检验的内容，尽管 Kappa 架构在一般情况下更容易和更轻量化实现，但仍可能存在需要
    Lambda 架构的情况，以及为什么。
- en: The Use of a Batch-Processing Component in a Streaming Application
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在流应用程序中使用批处理组件
- en: Often, if we develop a batch application that runs on a periodic interval into
    a streaming application, we are provided with batch datasets already—and a batch
    program representing this periodic analysis, as well. In this evolution use case,
    as described in the prior chapters, we want to evolve to a streaming application
    to reap the benefits of a lighter, simpler application that gives faster results.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，如果我们开发一个批处理应用程序，它以周期性间隔运行成为一个流应用程序，我们已经提供了批处理数据集，并且表示这种周期性分析的批处理程序。在这个演化使用案例中，如前几章所述，我们希望演变成一个流应用程序，以获得一个更轻、更简单的应用程序，能够更快地提供结果。
- en: 'In a greenfield application, we might also be interested in creating a reference
    batch dataset: most data engineers don’t work on merely solving a problem once,
    but revisit their solution, and continuously improve it, especially if value or
    revenue is tied to the performance of their solution. For this purpose, a batch
    dataset has the advantage of setting a benchmark: after it’s collected, it does
    not change anymore and can be used as a “test set.” We can indeed replay a batch
    dataset to a streaming system to compare its performance to prior iterations or
    to a known benchmark.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个全新的应用程序中，我们可能也有兴趣创建一个参考批处理数据集：大多数数据工程师不仅仅是解决问题一次，而是重新审视他们的解决方案，并持续改进，特别是如果价值或收入与他们的解决方案的性能有关。出于这个目的，批处理数据集有设置基准的优势：一旦收集完毕，它就不再改变，可以用作“测试集”。确实，我们可以将批处理数据集重新播放到流系统中，以比较其性能与先前的迭代或已知基准。
- en: 'In this context, we identify three levels of interaction between the batch
    and the stream-processing components, from the least to the most mixed with batch
    processing:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们识别出批处理和流处理组件之间的三个交互级别，从最少混合到最多混合的批处理：
- en: '*Code reuse*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码重用*'
- en: Often born out of a reference batch implementation, seeks to reemploy as much
    of it as possible, so as not to duplicate efforts. This is an area in which Spark
    shines, since it is particularly easy to call functions that transform Resilient
    Distributed Databases (RDDs) and DataFrames—they share most of the same APIs,
    and only the setup of the data input and output is distinct.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通常源于参考批处理实现，寻求尽可能多地重新使用它，以避免重复努力。这是 Spark 突出的领域，因为可以特别容易地调用转换 Resilient Distributed
    Databases (RDDs) 和 DataFrames 的函数 — 它们共享大部分相同的 API，只有数据输入和输出的设置是不同的。
- en: '*Data reuse*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据重用*'
- en: 'Wherein a streaming application feeds itself from a feature or data source
    prepared, at regular intervals, from a batch processing job. This is a frequent
    pattern: for example, some international applications must handle time conversions,
    and a frequent pitfall is that daylight saving rules change on a more frequent
    basis than expected. In this case, it is good to be thinking of this data as a
    new dependent source that our streaming application feeds itself off.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，流应用程序从一个特征或数据源中定期准备的批处理作业中获得自身的数据。这是一种常见模式：例如，一些国际应用程序必须处理时间转换，而一个常见的陷阱是夏令时规则比预期更频繁地改变。在这种情况下，考虑这些数据作为我们的流应用程序自身依赖的新依赖源是很好的。
- en: '*Mixed processing*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*混合处理*'
- en: Wherein the application itself is understood to have both a batch and a streaming
    component during its lifetime. This pattern does happen relatively frequently,
    out of a will to manage both the precision of insights provided by an application,
    and as a way to deal with the versioning and the evolution of the application
    itself.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序在其生命周期内被理解为同时具有批处理和流处理组件。这种模式相对频繁地发生，出于希望管理应用程序提供的洞察力的精度以及处理应用程序自身版本控制和演进的意愿。
- en: 'The first two uses are uses of convenience, but the last one introduces a new
    notion: using a batch dataset as a benchmark. In the next subsections, we see
    how this affects the architecture of a streaming application.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种用途是便利性的用途，但最后一个引入了一个新概念：使用批处理数据集作为基准。在接下来的小节中，我们看到这如何影响流应用程序的架构。
- en: Referential Streaming Architectures
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考性数据流架构
- en: 'In the world of replay-ability and performance analysis over time, there are
    two historical but conflicting recommendations. Our main concern is about how
    to measure and test the performance of a streaming application. When we do so,
    there are two things that can change in our setup: the nature of our model (as
    a result of our attempt at improving it) and the data that the model operates
    on (as a result of organic change). For instance, if we are processing data from
    weather sensors, we can expect a seasonal pattern of change in the data.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在重放性和随时间性能分析的世界中，有两个历史悠久但相互冲突的建议。我们的主要关注点是如何测量和测试流应用程序的性能。在这样做时，我们的设置可能会发生两种变化：模型的性质（由于改进尝试的结果）和模型操作的数据（由于有机变化的结果）。例如，如果我们处理来自气象传感器的数据，我们可以预期数据中会有季节性变化模式。
- en: 'To ensure we compare apples to apples, we have already established that replaying
    a *batch dataset* to two versions of our streaming application is useful: it lets
    us make sure that we are not seeing a change in performance that is really reflecting
    a change in the data. Ideally, in this case, we would test our improvements in
    yearly data, making sure we’re not overoptimizing for the current season at the
    detriment of performance six months after.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们能进行苹果与苹果的比较，我们已经确定，重播*批处理数据集*到我们流应用程序的两个版本是有用的：它确保我们没有看到真正反映数据变化的性能变化。理想情况下，在这种情况下，我们将测试我们在年度数据中的改进，确保我们在当前季节的优化不至于在六个月后损害性能。
- en: However, we want to contend that a comparison with a *batch analysis* is necessary,
    as well, beyond the use of a benchmark dataset—and this is where the architecture
    comparison helps.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们希望争辩说，与*批处理分析*的比较同样是必要的，超出使用基准数据集的使用——这正是架构比较发挥作用的地方。
- en: The Lambda Architecture
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lambda 架构
- en: The Lambda architecture ([Figure 3-2](#lambda-arch)) suggests taking a batch
    analysis performed on a periodic basis—say, nightly—and to supplement the model
    thus created with streaming refinements as data comes, until we are able to produce
    a new version of the batch analysis based on the entire day’s data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 架构（[图 3-2](#lambda-arch)）建议定期执行批处理分析（例如每晚一次），并在数据流入时补充模型，直至能够基于整天的数据生成新版本的批处理分析。
- en: 'It was introduced as such by Nathan Marz in a blog post, [“How to beat the
    CAP Theorem”](http://bit.ly/1ATyjbD).^([1](ch03.xhtml#idm46385833157224)) It proceeds
    from the idea that we want to emphasize two novel points beyond the precision
    of the data analysis:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 它是由 Nathan Marz 在博客文章 [“如何战胜 CAP 定理”](http://bit.ly/1ATyjbD).^([1](ch03.xhtml#idm46385833157224))
    中作为此类引入的。它源于这样一个理念：我们希望强调数据分析精度以外的两个新颖点。
- en: The historical replay-ability of data analysis is important
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分析的历史重放性非常重要
- en: The availability of results proceeding from fresh data is also a very important
    point
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由新数据产生的结果的可用性也是非常重要的一点。
- en: '![spas 0302](Images/spas_0302.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0302](Images/spas_0302.png)'
- en: Figure 3-2\. The Lambda architecture
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. Lambda 架构
- en: 'This is a useful architecture, but its drawbacks seem obvious, as well: such
    a setup is complex and requires maintaining two versions of the same code, for
    the same purpose. Even if Spark helps in letting us reuse most of our code between
    the batch and streaming versions of our application, the two versions of the application
    are distinct in life cycles, which might seem complicated.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有用的架构，但其缺点显而易见：这样的设置很复杂，需要维护两个用于相同目的的相同代码版本。即使 Spark 在让我们在批处理和流处理版本的应用程序之间重用大部分代码方面提供了帮助，应用程序的这两个版本在生命周期上是不同的，这可能看起来很复杂。
- en: An alternative view on this problem suggests that it would be enough to keep
    the ability to feed the same dataset to two versions of a streaming application
    (the new, improved experiment, and the older, stable workhorse), helping with
    the maintainability of our solution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的另一个观点建议，保留能够向两个版本的流式应用程序（新的、改进的实验和旧的、稳定的工作马）提供相同数据集的能力，有助于我们解决方案的可维护性。
- en: The Kappa Architecture
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kappa 架构
- en: This architecture, as outlined in [Figure 3-3](#kappa-arch), compares two streaming
    applications and does away with any batching, noting that if reading a batch file
    is needed, a simple component can replay the contents of this file, record by
    record, as a streaming data source. This simplicity is still a great benefit,
    since even the code that consists in feeding data to the two versions of this
    application can be reused. In this paradigm, called the *Kappa architecture* ([[Kreps2014]](app01.xhtml#Kreps2014)),
    there is no deduplication and the mental model is simpler.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构，如 [图3-3](#kappa-arch) 所述，比较了两个流式应用程序，并且取消了任何批处理，指出如果需要读取批处理文件，则可以简单地组件可以逐记录地重放此文件的内容作为流式数据源。这种简单性仍然是一个巨大的优势，因为甚至是向这两个应用程序版本提供数据的代码也可以被重复使用。在这种被称为
    *Kappa 架构* 的范式中（[[Kreps2014]](app01.xhtml#Kreps2014)），没有去重，并且心理模型更简单。
- en: '![spas 0303](Images/spas_0303.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0303](Images/spas_0303.png)'
- en: Figure 3-3\. The Kappa architecture
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3\. Kappa 架构
- en: 'This begs the question: is batch computation still relevant? Should we convert
    our applications to be all streaming, all the time?'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了一个问题：批处理计算仍然重要吗？我们应该把我们的应用程序全部转换为全天候的流式处理吗？
- en: We think some concepts stemming from the Lambda architecture are still relevant;
    in fact, they’re vitally useful in some cases, although those are not always easy
    to figure out.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为 Lambda 架构中衍生出的一些概念仍然相关；事实上，在某些情况下它们非常有用，尽管这些情况并不总是容易理解。
- en: There are some use cases for which it is still useful to go through the effort
    of implementing a batch version of our analysis and then compare it to our streaming
    solution.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有一些用例是值得付出努力实现我们的分析的批处理版本，然后与我们的流式解决方案进行比较的。
- en: Streaming Versus Batch Algorithms
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式与批处理算法比较
- en: 'There are two important considerations that we need to take into account when
    selecting a general architectural model for our streaming application:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择流式应用的通用架构模型时，有两个重要的考虑因素需要考虑：
- en: Streaming algorithms are sometimes completely different in nature
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式算法有时在性质上完全不同
- en: Streaming algorithms can’t be guaranteed to measure well against batch algorithms
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式算法不能保证在与批处理算法的比较中表现良好。
- en: Let’s explore these thoughts in the next two sections using motivating examples.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的两个部分中使用激励性的例子来探讨这些想法。
- en: Streaming Algorithms Are Sometimes Completely Different in Nature
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式算法在性质上有时完全不同
- en: Sometimes, it is difficult to deduce batch from streaming, or the reverse, and
    those two classes of algorithms have different characteristics. This means that
    at first glance we might not be able to reuse code between both approaches, but
    also, and more important, that relating the performance characteristics of those
    two modes of processing should be done with high care.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，很难从流式计算中推断出批处理，或者反过来，这两类算法具有不同的特性。这意味着乍看之下我们可能无法在这两种方法之间重复使用代码，但更重要的是，应该非常谨慎地比较这两种处理模式的性能特征。
- en: 'To make things more precise, let’s look at an example: the buy or rent problem.
    In this case, we decide to go skiing. We can buy skis for $500 or rent them for
    $50\. Should we rent or buy?'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使事情更加明确，让我们看一个例子：购买或租赁问题。在这种情况下，我们决定去滑雪。我们可以花$500购买滑雪板，或者花$50租借滑雪板。我们应该租借还是购买？
- en: 'Our intuitive strategy is to first rent, to see if we like skiing. But suppose
    we do: in this case, we will eventually realize we will have spent more money
    than we would have if we had bought the skis in the first place.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的直觉策略是先租借，看看我们是否喜欢滑雪。但假设我们确实喜欢：在这种情况下，我们最终会意识到我们花费的钱比如果我们一开始就买了滑雪板要多。
- en: In the batch version of this computation, we proceed “in hindsight,” being given
    the total number of times we will go skiing in a lifetime. In the streaming, or
    online version of this problem, we are asked to make a decision (produce an output)
    on each discrete skiing event, as it happens. The strategy is fundamentally different.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种计算的批处理版本中，我们“事后”进行，被告知我们一生中将要滑雪的总次数。在这个问题的流或在线版本中，我们被要求在每个离散的滑雪事件发生时做出决策（产生输出）。策略是根本不同的。
- en: In this case, we can consider the competitive ratio of a streaming algorithm.
    We run the algorithm on the worst possible input, and then compare its “cost”
    to the decision that a batch algorithm would have taken, “in hindsight.”
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以考虑流算法的竞争比率。我们在最坏的输入上运行算法，然后将其“成本”与批算法在“事后”所做的决策进行比较。
- en: 'In our buy-or-rent problem, let’s consider the following streaming strategy:
    we rent until renting makes our total spending as much as buying, in which case
    we buy.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的买房或租房问题中，让我们考虑以下的流策略：我们租房直到租房支出与买房相等，这时我们选择买房。
- en: If we go skiing nine times or fewer, we are optimal, because we spend as much
    as what we would have in hindsight. The competitive ratio is one. If we go skiing
    10 times or more, we pay $450 + $500 = $950\. The worst input is to receive 10
    “ski trip” decision events, in which case the batch algorithm, in hindsight, would
    have paid $500\. The competitive ratio of this strategy is (2 – 1/10).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们滑雪九次或更少，我们是最优的，因为我们的支出与事后的支出相同。竞争比率为一。如果我们滑雪十次或更多，我们支付 $450 + $500 = $950。最坏的输入是接收到10个“滑雪旅行”决策事件，在这种情况下，批处理算法在事后将支付
    $500。这种策略的竞争比率是（2 - 1/10）。
- en: If we were to choose another algorithm, say “always buy on the first occasion,”
    then the worst possible input is to go skiing only once, which means that the
    competitive ratio is $500 / $50 = 10.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择另一种算法，比如“总是第一次买”，那么最坏的输入是只滑雪一次，这意味着竞争比率是 $500 / $50 = 10。
- en: Note
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The performance ratio or competitive ratio is a measure of how far from the
    optimal the values returned by an algorithm are, given a measure of optimality.
    An algorithm is formally ρ-competitive if its objective value is no more than
    ρ times the optimal offline value for all instances.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 性能比率或竞争比率是算法返回的值距离最优解有多远的度量标准。如果一个算法在所有实例中的目标值不超过最优离线值的ρ倍，则形式上称为ρ-竞争算法。
- en: A better competitive ratio is smaller, whereas a competitive ratio above one
    shows that the streaming algorithm performs measurably worse on some inputs. It
    is easy to see that with the worst input condition, the batch algorithm, which
    proceeds in hindsight with strictly more information, is always expected to perform
    better (the competitive ratio of any streaming algorithm is greater than one).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的竞争比率更小，而竞争比率大于一显示流算法在某些输入上表现明显较差。很容易看出，在最坏的输入条件下，批处理算法，以严格更多信息事后进行，总是预计能表现更好（任何流算法的竞争比率大于一）。
- en: Streaming Algorithms Can’t Be Guaranteed to Measure Well Against Batch Algorithms
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流算法不能保证在与批处理算法的性能上表现良好
- en: Another example of those unruly cases is the bin-packing problem. In the bin-packing
    problem, an input of a set of objects of different sizes or different weights
    must be fitted into a number of bins or containers, each of them having a set
    volume or set capacity in terms of weight or size. The challenge is to find an
    assignment of objects into bins that minimizes the number of containers used.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不服管教的例子是**装箱问题**。在装箱问题中，一组不同大小或不同重量的物体必须装入若干个箱子或容器中，每个箱子或容器具有一定的体积或重量容量。挑战在于找到一种物体分配到箱子中的方式，使使用的容器数量最小化。
- en: 'In computational complexity theory, the offline ration of that algorithm is
    known to be *NP-hard*. The simple variant of the problem is the *decision* question:
    knowing whether that set of objects will fit into a specified number of bins.
    It is itself NP-complete, meaning (for our purposes here) computationally very
    difficult in and of itself.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算复杂性理论中，该算法的离线版本被称为*NP-hard*。问题的简单变体是*决策*问题：知道这些物体是否能够装入指定数量的箱子中。它本身是NP完全的，这意味着（对于我们这里的目的）在计算上非常困难。
- en: In practice, this algorithm is used very frequently, from the shipment of actual
    goods in containers, to the way operating systems match memory allocation requests,
    to blocks of free memory of various sizes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这种算法被广泛使用，从实际货物在集装箱中的装运，到操作系统匹配各种大小的空闲内存块的内存分配请求的方式。
- en: There are many variations of these problems, but we want to focus on the distinction
    between online versions—for which the algorithm has as input a stream of objects—and
    offline versions—for which the algorithm can examine the entire set of input objects
    before it even starts the computing process.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题有许多变体，但我们想要关注在线版本与离线版本之间的区别——在线版本的算法以对象流作为输入，而离线版本的算法在甚至开始计算过程之前就可以检查整组输入对象。
- en: The online algorithm processes the items in arbitrary order and then places
    each item in the first bin that can accommodate it, and if no such bin exists,
    it opens a new bin and puts the item within that new bin. This greedy approximation
    algorithm always allows placing the input objects into a set number of bins that
    is, at worst, suboptimal; meaning we might use more bins than necessary.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在线算法按任意顺序处理项目，然后将每个项目放入第一个能容纳它的箱子中，如果没有这样的箱子存在，则打开一个新箱子并将项目放入其中。这种贪婪逼近算法总是允许将输入对象放入最多情况下是次优的箱子数中，这意味着我们可能会使用比必要更多的箱子。
- en: A better algorithm, which is still relatively intuitive to understand, is the
    *first fit decreasing strategy*, which operates by first sorting the items to
    be inserted in decreasing order of their sizes, and then inserting each item into
    the first bin in the list with sufficient remaining space. That algorithm was
    proven in 2007 to be much closer to the optimal algorithm producing the absolute
    minimum number of bins ([[Dosa2007]](app01.xhtml#Dosa2007)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更好的算法，仍然相对直观易懂，是*首次适合减少策略*，它通过首先按其大小降序排序要插入的项目，然后将每个项目插入列表中第一个具有足够剩余空间的箱子。该算法在2007年被证明非常接近最优算法，可以产生绝对最小的箱子数（[[Dosa2007]](app01.xhtml#Dosa2007)）。
- en: The first fit decreasing strategy, however, relies on the idea that we can first
    sort the items in decreasing order of sizes before we begin processing them and
    packing them into bins.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，首次适合减少策略依赖于我们可以在开始处理和将其放入箱子之前按大小降序对项目进行排序的想法。
- en: Now, attempting to apply such a method in the case of the online bin-packing
    problem, the situation is completely different in that we are dealing with a stream
    of elements for which sorting is not possible. Intuitively, it is thus easy to
    understand that the online bin-packing problem—which by its nature lacks foresight
    when it operates—is much more difficult than the offline bin-packing problem.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，试图在在线装箱问题的情况下应用这样的方法，情况完全不同，因为我们处理的是一个无法排序的元素流。直觉上，因此很容易理解，在线装箱问题——在其操作时缺乏预见性——比离线装箱问题要困难得多。
- en: Warning
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: That intuition is in fact supported by proof if we consider the competitive
    ratio of streaming algorithms. This is the ratio of resources consumed by the
    online algorithm to those used by an online optimal algorithm delivering the minimal
    number of bins by which the input set of objects encountered so far can be packed.
    This competitive ratio for the knapsack (or bin-packing) problem is in fact arbitrarily
    bad (that is, large; see [[Sharp2007]](app01.xhtml#Sharp2007)), meaning that it
    is always possible to encounter a “bad” sequence in which the performance of the
    online algorithm will be arbitrarily far from that of the optimal algorithm.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果考虑流算法的竞争比，这种直觉事实上得到了证明支持。这是在线算法消耗的资源与通过最小化输入对象集合中可以打包的箱子数量的在线最优算法使用的资源之比。背包（或装箱）问题的这种竞争比实际上是任意糟糕的（也就是说，很大；见[[Sharp2007]](app01.xhtml#Sharp2007)），这意味着总是可能遇到一种“糟糕”的序列，在这种序列中，在线算法的表现将与最优算法的表现有任意远的距离。
- en: The larger issue presented in this section is that there is no guarantee that
    a streaming algorithm will perform better than a batch algorithm, because those
    algorithms must function without foresight. In particular, some online algorithms,
    including the knapsack problem, have been proven to have an arbitrarily large
    performance ratio when compared to their offline algorithms.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中涉及的更大问题是，不能保证流算法的表现优于批处理算法，因为这些算法必须在没有预见的情况下运行。特别是一些在线算法，包括背包问题，已经被证明在与其离线算法比较时存在任意大的性能比率。
- en: What this means, to use an analogy, is that we have one worker that receives
    the data as batch, as if it were all in a *storage room* from the beginning, and
    the other worker receiving the data in a streaming fashion, as if it were on a
    *conveyor belt*, then *no matter how clever our streaming worker is, there is
    always a way to place items on the conveyor belt in such a pathological way that
    he will finish his task with an arbitrarily worse result than the batch worker*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，打个比方，我们有一个工人像批处理一样接收数据，好像一开始数据就在一个*储藏室*里，另一个工人则像流处理一样接收数据，好像数据在一个*传送带*上，然后*无论我们的流处理工人多么聪明，总有办法以一种病态的方式将项目放在传送带上，使他完成的任务结果比批处理工人任意糟糕*。
- en: 'The takeaway message from this discussion is twofold:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从这次讨论中得出的要点有两个：
- en: 'Streaming systems are indeed “lighter”: their semantics can express a lot of
    low-latency analytics in expressive terms.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流处理系统确实“更轻量”：它们的语义可以用富有表现力的术语表达许多低延迟分析。
- en: Streaming APIs invite us to implement analytics using streaming or online algorithms
    in which heuristics are sadly limited, as we’ve seen earlier.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式 API 鼓励我们使用流处理或在线算法来实现分析，其中启发式方法很遗憾地受限，正如我们之前看到的。
- en: Summary
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: 'In conclusion, the news of batch processing’s demise is overrated: batch processing
    is still relevant, at least to provide a baseline of performance for a streaming
    problem. Any responsible engineer should have a good idea of the performance of
    a batch algorithm operating “in hindsight” on the same input as their streaming
    application:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，批处理处理即使已经过时的说法也是不准确的：批处理处理仍然相关，至少对于为流处理问题提供性能基准。任何负责任的工程师应该对批处理算法在“事后”运行与其流应用相同输入时的性能有一个良好的了解：
- en: If there is a known competitive ratio for the streaming algorithm at hand, and
    the resulting performance is acceptable, running just the stream processing might
    be enough.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果当前的流处理算法有已知的竞争比率，并且结果表现可以接受，仅运行流处理可能就足够了。
- en: If there is no known competitive ratio between the implemented stream processing
    and a batch version, running a batch computation on a regular basis is a valuable
    benchmark to which to hold one’s application.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果当前实施的流处理与批处理版本之间没有已知的竞争比率，定期运行批量计算是一个有价值的基准，可以用来评估其应用。
- en: ^([1](ch03.xhtml#idm46385833157224-marker)) We invite you to consult the original
    article if you want to know more about the link with the CAP theorem (also called
    Brewer’s theorem). The idea was that it concentrated some limitations fundamental
    to distributed computing described by the theorem to a limited part of the data-processing
    system. In our case, we’re focusing on the practical implications of that constraint.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.xhtml#idm46385833157224-marker)) 如果您想了解与 CAP 定理（也称为 Brewer 定理）的联系更多信息，我们建议您查阅原始文章。该定理集中描述了分布式计算中一些限制到数据处理系统的一个有限部分的基本限制。在我们的情况下，我们关注的是该约束的实际影响。
