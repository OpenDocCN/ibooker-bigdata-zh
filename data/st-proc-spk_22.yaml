- en: Chapter 16\. Introducing Spark Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 16 章。介绍 Spark Streaming
- en: Spark Streaming was the first stream-processing framework built on top of the
    distributed processing capabilities of Spark. Nowadays, it offers a mature API
    that’s widely adopted in the industry to process large-scale data streams.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 是建立在 Spark 分布式处理能力之上的第一个流处理框架。如今，它提供了一个成熟的 API，在行业中被广泛采用来处理大规模数据流。
- en: Spark is, by design, a system that is really good at processing data distributed
    over a cluster of machines. Spark’s core abstraction, the *Resilient Distributed
    Dataset* (RDD), and its fluent functional API permits the creation of programs
    that treat distributed data as a collection. That abstraction lets us reason about
    data-processing logic in the form of transformation of the distributed dataset.
    By doing so, it reduces the cognitive load previously required to create and execute
    scalable and distributed data-processing programs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个通过设计在处理分布在一组机器上的数据方面非常出色的系统。Spark 的核心抽象化是*弹性分布式数据集*（RDD），其流畅的函数式 API
    允许创建将分布式数据视为集合的程序。这种抽象化使我们能够通过对分布式数据集的转换来推理数据处理逻辑。通过这样做，它减少了以前创建和执行可扩展和分布式数据处理程序所需的认知负荷。
- en: 'Spark Streaming was created upon a simple yet powerful premise: apply Spark’s
    distributed computing capabilities to stream processing by transforming a continuous
    stream of data into discrete data collections on which Spark could operate.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 是基于简单而强大的前提创建的：通过将 Spark 的分布式计算能力应用于流处理，将连续的数据流转换为 Spark 可操作的离散数据集。
- en: As we can see in [Figure 16-1](#spark-streaming-flow), the main task of Spark
    Streaming is to take data from the stream, package it down into small batches,
    and provide them to Spark for further processing. The output is then produced
    to some downstream system.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们可以在[图 16-1](#spark-streaming-flow)中看到的那样，Spark Streaming 的主要任务是从流中获取数据，将其打包成小批次，并提供给
    Spark 进行进一步处理。然后将输出产生到某个下游系统。
- en: '![spas 1601](Images/spas_1601.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![spas 1601](Images/spas_1601.png)'
- en: Figure 16-1\. Spark and Spark Streaming in action
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 16-1。Spark 和 Spark Streaming 的实际应用
- en: The DStream Abstraction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DStream 抽象化
- en: 'Whereas Structured Streaming, which you learned in [Part II](part02.xhtml#str-str),
    builds its streaming capabilities on top of the *Spark SQL* abstractions of `DataFrame`
    and `Dataset`, Spark Streaming relies on the much more fundamental Spark abstraction
    of RDD. At the same time, Spark Streaming introduces a new concept: the *Discretized
    Stream* or DStream. A DStream represents a stream in terms of discrete blocks
    of data that in turn are represented as RDDs over time, as we can see in [Figure 16-2](#spark-streaming-spark-detail).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 而结构化流，你在[第 II 部分](part02.xhtml#str-str)中学到的，将其流能力构建在*Spark SQL*抽象化的`DataFrame`和`Dataset`之上，而
    Spark Streaming 则依赖于更基础的 Spark RDD 抽象化。与此同时，Spark Streaming 引入了一个新概念：*离散化流*或 DStream。DStream
    表示一种数据的离散块，这些块随时间表现为 RDD。正如我们可以在[图 16-2](#spark-streaming-spark-detail)中看到的那样。
- en: '![spas 1602](Images/spas_1602.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![spas 1602](Images/spas_1602.png)'
- en: Figure 16-2\. DStreams and RDDs in Spark Streaming
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 16-2。Spark Streaming 中的 DStreams 和 RDDs
- en: The DStream abstraction is primarily an execution model that, when combined
    with a functional programming model, provides us with a complete framework to
    develop and execute streaming applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: DStream 抽象化主要是一个执行模型，当与函数式编程模型结合时，为我们提供了一个完整的框架来开发和执行流应用程序。
- en: DStreams as a Programming Model
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DStreams 作为编程模型
- en: 'The code representation of DStreams give us a functional programming API consistent
    with the RDD API and augmented with stream-specific functions to deal with aggregations,
    time-based operations, and stateful computations. In Spark Streaming, we consume
    a stream by creating a DStream from one of the native implementations, such as
    a `SocketInputStream` or using one of the many connectors available that provide
    a DStream implementation specific to a stream provider (this is the case of Kafka,
    Twitter, or Kinesis connectors for Spark Streaming, just to name a few):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: DStreams 的代码表示给了我们一个与 RDD API 一致的函数式编程 API，并增加了处理聚合、基于时间的操作和有状态计算的流特定函数。在 Spark
    Streaming 中，我们通过从其中一个原生实现（如 `SocketInputStream`）创建 DStream 或使用提供特定于流提供程序的 DStream
    实现的众多连接器之一来消费流（这是 Kafka、Twitter 或 Kinesis 连接器为 Spark Streaming 提供的情况，仅举几例）：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After we have obtained a DStream reference, we can implement our application
    logic using the functions provided by the `DStream` API. For example, if the `textDStream`
    in the preceding code is connected to a log server, we could count the number
    of error occurrences:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 获得了一个`DStream`的引用后，我们可以使用`DStream` API提供的函数来实现我们的应用逻辑。例如，如果前面代码中的`textDStream`连接到一个日志服务器，我们可以统计错误发生的次数：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can then count the totals and compute the error rate by using an aggregation
    function called `reduce`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过使用称为`reduce`的聚合函数计算总数并计算错误率：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To obtain our error rate, we perform a safe division:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取我们的错误率，我们执行一个安全的除法：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It’s important to note that up until now, we have been using transformations
    on the DStream but there is still no data processing happening. All transformations
    on DStreams are lazy. This process of defining the logic of a stream-processing
    application is better seen as the set of transformations that will be applied
    to the data after the stream processing is started. As such, it’s a plan of action
    that Spark Streaming will recurrently execute on the data consumed from the source
    DStream. DStreams are immutable. It’s only through a chain of transformations
    that we can process and obtain a result from our data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，到目前为止，我们一直在DStream上使用变换，但尚未进行数据处理。所有DStream上的变换都是惰性的。定义流处理应用程序逻辑的过程更好地看作是将在启动流处理后应用于数据的一组变换。因此，这是Spark
    Streaming将在来自源DStream的数据上重复执行的操作计划。DStreams是不可变的。只有通过一系列变换，我们才能处理并从我们的数据中获取结果。
- en: 'Finally, the DStream programming model requires that the transformations are
    ended by an output operation. This particular operation specifies how the DStream
    is materialized. In our case, we are interested in printing the results of this
    stream computation to the console:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，DStream编程模型要求变换必须以输出操作结束。这种特定操作指定了DStream的具体实现方式。在我们的情况下，我们有兴趣将此流计算的结果打印到控制台上：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In summary, the DStream programming model consists of the functional composition
    of transformations over the stream payload, materialized by one or more *output
    operations* and recurrently executed by the Spark Streaming engine.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，DStream编程模型由对流载荷进行的变换的功能组合组成，由一个或多个*输出操作*实现，并由Spark Streaming引擎重复执行。
- en: DStreams as an Execution Model
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DStreams作为执行模型
- en: In the preceding introduction to the Spark Streaming programming model, we could
    see how data is transformed from its original form into our intended result as
    a series of lazy functional transformations. The Spark Streaming engine is responsible
    for taking that chain of functional transformations and turning it into an actual
    execution plan. That happens by receiving data from the input stream(s), collecting
    that data into batches, and feeding it to Spark in a timely manner.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面介绍的Spark Streaming编程模型中，我们可以看到数据如何从其原始形式转换为我们预期的结果，作为一系列惰性功能变换。Spark Streaming引擎负责将这些功能变换链转化为实际的执行计划。这是通过从输入流(s)接收数据，将数据收集到批次中，并及时将其传递给Spark来实现的。
- en: The measure of time to wait for data is known as the *batch interval*. It is
    usually a short amount of time, ranging from approximately two hundred milliseconds
    to minutes depending on the application requirements for latency. The batch interval
    is the central unit of time in Spark Streaming. At each batch interval, the data
    corresponding to the previous interval is sent to Spark for processing while new
    data is received. This process repeats as long as the Spark Streaming job is active
    and healthy. A natural consequence of this recurring microbatch operation is that
    the computation on the batch’s data has to complete within the duration of the
    batch interval so that computing resources are available when the new microbatch
    arrives. As you will learn in this part of the book, the batch interval dictates
    the time for most other functions in Spark Streaming.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 等待数据的时间度量称为*批处理间隔*。通常是一个较短的时间，从大约两百毫秒到几分钟不等，具体取决于应用程序对延迟的要求。批处理间隔是Spark Streaming中的中心时间单位。在每个批处理间隔内，将前一个间隔对应的数据发送给Spark进行处理，同时接收新数据。这个过程在Spark
    Streaming作业处于活动和健康状态时重复进行。这种重复的微批处理操作的自然结果是，在批处理间隔的持续时间内，对批处理数据的计算必须完成，以便在新的微批处理到达时仍然有可用的计算资源。正如本书的这一部分将会介绍的那样，批处理间隔决定了Spark
    Streaming中大多数其他功能的时间。
- en: In summary, the DStream model dictates that a continuous stream of data is discretized
    into microbatches using a regular time interval defined as the batch interval.
    At each batch interval, Spark Streaming delivers to Spark the corresponding data
    for that period, together with the functional transformations to be applied to
    it. In turn, Spark applies the computation to the data and produces a result,
    usually to an external system. Spark has—at most—the same batch interval of time
    to process the data. Otherwise, bad things could happen, as you will learn later
    on.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，DStream模型规定数据的连续流通过常规时间间隔（称为批处理间隔）离散化为微批次。在每个批处理间隔，Spark Streaming将相应时间段内的数据以及要应用的功能转换交给Spark处理。然后，Spark对数据进行计算并生成结果，通常发送到外部系统。Spark最多有相同的批处理间隔时间来处理数据，否则会出现问题，稍后您将了解到。
- en: The Structure of a Spark Streaming Application
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming应用程序的结构
- en: To gain intuition into the structure and programming model of Spark Streaming
    applications, we are going to begin with an example.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对Spark Streaming应用程序的结构和编程模型有直观的理解，我们将从一个例子开始。
- en: 'Any Spark Streaming application needs to do the following four things:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 任何Spark Streaming应用程序都需要执行以下四项操作：
- en: Create a *Spark Streaming Context*
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个*Spark Streaming Context*
- en: Define one or several DStreams from data sources or other DStreams
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据源或其他DStreams定义一个或多个DStreams
- en: Define one or more output operations to materialize the results of these DStream
    operations
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个或多个输出操作，以实现这些DStream操作的结果
- en: Start the Spark Streaming Context to get the stream processing going
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动Spark Streaming Context以启动流处理。
- en: For the sake of our example, we will stop the process after is has run for a
    while.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 出于我们例子的考虑，在它运行一段时间后我们将停止这个过程。
- en: The behavior of the job is defined in the operations defined between the moment
    that the instance for the Streaming Context is defined and the moment it is started.
    In that sense, the manipulation of the context before it is started defines the
    scaffolding for the streaming application, which will be its behavior and execution
    for the duration of the streaming application.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作业的行为是在定义流处理上下文实例之后并启动之间定义的操作中定义的。从这个意义上讲，在启动之前对上下文的操作定义了流应用程序的脚手架，其行为和执行将在流应用程序的整个持续期间内定义。
- en: It is during this definition phase that all the DStreams and their transformations
    will be defined and the behavior of the Spark Streaming application will be “wired,”
    so to speak.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义阶段，将定义所有的DStreams及其转换，并且“连接”了Spark Streaming应用程序的行为。
- en: Note that after the Spark Streaming Context has started, no new DStream can
    be added to it nor can any existing DStream can be structurally modified.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一旦启动了Spark Streaming Context，就不能向其添加新的DStream，也不能结构上修改任何现有的DStream。
- en: Creating the Spark Streaming Context
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Spark Streaming Context
- en: The Spark Streaming Context has as a goal to keep tabs on the creation, provisioning,
    and processing of DStreams in the Spark cluster. As such, it creates jobs based
    on the Spark RDDs produced at each interval, and it keeps track of the DStream
    lineage.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming Context的目标是跟踪在Spark集群中创建、配置和处理DStreams。因此，它根据每个间隔产生的Spark RDD创建作业，并跟踪DStream的血统。
- en: 'To gain a clearer idea of this picture, we are going to have a look at how
    to create a Streaming Context to host our stream. The simplest way is, in the
    Spark shell, to wrap a streaming context around the Spark Context, which in the
    shell is available under the name `sc`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地了解这个情况，我们将看看如何创建一个流上下文来托管我们的流。在Spark shell中，最简单的方法是在Spark Context周围包装一个流上下文，而在shell中可通过名称`sc`访问它：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The Streaming Context is in charge of starting the ingestion process, which
    is deferred until the `start()` method is called on the corresponding `streamingContext`
    instance.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理上下文负责启动延迟到对应的`streamingContext`实例上调用`start()`方法的摄入过程。
- en: Warning
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'A frequent beginner’s mistake is to attempt to test Spark Streaming either
    by allocating a single core in local mode (`--master "local[1]"` ) or by launching
    it in a virtual machine with a single core: the consumption of data by the receiver
    would block any Spark processing on the same machine, resulting in a streaming
    job that does not progress.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 初学者常见的错误是尝试通过在本地模式中分配一个单核（`--master "local[1]"`）或者在单核虚拟机中启动Spark Streaming来测试，接收器消耗数据会阻塞同一机器上的任何Spark处理，导致流作业无法进展。
- en: Defining a DStream
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个DStream
- en: 'Let’s now declare a DStream that will listen for data over an arbitrary local
    port. A DStream on its own does not do anything. In the same way that RDDs need
    `actions` to materialize a computation, DStreams require the declaration of output
    operations in order to trigger the scheduling of the execution of the DStream
    transformations. In this example, we use the `count` transformation, which counts
    the number of elements received at each batch interval:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们声明一个 DStream，它将监听任意本地端口上的数据。单独的 DStream 不会做任何事情。与需要 `actions` 来实现计算的 RDD
    类似，DStreams 需要声明输出操作，以触发 DStream 转换执行的调度。在本例中，我们使用了 `count` 转换，它计算每个批次间隔接收到的元素数量：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Defining Output Operations
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义输出操作
- en: 'For this example, we use `print`, an output operation that outputs a small
    sample of elements in `DStream` at each batch interval:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们使用了 `print`，这是一个输出操作，每个批次间隔输出 `DStream` 中的少量元素样本：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, in a separate console, we can loop through our file of common last names
    and send it over a local TCP socket by using a small Bash script:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在另一个控制台中，我们可以循环遍历我们的姓氏常见文件，并通过使用一个小型 Bash 脚本将其发送到本地 TCP socket 中：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This iterates through the file we have and continuously loops over it, sending
    it infinitely over the TCP socket.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码遍历我们拥有的文件，并持续通过 TCP socket 无限发送。
- en: Starting the Spark Streaming Context
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动 Spark Streaming 上下文
- en: 'At this point, we have created a DStream, declared a simple `count` transformation,
    and used `print` as an output operation to observe the result. We also started
    our server socket data producer, which is looping over a file of names and sending
    each entry to a network socket. Yet, we don’t see any results. To materialize
    the transformations declared on the DStream into a running process, we need to
    start the Streaming Context, represented by `ssc`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经创建了一个 DStream，声明了一个简单的 `count` 转换，并使用 `print` 作为输出操作来观察结果。我们还启动了我们的服务器套接字数据生产者，它正在循环遍历一个名字文件，并将每个条目发送到网络套接字。然而，我们看不到任何结果。要将在
    DStream 上声明的转换物化为运行中的进程，我们需要启动 Streaming Context，表示为 `ssc`：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Stopping the Streaming Process
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停止流处理过程
- en: 'The final step in our initial Spark Streaming exploration is to stop the stream
    process. After the `streamingContext` is stopped, all DStreams declared within
    its scope are stopped and no further data is consumed:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们最初的 Spark Streaming 探索中的最后一步是停止流程。在停止 `streamingContext` 后，其范围内声明的所有 DStreams
    都将停止，并且不再消耗数据：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You cannot restart a stopped `streamingContext`. To restart a stopped job, the
    complete setup, from the point of creating the `streamingContext` instance needs
    to be re-executed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 无法重新启动已停止的 `streamingContext`。要重新启动已停止的作业，需要重新执行从创建 `streamingContext` 实例开始的完整设置。
- en: Summary
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced Spark Streaming as the first and most mature
    stream-processing API in Spark.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 Spark Streaming 作为 Spark 中第一个也是最成熟的流处理 API。
- en: 'We learned about DStreams: the core abstraction in Spark Streaming'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们了解了 DStreams：Spark Streaming 中的核心抽象。
- en: We explored the DStream functional API and applied it in our first application
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们探索了 DStream 的函数式 API，并在我们的第一个应用程序中应用它。
- en: We got a notion of the DStream model and how microbatches are defined by a time
    measure known as the interval
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们掌握了 DStream 模型的概念以及如何通过称为间隔的时间度量来定义微批次。
- en: In the next chapters, you gain a deeper understanding of the programming model
    and execution aspects in Spark Streaming.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，您将更深入地了解 Spark Streaming 中的编程模型和执行方面。
