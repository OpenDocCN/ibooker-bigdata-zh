- en: Chapter 5\. Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章 管道
- en: You will never walk again, but you will fly!
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你再也不会步行，但你会飞！
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Three-Eyed Raven
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —三眼乌鸦
- en: In [Chapter 4](ch04.html#modeling), you learned how to build predictive models
    using the high-level functions Spark provides and well-known R packages that work
    well together with Spark. You learned about supervised methods first and finished
    the chapter with an unsupervised method over raw text.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第四章](ch04.html#modeling)中，您学习了如何使用Spark提供的高级功能以及与Spark良好配合的知名R包来构建预测模型。您首先了解了监督方法，然后通过原始文本完成了该章节的无监督方法。
- en: In this chapter, we dive into Spark Pipelines, which is the engine that powers
    the features we demonstrated in [Chapter 4](ch04.html#modeling). So, for instance,
    when you invoke an `MLlib` function via the formula interface in R—for example,
    `ml_logistic_regression(cars, am ~ .)`—a *pipeline* is constructed for you under
    the hood. Therefore, Pipelines also allow you to make use of advanced data processing
    and modeling workflows. In addition, a pipeline also facilitates collaboration
    across data science and engineering teams by allowing you to *deploy* pipelines
    into production systems, web applications, mobile applications, and so on.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了Spark Pipelines，这是支持我们在[第四章](ch04.html#modeling)中演示功能的引擎。例如，当您通过R中的公式接口调用MLlib函数，例如`ml_logistic_regression(cars,
    am ~ .)`，则在幕后为您构建了一个*pipeline*。因此，管道还允许您利用高级数据处理和建模工作流程。此外，管道还通过允许您将管道部署到生产系统、Web应用程序、移动应用程序等，促进了数据科学和工程团队之间的协作。
- en: This chapter also happens to be the last chapter that encourages using your
    local computer as a Spark cluster. You are just one chapter away from getting
    properly introduced to cluster computing and beginning to perform data science
    or machine learning that can scale to the most demanding computation problems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 该章节也恰好是鼓励您将本地计算机作为Spark集群使用的最后一章。您只需再阅读一章，就可以开始执行能够扩展到最具挑战性计算问题的数据科学或机器学习。
- en: Overview
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'The building blocks of pipelines are objects called *transformers* and *estimators*,
    which are collectively referred to as *pipeline stages*. A *transformer* can be
    used to apply transformations to a DataFrame and return another DataFrame; the
    resulting DataFrame often comprises the original DataFrame with new columns appended
    to it. An *estimator*, on the other hand, can be used to create a transformer
    giving some training data. Consider the following example to illustrate this relationship:
    a “center and scale” estimator can learn the mean and standard deviation of some
    data and store the statistics in a resulting transformer object; this transformer
    can then be used to normalize the data that it was trained on and also any new,
    yet unseen, data.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的构建模块是称为*transformers*和*estimators*的对象，它们被统称为*pipeline stages*。*Transformer*用于对DataFrame应用转换并返回另一个DataFrame；结果DataFrame通常包含原始DataFrame及其附加的新列。另一方面，*estimator*可用于根据一些训练数据创建transformer。考虑以下示例来说明这种关系：一个“中心和缩放”的estimator可以学习数据的均值和标准差，并将统计信息存储在生成的transformer对象中；然后可以使用此transformer来规范化训练数据以及任何新的未见数据。
- en: 'Here is an example of how to define an estimator:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是定义estimator的示例：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now create some data (for which we know the mean and standard deviation)
    and then fit our scaling model to it using the `ml_fit()` function:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一些数据（我们知道其均值和标准差），然后使用`ml_fit()`函数将我们的缩放模型拟合到数据中：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In Spark ML, many algorithms and feature transformers require that the input
    be a vector column. The function `ft_vector_assembler()` performs this task. You
    can also use the function to initialize a transformer to be used in a pipeline.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark ML中，许多算法和特征转换器要求输入为向量列。函数`ft_vector_assembler()`执行此任务。您还可以使用该函数初始化一个用于pipeline中的transformer。
- en: 'We see that the mean and standard deviation are very close to 0 and 1, respectively,
    which is what we expect. We then can use the transformer to *transform* a DataFrame,
    using the `ml_transform()` function:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到均值和标准差非常接近0和1，这是我们预期的结果。然后我们可以使用transformer来*transform*一个DataFrame，使用`ml_transform()`函数：
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that you’ve seen basic examples of estimators and transformers, we can move
    on to pipelines.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了estimators和transformers的基本示例，我们可以继续讨论管道。
- en: Creation
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建
- en: A *pipeline* is simply a sequence of transformers and estimators, and a *pipeline
    model* is a pipeline that has been trained on data so all of its components have
    been converted to transformers.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*管道*只是一系列转换器和评估器，*管道模型*是在数据上训练过的管道，因此其所有组件都已转换为转换器。'
- en: There are a couple of ways to construct a pipeline in `sparklyr`, both of which
    use the `ml_pipeline()` function.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在`sparklyr`中构建管道的几种方法，都使用`ml_pipeline()`函数。
- en: 'We can initialize an empty pipeline with `ml_pipeline(sc)` and append stages
    to it:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用`ml_pipeline(sc)`初始化一个空管道，并将阶段追加到其中：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Alternatively, we can pass stages directly to `ml_pipeline()`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以直接将阶段传递给`ml_pipeline()`：
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We fit a pipeline as we would fit an estimator:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像拟合评估器一样拟合管道：
- en: '[PRE9]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As a result of the design of Spark ML, pipelines are always estimator objects,
    even if they comprise only transformers. This means that if you have a pipeline
    with only transformers, you still need to call `ml_fit()` on it to obtain a transformer.
    The “fitting” procedure in this case wouldn’t actually modify any of the transformers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark ML的设计，管道始终是评估器对象，即使它们只包含转换器。这意味着，如果您有一个只包含转换器的管道，仍然需要对其调用`ml_fit()`来获取转换器。在这种情况下，“拟合”过程实际上不会修改任何转换器。
- en: Use Cases
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用案例
- en: Now that you have an understanding of the rudimentary concepts for ML Pipelines,
    let’s apply them to the predictive modeling problem from the previous chapter
    in which we are trying to predict whether people are currently employed by looking
    at their profiles. Our starting point is the `okc_train` DataFrame with the relevant
    columns.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了ML Pipelines的基本概念，让我们将其应用于前一章节中的预测建模问题，我们试图通过查看其档案来预测人们当前是否就业。我们的起点是具有相关列的`okc_train`
    DataFrame。
- en: '[PRE12]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We first exhibit the pipeline, which includes feature engineering and modeling
    steps, and then walk through it:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先展示管道，包括特征工程和建模步骤，然后逐步介绍它：
- en: '[PRE13]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first three stages index the `sex`, `drinks`, and `drugs` columns, which
    are characters, into numeric indices via `ft_string_indexer()`. This is necessary
    for the `ft_one_hot_encoder_estimator()` that comes next, which requires numeric
    column inputs. When all of our predictor variables are of numeric type (recall
    that `age` is numeric already), we can create our features vector using `ft_vector_assembler()`,
    which concatenates all of its inputs together into one column of vectors. We can
    then use `ft_standard_scaler()` to normalize all elements of the features column
    (including the one-hot-encoded 0/1 values of the categorical variables), and finally
    apply a logistic regression via `ml_logistic_regression()`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 前三个阶段索引`sex`、`drinks`和`drugs`列，这些列是字符型，通过`ft_string_indexer()`将它们转换为数值索引。这对于接下来的`ft_one_hot_encoder_estimator()`是必要的，后者需要数值列输入。当所有预测变量都是数值类型时（回想一下`age`已经是数值型），我们可以使用`ft_vector_assembler()`创建特征向量，它将所有输入连接到一个向量列中。然后，我们可以使用`ft_standard_scaler()`来归一化特征列的所有元素（包括分类变量的独热编码0/1值），最后通过`ml_logistic_regression()`应用逻辑回归。
- en: 'During prototyping, you might want to execute these transformations *eagerly*
    on a small subset of the data, by passing the DataFrame to the `ft_` and `ml_`
    functions, and inspecting the transformed DataFrame. The immediate feedback allows
    for rapid iteration of ideas; when you have arrived at the desired processing
    steps, you can roll them up into a pipeline. For example, you can do the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在原型设计阶段，您可能希望将这些转换*急切地*应用于数据的一个小子集，通过将DataFrame传递给`ft_`和`ml_`函数，并检查转换后的DataFrame。即时反馈允许快速迭代想法；当您已经找到所需的处理步骤时，可以将它们整合成一个管道。例如，您可以执行以下操作：
- en: '[PRE14]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: After you have found the appropriate transformations for your dataset, you can
    replace the DataFrame input with `ml_pipeline(sc)`, and the result will be a pipeline
    that you can apply to any DataFrame with the appropriate schema. In the next section,
    we’ll see how pipelines can make it easier for us to test different model specifications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 找到数据集的适当转换后，您可以用`ml_pipeline(sc)`替换DataFrame输入，结果将是一个可以应用于具有适当架构的任何DataFrame的管道。在下一节中，我们将看到管道如何使我们更容易测试不同的模型规格。
- en: Hyperparameter Tuning
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整
- en: 'Going back to the pipeline we have created earlier, we can use `ml_cross_validator()`
    to perform the cross-validation workflow we demonstrated in the previous chapter
    and easily test different hyperparameter combinations. In this example, we test
    whether centering the variables improves predictions together with various regularization
    values for the logistic regression. We define the cross-validator as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们之前创建的管道，我们可以使用 `ml_cross_validator()` 来执行我们在前一章中演示的交叉验证工作流，并轻松测试不同的超参数组合。在这个例子中，我们测试了是否通过中心化变量以及逻辑回归的各种正则化值来改进预测。我们定义交叉验证器如下：
- en: '[PRE16]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `estimator` argument is simply the estimator that we want to tune, and
    in this case it is the `pipeline` that we defined. We provide the hyperparameter
    values we are interested in via the `estimator_param_maps` parameter, which takes
    a nested named list. The names at the first level correspond to UIDs, which are
    unique identifiers associated with each pipeline stage object, of the stages we
    want to tune (if a partial UID is provided, `sparklyr` will attempt to match it
    to a pipeline stage), and the names at the second level correspond to parameters
    of each stage. In the preceding snippet, we are specifying that we want to test
    the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`estimator` 参数就是我们要调整的估算器，本例中是我们定义的 `pipeline`。我们通过 `estimator_param_maps`
    参数提供我们感兴趣的超参数值，它接受一个嵌套的命名列表。第一级的名称对应于我们想要调整的阶段的唯一标识符（UID），这些 UID 是与每个管道阶段对象关联的唯一标识符（如果提供部分
    UID，`sparklyr` 将尝试将其与管道阶段匹配），第二级的名称对应于每个阶段的参数。在上面的片段中，我们正在指定我们要测试以下内容：'
- en: Standard scaler
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 标准缩放器
- en: The values `TRUE` and `FALSE` for `with_mean`, which denotes whether predictor
    values are centered.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 值 `TRUE` 和 `FALSE` 对于 `with_mean`，它表示预测值是否被居中。
- en: Logistic regression
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: The values `0.25` and `0.75` for <math><mi>α</mi></math> , and the values `1e-2`
    and `1e-3` for <math><mi>λ</mi></math> .
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 值 `0.25` 和 `0.75` 对于 <math><mi>α</mi></math> ，以及值 `1e-2` 和 `1e-3` 对于 <math><mi>λ</mi></math>
    。
- en: 'We expect this to give rise to <math><mrow><mn>2</mn> <mo>×</mo> <mn>2</mn>
    <mo>×</mo> <mn>2</mn> <mo>=</mo> <mn>8</mn></mrow></math> hyperparameter combinations,
    which we can confirm by printing the `cv` object:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计这将产生 <math><mrow><mn>2</mn> <mo>×</mo> <mn>2</mn> <mo>×</mo> <mn>2</mn>
    <mo>=</mo> <mn>8</mn></mrow></math> 种超参数组合，我们可以通过打印 `cv` 对象来确认：
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As with any other estimator, we can fit the cross-validator by using `ml_fit()`
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他估算器一样，我们可以使用 `ml_fit()` 来拟合交叉验证器
- en: '[PRE19]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'and then inspect the results:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后检查结果：
- en: '[PRE20]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now that we have seen the pipelines API in action, let’s talk more formally
    about how they behave in various contexts.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到管道 API 在操作中的样子，让我们更正式地讨论它们在各种上下文中的行为。
- en: Operating Modes
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作模式
- en: By now, you have likely noticed that the pipeline stage functions, such as `ft_string_indexer()`
    and `ml_logistic_regression()`, return different types of objects depending on
    the first argument passed to them. [Table 5-1](#table0501) presents the full pattern.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您可能已经注意到管道阶段函数（如 `ft_string_indexer()` 和 `ml_logistic_regression()`）根据传递给它们的第一个参数返回不同类型的对象。[表 5-1](#table0501)
    展示了完整的模式。
- en: Table 5-1\. Operating modes in machine learning functions
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-1\. 机器学习函数中的操作模式
- en: '| First argument | Returns | Example |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 第一个参数 | 返回值 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Spark connection | Estimator or transformer object | `ft_string_indexer(sc)`
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Spark 连接 | 估算器或转换器对象 | `ft_string_indexer(sc)` |'
- en: '| Pipeline | Pipeline | `ml_pipeline(sc) %>% ft_string_indexer()` |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 管道 | 管道 | `ml_pipeline(sc) %>% ft_string_indexer()` |'
- en: '| DataFrame, without formula | DataFrame | `ft_string_indexer(iris, "Species",
    "indexed")` |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 不带公式的 DataFrame | DataFrame | `ft_string_indexer(iris, "Species", "indexed")`
    |'
- en: '| DataFrame, with formula | sparklyr ML model object | `ml_logistic_regression(iris,
    Species ~ .)` |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 带公式的 DataFrame | sparklyr ML 模型对象 | `ml_logistic_regression(iris, Species
    ~ .)` |'
- en: 'These functions are implemented using [S3](https://adv-r.hadley.nz/s3.html),
    which is the most popular object-oriented programming paradigm provided by R.
    For our purposes, it suffices to know that the behavior of an `ml_` or `ft_` function
    is dictated by the class of the first argument provided. This allows us to provide
    a wide range of features without introducing additional function names. We now
    can summarize the behavior of these functions:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数是使用 [S3](https://adv-r.hadley.nz/s3.html) 实现的，它是 R 提供的最流行的面向对象编程范式。对于我们的目的，知道
    `ml_` 或 `ft_` 函数的行为由提供的第一个参数的类别决定就足够了。这使我们能够提供广泛的功能而不引入额外的函数名称。现在我们可以总结这些函数的行为：
- en: If a Spark connection is provided, the function returns a transformer or estimator
    object, which can be utilized directly using `ml_fit()` or `ml_transform()` or
    be included in a pipeline.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果提供了 Spark 连接，则该函数返回一个转换器或估计器对象，可以直接使用`ml_fit()`或`ml_transform()`，也可以包含在管道中。
- en: If a pipeline is provided, the function returns a pipeline object with the stage
    appended to it.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果提供了管道，则该函数返回一个具有附加到其中的阶段的管道对象。
- en: If a DataFrame is provided to a feature transformer function (those with prefix
    `ft_`), or an ML algorithm without also providing a formula, the function instantiates
    the pipeline stage object, fits it to the data if necessary (if the stage is an
    estimator), and then transforms the DataFrame returning a DataFrame.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果将 DataFrame 提供给特征转换器函数（带有前缀`ft_`）或不提供公式就提供 ML 算法，则该函数实例化管道阶段对象，如果必要的话（如果阶段是估计器），将其拟合到数据，然后转换
    DataFrame 并返回一个 DataFrame。
- en: If a DataFrame and a formula are provided to an ML algorithm that supports the
    formula interface, `sparklyr` builds a pipeline model under the hood and returns
    an ML model object that contains additional metadata information.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果将 DataFrame 和公式提供给支持公式接口的 ML 算法，`sparklyr`在后台构建一个管道模型，并返回一个包含附加元数据信息的 ML 模型对象。
- en: The formula interface approach is what we studied in [Chapter 4](ch04.html#modeling),
    and this is what we recommend users new to Spark start with, since its syntax
    is similar to existing R modeling packages and abstracts away some Spark ML peculiarities.
    However, to take advantage of the full power of Spark ML and leverage pipelines
    for workflow organization and interoperability, it is worthwhile to learn the
    ML Pipelines API.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 公式接口方法是我们在[第四章](ch04.html#modeling)中学习的内容，也是我们建议刚接触 Spark 的用户从这里开始的原因，因为它的语法类似于现有的
    R 建模包，并且可以摆脱一些 Spark ML 的特殊性。然而，要充分利用 Spark ML 的全部功能并利用管道进行工作流组织和互操作性，学习 ML 管道
    API 是值得的。
- en: With the basics of pipelines down, we are now ready to discuss the collaboration
    and model deployment aspects hinted at in the introduction of this chapter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握了管道的基础知识后，我们现在可以讨论在本章引言中提到的协作和模型部署方面的内容。
- en: Interoperability
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 互操作性
- en: 'One of the most powerful aspects of pipelines is that they can be serialized
    to disk and are fully interoperable with the other Spark APIs such as Python and
    Scala. This means that you can easily share them among users of Spark working
    in different languages, which might include other data scientists, data engineers,
    and deployment engineers. To save a pipeline model, call `ml_save()` and provide
    a path:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 管道最强大的一点之一是它们可以序列化到磁盘，并且与其他 Spark API（如 Python 和 Scala）完全兼容。这意味着您可以轻松地在使用不同语言的
    Spark 用户之间共享它们，这些用户可能包括其他数据科学家、数据工程师和部署工程师。要保存管道模型，请调用`ml_save()`并提供路径：
- en: '[PRE22]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s take a look at the directory to which we just wrote:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下刚写入的目录：
- en: '[PRE24]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can dive into a couple of the files to see what type of data was saved:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以深入几个文件，看看保存了什么类型的数据：
- en: '[PRE26]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We see that quite a bit of information has been exported, from the SQL statement
    in the `dplyr` transformer to the fitted coefficient estimates of the logistic
    regression. We can then (in a new Spark session) reconstruct the model by using
    `ml_load()`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到已导出了相当多的信息，从`dplyr`转换器中的 SQL 语句到逻辑回归的拟合系数估计。然后（在一个新的 Spark 会话中），我们可以使用`ml_load()`来重建模型：
- en: '[PRE30]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let’s see if we can retrieve the logistic regression stage from this pipeline
    model:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看是否可以从这个管道模型中检索逻辑回归阶段：
- en: '[PRE31]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note that the exported JSON and parquet files are agnostic of the API that exported
    them. This means that in a multilingual machine learning engineering team, you
    can pick up a data preprocessing pipeline from a data engineer working in Python,
    build a prediction model on top of it, and then hand off the final pipeline to
    a deployment engineering working in Scala. In the next section, we discuss deployment
    of models in more detail.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，导出的 JSON 和 parquet 文件与导出它们的 API 无关。这意味着在多语言机器学习工程团队中，您可以从使用 Python 的数据工程师那里获取数据预处理管道，构建一个预测模型，然后将最终管道交给使用
    Scala 的部署工程师。在下一节中，我们将更详细地讨论模型的部署。
- en: Note
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When `ml_save()` is called for `sparklyr` ML models (created using the formula
    interface), the associated pipeline model is saved, but any `sparklyr`-specific
    metadata, such as index labels, is not. In other words, saving a `sparklyr` `ml_model`
    object and then loading it will yield a pipeline model object, as if you created
    it via the ML Pipelines API. This behavior is required to use pipelines with other
    programming languages.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当为使用公式接口创建的`sparklyr` ML模型调用`ml_save()`时，关联的管道模型将被保存，但不会保存任何`sparklyr`特定的元数据，如索引标签。换句话说，保存一个`sparklyr`的`ml_model`对象，然后加载它将产生一个管道模型对象，就像您通过ML流水线API创建它一样。这种行为对于在其他编程语言中使用流水线是必需的。
- en: 'Before we move on to discuss how to run pipelines in production, make sure
    you disconnect from Spark:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续讨论如何在生产环境中运行流水线之前，请确保断开与Spark的连接：
- en: '[PRE33]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: That way, we can start from a brand new environment, which is also expected
    when you are deploying pipelines to production.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以从全新的环境开始，这也是在部署流水线时预期的情况。
- en: Deployment
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: 'What we have just demonstrated bears emphasizing: by collaborating within the
    framework of ML Pipelines, we reduce friction among different personas in a data
    science team. In particular, we cut down on the time from modeling to deployment.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚展示的值得强调的是：通过在ML流水线框架内协作，我们减少了数据科学团队中不同角色之间的摩擦。特别是，我们缩短了从建模到部署的时间。
- en: 'In many cases, a data science project does not end with just a slide deck with
    insights and recommendations. Instead, the business problem at hand might require
    scoring new data points on a schedule or on-demand in real time. For example,
    a bank might want to evaluate its mortgage portfolio risk nightly or provide instant
    decisions on credit card applications. This process of taking a model and turning
    it into a service that others can consume is usually referred to as *deployment*
    or *productionization*. Historically, there was a large gap between the analyst
    who built the model and the engineer who deployed it: the former might work in
    R and develop extensive documentation on the scoring mechanism, so that the latter
    can reimplement the model in C++ or Java. This practice, which might easily take
    months in some organizations, is less prevalent today, but is almost always unnecessary
    in Spark ML workflows.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，数据科学项目并不仅仅以幻灯片展示洞见和建议而告终。相反，手头的业务问题可能需要按计划或按需实时评分新数据点。例如，银行可能希望每晚评估其抵押贷款组合的风险或即时决策信用卡申请。将模型转化为其他人可以消费的服务的过程通常称为*部署*或*产品化*。历史上，在构建模型的分析师和部署模型的工程师之间存在很大的鸿沟：前者可能在R中工作，并且开发了关于评分机制的详尽文档，以便后者可以用C++或Java重新实现模型。这种做法在某些组织中可能需要数月时间，但在今天已经不那么普遍，而且在Spark
    ML工作流中几乎总是不必要的。
- en: The aforementioned nightly portfolio risk and credit application scoring examples
    represent two modes of ML deployment known as *batch* and *real time*. Loosely,
    batch processing implies processing many records at the same time, and that execution
    time is not important as long it is reasonable (often on the scale of minutes
    to hours). On the other hand, real-time processing implies scoring one or a few
    records at a time, but the latency is crucial (on the scale of <1 second). Let’s
    turn now to see how we can take our `OKCupid` pipeline model to “production.”
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的夜间投资组合风险和信用申请评分示例代表了两种机器学习部署模式，称为*批处理*和*实时*。粗略地说，批处理意味着同时处理许多记录，执行时间不重要，只要合理（通常在几分钟到几小时的范围内）。另一方面，实时处理意味着一次评分一个或几个记录，但延迟至关重要（在小于1秒的范围内）。现在让我们看看如何将我们的`OKCupid`流水线模型带入“生产”环境。
- en: Batch Scoring
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量评分
- en: For both scoring methods, batch and real time, we will expose our model as web
    services, in the form of an API over the Hypertext Transfer Protocol (HTTP). This
    is the primary medium over which software communicates. By providing an API, other
    services or end users can utilize our model without any knowledge of R or Spark.
    The [`plumber`](https://www.rplumber.io/) R package enables us to do this very
    easily by annotating our prediction function.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批处理和实时两种评分方法，我们将以Web服务的形式公开我们的模型，通过超文本传输协议（HTTP）的API提供。这是软件进行通信的主要媒介。通过提供API，其他服务或最终用户可以使用我们的模型，而无需了解R或Spark。[`plumber`](https://www.rplumber.io/)
    R包使我们可以通过注释我们的预测函数来轻松实现这一点。
- en: 'You will need make sure that `plumber`, `callr`, and the `httr` package are
    installed by running the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要确保通过运行以下命令安装`plumber`、`callr`和`httr`包：
- en: '[PRE34]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `callr` package provides support to run R code in separate R sessions; it
    is not strictly required, but we will use it to start a web service in the background.
    The `httr` package allows us to use web APIs from R.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`callr` 包支持在单独的 R 会话中运行 R 代码；虽然不是必需的，但我们将使用它来在后台启动 Web 服务。`httr` 包允许我们从 R 使用
    Web API。'
- en: 'In the batch scoring use case, we simply initiate a Spark connection and load
    the saved model. Save the following script as *plumber/spark-plumber.R*:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在批处理评分用例中，我们只需初始化一个 Spark 连接并加载保存的模型。将以下脚本保存为 *plumber/spark-plumber.R*：
- en: '[PRE35]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can then initialize the service by executing the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以通过执行以下操作来初始化服务：
- en: '[PRE36]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This starts the web service locally, and then we can query the service with
    new data to be scored; however, you might need to wait a few seconds for the Spark
    service to initialize:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这会在本地启动 Web 服务，然后我们可以用新数据查询服务进行评分；但是，您可能需要等待几秒钟以便 Spark 服务初始化：
- en: '[PRE37]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This reply informs us that this particular profile is likely to not be unemployed—that
    is, employed. We can now terminate the `plumber` service by stopping the `callr`
    service:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此回复告诉我们，这个特定的配置文件可能不会是失业的，即是受雇用的。现在我们可以通过停止 `callr` 服务来终止 `plumber` 服务：
- en: '[PRE39]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: If we were to time this operation (e.g., with `system.time()`), we see that
    the latency is on the order of hundreds of milliseconds, which might be appropriate
    for batch applications but is insufficient for real time. The main bottleneck
    is the serialization of the R DataFrame to a Spark DataFrame and back. Also, it
    requires an active Spark session, which is a heavy runtime requirement. To ameliorate
    these issues, we discuss next a deployment method more suitable for real-time
    deployment.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计时这个操作（例如使用 `system.time()`），我们会发现延迟在数百毫秒的数量级上，这对于批处理应用可能是合适的，但对于实时应用来说不够。主要瓶颈是将
    R DataFrame 序列化为 Spark DataFrame，然后再转换回来。此外，它需要一个活跃的 Spark 会话，这是一个重量级的运行时要求。为了改善这些问题，接下来我们讨论一个更适合实时部署的部署方法。
- en: Real-Time Scoring
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时评分
- en: For real-time production, we want to keep dependencies as light as possible
    so we can target more platforms for deployment. We now show how we can use the
    [`mleap`](http://bit.ly/2Z7jgSV) package, which provides an interface to the [MLeap](http://bit.ly/33G271R)
    library, to serialize and serve Spark ML models. MLeap is open source (Apache
    License 2.0) and supports a wide range of, though not all, Spark ML transformers.
    At runtime, the only prerequisites for the environment are the Java Virtual Machine
    (JVM) and the MLeap runtime library. This avoids both the Spark binaries and expensive
    overhead in converting data to and from Spark DataFrames.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实时生产，我们希望尽可能保持依赖项轻量化，以便可以针对更多平台进行部署。现在我们展示如何使用 [`mleap`](http://bit.ly/2Z7jgSV)
    包，它提供了一个接口给 [MLeap](http://bit.ly/33G271R) 库，用于序列化和提供 Spark ML 模型。MLeap 是开源的（Apache
    License 2.0），支持广泛的 Spark ML 转换器，尽管不是所有。在运行时，环境的唯一先决条件是 Java 虚拟机（JVM）和 MLeap 运行时库。这避免了
    Spark 二进制文件和昂贵的将数据转换为 Spark DataFrames 的开销。
- en: 'Since `mleap` is a `sparklyr` extension and an R package, first we need to
    install it from CRAN:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `mleap` 是 `sparklyr` 的扩展和一个 R 包，因此我们首先需要从 CRAN 安装它：
- en: '[PRE40]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'It then must be loaded when `spark_connect()` is called; so let’s restart your
    R session, establish a new Spark connection,^([1](ch05.html#idm46099151778536))
    and load the pipeline model that we previously saved:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用 `spark_connect()` 时，必须加载它；所以让我们重新启动您的 R 会话，建立一个新的 Spark 连接，并加载我们之前保存的管道模型：
- en: '[PRE41]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The way we save a model to MLeap bundle format is very similar to saving a
    model using the Spark ML Pipelines API; the only additional argument is `sample_input`,
    which is a Spark DataFrame with schema that we expect new data to be scored to
    have:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 保存模型到 MLeap bundle 格式的方法与使用 Spark ML Pipelines API 保存模型非常相似；唯一的附加参数是 `sample_input`，它是一个具有我们期望对新数据进行评分的模式的
    Spark DataFrame：
- en: '[PRE44]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can now deploy the artifact we just created, *mleap_model.zip*, in any device
    that runs Java and has the open source MLeap runtime dependencies, without needing
    Spark or R! In fact, we can go ahead and disconnect from Spark already:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在运行 Java 并具有开源 MLeap 运行时依赖项的任何设备上部署我们刚刚创建的文件 **mleap_model.zip**，而不需要
    Spark 或 R！事实上，我们可以继续断开与 Spark 的连接：
- en: '[PRE45]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Before we use this MLeap model, make sure the runtime dependencies are installed:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用此 MLeap 模型之前，请确保安装了运行时依赖项：
- en: '[PRE46]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'To test this model, we can create a new plumber API to expose it. The script
    *plumber/mleap-plumber.R* is very similar to the previous example:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试此模型，我们可以创建一个新的 plumber API 来公开它。脚本 *plumber/mleap-plumber.R* 与前面的示例非常相似：
- en: '[PRE47]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'And the way we launch the service is exactly the same:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 启动服务的方式也完全相同：
- en: '[PRE48]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We can run the exact same code we did previously to test unemployment predictions
    in this new service:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以运行与之前相同的代码来测试这个新服务中的失业预测：
- en: '[PRE49]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: If we were to time this operation, we would see that the service now returns
    predictions in tens of milliseconds.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计时这个操作，我们会看到现在服务在几十毫秒内返回预测结果。
- en: 'Let’s stop this service and then wrap up this chapter:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们停止这项服务，然后结束本章：
- en: '[PRE51]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Recap
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed Spark Pipelines, which is the engine behind the
    modeling functions introduced in [Chapter 4](ch04.html#modeling). You learned
    how to tidy up your predictive modeling workflows by organizing data processing
    and modeling algorithms into pipelines. You learned that pipelines also facilitate
    collaboration among members of a multilingual data science and engineering team
    by sharing a language-agnostic serialization format—you can export a Spark pipeline
    from R and let others reload your pipeline into Spark using Python or Scala, which
    allows them to collaborate without changing their language of choice.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了Spark管道，这是引擎在[第4章](ch04.html#modeling)介绍的建模功能背后的驱动力。您学会了通过将数据处理和建模算法组织到管道中来整理预测建模工作流程。您了解到管道还通过共享一种语言无关的序列化格式促进了多语言数据科学和工程团队成员之间的协作——您可以从R导出一个Spark管道，让其他人在Python或Scala中重新加载您的管道到Spark中，这使他们可以在不改变自己选择的语言的情况下进行协作。
- en: You also learned how to deploy pipelines using `mleap`, a Java runtime that
    provides another path to productionize Spark models—you can export a pipeline
    and integrate it to Java-enabled environments without requiring the target environment
    to support Spark or R.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您还学会了如何使用`mleap`部署管道，这是一个提供另一种将Spark模型投入生产的Java运行时——您可以导出管道并将其集成到支持Java的环境中，而不需要目标环境支持Spark或R。
- en: You probably noticed that some algorithms, especially the unsupervised learning
    kind, were slow, even for the `OKCupid` dataset, which can be loaded into memory.
    If we had access to a proper Spark cluster, we could spend more time modeling
    and less time waiting! Not only that, but we could use cluster resources to run
    broader hyperparameter-tuning jobs and process large datasets. To get there, [Chapter 6](ch06.html#clusters)
    presents what exactly a computing cluster is and explains the various options
    you can consider, like building your own or using cloud clusters on demand.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，一些算法，尤其是无监督学习类型的算法，对于可以加载到内存中的`OKCupid`数据集来说，速度较慢。如果我们能够访问一个合适的Spark集群，我们就可以花更多时间建模，而不是等待！不仅如此，我们还可以利用集群资源来运行更广泛的超参数调整作业和处理大型数据集。为了达到这个目标，[第6章](ch06.html#clusters)介绍了计算集群的具体内容，并解释了可以考虑的各种选项，如建立自己的集群或按需使用云集群。
- en: ^([1](ch05.html#idm46099151778536-marker)) As of the writing of this book, MLeap
    does not support Spark 2.4.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.html#idm46099151778536-marker)) 截至本书撰写时，MLeap不支持Spark 2.4。
