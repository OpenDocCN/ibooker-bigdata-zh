- en: Chapter 9\. Structured Streaming in Action
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 结构化流实战
- en: Now that we have a better understanding of the Structured Streaming API and
    programming model, in this chapter, we create a small but complete Internet of
    Things (IoT)-inspired streaming program.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们更好地理解了结构化流API和编程模型，在本章中，我们创建了一个小而完整的受物联网（IoT）启发的流程程序。
- en: Online Resources
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线资源
- en: For this example, we will use the `Structured-Streaming-in-action` notebook
    in the online resources for the book, located on [*https://github.com/stream-processing-with-spark*](https://github.com/stream-processing-with-spark).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，我们将使用书籍在线资源中的`Structured-Streaming-in-action`笔记本，位于[*https://github.com/stream-processing-with-spark*](https://github.com/stream-processing-with-spark)。
- en: Our use case will be to consume a stream of sensor readings from Apache Kafka
    as the streaming source.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的用例将是作为流源从Apache Kafka消耗传感器读数流。
- en: We are going to correlate incoming IoT sensor data with a static reference file
    that contains all known sensors with their configuration. That way, we enrich
    each incoming record with specific sensor parameters that we require to process
    the reported data. We then save all correctly processed records to a file in Parquet
    format.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将会将传入的IoT传感器数据与包含所有已知传感器及其配置的静态参考文件进行关联。通过这种方式，我们会为每个传入的记录添加特定的传感器参数，以便处理报告的数据。然后，我们将所有处理正确的记录保存到Parquet格式的文件中。
- en: Apache Kafka
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Kafka
- en: 'Apache Kafka is one of the most popular choices for a scalable messaging broker
    that is used to decouple producers from consumers in an event-driven system. It
    is is a highly scalable distributed streaming platform based on the abstraction
    of a distributed commit log. It provides functionality similar to message queues
    or enterprise messaging systems but differentiates from its predecessors in three
    important areas:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka 是最流行的可扩展消息代理之一，用于在事件驱动系统中解耦生产者和消费者。它是基于分布式提交日志抽象的高度可伸缩的分布式流平台。它提供类似于消息队列或企业消息系统的功能，但在以下三个重要领域与其前身有所不同：
- en: Runs are distributed on a commodity cluster, making it highly scalable.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行在商品集群上，使其具有高度可伸缩性。
- en: Fault-tolerant data storage guarantees consistency of data reception and delivery.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容错数据存储确保数据接收和传递的一致性。
- en: Pull-based consumers allow consumption of the data at a different time and pace,
    from real time, to microbatch, to batch, creating the possibility of feeding data
    to a wide range of applications.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉取型消费者允许在不同的时间和速率下消费数据，从实时到微批到批处理，从而为各种应用程序提供数据提供的可能性。
- en: You can find Kafka at [*http://kafka.apache.org*](http://kafka.apache.org).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[*http://kafka.apache.org*](http://kafka.apache.org)找到Kafka。
- en: Consuming a Streaming Source
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消费流源
- en: 'The first part of our program deals with the creation of the streaming `Dataset`:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们程序的第一部分涉及创建流`Dataset`：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The entry point of Structured Streaming is an existing Spark Session (`sparkSession`).
    As you can appreciate on the first line, the creation of a streaming `Dataset`
    is almost identical to the creation of a static `Dataset` that would use a `read`
    operation instead. `sparkSession.readStream` returns a `DataStreamReader`, a class
    that implements the builder pattern to collect the information needed to construct
    the streaming source using a *fluid* API. In that API, we find the `format` option
    that lets us specify our source provider, which, in our case, is `kafka`. The
    options that follow it are specific to the source:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流的入口是现有的Spark Session（`sparkSession`）。正如您在第一行所看到的那样，创建流`Dataset`几乎与创建使用`read`操作的静态`Dataset`相同。`sparkSession.readStream`返回一个`DataStreamReader`，这是一个实现构建器模式以收集构建流源所需信息的类，使用*流畅*API。在该API中，我们找到了`format`选项，让我们指定我们的源提供程序，而在我们的情况下，这是`kafka`。随后的选项特定于源：
- en: '`kafka.bootstrap.servers`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`kafka.bootstrap.servers`'
- en: Indicates the set of bootstrap servers to contact as a comma-separated list
    of `host:port` addresses
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 指示要联系的引导服务器集的集合，作为逗号分隔的`host:port`地址列表
- en: '`subscribe`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`subscribe`'
- en: Specifies the topic or topics to subscribe to
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 指定要订阅的主题或主题集
- en: '`startingOffsets`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`startingOffsets`'
- en: The offset reset policy to apply when this application starts out fresh
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当此应用程序刚开始运行时应用的偏移复位策略
- en: We cover the details of the Kafka streaming provider later in [Chapter 10](ch10.xhtml#ss-sources).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第10章](ch10.xhtml#ss-sources)中详细介绍Kafka流处理提供程序。
- en: 'The `load()` method evaluates the `DataStreamReader` builder and creates a
    `DataFrame` as a result, as we can see in the returned value:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`load()` 方法评估 `DataStreamReader` 构建器，并创建一个 `DataFrame` 作为结果，正如我们在返回的值中看到的那样：'
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A `DataFrame` is an alias for `Dataset[Row]` with a known schema. After creation,
    you can use streaming `Dataset`s just like regular `Dataset`s. This makes it possible
    to use the full-fledged `Dataset` API with Structured Streaming, albeit some exceptions
    apply because not all operations, such as `show()` or `count()`, make sense in
    a streaming context.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `DataFrame` 是 `Dataset[Row]` 的别名，并且有已知的模式。创建后，您可以像常规的 `Dataset` 一样使用流 `Dataset`。这使得在结构化流中可以使用完整的
    `Dataset` API，尽管某些例外情况存在，因为并非所有操作（例如 `show()` 或 `count()`）在流上下文中都有意义。
- en: 'To programmatically differentiate a streaming `Dataset` from a static one,
    we can ask a `Dataset` whether it is of the streaming kind:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要以编程方式区分流 `Dataset` 和静态 `Dataset`，我们可以询问 `Dataset` 是否属于流类型：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And we can also explore the schema attached to it, using the existing `Dataset`
    API, as demonstrated in [Example 9-1](#kafka-schema).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用现有的 `Dataset` API 探索附加到其上的模式，正如在 [示例 9-1](#kafka-schema) 中所示。
- en: Example 9-1\. The Kafka schema
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-1\. Kafka 架构
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In general, Structured Streaming requires the explicit declaration of a schema
    for the consumed stream. In the specific case of `kafka`, the schema for the resulting
    `Dataset` is fixed and is independent of the contents of the stream. It consists
    of a set of fields specific to the Kakfa `source`: `key`, `value`, `topic`, `partition`,
    `offset`, `timestamp`, and `timestampType`, as we can see in [Example 9-1](#kafka-schema).
    In most cases, applications will be mostly interested in the contents of the `value`
    field where the actual payload of the stream resides.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，结构化流需要对消费流的模式进行明确声明。在 `kafka` 的特定情况下，结果 `Dataset` 的模式是固定的，与流的内容无关。它由一组字段组成，特定于
    Kakfa `source`：`key`、`value`、`topic`、`partition`、`offset`、`timestamp` 和 `timestampType`，正如我们在
    [示例 9-1](#kafka-schema) 中看到的那样。在大多数情况下，应用程序将主要关注流的实际负载所在的 `value` 字段的内容。
- en: Application Logic
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用逻辑
- en: Recall that the intention of our job is to correlate the incoming IoT sensor
    data with a reference file that contains all known sensors with their configuration.
    That way, we would enrich each incoming record with specific sensor parameters
    that would allow us to interpret the reported data. We would then save all correctly
    processed records to a Parquet file. The data coming from unknown sensors would
    be saved to a separate file for later analysis.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的工作意图是将传入的物联网传感器数据与包含所有已知传感器及其配置的参考文件相关联。这样，我们将丰富每个传入记录的特定传感器参数，以便解释报告的数据。然后，我们将所有处理正确的记录保存到一个
    Parquet 文件中。来自未知传感器的数据将保存到一个单独的文件中，以供后续分析。
- en: 'Using Structured Streaming, our job can be implemented in terms of `Dataset`
    operations:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用结构化流，我们的工作可以通过 `Dataset` 操作来实现：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the first step, we transform our CSV-formatted records back into `SensorData`
    entries. We apply Scala functional operations on the typed `Dataset[String]` that
    we obtained from extracting the `value` field as a `String`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们将 CSV 格式的记录转换回 `SensorData` 条目。我们在从 `value` 字段提取的 `String` 类型的 `Dataset[String]`
    上应用 Scala 函数操作。
- en: Then, we use a streaming `Dataset` to static `Dataset` `inner join` to correlate
    the sensor data with the corresponding reference using the `sensorId` as key.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用流 `Dataset` 对静态 `Dataset` 进行 `inner join`，以将传感器数据与相应的参考数据关联，使用 `sensorId`
    作为键。
- en: To complete our application, we compute the real values of the sensor reading
    using the minimum-maximum ranges in the reference data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成我们的应用程序，我们使用参考数据中的最小-最大范围来计算传感器读数的实际值。
- en: Writing to a Streaming Sink
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 写入流式接收器
- en: 'The final step of our streaming application is to write the enriched IoT data
    to a Parquet-formatted file. In Structured Streaming, the `write` operation is
    crucial: it marks the completion of the declared transformations on the stream,
    defines a *write mode*, and upon calling start(), the processing of the continuous
    query will begin.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们流应用的最后一步是将丰富的物联网数据写入 Parquet 格式的文件。在结构化流中，`write` 操作至关重要：它标志着对流上声明的转换的完成，定义了一个
    *写模式*，并在调用 start() 后，连续查询的处理将开始。
- en: 'In Structured Streaming, all operations are lazy declarations of what we want
    to do with the streaming data. Only when we call `start()` will the actual consumption
    of the stream begin and the query operations on the data materialize into actual
    results:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化流处理中，所有操作都是对我们希望对流数据进行的操作的延迟声明。只有当我们调用`start()`时，流的实际消费才会开始，并且对数据的查询操作会实现为实际结果：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s break this operation down:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细分解这个操作：
- en: '`writeStream` creates a builder object where we can configure the options for
    the desired write operation, using a *fluent* interface.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`writeStream` 创建一个构建器对象，在这里我们可以使用*流畅*接口配置所需写入操作的选项。'
- en: With `format`, we specify the sink that will materialize the result downstream.
    In our case, we use the built-in `FileStreamSink` with Parquet format.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`format`，我们指定了将结果材料化到下游的接收器。在我们的情况下，我们使用内置的`FileStreamSink`和Parquet格式。
- en: '`mode` is a new concept in Structured Streaming: given that we, theoretically,
    have access to all the data seen in the stream so far, we also have the option
    to produce different views of that data.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode` 是结构化流处理中的一个新概念：理论上，我们可以访问到迄今为止在流中看到的所有数据，因此我们也有选项来生成该数据的不同视图。'
- en: The `append` mode, used here, implies that the new records affected by our streaming
    computation are produced to the output.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里使用的`append`模式意味着我们的流处理计算产生的新记录会被输出。
- en: The result of the `start` call is a `StreamingQuery` instance. This object provides
    methods to control the execution of the query and request information about the
    status of our running streaming query, as shown in [Example 9-2](#query-recent-progress).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`start`的结果是一个`StreamingQuery`实例。该对象提供了控制查询执行和请求有关我们正在运行的流查询状态信息的方法，正如示例 [9-2](#query-recent-progress)
    所示。
- en: Example 9-2\. Query progress
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-2\. 查询进度
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In [Example 9-2](#query-recent-progress), we can see the `StreamingQueryProgress`
    as a result of calling `knownSensorsQuery.recentProgress`. If we see nonzero values
    for the `numInputRows`, we can be certain that our job is consuming data. We now
    have a Structured Streaming job running properly.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 9-2](#query-recent-progress) 中，通过调用`knownSensorsQuery.recentProgress`，我们可以看到`StreamingQueryProgress`的结果。如果`numInputRows`有非零值，我们可以确定我们的作业正在消费数据。我们现在有一个正常运行的结构化流处理作业。
- en: Summary
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Hopefully, this hands-on chapter has shown you how to create your first nontrivial
    application using Structured Streaming.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 希望通过这个实践章节，能够向你展示如何使用结构化流处理创建你的第一个非平凡应用程序。
- en: After reading this chapter, you should have a better understanding of the structure
    of a Structured Streaming program and how to approach a streaming application,
    from consuming the data, to processing it using the `Dataset` and `DataFrames`
    APIs, to producing the data to an external output. At this point, you should be
    just about ready to take on the adventure of creating your own streaming processing
    jobs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，你应该更好地理解了结构化流处理程序的结构以及如何处理流应用程序，从消费数据、使用`Dataset`和`DataFrames` API进行处理，到将数据输出到外部。此时，你几乎准备好迎接创建自己的流处理作业的挑战了。
- en: In the next chapters, you learn in depth the different aspects of Structured
    Streaming.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将深入学习结构化流处理的不同方面。
