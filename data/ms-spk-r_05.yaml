- en: Chapter 4\. Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 模型化
- en: I’ve trusted in your visions, in your prophecies, for years.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 多年来，我一直相信你的幻象和预言。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Stannis Baratheon
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —斯坦尼斯·拜拉席恩
- en: In [Chapter 3](ch03.html#analysis) you learned how to scale up data analysis
    to large datasets using Spark. In this chapter, we detail the steps required to
    build prediction models in Spark. We explore `MLlib`, the component of Spark that
    allows you to write high-level code to perform predictive modeling on distributed
    data, and use data wrangling in the context of feature engineering and exploratory
    data analysis.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html#analysis)中，您学习了如何使用Spark扩展数据分析到大数据集。在本章中，我们详细介绍了在Spark中构建预测模型所需的步骤。我们探索了`MLlib`，这是Spark的一个组件，允许您编写高级代码在分布式数据上执行预测建模，并在特征工程和探索性数据分析的上下文中使用数据整理。
- en: We will start this chapter by introducing modeling in the context of Spark and
    the dataset you will use throughout the chapter. We then demonstrate a supervised
    learning workflow that includes exploratory data analysis, feature engineering,
    and model building. Then we move on to an unsupervised topic modeling example
    using unstructured text data. Keep in mind that our goal is to show various techniques
    of executing data science tasks on large data rather than conducting a rigorous
    and coherent analysis. There are also many other models available in Spark that
    won’t be covered in this chapter, but by the end of the chapter, you will have
    the right tools to experiment with additional ones on your own.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从介绍Spark中的建模和本章节中将使用的数据集开始。然后我们展示一个包括探索性数据分析、特征工程和模型构建的监督学习工作流程。接着，我们将演示一个使用非结构化文本数据的无监督主题建模示例。请记住，我们的目标是展示在大数据上执行数据科学任务的各种技术，而不是进行严格和连贯的分析。在本章中，还有许多其他Spark中可用但本章未涵盖的模型，但到了本章结束时，您将具备自行尝试其他模型的正确工具。
- en: While predicting datasets manually is often a reasonable approach (by “manually,”
    we mean someone imports a dataset into Spark and uses the fitted model to enrich
    or predict values), it does beg the question, could we automate this process into
    systems that anyone can use? For instance, how can we build a system that automatically
    identifies an email as spam without having to manually analyze each email account?
    [Chapter 5](ch05.html#pipelines) presents the tools to automate data analysis
    and modeling with pipelines, but to get there, we need to first understand how
    to train models “by hand.”
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然手动预测数据集通常是一个合理的方法（“手动”指的是将数据集导入Spark并使用已拟合的模型来丰富或预测值），但这引出了一个问题，我们是否可以将这一过程自动化到任何人都可以使用的系统中？例如，我们如何构建一个系统，可以自动识别电子邮件是否为垃圾邮件，而无需手动分析每个电子邮件账户？[第5章](ch05.html#pipelines)
    提供了用于通过流水线自动化数据分析和建模的工具，但要实现这一目标，我们首先需要了解如何“手动”训练模型。
- en: Overview
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概览
- en: The R interface to Spark provides modeling algorithms that should be familiar
    to R users. For instance, we’ve already used `ml_linear_regression(cars, mpg ~
    .)`, but we could just as easily run `ml_logistic_regression(cars, am ~ .)`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: R接口到Spark提供了模型算法，这对R用户来说应该是熟悉的。例如，我们已经使用了 `ml_linear_regression(cars, mpg ~
    .)`，但我们同样可以运行 `ml_logistic_regression(cars, am ~ .)`。
- en: Take a moment to look at the long list of `MLlib` functions included in the
    appendix of this book; a quick glance at this list shows that Spark supports Decision
    Trees, Gradient-Boosted Trees, Accelerated Failure Time Survival Regression, Isotonic
    Regression, *K*-Means Clustering, Gaussian Mixture Clustering, and more.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 请花点时间查看本书附录中包含的`MLlib`函数的长列表；快速浏览此列表可以看出，Spark支持决策树、梯度提升树、加速失效时间生存回归、等距回归、*K*-均值聚类、高斯混合聚类等等。
- en: 'As you can see, Spark provides a wide range of algorithms and feature transformers,
    and here we touch on a representative portion of the functionality. A complete
    treatment of predictive modeling concepts is beyond the scope of this book, so
    we recommend complementing this discussion with [*R for Data Science*](https://r4ds.had.co.nz/)
    by Hadley Wickham and Garrett Grolemund G (O’Reilly) and *Feature Engineering
    and Selection: A Practical Approach for Predictive Models*,^([1](ch04.html#idm46099157208232))
    from which we adopted (sometimes verbatim) some of the examples and visualizations
    in this chapter.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '如您所见，Spark 提供了广泛的算法和特征转换器，在这里我们涉及了功能的代表部分。对预测建模概念的全面讨论超出了本书的范围，因此我们建议结合 Hadley
    Wickham 和 Garrett Grolemund G（O''Reilly）的[*R for Data Science*](https://r4ds.had.co.nz/)以及*Feature
    Engineering and Selection: A Practical Approach for Predictive Models*^([1](ch04.html#idm46099157208232))，本章中的一些示例和可视化有时候是直接引用的。'
- en: This chapter focuses on predictive modeling, since Spark aims to enable machine
    learning as opposed to statistical inference. Machine learning is often more concerned
    about forecasting the future rather than inferring the process by which our data
    is generated,^([2](ch04.html#idm46099157204216)) which is then used to create
    automated systems. Machine learning can be categorized into *supervised learning*
    (predictive modeling) and *unsupervised learning*. In supervised learning, we
    try to learn a function that will map from X to Y, from a dataset of (x, y) examples.
    In unsupervised learning, we just have X and not the Y labels, so instead we try
    to learn something about the structure of X. Some practical use cases for supervised
    learning include forecasting tomorrow’s weather, determining whether a credit
    card transaction is fraudulent, and coming up with a quote for your car insurance
    policy. With unsupervised learning, examples include automated grouping of photos
    of individuals, segmenting customers based on their purchase history, and clustering
    of documents.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍预测建模，因为 Spark 的目标是实现机器学习，而不是统计推断。机器学习通常更关注预测未来，而不是推断生成数据的过程^([2](ch04.html#idm46099157204216))，然后用于创建自动化系统。机器学习可分为*监督学习*（预测建模）和*无监督学习*。在监督学习中，我们试图学习一个函数，将从数据集中的(x,
    y)示例映射到Y。在无监督学习中，我们只有X而没有Y标签，因此我们尝试学习有关X结构的信息。监督学习的一些实际用例包括预测明天的天气、确定信用卡交易是否欺诈以及为您的汽车保险报价。无监督学习的例子包括自动分组个人照片、基于购买历史对客户进行分段以及文档聚类。
- en: The ML interface in `sparklyr` has been designed to minimize the cognitive effort
    for moving from a local, in-memory, native-R workflow to the cluster, and back.
    While the Spark ecosystem is very rich, there is still a tremendous number of
    packages from CRAN, with some implementing functionality that you might require
    for a project. Also, you might want to leverage your skills and experience working
    in R to maintain productivity. What we learned in [Chapter 3](ch03.html#analysis)
    also applies here—it is important to keep track of where you are performing computations
    and move between the cluster and your R session as appropriate.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`sparklyr` 中的 ML 接口旨在最小化从本地内存中的本机 R 工作流程到集群的认知工作量，以及反向过程。虽然 Spark 生态系统非常丰富，但
    CRAN 中仍然有大量的软件包，其中一些实现了您可能需要的功能。此外，您可能希望利用您在 R 中的技能和经验来保持生产力。我们在[第三章](ch03.html#analysis)中学到的内容在这里同样适用——重要的是要跟踪您执行计算的位置，并根据需要在集群和您的
    R 会话之间移动。'
- en: The examples in this chapter utilize the [`OkCupid` dataset](https://oreil.ly/Uv9r_).^([3](ch04.html#idm46099157194104))
    The dataset consists of user profile data from an online dating site and contains
    a diverse set of features, including biographical characteristics such as gender
    and profession, as well as free text fields related to personal interests. There
    are about 60,000 profiles in the dataset, which fits comfortably into memory on
    a modern laptop and wouldn’t be considered “big data,” so you can easily follow
    along running Spark in local mode.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例使用了[`OkCupid`数据集](https://oreil.ly/Uv9r_)^([3](ch04.html#idm46099157194104))。该数据集包含来自在线约会网站的用户资料数据，包括性别、职业等生物特征以及与个人兴趣相关的自由文本字段。数据集中约有60,000个用户资料，可以轻松地在现代笔记本电脑的内存中运行，不被视为“大数据”，因此您可以轻松地在本地模式下运行
    Spark 进行跟踪。
- en: 'You can download this dataset as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按以下方式下载此数据集：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We don’t recommend sampling this dataset since the model won’t be nearly as
    rich; however, if you have limited hardware resources, you are welcome to sample
    it as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不建议对该数据集进行抽样，因为模型将远不如丰富；但是，如果您的硬件资源有限，您可以按以下方式对其进行抽样：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The examples in this chapter utilize small datasets so that you can easily follow
    along in local mode. In practice, if your dataset fits comfortably in memory on
    your local machine, you might be better off using an efficient, nondistributed
    implementation of the modeling algorithm. For example, you might want to use the
    `ranger` package instead of `ml_random_forest_classifier()`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的示例使用了小型数据集，以便您可以轻松在本地模式下跟踪。 实际上，如果您的数据集在本地机器的内存中能轻松容纳，您可能最好使用一种高效的、非分布式的建模算法实现。
    例如，您可能想使用`ranger`包，而不是`ml_random_forest_classifier()`。
- en: 'In addition, to follow along, you will need to install a few additional packages:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了跟进，您需要安装一些额外的包：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To motivate the examples, we consider the following problem:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明示例的动机，我们考虑以下问题：
- en: Predict whether someone is actively working—that is, not retired, a student,
    or unemployed.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 预测某人目前是否在积极工作——即不是退休、学生或失业。
- en: Next up, we explore this dataset.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来探索这个数据集。
- en: Exploratory Data Analysis
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: 'Exploratory data analysis (EDA), in the context of predictive modeling, is
    the exercise of looking at excerpts and summaries of the data. The specific goals
    of the EDA stage are informed by the business problem, but here are some common
    objectives:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测建模的背景下，探索性数据分析（EDA）是查看数据摘录和摘要的练习。 EDA阶段的具体目标受业务问题的启发，但以下是一些常见目标：
- en: Check for data quality; confirm meaning and prevalence of missing values and
    reconcile statistics against existing controls.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查数据质量；确认缺失值的含义和普遍性，并将统计数据与现有控制进行调和。
- en: Understand univariate relationships between variables.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解变量之间的单变量关系。
- en: Perform an initial assessment on what variables to include and what transformations
    need to be done on them.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对要包括的变量进行初步评估，以及对它们要进行的变换进行评估。
- en: 'To begin, we connect to Spark, load libraries, and read in the data:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们连接到Spark，加载库，并读取数据：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We specify `escape = "\""` and `options = list(multiline = TRUE)` here to accommodate
    embedded quote characters and newlines in the essay fields. We also convert the
    `height` and `income` columns to numeric types and recode missing values in the
    string columns. Note that it might very well take a few tries of specifying different
    parameters to get the initial data ingest correct, and sometimes you might need
    to revisit this step after you learn more about the data during modeling.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里指定了`escape = "\""`和`options = list(multiline = TRUE)`，以适应论文字段中嵌入的引号字符和换行符。
    我们还将`height`和`income`列转换为数值类型，并重新编码字符串列中的丢失值。 请注意，可能需要多次尝试指定不同的参数才能正确进行初始数据摄取，有时在您在建模过程中了解更多关于数据后，您可能需要重新访问这一步。
- en: 'We can now take a quick look at our data by using `glimpse()`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`glimpse()`来快速查看我们的数据：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we add our response variable as a column in the dataset and look at its
    distribution:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将我们的响应变量作为数据集中的一列，并查看其分布：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Before we proceed further, let’s perform an initial split of our data into
    a training set and a testing set and put away the latter. In practice, this is
    a crucial step because we would like to have a holdout set that we set aside at
    the end of the modeling process to evaluate model performance. If we were to include
    the entire dataset during EDA, information from the testing set could “leak” into
    the visualizations and summary statistics and bias our model-building process
    even though the data is not used directly in a learning algorithm. This would
    undermine the credibility of our performance metrics. We can easily split the
    data by using the `sdf_random_split()` function:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步进行之前，让我们将数据进行初步分割，分为训练集和测试集，并将后者放在一边。 在实践中，这是一个关键的步骤，因为我们希望在建模过程的最后留出一个留置集，以评估模型性能。
    如果我们在EDA期间包含整个数据集，测试集的信息可能会“泄漏”到可视化和摘要统计数据中，并且偏向我们的模型构建过程，即使数据并没有直接用在学习算法中。 这将损害我们性能指标的可信度。
    我们可以使用`sdf_random_split()`函数轻松地将数据进行分割：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can quickly look at the distribution of our response variable:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以快速查看我们响应变量的分布：
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Using the `sdf_describe()` function, we can obtain numerical summaries of specific
    columns:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sdf_describe()`函数，我们可以获得特定列的数值摘要：
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Like we saw in [Chapter 3](ch03.html#analysis), we can also utilize the `dbplot`
    package to plot distributions of these variables. In [Figure 4-1](#age-histogram)
    we show a histogram of the distribution of the `age` variable, which is the result
    of the following code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第三章](ch03.html#analysis)中所见，我们也可以利用` dbplot `包来绘制这些变量的分布。在[图 4-1](#age-histogram)中，我们展示了`
    age `变量的直方图分布，其代码如下所示：
- en: '[PRE13]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Distribution of age](assets/mswr_0401.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![年龄分布](assets/mswr_0401.png)'
- en: Figure 4-1\. Distribution of age
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 年龄分布
- en: 'A common EDA exercise is to look at the relationships between the response
    and the individual predictors. Often, you might have prior business knowledge
    of what these relationships should be, so this can serve as a data quality check.
    Also, unexpected trends can inform variable interactions that you might want to
    include in the model. As an example, we can explore the `religion` variable:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的探索性数据分析（EDA）练习是查看响应变量和各个预测变量之间的关系。通常情况下，您可能已经具备了这些关系应该是什么样子的业务知识，因此这可以作为数据质量检查。此外，意外的趋势可以提示您可能希望在模型中包含的变量交互。例如，我们可以探索`
    religion `变量：
- en: '[PRE14]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Note that `prop_data` is a small DataFrame that has been collected into memory
    in our R session, we can take advantage of `ggplot2` to create an informative
    visualization (see [Figure 4-2](#modeling-eda-prop-render)):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，` prop_data `是一个小的DataFrame，已经在我们的R会话中收集到内存中，我们可以利用` ggplot2 `创建一个信息丰富的可视化（参见[图 4-2](#modeling-eda-prop-render)）：
- en: '[PRE16]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Proportion of individuals not currently employed, by religion](assets/mswr_0402.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![不同宗教信仰的当前失业者比例](assets/mswr_0402.png)'
- en: Figure 4-2\. Proportion of individuals not currently employed, by religion
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 不同宗教信仰的当前失业者比例
- en: 'Next, we take a look at the relationship between a couple of predictors: alcohol
    use and drug use. We would expect there to be some correlation between them. You
    can compute a contingency table via `sdf_crosstab()`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一下两个预测变量之间的关系：酒精使用和药物使用。我们预计它们之间会存在一定的相关性。您可以通过` sdf_crosstab() `计算一个列联表：
- en: '[PRE17]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can visualize this contingency table using a mosaic plot (see [Figure 4-3](#modeling-eda-mosaic)):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用马赛克图可视化这个列联表（参见[图 4-3](#modeling-eda-mosaic)）：
- en: '[PRE19]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Mosaic plot of drug and alcohol use](assets/mswr_0403.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![药物和酒精使用的马赛克图](assets/mswr_0403.png)'
- en: Figure 4-3\. Mosaic plot of drug and alcohol use
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 药物和酒精使用的马赛克图
- en: 'To further explore the relationship between these two variables, we can perform
    correspondence analysis^([4](ch04.html#idm46099156059992)) using the `FactoMineR`
    package. This technique enables us to summarize the relationship between the high-dimensional
    factor levels by mapping each level to a point on the plane. We first obtain the
    mapping using `FactoMineR::CA()` as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探索这两个变量之间的关系，我们可以使用` FactoMineR `包执行对应分析^([4](ch04.html#idm46099156059992))。这种技术使我们能够通过将每个水平映射到平面上的一个点来总结高维因子水平之间的关系。我们首先使用`
    FactoMineR::CA() `获取映射，如下所示：
- en: '[PRE20]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can then plot the results using `ggplot`, which you can see in [Figure 4-4](#modeling-eda-pcs):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用` ggplot `绘制结果，您可以在[图 4-4](#modeling-eda-pcs)中看到：
- en: '[PRE21]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Correspondence analysis principal coordinates for drug and alcohol use](assets/mswr_0404.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![药物和酒精使用的对应分析主坐标](assets/mswr_0404.png)'
- en: Figure 4-4\. Correspondence analysis principal coordinates for drug and alcohol
    use
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 药物和酒精使用的对应分析主坐标
- en: In [Figure 4-4](#modeling-eda-pcs), we see that the correspondence analysis
    procedure has transformed the factors into variables called *principal coordinates*,
    which correspond to the axes in the plot and represent how much information in
    the contingency table they contain. We can, for example, interpret the proximity
    of “drinking often” and “using drugs very often” as indicating association.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 4-4](#modeling-eda-pcs)中，我们看到对应分析过程已将因子转换为称为*主坐标*的变量，这些坐标对应于图中的轴，并表示它们在列联表中包含的信息量。例如，我们可以解释“经常饮酒”和“非常频繁使用药物”的接近表示它们之间的关联。
- en: This concludes our discussion on EDA. Let’s proceed to feature engineering.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对EDA的讨论。让我们继续进行特征工程。
- en: Feature Engineering
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: The feature engineering exercise comprises transforming the data to increase
    the performance of the model. This can include things like centering and scaling
    numerical values and performing string manipulation to extract meaningful variables.
    It also often includes variable selection—the process of selecting which predictors
    are used in the model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的练习包括将数据转换以提高模型的性能。这可以包括将数值值居中和缩放以及执行字符串操作以提取有意义的变量。它通常还包括变量选择——选择在模型中使用哪些预测变量的过程。
- en: 'In [Figure 4-1](#age-histogram) we see that the `age` variable has a range
    from 18 to over 60\. Some algorithms, especially neural networks, train faster
    if we normalize our inputs so that they are of the same magnitude. Let’s now normalize
    the `age` variable by removing the mean and scaling to unit variance, beginning
    by calculating its mean and standard deviation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 4-1](#age-histogram)中，我们看到`age`变量的范围从18到60多岁。一些算法，特别是神经网络，在训练时如果我们对输入进行归一化使其具有相同的量级，会更快。现在让我们通过计算其均值和标准差来归一化`age`变量：
- en: '[PRE22]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can then use these to transform the dataset:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用这些来转换数据集：
- en: '[PRE24]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In [Figure 4-5](#modeling-eda-scaled-dist), we see that the scaled age variable
    has values that are closer to zero. We now move on to discussing other types of
    transformations, but during your feature engineering workflow you might want to
    perform the normalization for all numeric variables that you want to include in
    the model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 4-5](#modeling-eda-scaled-dist)中，我们看到缩放年龄变量的值接近零。现在我们继续讨论其他类型的转换，在特征工程工作流程中，您可能希望对要包含在模型中的所有数值变量执行归一化。
- en: '![Distribution of scaled age](assets/mswr_0405.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![缩放年龄分布](assets/mswr_0405.png)'
- en: Figure 4-5\. Distribution of scaled age
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 缩放年龄分布
- en: 'Since some of the profile features are multiple-select—in other words, a person
    can choose to associate multiple options for a variable—we need to process them
    before we can build meaningful models. If we take a look at the ethnicity column,
    for example, we see that there are many different combinations:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些个人资料特征是多选的——换句话说，一个人可以选择为变量关联多个选项——我们需要在构建有意义的模型之前对它们进行处理。例如，如果我们看一下种族列，我们会发现有许多不同的组合：
- en: '[PRE26]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'One way to proceed would be to treat each combination of races as a separate
    level, but that would lead to a very large number of levels, which becomes problematic
    in many algorithms. To better encode this information, we can create dummy variables
    for each race, as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一种处理方法是将每种种族组合视为一个单独的级别，但这将导致许多算法中的级别数量非常庞大，从而产生问题。为了更好地编码这些信息，我们可以为每个种族创建虚拟变量，如下所示：
- en: '[PRE28]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: For the free text fields, a straightforward way to extract features is counting
    the total number of characters. We will store the train dataset in Spark’s memory
    with `compute()` to speed up computation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自由文本字段，提取特征的一种简单方法是计算字符的总数。我们将使用`compute()`将训练数据集存储在Spark的内存中，以加快计算速度。
- en: '[PRE30]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We can see the distribution of the `essay_length` variable in [Figure 4-6](#modeling-essay-length-distribution).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在[图 4-6](#modeling-essay-length-distribution)中`essay_length`变量的分布。
- en: '![Distribution of essay length](assets/mswr_0406.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![文章长度分布](assets/mswr_0406.png)'
- en: Figure 4-6\. Distribution of essay length
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 文章长度分布
- en: 'We use this dataset in [Chapter 5](ch05.html#pipelines), so let’s save it first
    as a Parquet file—an efficient file format ideal for numeric data:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第五章](ch05.html#pipelines)中使用这个数据集，所以让我们首先将其保存为Parquet文件——这是一个非常适合数值数据的高效文件格式：
- en: '[PRE32]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now that we have a few more features to work with, we can begin running some
    unsupervised learning algorithms.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了更多特征可以使用，我们可以开始运行一些无监督学习算法了。
- en: Supervised Learning
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: Once we have a good grasp on our dataset, we can start building some models.
    Before we do so, however, we need to come up with a plan to tune and validate
    the “candidate” models—in modeling projects, we often try different types of models
    and ways to fit them to see which ones perform the best. Since we are dealing
    with a binary classification problem, the metrics we can use include accuracy,
    precision, sensitivity, and area under the receiver operating characteristic curve
    (ROC AUC), among others. The metric you optimize depends on your specific business
    problem, but for this exercise, we will focus on the ROC AUC.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对数据集有了充分的掌握，我们可以开始构建一些模型。然而，在此之前，我们需要制定一个计划来调整和验证“候选”模型——在建模项目中，我们通常尝试不同类型的模型和拟合方式，以确定哪些模型效果最佳。由于我们处理的是二元分类问题，可以使用的指标包括准确率、精确度、敏感度和接收者操作特征曲线下面积（ROC
    AUC），等等。优化的指标取决于具体的业务问题，但在本次练习中，我们将专注于ROC AUC。
- en: 'It is important that we don’t peek at the testing holdout set until the very
    end, because any information we obtain could influence our modeling decisions,
    which would in turn make our estimates of model performance less credible. For
    tuning and validation, we perform 10-fold cross-validation, which is a standard
    approach for model tuning. The scheme works as follows: we first divide our dataset
    into 10 approximately equal-sized subsets. We take the 2nd to 10th sets together
    as the training set for an algorithm and validate the resulting model on the 1st
    set. Next, we reserve the 2nd set as the validation set and train the algorithm
    on the 1st and 3rd to 10th sets. In total, we train 10 models and average the
    performance. If time and resources allow, you can also perform this procedure
    multiple times with different random partitions of the data. In our case, we will
    demonstrate how to perform the cross-validation once. Hereinafter, we refer to
    the training set associated with each split as the *analysis* data, and the validation
    set as *assessment* data.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 很重要的一点是，在最后阶段之前我们不能窥视测试保留集，因为我们获取的任何信息都可能影响我们的建模决策，从而使我们对模型性能的估计不够可信。为了调整和验证，我们执行10折交叉验证，这是模型调整的标准方法。该方案的工作方式如下：首先将数据集分成10个大致相等的子集。我们将第2到第10个集合作为算法的训练集，并在第1个集合上验证结果模型。接下来，我们将第2个集合作为验证集，并在第1和第3到第10个集合上训练算法。总共，我们训练了10个模型并平均性能。如果时间和资源允许，您还可以多次使用不同的随机数据分割执行此过程。在我们的案例中，我们将演示如何执行一次交叉验证。此后，我们将每个分割的训练集称为*分析*数据，将验证集称为*评估*数据。
- en: 'Using the `sdf_random_split()` function, we can create a list of subsets from
    our `okc_train` table:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sdf_random_split()`函数，我们可以从我们的`okc_train`表中创建一个子集列表：
- en: '[PRE33]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We then create our first analysis/assessment split as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们按以下方式创建我们的第一个分析/评估分割：
- en: '[PRE34]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'One item we need to carefully treat here is the scaling of variables. We need
    to make sure that we do not leak any information from the assessment set to the
    analysis set, so we calculate the mean and standard deviation on the analysis
    set only and apply the same transformation to both sets. Here is how we would
    handle this for the `age` variable:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们需要特别注意的一点是变量的缩放。我们必须确保不会从评估集泄漏任何信息到分析集，因此我们仅在分析集上计算均值和标准差，并将相同的变换应用于两个集合。以下是我们如何处理`age`变量的方法：
- en: '[PRE35]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: For brevity, here we show only how to transform the `age` variable. In practice,
    however, you would want to normalize each one of your continuous predictors, such
    as the `essay_length` variable we derived in the previous section.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为简洁起见，这里只展示如何转换`age`变量。然而，在实践中，您会希望归一化每一个连续预测变量，例如我们在前一节中推导的`essay_length`变量。
- en: 'Logistic regression is often a reasonable starting point for binary classification
    problems, so let’s give it a try. Suppose also that our domain knowledge provides
    us with an initial set of predictors. We can then fit a model by using the `Formula`
    interface:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归通常是二元分类问题的合理起点，所以让我们试试看。假设我们的领域知识还提供了一组初始预测变量。然后我们可以使用`Formula`接口拟合模型：
- en: '[PRE36]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To obtain a summary of performance metrics on the assessment set, we can use
    the `ml_evaluate()` function:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得评估集上性能指标的摘要，我们可以使用`ml_evaluate()`函数：
- en: '[PRE38]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You can print `validation_summary` to see the available metrics:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以打印`validation_summary`以查看可用的指标：
- en: '[PRE39]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can plot the ROC curve by collecting the output of `validation_summary$roc()`
    and using `ggplot2`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过收集`validation_summary$roc()`的输出并使用`ggplot2`来绘制ROC曲线：
- en: '[PRE41]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[Figure 4-7](#modeling-super-roc1) shows the results of the plot.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-7](#modeling-super-roc1) 展示了绘图结果。'
- en: The ROC curve plots the true positive rate (sensitivity) against the false positive
    rate (1–specificity) for varying values of the classification threshold. In practice,
    the business problem helps to determine where on the curve one sets the threshold
    for classification. The AUC is a summary measure for determining the quality of
    a model, and we can compute it by calling the `area_under_roc()` function.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线将真正例率（灵敏度）绘制为分类阈值的假正例率（1–特异性）。在实践中，业务问题有助于确定在曲线上设置分类阈值的位置。AUC 是用于确定模型质量的总结性测量，我们可以通过调用
    `area_under_roc()` 函数来计算它。
- en: '![ROC curve for the logistic regression model](assets/mswr_0407.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归模型的 ROC 曲线](assets/mswr_0407.png)'
- en: Figure 4-7\. ROC curve for the logistic regression model
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 逻辑回归模型的 ROC 曲线
- en: '[PRE42]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Spark provides evaluation methods for only generalized linear models (including
    linear models and logistic regression). For other algorithms, you can use the
    evaluator functions (e.g., `ml_binary_classification_evaluator()` on the prediction
    DataFrame) or compute your own metrics.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于广义线性模型（包括线性模型和逻辑回归），Spark 提供了评估方法。对于其他算法，您可以使用评估器函数（例如，在预测 DataFrame 上使用 `ml_binary_classification_evaluator()`）或计算自己的指标。
- en: 'Now, we can easily repeat the logic we already have and apply it to each analysis/assessment
    split:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以轻松重复已有的逻辑，并将其应用于每个分析/评估拆分：
- en: '[PRE44]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This gives us 10 ROC curves:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了 10 条 ROC 曲线：
- en: '[PRE45]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[Figure 4-8](#modeling-super-roc2) shows the results of the plot.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-8](#modeling-super-roc2) 展示了绘图结果。'
- en: '![Cross-validated ROC curves for the logistic regression model](assets/mswr_0408.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归模型的交叉验证 ROC 曲线](assets/mswr_0408.png)'
- en: Figure 4-8\. Cross-validated ROC curves for the logistic regression model
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. 逻辑回归模型的交叉验证 ROC 曲线
- en: 'And we can obtain the average AUC metric:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 平均 AUC 指标可以通过以下方式获取：
- en: '[PRE46]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Generalized Linear Regression
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广义线性回归
- en: 'If you are interested in generalized linear model (GLM) diagnostics,you can
    also fit a logistic regression via the generalized linear regression interface
    by specifying `family = "binomial"`. Because the result is a regression model,
    the `ml_predict()` method does not give class probabilities. However, it includes
    confidence intervals for coefficient estimates:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对广义线性模型（GLM）诊断感兴趣，还可以通过指定 `family = "binomial"` 来通过广义线性回归界面拟合逻辑回归。因为结果是一个回归模型，所以
    `ml_predict()` 方法不会给出类别概率。然而，它包括系数估计的置信区间：
- en: '[PRE48]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We can extract the coefficient estimates into a tidy DataFrame, which we can
    then process further—for example, to create a coefficient plot, which you can
    see in [Figure 4-9](#modeling-super-glr-coefs):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将系数估计提取到一个整洁的 DataFrame 中，然后进一步处理——例如，创建一个系数图，您可以在 [图 4-9](#modeling-super-glr-coefs)
    中看到：
- en: '[PRE49]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![Coefficient estimates with 95% confidence intervals](assets/mswr_0409.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![系数估计及其 95% 置信区间](assets/mswr_0409.png)'
- en: Figure 4-9\. Coefficient estimates with 95% confidence intervals
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-9\. 系数估计及其 95% 置信区间
- en: Note
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Both `ml_logistic_regression()` and `ml_linear_regression()` support elastic
    net regularization^([5](ch04.html#idm46099153772712)) through the `reg_param`
    and `elastic_net_param` parameters. `reg_param` corresponds to <math><mi>λ</mi></math>
    , whereas `elastic_net_param` corresponds to <math><mi>α</mi></math> . `ml_generalized_linear_regression()`
    supports only `reg_param`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`ml_logistic_regression()` 和 `ml_linear_regression()` 都支持通过 `reg_param` 和 `elastic_net_param`
    参数进行弹性网正则化^([5](ch04.html#idm46099153772712))。`reg_param` 对应于 <math><mi>λ</mi></math>
    ，而 `elastic_net_param` 对应于 <math><mi>α</mi></math> 。`ml_generalized_linear_regression()`
    仅支持 `reg_param`。'
- en: Other Models
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他模型
- en: 'Spark supports many of the standard modeling algorithms and it’s easy to apply
    these models and hyperparameters (values that control the model-fitting process)
    for your particular problem. You can find a list of supported ML-related functions
    in the appendix. The interfaces to access these functionalities are largely identical,
    so it is easy to experiment with them. For example, to fit a neural network model
    we can run the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 支持许多标准建模算法，并且可以轻松将这些模型和超参数（控制模型拟合过程的值）应用于特定问题。您可以在附录中找到支持的 ML 相关函数列表。访问这些功能的接口基本相同，因此可以轻松进行实验。例如，要拟合神经网络模型，我们可以运行以下操作：
- en: '[PRE50]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This gives us a feedforward neural network model with two hidden layers of
    64 nodes each. Note that you have to specify the correct values for the input
    and output layers in the `layers` argument. We can obtain predictions on a validation
    set using `ml_predict()`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个具有两个隐藏层，每层64个节点的前馈神经网络模型。请注意，您必须在`layers`参数中指定输入和输出层的正确值。我们可以使用`ml_predict()`在验证集上获取预测结果：
- en: '[PRE51]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Then, we can compute the AUC via `ml_binary_classification_evaluator()`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过`ml_binary_classification_evaluator()`计算AUC：
- en: '[PRE52]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Up until now, we have not looked into the unstructured text in the essay fields
    apart from doing simple character counts. In the next section, we explore the
    textual data in more depth.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们尚未深入研究除了进行简单字符计数之外的论文字段中的非结构化文本。在接下来的部分中，我们将更深入地探索文本数据。
- en: Unsupervised Learning
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Along with speech, images, and videos, textual data is one of the components
    of the big data explosion. Prior to modern text-mining techniques and the computational
    resources to support them, companies had little use for freeform text fields.
    Today, text is considered a rich source of insights that can be found anywhere
    from physician’s notes to customer complaints. In this section, we show some basic
    text analysis capabilities of `sparklyr`. If you would like more background on
    text-mining techniques, we recommend reading [*Text Mining with R*](https://oreil.ly/OrjWr)
    by David Robinson and Julie Silge (O’Reilly).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 除了语音、图像和视频外，文本数据是大数据爆炸的组成部分之一。在现代文本挖掘技术及其支持的计算资源出现之前，公司很少使用自由文本字段。如今，文本被视为从医生的笔记到客户投诉等各处获得洞察的丰富信息源。在本节中，我们展示了`sparklyr`的一些基本文本分析功能。如果您希望了解更多有关文本挖掘技术背景的信息，我们建议阅读David
    Robinson和Julie Silge的《*Text Mining with R*》（O’Reilly）。
- en: In this section, we show how to perform a basic topic-modeling task on the essay
    data in the `OKCupid` dataset. Our plan is to concatenate the essay fields (of
    which there are 10) of each profile and regard each profile as a document, then
    attempt to discover *topics* (we define these soon) using Latent Dirichlet Allocation
    (LDA).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了如何在`OKCupid`数据集中的论文数据上执行基本的主题建模任务。我们的计划是将每个档案的10个论文字段连接起来，并将每个档案视为一个文档，然后尝试使用潜在狄利克雷分配（LDA）来发现*主题*（我们很快会定义这些主题）。
- en: Data Preparation
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: As always, before analyzing a dataset (or a subset of one), we want to take
    a quick look at it to orient ourselves. In this case, we are interested in the
    freeform text that the users entered into their dating profiles.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析数据集（或其子集）之前，我们总是希望快速查看以对其进行定位。在这种情况下，我们对用户在其约会档案中输入的自由文本感兴趣。
- en: '[PRE54]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Just from this output, we see the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 就从这个输出中，我们看到以下内容：
- en: The text contains HTML tags
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本包含HTML标签
- en: The text contains the newline (`\n`) character
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本包含换行符（`\n`）字符
- en: There are missing values in the data
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中存在缺失值。
- en: The HTML tags and special characters pollute the data since they are not directly
    input by the user and do not provide interesting information. Similarly, since
    we have encoded missing character fields with the *missing* string, we need to
    remove it. (Note that by doing this we are also removing instances of the word
    “missing” written by the users, but the information lost from this removal is
    likely to be small.)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: HTML标签和特殊字符会污染数据，因为它们不是用户直接输入的，并且不提供有趣的信息。类似地，由于我们使用“*missing*”字符串对缺失的字符字段进行了编码，我们需要将其删除。（请注意，通过这样做，我们也在删除用户写入的“missing”单词的实例，但是由于这种删除而丢失的信息可能很少。）
- en: 'As you analyze your own text data, you will quickly come across and become
    familiar with the peculiarities of the specific dataset. As with tabular numerical
    data, preprocessing text data is an iterative process, and after a few tries we
    have the following transformations:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析您自己的文本数据时，您将很快遇到并熟悉特定数据集的特殊情况。与表格数值数据一样，预处理文本数据是一个迭代过程，经过几次尝试后，我们有了以下转换：
- en: '[PRE56]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Note here we are using `regex_replace()`, which is a Spark SQL function. Next,
    we discuss LDA and how to apply it to our cleaned dataset.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里我们使用了`regex_replace()`，这是一个Spark SQL函数。接下来，我们讨论LDA以及如何将其应用于我们的清理数据集。
- en: Topic Modeling
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题建模
- en: LDA is a type of topic model for identifying abstract “topics” in a set of documents.
    It is an unsupervised algorithm in that we do not provide any labels, or topics,
    for the input documents. LDA posits that each document is a mixture of topics,
    and each topic is a mixture of words. During training, it attempts to estimate
    both of these simultaneously. A typical use case for topic models involves categorizing
    many documents, for which the large number of documents renders manual approaches
    infeasible. The application domains range from GitHub issues to legal documents.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 是一种用于识别文档集中抽象“主题”的主题模型。它是一种无监督算法，因为我们不为输入文档提供任何标签或主题。LDA 假定每个文档是主题的混合物，而每个主题是单词的混合物。在训练期间，它试图同时估计这两者。主题模型的典型应用包括对许多文档进行分类，其中文档的大量数量使得手动方法不可行。应用领域涵盖从
    GitHub 问题到法律文件等多个领域。
- en: 'After we have a reasonably clean dataset following the workflow in the previous
    section, we can fit an LDA model with `ml_lda()`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成上一节中的工作流程后，我们可以使用 `ml_lda()` 拟合一个 LDA 模型：
- en: '[PRE57]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We are also including a `stop_words` vector, consisting of commonly used English
    words and common words in our dataset, that instructs the algorithm to ignore
    them. After the model is fit, we can use the `tidy()` function to extract the
    associated betas, which are the per-topic-per-word probabilities, from the model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还包括一个 `stop_words` 向量，包含常用的英语单词和数据集中常见的单词，指示算法忽略它们。模型拟合后，我们可以使用 `tidy()` 函数从模型中提取相关的
    beta 值，即每个主题每个单词的概率。
- en: '[PRE58]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We can then visualize this output by looking at word probabilities by topic.
    In [Figure 4-10](#modeling-unsuper-lda-betas-topics) and [Figure 4-11](#modeling-unsuper-lda-betas-topics-2),
    we show the results at 1 iteration and 100 iterations. The code that generates
    [Figure 4-10](#modeling-unsuper-lda-betas-topics) follows; to generate [Figure 4-11](#modeling-unsuper-lda-betas-topics-2),
    you would need to set `max_iter = 100` when running `ml_lda()`, but beware that
    this can take a really long time in a single machine—this is the kind of big-compute
    problem that a proper Spark cluster would be able to easily tackle.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看每个主题的单词概率来可视化这些输出。在 [图 4-10](#modeling-unsuper-lda-betas-topics) 和 [图 4-11](#modeling-unsuper-lda-betas-topics-2)
    中，我们展示了在 1 次迭代和 100 次迭代时的结果。生成 [图 4-10](#modeling-unsuper-lda-betas-topics) 的代码如下；要生成
    [图 4-11](#modeling-unsuper-lda-betas-topics-2)，你需要在运行 `ml_lda()` 时设置 `max_iter
    = 100`，但要注意，在单台机器上这可能需要很长时间——这是 Spark 集群可以轻松处理的大计算问题。
- en: '[PRE60]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '![The most common terms per topic in the first iteration](assets/mswr_0410.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![第一次迭代中每个主题中最常见的术语](assets/mswr_0410.png)'
- en: Figure 4-10\. The most common terms per topic in the first iteration
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-10\. 第一次迭代中每个主题中最常见的术语
- en: At 100 iterations, we can see “topics” starting to emerge. This could be interesting
    information in its own right if you were digging into a large collection of documents
    with which you aren’t familiar. The learned topics can also serve as features
    in a downstream supervised learning task; for example, we could consider using
    the topic number as a predictor in our model to predict employment status in our
    predictive modeling example.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在 100 次迭代后，“主题”开始显现出来。如果你正在研究一大批你不熟悉的文件，这些信息本身可能是很有趣的。学到的主题还可以作为下游监督学习任务中的特征；例如，在我们的预测建模示例中，我们可以考虑使用主题编号作为模型中的预测因子来预测就业状态。
- en: '![The most common terms per topic after 100 iterations](assets/mswr_0411.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![100 次迭代后每个主题中最常见的术语](assets/mswr_0411.png)'
- en: Figure 4-11\. The most common terms per topic after 100 iterations
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-11\. 100 次迭代后每个主题中最常见的术语
- en: 'Finally, to conclude this chapter you should disconnect from Spark. [Chapter 5](ch05.html#pipelines)
    also makes use of the `OKCupid` dataset, but we provide instructions to reload
    it from scratch:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在结束本章时，你应该从 Spark 断开连接。第 [第五章](ch05.html#pipelines) 也使用了 `OKCupid` 数据集，但我们提供了从头重新加载该数据集的指导：
- en: '[PRE61]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Recap
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the basics of building predictive models in Spark
    with R by presenting the topics of EDA, feature engineering, and building supervised
    models, in which we explored using logistic regression and neural networks—just
    to pick a few from dozens of models available in Spark through `MLlib`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过介绍 EDA、特征工程和构建监督模型的主题，探讨了使用逻辑回归和神经网络等多种模型（从 Spark 的数十种模型中挑选出来的）构建预测模型的基础，涵盖了在
    Spark 中使用 R 构建预测模型的基础知识。
- en: 'We then explored how to use unsupervised learning to process raw text, in which
    you created a topic model that automatically grouped the profiles into six categories.
    We demonstrated that building the topic model can take a significant amount of
    time using a single machine, which is a nearly perfect segue to introduce full-sized
    computing clusters! But hold that thought: we first need to consider how to automate
    data science workflows.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们探讨了如何使用无监督学习处理原始文本，您创建了一个可以自动将配置文件分组为六个类别的主题模型。我们演示了使用单台机器构建主题模型可能需要大量时间，这几乎是引入全尺寸计算集群的完美过渡！但请先暂停思考：我们首先需要考虑如何自动化数据科学工作流程。
- en: As we mentioned when introducing this chapter, emphasis was placed on predictive
    modeling. Spark can help with data science at scale, but it can also assist in
    productionizing data science workflows into automated processes, known by many
    as machine learning. [Chapter 5](ch05.html#pipelines) presents the tools we will
    need to take our predictive models, and even our entire training workflows, into
    automated environments that can run continuously or be exported and consumed in
    web applications, mobile applications, and more.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在介绍本章时提到的，重点放在了预测建模上。Spark可以帮助规模化数据科学，但它也可以将数据科学工作流程投入自动化流程中，许多人称之为机器学习。[第五章](ch05.html#pipelines)介绍了我们将需要的工具，以将我们的预测模型，甚至整个训练工作流程，带入可以持续运行或导出并在Web应用程序、移动应用程序等中消费的自动化环境中。
- en: '^([1](ch04.html#idm46099157208232-marker)) Kuhn M, Johnson K (2019). *Feature
    Engineering and Selection: A Practical Approach for Predictive Models*. (CRC PRess.)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.html#idm46099157208232-marker)) Kuhn M，Johnson K（2019）。*特征工程和选择：预测模型的实用方法*。（CRC出版社。）
- en: ^([2](ch04.html#idm46099157204216-marker)) We acknowledge that the terms here
    might mean different things to different people and that there is a continuum
    between the two approaches; however, they are defined.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.html#idm46099157204216-marker)) 我们承认这些术语可能对不同的人有不同的含义，并且这两种方法之间存在一个连续体；然而，它们是被定义好的。
- en: ^([3](ch04.html#idm46099157194104-marker)) Kim AY, Escobedo-Land A (2015). “OKCupid
    data for introductory statistics and data science courses.” *Journal of Statistics
    Education*, 23(2).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.html#idm46099157194104-marker)) Kim AY，Escobedo-Land A（2015）。“用于初步统计和数据科学课程的OKCupid数据。”*统计教育杂志*，23（2）。
- en: ^([4](ch04.html#idm46099156059992-marker)) Greenacre M (2017). *Correspondence
    analysis in practice*. Chapman and Hall/CRC.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.html#idm46099156059992-marker)) Greenacre M（2017）。*实践中的对应分析*。Chapman
    and Hall/CRC出版社。
- en: '^([5](ch04.html#idm46099153772712-marker)) Zou H, Hastie T (2005). “Regularization
    and variable selection via the elastic net.” *Journal of the royal statistical
    society: series B (statistical methodology)*, 67(2), 301–320.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.html#idm46099153772712-marker)) Zou H，Hastie T（2005）。“通过弹性网络进行正则化和变量选择。”*皇家统计学会：B系列（统计方法学）*，67（2），301–320。
