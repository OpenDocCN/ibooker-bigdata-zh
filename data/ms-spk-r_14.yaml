- en: Chapter 13\. Contributing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章. 贡献
- en: Hold the door, hold the door.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 留住门，留住门。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Hodor
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —霍德尔
- en: In [Chapter 12](ch12.html#streaming), we equipped you with the tools to tackle
    large-scale and real-time data processing in Spark using R. In this final chapter
    we focus less on learning and more on giving back to the Spark and R communities
    or colleagues in your professional career. It really takes an entire community
    to keep this going, so we are counting on you!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第12章](ch12.html#streaming)中，我们为您提供了使用R在Spark中处理大规模和实时数据的工具。在这最后一章中，我们不再专注于学习，而是更多地回馈给Spark和R社区，或者您职业生涯中的同事。要让这一切持续下去，真的需要整个社区的参与，所以我们期待您的加入！
- en: There are many ways to contribute, from helping community members and opening
    GitHub issues to providing new functionality for yourself, colleagues, or the
    R and Spark community. However, we’ll focus here on writing and sharing code that
    extends Spark, to help others use new functionality you can provide as an author
    of Spark extensions using R. Specifically, you’ll learn what an extension is,
    the different types of extensions you can build, what building tools are available,
    and when and how to build an extension from scratch.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方式可以贡献，从帮助社区成员和开放GitHub问题到为您自己、同事或R和Spark社区提供新功能。然而，我们将在这里重点介绍编写和共享扩展Spark的代码，以帮助其他人使用您可以作为Spark扩展作者提供的新功能。具体来说，您将学习什么是扩展，可以构建哪些类型的扩展，可用的构建工具，以及从头开始构建扩展的时间和方式。
- en: You will also learn how to make use of the hundreds of extensions available
    in Spark and the millions of components available in Java that can easily be used
    in R. We’ll also cover how to create code natively in Scala that makes use of
    Spark. As you might know, R is a great language for interfacing with other languages,
    such as C++, SQL, Python, and many others. It’s no surprise, then, that working
    with Scala from R will follow similar practices that make R ideal for providing
    easy-to-use interfaces that make data processing productive and that are loved
    by many of us.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您还将学习如何利用Spark中的数百个扩展和Java中的数百万组件，这些组件可以轻松在R中使用。我们还将介绍如何在Scala中创建原生的利用Spark的代码。您可能知道，R是一个很好的语言，可以与其他语言（如C++、SQL、Python等）进行接口操作。因此，与R一起使用Scala将遵循使R成为理想选择的类似实践，提供易于使用的接口，使数据处理高效且受到许多人喜爱。
- en: Overview
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'When you think about giving back to your larger coding community, the most
    important question you can ask about any piece of code you write is: would this
    code be useful to someone else?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当您考虑回馈您更大的编码社区时，您可以询问关于您编写的任何代码的最重要的问题是：这段代码对其他人有用吗？
- en: 'Let’s start by considering one of the first and simplest lines of code presented
    in this book. This code was used to load a simple CSV file:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从考虑本书中呈现的第一行最简单的代码之一开始。这段代码用于加载一个简单的CSV文件：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Code this basic is probably not useful to someone else. However, you could
    tailor that same example to something that generates more interest, perhaps the
    following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基础的代码可能对其他人没有用处。然而，您可以将同样的例子调整为生成更多兴趣的内容，也许是以下内容：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This code is quite similar to the first. But if you work with others who are
    working with this dataset, the answer to the question about usefulness would be
    yes—this would very likely be useful to someone else!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与第一个非常相似。但是，如果您与正在使用这个数据集的其他人一起工作，关于有用性的问题的答案将是肯定的——这很可能对其他人有用！
- en: This is surprising since it means that not all useful code needs to be advanced
    or complicated. However, for it to be useful to others, it does need to be packaged,
    presented, and shared in a format that is easy to consume.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这很令人惊讶，因为这意味着并不是所有有用的代码都需要高级或复杂。但是，为了对其他人有用，它确实需要以易于消费的格式进行打包、呈现和分享。
- en: 'A first attempt would be to save this into a *`teamdata.R`* file and write
    a function wrapping it:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先尝试将其保存为*`teamdata.R`*文件，并编写一个包装它的函数：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is an improvement, but it would require users to manually share this file
    again and again. Fortunately, this problem is easily solved in R, through *R packages*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个进步，但它将要求用户一遍又一遍地手动共享这个文件。幸运的是，这个问题在R中很容易解决，通过*R包*。
- en: An R package contains R code in a format that is installable using the function
    `install.packages()`. One example is `sparklyr`. There are many other R packages
    available; you can also create your own. For those of you new to creating them,
    we encourage you to read Hadley Wickham’s book, [*R Packages*](http://shop.oreilly.com/product/0636920034421.do)
    (O’Reilly). Creating an R package allows you to easily share your functions with
    others by sharing the package file in your organization.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 R 包包含了 R 代码，可以通过函数 `install.packages()` 进行安装。一个例子是 `sparklyr`。还有许多其他的 R 包可用；你也可以创建自己的包。对于那些刚开始创建包的人，我们鼓励你阅读
    Hadley Wickham 的书，[*R Packages*](http://shop.oreilly.com/product/0636920034421.do)（O’Reilly）。创建一个
    R 包允许你通过在组织中分享包文件轻松地与其他人分享你的函数。
- en: Once you create a package, there are many ways of sharing it with colleagues
    or the world. For instance, for packages meant to be private, consider using [Drat](http://bit.ly/2N8J1f7)
    or products like [RStudio Package Manager](http://bit.ly/2H5k807). R packages
    meant for public consumption are made available to the R community in [CRAN](https://cran.r-project.org/)
    (the Comprehensive R Archive Network).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了一个包，有许多方法可以与同事或全世界分享它。例如，对于要保密的包，考虑使用 [Drat](http://bit.ly/2N8J1f7) 或像
    [RStudio Package Manager](http://bit.ly/2H5k807) 这样的产品。用于公共消费的 R 包可以在 [CRAN](https://cran.r-project.org/)（全面的
    R 存档网络）中向 R 社区提供。
- en: These repositories of R packages allow users to install packages through `install.packages("teamdata")`
    without having to worry where to download the package from. It also allows other
    packages to reuse your package.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 R 包的存储库允许用户通过 `install.packages("teamdata")` 安装包，而不必担心从哪里下载包。它还允许其他包重用你的包。
- en: In addition to using R packages like `sparklyr`, `dplyr`, `broom`, and others
    to create new R packages that extend Spark, you can also use all the functionality
    available in the Spark API or Spark Extensions or write custom Scala code.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用像 `sparklyr`、`dplyr`、`broom` 等 R 包创建扩展 Spark 的新 R 包外，还可以使用 Spark API 或 Spark
    扩展的所有功能，或编写自定义 Scala 代码。
- en: For instance, suppose that there is a new file format similar to a CSV but not
    quite the same. We might want to write a function named `spark_read_file()` that
    would take a path to this new file type and read it in Spark. One approach would
    be to use `dplyr` to process each line of text or any other R library using `spark_apply()`.
    Another would be to use the Spark API to access methods provided by Spark. A third
    approach would be to check whether someone in the Spark community has already
    provided a Spark extension that supports this new file format. Last but not least,
    you could write your own custom Scala code that makes use of any Java library,
    including Spark and its extensions. This is shown in [Figure 13-1](#contributing-types-of-extensions).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设有一种类似 CSV 但不完全相同的新文件格式。我们可能想编写一个名为 `spark_read_file()` 的函数，该函数将获取到这种新文件类型的路径并在
    Spark 中读取它。一种方法是使用 `dplyr` 处理每行文本或任何其他 R 库使用 `spark_apply()`。另一种方法是使用 Spark API
    访问 Spark 提供的方法。第三种方法是检查 Spark 社区中是否已经提供了支持这种新文件格式的 Spark 扩展。最后但并非最不重要的是，你可以编写自定义的
    Scala 代码，其中包括使用任何 Java 库，包括 Spark 及其扩展。这在 [图 13-1](#contributing-types-of-extensions)
    中显示。
- en: '![Extending Spark using the Spark API, Spark extensions or Scala code](assets/mswr_1301.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Spark API、Spark 扩展或 Scala 代码扩展 Spark](assets/mswr_1301.png)'
- en: Figure 13-1\. Extending Spark using the Spark API or Spark extensions, or writing
    Scala code
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-1\. 使用 Spark API 或 Spark 扩展，或编写 Scala 代码扩展 Spark
- en: We will focus first on extending Spark using the Spark API, since the techniques
    required to call the Spark API are also applicable while calling Spark extensions
    or custom Scala code.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先专注于使用 Spark API 扩展 Spark，因为调用 Spark API 所需的技术在调用 Spark 扩展或自定义 Scala 代码时也是适用的。
- en: The Spark API
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark API
- en: 'Before we introduce the Spark API, let’s consider a simple and well-known problem.
    Suppose we want to count the number of lines in a distributed and potentially
    large text file—say, *`cars.csv`*, which we initialize as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍 Spark API 之前，让我们考虑一个简单而又众所周知的问题。假设我们想要计算一个分布式且可能很大的文本文件中行数的问题，比如说，*`cars.csv`*，我们初始化如下：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, to count how many lines are available in this file, we can run the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要计算此文件中有多少行，我们可以运行以下命令：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Easy enough: we used `spark_read_text()` to read the entire text file, followed
    by counting lines using `dplyr’s` `count()`. Now, suppose that neither `spark_read_text()`,
    nor `dplyr`, nor any other Spark functionality, is available to you. How would
    you ask Spark to count the number of rows in *`cars.csv`*?'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单：我们使用 `spark_read_text()` 读取整个文本文件，然后使用 `dplyr` 的 `count()` 来计算行数。现在，假设你既不能使用
    `spark_read_text()`，也不能使用 `dplyr` 或任何其他 Spark 功能，你将如何要求 Spark 统计 *`cars.csv`*
    中的行数？
- en: 'If you do this in Scala, you find in the Spark documentation that by using
    the Spark API you can count lines in a file as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 Scala 中这样做，您会发现在 Spark 文档中，通过使用 Spark API，可以像下面这样计算文件中的行数：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'So, to use the functionality available in the Spark API from R, like `spark.read.textFile`,
    you can use `invoke()`, `invoke_static()`, or `invoke_new()`. (As their names
    suggest, the first invokes a method from an object, the second invokes a method
    from a static object, and the third creates a new object.) We then use these functions
    to call Spark’s API and execute code similar to the one provided in Scala:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要从 R 中使用类似 `spark.read.textFile` 的 Spark API 功能，可以使用 `invoke()`、`invoke_static()`
    或 `invoke_new()`。 （正如它们的名称所示，第一个从对象中调用方法，第二个从静态对象中调用方法，第三个创建一个新对象。）然后，我们使用这些函数调用
    Spark 的 API 并执行类似 Scala 中提供的代码：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'While the `invoke()` function was originally designed to call Spark code, it
    can also call any code available in Java. For instance, we can create a Java `BigInteger`
    with the following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `invoke()` 函数最初设计用于调用 Spark 代码，但也可以调用 Java 中的任何可用代码。例如，我们可以使用以下代码创建一个 Java
    的 `BigInteger`：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, the object created is not an R object but rather a proper Java
    object. In R, this Java object is represented by the `spark_jobj`. These objects
    are meant to be used with the `invoke()` functions or `spark_dataframe()` and
    `spark_connection()`. `spark_dataframe()` transforms a `spark_jobj` into a Spark
    DataFrame when possible, whereas `spark_connect()` retrieves the original Spark
    connection object, which can be useful to avoid passing the `sc` object across
    functions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，创建的对象不是 R 对象，而是一个真正的 Java 对象。在 R 中，这个 Java 对象由 `spark_jobj` 表示。这些对象可以与
    `invoke()` 函数或 `spark_dataframe()` 和 `spark_connection()` 一起使用。`spark_dataframe()`
    将 `spark_jobj` 转换为 Spark DataFrame（如果可能的话），而 `spark_connect()` 则检索原始的 Spark 连接对象，这对于避免在函数间传递
    `sc` 对象很有用。
- en: While calling the Spark API can be useful in some cases, most of the functionality
    available in Spark is already supported in `sparklyr`. Therefore, a more interesting
    way to extend Spark is by using one of its many existing extensions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在某些情况下调用 Spark API 可以很有用，但 Spark 中大部分功能已经在 `sparklyr` 中支持。因此，扩展 Spark 的一个更有趣的方法是使用其众多现有的扩展。
- en: Spark Extensions
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 扩展
- en: Before we get started with this section, consider navigating to [spark-packages.org](https://spark-packages.org/),
    a site that tracks Spark extensions provided by the Spark community. Using the
    same techniques presented in the previous section, you can use these extensions
    from R.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本节之前，请考虑导航至 [spark-packages.org](https://spark-packages.org/)，这是一个跟踪由 Spark
    社区提供的 Spark 扩展的网站。使用前一节中介绍的相同技术，您可以从 R 中使用这些扩展。
- en: For instance, there is [Apache Solr](http://bit.ly/2MmBfim), a system designed
    to perform full-text searches over large datasets, something that Apache Spark
    currently does not support natively. Also, as of this writing, there is no extension
    for R to support Solr. So, let’s try to solve this by using a Spark extension.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，有 [Apache Solr](http://bit.ly/2MmBfim)，一个专为在大型数据集上执行全文搜索而设计的系统，目前 Apache
    Spark 并不原生支持。此外，在撰写本文时，还没有为 R 提供支持 Solr 的扩展。因此，让我们尝试使用 Spark 扩展来解决这个问题。
- en: 'First, if you search “spark-packages.org” to find a Solr extension, you should
    be able to locate [`spark-solr`](http://bit.ly/2YQnwXw). The “How to” extension
    mentions that the `com.lucidworks.spark:spark-solr:2.0.1` should be loaded. We
    can accomplish this in R using the `sparklyr.shell.packages` configuration option:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果您搜索“spark-packages.org”以找到 Solr 扩展，您应该能够找到 [`spark-solr`](http://bit.ly/2YQnwXw)。扩展的“如何”部分提到应加载
    `com.lucidworks.spark:spark-solr:2.0.1`。我们可以在 R 中使用 `sparklyr.shell.packages`
    配置选项来完成这个任务。
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: While specifying the `sparklyr.shell.packages` parameter is usually enough,
    for this particular extension, dependencies failed to download from the Spark
    packages repository. You would need to manually find the failed dependencies in
    the [Maven repo](https://mvnrepository.com) and add further repositories under
    the `sparklyr.shell.repositories` parameter.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通常指定 `sparklyr.shell.packages` 参数就足够了，但是对于这个特定的扩展，依赖项未能从 Spark 包仓库下载。你需要在
    `sparklyr.shell.repositories` 参数下手动查找失败的依赖项，并添加更多的仓库。
- en: Note
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When you use an extension, Spark connects to the Maven package repository to
    retrieve it. This can take significant time depending on the extension and your
    download speeds. In this case, consider using the `sparklyr.connect.timeout` configuration
    parameter to allow Spark to download the required files.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用扩展时，Spark 将连接到 Maven 包仓库以检索它。这可能需要很长时间，具体取决于扩展和你的下载速度。在这种情况下，考虑使用 `sparklyr.connect.timeout`
    配置参数来允许 Spark 下载所需的文件。
- en: 'From the `spark-solr` documentation, you would find that this extension can
    be used with the following Scala code:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 `spark-solr` 文档，你可以使用以下 Scala 代码：
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can translate this to R code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其翻译为 R 代码：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This code will fail, however, since it would require a valid Solr instance and
    configuring Solr goes beyond the scope of this book. But this example provides
    insights as to how you can create Spark extensions. It’s also worth mentioning
    that you can use `spark_read_source()` to read from generic sources to avoid writing
    custom `invoke()` code.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这段代码将失败，因为它需要一个有效的 Solr 实例，并且配置 Solr 超出了本书的范围。但是这个例子揭示了你如何创建 Spark 扩展的见解。值得一提的是，你可以使用
    `spark_read_source()` 从通用源中读取，而不必编写自定义的 `invoke()` 代码。
- en: 'As pointed out in [“Overview”](#contributing-overview), consider sharing code
    with others using R packages. While you could require users of your package to
    specify `sparklyr.shell.packages`, you can avoid this by registering dependencies
    in your R package. Dependencies are declared under a `spark_dependencies()` function;
    thus, for the example in this section:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在 [“概述”](#contributing-overview) 中指出的那样，考虑使用 R 包与其他人共享代码。虽然你可以要求你的包的用户指定 `sparklyr.shell.packages`，但你可以通过在你的
    R 包中注册依赖项来避免这样做。依赖项在 `spark_dependencies()` 函数下声明；因此，对于本节中的示例：
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `onLoad` function is automatically called by R when your library loads.
    It should call `register_extension()`, which will then call back `spark_dependencies()`,
    to allow your extension to provide additional dependencies. This example supports
    Spark 2.4, but you should also support mapping Spark and Scala versions to the
    correct Spark extension version.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的库加载时，`onLoad` 函数会被 R 自动调用。它应该调用 `register_extension()`，然后回调 `spark_dependencies()`，以允许你的扩展提供额外的依赖项。本例支持
    Spark 2.4，但你还应该支持将 Spark 和 Scala 版本映射到正确的 Spark 扩展版本。
- en: There are about 450 Spark extensions you can use; in addition, you can also
    use any Java library from a [Maven repository](http://bit.ly/2Mp0wrR), where Maven
    Central has over 3 million artifacts. While not all Maven Central libraries might
    be relevant to Spark, the combination of Spark extensions and Maven repositories
    certainly opens many interesting possibilities for you to consider!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用约 450 个 Spark 扩展；此外，你还可以从 [Maven 仓库](http://bit.ly/2Mp0wrR) 使用任何 Java 库，Maven
    Central 包含超过 300 万个构件。虽然并非所有 Maven Central 的库都与 Spark 相关，但 Spark 扩展和 Maven 仓库的结合无疑会为你带来许多有趣的可能性！
- en: However, for those cases where no Spark extension is available, the next section
    will teach you how to use custom Scala code from your own R package.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在没有 Spark 扩展可用的情况下，下一节将教你如何从你自己的 R 包中使用自定义的 Scala 代码。
- en: Using Scala Code
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Scala 代码
- en: Scala code enables you to use any method in the Spark API, Spark extensions,
    or Java library. In addition, writing Scala code when running in Spark can provide
    performance improvements over R code using `spark_apply()`. In general, the structure
    of your R package will contain R code and Scala code; however, the Scala code
    will need to be compiled as JARs (Java ARchive files) and included in your package.
    [Figure 13-2](#contributing-scala-code) shows this structure.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中运行时，Scala 代码使你能够使用 Spark API、Spark 扩展或 Java 库中的任何方法。此外，在 Spark 中编写
    Scala 代码可以提高性能，超过使用 `spark_apply()` 的 R 代码。通常情况下，你的 R 包结构将包含 R 代码和 Scala 代码；然而，Scala
    代码需要编译为 JAR 文件（Java ARchive 文件）并包含在你的包中。[图 13-2](#contributing-scala-code)展示了这种结构。
- en: '![R package structure when using Scala code](assets/mswr_1302.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Scala 代码时的 R 包结构](assets/mswr_1302.png)'
- en: Figure 13-2\. R package structure when using Scala code
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-2\. 使用Scala代码时的R包结构
- en: As usual, the R code should be placed under a top-level *R* folder, Scala code
    under a *java* folder, and the compiled JARs under an *inst/java* folder. Though
    you are certainly welcome to manually compile the Scala code, you can use helper
    functions to download the required compiler and compile Scala code.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，R代码应放在顶层的*R*文件夹下，Scala代码放在*java*文件夹下，编译后的JAR包放在*inst/java*文件夹下。虽然您当然可以手动编译Scala代码，但可以使用辅助函数下载所需的编译器并编译Scala代码。
- en: To compile Scala code, you’ll need to install the Java Development Kit 8 (JDK8,
    for short). Download the JDK from [Oracle’s Java download page](http://bit.ly/2P2UkYM);
    this will require you to restart your R session.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要编译Scala代码，您需要安装Java开发工具包8（简称JDK8）。从[Oracle的Java下载页面](http://bit.ly/2P2UkYM)下载JDK，这将需要您重新启动R会话。
- en: 'You’ll also need a [Scala compiler for Scala 2.11 and 2.12](https://www.scala-lang.org/).
    The Scala compilers can be automatically downloaded and installed using `download_scalac()`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要[Scala 2.11和2.12的Scala编译器](https://www.scala-lang.org/)。Scala编译器可以使用`download_scalac()`自动下载和安装：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, you’ll need to compile your Scala sources using `compile_package_jars()`.
    By default, it uses `spark_compilation_spec()`, which compiles your sources for
    the following Spark versions:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要使用`compile_package_jars()`编译您的Scala源代码。默认情况下，它使用`spark_compilation_spec()`，该函数为以下Spark版本编译您的源代码：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can also customize this specification by creating custom entries with `spark_compilation_spec()`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过使用`spark_compilation_spec()`创建自定义条目来自定义此规范。
- en: While you could create the project structure for the Scala code from scratch,
    you can also simply call *`spark_extension(path)`* to create an extension in the
    given path. This extension will be mostly empty but will contain the appropriate
    project structure to call the Scala code.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以从头开始创建Scala代码的项目结构，但也可以简单地调用*`spark_extension(path)`*来在指定路径创建一个扩展。该扩展将主要为空，但将包含适当的项目结构以调用Scala代码。
- en: Since `spark_extension()` is registered as a custom project extension in RStudio,
    you can also create an R package that extends Spark using Scala code from the
    File menu; click New Project and then select “R Package using Spark” as shown
    in [Figure 13-3](#contributing-r-rstudio-project).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`spark_extension()`在RStudio中注册为自定义项目扩展，您还可以通过“文件”菜单中的新项目，选择“使用Spark的R包”，如[图13-3](#contributing-r-rstudio-project)所示。
- en: '![Creating a Scala extension package from RStudio](assets/mswr_1303.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![从RStudio创建Scala扩展包](assets/mswr_1303.png)'
- en: Figure 13-3\. Creating a Scala extension package from RStudio
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-3\. 从RStudio创建Scala扩展包
- en: 'Once you are ready to compile your package JARs, you can simply run the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦准备好编译您的包JARs，您可以简单地运行以下命令：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Since the JARs are compiled by default into the *inst/* package path, when you
    are building the R package all the JARs will also be included within the package.
    This means that you can share or publish your R package, and it will be fully
    functional by R users. For advanced Spark users with most of their expertise in
    Scala, it’s compelling to consider writing libraries for R users and the R community
    in Scala and then easily packaging these into R packages that are easy to consume,
    use, and share among them.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于默认情况下JAR包编译到*inst/*包路径中，在构建R包时所有JAR包也将包含在内。这意味着您可以分享或发布您的R包，并且R用户可以完全使用它。对于大多数专业的Scala高级Spark用户来说，考虑为R用户和R社区编写Scala库，然后轻松将其打包成易于消费、使用和分享的R包，这是非常吸引人的。
- en: If you are interested in developing a Spark extension with R and you get stuck
    along the way, consider joining the `sparklyr` [Gitter channel](http://bit.ly/33ESccY),
    where many of us hang out to help this wonderful community to grow. We hope to
    hear from you soon!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣开发带有R的Spark扩展，并且在开发过程中遇到困难，请考虑加入`sparklyr`的[Gitter频道](http://bit.ly/33ESccY)，我们很乐意帮助这个美好的社区成长。期待您的加入！
- en: Recap
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小结
- en: This final chapter introduced you to a new set of tools to use to expand Spark
    functionality beyond what R and R packages currently support. This vast new space
    of libraries includes more than 450 Spark extensions and millions of Java artifacts
    you can use in Spark from R. Beyond these resources, you also learned how to build
    Java artifacts using Scala code that can be easily embedded and compiled from
    R.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一组新工具，用于扩展 Spark 的功能，超越了当前 R 和 R 包的支持范围。这个庞大的新库空间包括超过 450 个 Spark 扩展和数百万个可以在
    R 中使用的 Java 工件。除了这些资源，您还学习了如何使用 Scala 代码构建 Java 工件，这些工件可以轻松地从 R 中嵌入和编译。
- en: This brings us back to this book’s purpose, presented early on; while we know
    you’ve learned how to perform large-scale computing using Spark in R, we’re also
    confident that you have acquired the knowledge required to help other community
    members through Spark extensions. We can’t wait to see your new creations, which
    will surely help grow the Spark and R communities at large.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们回到了本书早期提出的目的；虽然我们知道您已经学会了如何在 R 中使用 Spark 执行大规模计算，但我们也确信您已经掌握了通过 Spark 扩展帮助其他社区成员所需的知识。我们迫不及待地想看到您的新创作，这些创作肯定会帮助扩展整个
    Spark 和 R 社区。
- en: 'To close, we hope the first chapters gave you an easy introduction to Spark
    and R. Following that, we presented analysis and modeling as foundations for using
    Spark, with the familiarity of R packages you already know and love. You moved
    on to learning how to perform large-scale computing in proper Spark clusters.
    The last third of this book focused on advanced topics: using extensions, distributing
    R code, processing real-time data, and, finally, giving back by using Spark extensions
    using R and Scala code.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们希望前几章为您介绍了 Spark 和 R 的简易入门。在此之后，我们提出了分析和建模作为使用 Spark 的基础，结合您已经熟悉和喜爱的 R
    包。您进一步学习了如何在合适的 Spark 集群中执行大规模计算。本书的最后部分重点讨论了高级主题：使用扩展、分发 R 代码、处理实时数据，最后通过使用 R
    和 Scala 代码使用 Spark 扩展来回馈社区。
- en: We’ve tried to present the best possible content. However, if you know of any
    way to improve this book, please open a GitHub issue under the [the-r-in-spark](http://bit.ly/2HdkIZQ)
    repo, and we’ll address your suggestions in upcoming revisions. We hope you enjoyed
    reading this book, and that you’ve learned as much as we have learned while writing
    it. We hope it was worthy of your time—it has been an honor having you as our
    reader.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们努力呈现最佳内容。但是，如果您知道任何改进本书的方法，请在 [the-r-in-spark](http://bit.ly/2HdkIZQ) 存储库下开启
    GitHub 问题，我们将在即将推出的修订版本中解决您的建议。希望您喜欢阅读本书，并且在写作过程中像我们一样学到了很多。我们希望本书值得您的时间——您作为读者是我们的荣幸。
