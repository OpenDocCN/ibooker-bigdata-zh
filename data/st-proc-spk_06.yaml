- en: Chapter 4\. Apache Spark as a Stream-Processing Engine
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 4 章. Apache Spark 作为流处理引擎
- en: In [Chapter 3](ch03.xhtml#streaming-architectures), we pictured a general architectural
    diagram of a streaming data platform and identified where Spark, as a distributed
    processing engine, fits in a big data system.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 3 章](ch03.xhtml#streaming-architectures) 中，我们描绘了流数据平台的一般架构图，并确定了作为分布式处理引擎的
    Spark 在大数据系统中的位置。
- en: This architecture informed us about what to expect in terms of interfaces and
    links to the rest of the ecosystem, especially as we focus on stream data processing
    with Apache Spark. Stream processing, whether in its Spark Streaming or Structured
    Streaming incarnation, is another *execution mode* for Apache Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构告诉我们，在处理流数据时，特别是使用 Apache Spark 进行流数据处理时，可以期待接口和与生态系统其余部分的链接。无论是在其 Spark
    Streaming 还是 Structured Streaming 形式中，流处理都是 Apache Spark 的另一种 *执行模式*。
- en: In this chapter, we take a tour of the main features that make Spark stand out
    as a stream-processing engine.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍使 Spark 成为流处理引擎的主要特性。
- en: The Tale of Two APIs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两种 API 的故事
- en: 'As we mentioned in [“Introducing Apache Spark”](ch01.xhtml#spark-intro-in-intro),
    Spark offers two different stream-processing APIs, Spark Streaming and Structured
    Streaming:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [“介绍 Apache Spark”](ch01.xhtml#spark-intro-in-intro) 中提到的，Spark 提供了两种不同的流处理
    API，即 Spark Streaming 和 Structured Streaming：
- en: Spark Streaming
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: This is an API and a set of connectors, in which a Spark program is being served
    small batches of data collected from a stream in the form of microbatches spaced
    at fixed time intervals, performs a given computation, and eventually returns
    a result at every interval.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 API 和一组连接器，其中 Spark 程序以微批次的形式服务于从流中收集的数据，这些微批次在固定时间间隔内间隔，并执行给定的计算，并在每个间隔最终返回结果。
- en: Structured Streaming
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理
- en: This is an API and a set of connectors, built on the substrate of a SQL query
    optimizer, Catalyst. It offers an API based on `DataFrame`s and the notion of
    continuous queries over an unbounded table that is constantly updated with fresh
    records from the stream.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 API 和一组连接器，建立在 SQL 查询优化器 Catalyst 的基础上。它提供了基于 `DataFrame` 和不断更新的无界表的连续查询概念，从流中获取新记录。
- en: The interface that Spark offers on these fronts is particularly rich, to the
    point where this book devotes large parts explaining those two ways of processing
    streaming datasets. One important point to realize is that both APIs rely on the
    core capabilities of Spark and share many of the low-level features in terms of
    distributed computation, in-memory caching, and cluster interactions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 在这些方面提供的接口特别丰富，以至于本书大部分内容都在解释这两种处理流数据集的方式。需要注意的一点是，这两种 API 都依赖于 Spark
    的核心功能，并在分布式计算、内存缓存和集群交互等低级特性上共享许多特性。
- en: As a leap forward from its MapReduce predecessor, Spark offers a rich set of
    operators that allows the programmer to express complex processing, including
    machine learning or event-time manipulations. We examine more specifically the
    basic properties that allow Spark to perform this feat in a moment.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 作为其 MapReduce 前身的一大进步，Spark 提供了丰富的操作符集，允许程序员表达复杂的处理，包括机器学习或事件时间操作。我们将在稍后更详细地讨论允许
    Spark 实现这一功能的基本特性。
- en: We would just like to outline that these interfaces are by design as simple
    as their batch counterparts—operating on a `DStream` feels like operating on an
    `RDD`, and operating on a streaming `Dataframe` looks eerily like operating on
    a batch one.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只想概述一下，这些接口在设计上与其批处理对应物一样简单——在 `DStream` 上操作感觉就像在 `RDD` 上操作，在流式 `Dataframe`
    上操作看起来几乎与批处理类似。
- en: Apache Spark presents itself as a unified engine, offering developers a consistent
    environment whenever they want to develop a batch or a streaming application.
    In both cases, developers have all the power and speed of a distributed framework
    at hand.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 自称为统一引擎，为开发人员提供了一致的环境，无论他们想要开发批处理还是流处理应用程序。在这两种情况下，开发人员都可以利用分布式框架的全部能力和速度。
- en: This versatility empowers development agility. Before deploying a full-fledged
    stream-processing application, programmers and analysts first try to discover
    insights in interactive environments with a fast feedback loop. Spark offers a
    built-in shell, based on the Scala *REPL* (short for Read-Eval-Print-Loop) that
    can be used as prototyping grounds. There are several notebook implementations
    available, like Zeppelin, Jupyter, or the Spark Notebook, that take this interactive
    experience to a user-friendly web interface. This prototyping phase is essential
    in the early phases of development, and so is its velocity.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多功能性增强了开发的灵活性。在部署完整的流处理应用程序之前，程序员和分析师首先尝试在具有快速反馈循环的交互式环境中发现见解。Spark 提供了一个基于
    Scala *REPL*（Read-Eval-Print-Loop）的内置 shell，可用作原型开发场所。还有几种笔记本实现可供选择，如 Zeppelin、Jupyter
    或 Spark Notebook，它们将这种交互式体验带入用户友好的 Web 界面。这个原型开发阶段在开发的早期阶段至关重要，其速度也是如此。
- en: If you refer back to the diagram in [Figure 3-1](ch03.xhtml#big_data_parts),
    you will notice that what we called *results* in the chart are actionable insights—which
    often means revenue or cost-savings—are generated every time a loop (starting
    and ending at the business or scientific problem) is traveled fully. In sum, this
    loop is a crude representation of the experimental method, going through observation,
    hypothesis, experiment, measure, interpretation, and conclusion.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您回顾 [图 3-1](ch03.xhtml#big_data_parts) 中的图表，您会注意到我们在图表中称为 *results* 的内容实际上是可操作的见解——通常意味着收入或成本节省——每次完全遍历一个循环（从业务或科学问题开始并结束）时生成。总结来说，这个循环是实验方法的一个粗略表示，经历观察、假设、实验、测量、解释和结论。
- en: Apache Spark, in its streaming modules, has always made the choice to carefully
    manage the cognitive load of switching to a streaming application. It also has
    other major design choices that have a bearing on its stream-processing capabilities,
    starting with its in-memory storage.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 在其流处理模块中，始终致力于谨慎管理切换到流应用程序的认知负荷。它还有其他一些重要的设计选择，对其流处理能力有着重要影响，从其内存存储开始。
- en: Spark’s Memory Usage
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 的内存使用
- en: Spark offers in-memory storage of slices of a dataset, which must be initially
    loaded from a data source. The data source can be a distributed filesystem or
    another storage medium. Spark’s form of in-memory storage is analogous to the
    operation of caching data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了数据集片段的内存存储，必须从数据源初始加载。数据源可以是分布式文件系统或其他存储介质。Spark 的内存存储形式类似于数据缓存操作。
- en: Hence, a *value* in Spark’s in-memory storage has a *base*, which is its initial
    data source, and layers of successive operations applied to it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Spark 内存存储中的 *value* 具有一个 *base*，即其初始数据源，以及应用于它的连续操作层。
- en: Failure Recovery
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 失败恢复
- en: What happens in case of a failure? Because Spark knows exactly which data source
    was used to ingest the data in the first place, and because it also knows all
    the operations that were performed on it thus far, it can reconstitute the segment
    of lost data that was on a crashed executor, from scratch. Obviously, this goes
    faster if that reconstitution (*recovery*, in Spark’s parlance), does not need
    to be totally *from scratch*. So, Spark offers a replication mechanism, quite
    in a similar way to distributed filesystems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 发生故障时会发生什么？因为 Spark 精确知道首次用于摄取数据的数据源，也知道到目前为止对其执行的所有操作，因此它可以从头开始重建丢失数据片段，这是在崩溃执行器上的情况。显然，如果这种重建（在
    Spark 的术语中称为 *recovery*）不需要完全从头开始，速度会更快。因此，Spark 提供了一种复制机制，与分布式文件系统相似。
- en: However, because memory is such a valuable yet limited commodity, Spark makes
    (by default) the cache short lived.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于内存是如此宝贵但有限的资源，Spark 默认使缓存的存活时间很短。
- en: Lazy Evaluation
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 延迟评估
- en: As you will see in greater detail in later chapters, a good part of the operations
    that can be defined on values in Spark’s storage have a lazy execution, and it
    is the execution of a final, eager output operation that will trigger the actual
    execution of computation in a Spark cluster. It’s worth noting that if a program
    consists of a series of linear operations, with the previous one feeding into
    the next, the intermediate results *disappear* right after said next step has
    consumed its input.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将在后续章节中详细了解的那样，在 Spark 的存储中，可以定义在值上执行的大部分操作都是延迟执行的，只有在执行最终的渴望输出操作时，才会触发 Spark
    集群中的实际计算执行。值得注意的是，如果一个程序由一系列线性操作组成，前一个操作结果在下一个操作消耗其输入后会立即 *消失*。
- en: Cache Hints
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存提示
- en: On the other hand, what happens if we have several operations to do on a single
    intermediate result? Should we have to compute it several times? Thankfully, Spark
    lets users specify that an intermediate value is important and how its contents
    should be safeguarded for later.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们有多个操作要在单个中间结果上执行，会发生什么？我们是否必须多次计算它？幸运的是，Spark 允许用户指定中间值的重要性，并且如何保护其内容以供以后使用。
- en: '[Figure 4-1](#cache-dataflow-illustration) presents the data flow of such an
    operation.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-1](#cache-dataflow-illustration) 展示了此类操作的数据流程。'
- en: '![spas 0401](Images/spas_0401.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0401](Images/spas_0401.png)'
- en: Figure 4-1\. Operations on cached values
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1\. 对缓存值的操作
- en: Finally, Spark offers the opportunity to spill the cache to secondary storage
    in case it runs out of memory on the cluster, extending the in-memory operation
    to secondary—and significantly slower—storage to preserve the functional aspects
    of a data process when faced with temporary peak loads.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Spark 提供了在集群内存不足时将缓存溢出到二级存储的机会，将内存操作扩展到次级且明显较慢的存储，以保留数据处理过程的功能方面，当面对临时高峰负载时。
- en: Now that we have an idea of the main characteristics of Apache Spark, let’s
    spend some time focusing on one design choice internal to Spark, namely, the latency
    versus throughput trade-off.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对Apache Spark的主要特性有了一个大致了解，让我们花些时间专注于Spark内部的一个设计选择，即延迟与吞吐量的权衡。
- en: Understanding Latency
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解延迟
- en: Spark Streaming, as we mentioned, makes the choice of microbatching. It generates
    a chunk of elements on a fixed interval, and when that interval “tick” elapses,
    it begins processing the data collected over the last interval. Structured Streaming
    takes a slightly different approach in that it will make the interval in question
    as small as possible (the processing time of the last microbatch)—and proposing,
    in some cases, a continuous processing mode, as well. Yet, nowadays, microbatching
    is still the dominating internal execution mode of stream processing in Apache
    Spark.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，Spark Streaming 选择了微批处理。它在固定的时间间隔内生成一批元素，当该间隔“滴答”结束时，开始处理上个间隔收集到的数据。结构化流处理则采取了稍微不同的方式，它会尽可能缩小所讨论的间隔（上一个微批处理的处理时间），在某些情况下还提出了连续处理模式。然而，微批处理仍然是Apache
    Spark流处理中主导的内部执行模式。
- en: A consequence of microbatching is that any microbatch delays the processing
    of any particular element of a batch by at least the time of the batch interval.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 微批处理的一个结果是，任何微批至少延迟了批处理的任何特定元素的处理时间。
- en: 'Firstly, microbatches create a baseline latency. The jury is still out on how
    small it is possible to make this latency, though approximately one second is
    a relatively common number for the lower bound. For many applications, a latency
    in the space of a few minutes is sufficient; for example:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，微批处理创建了基准延迟。目前还不清楚可以将这种延迟减小到多小，尽管大约一秒钟是较为常见的下限数值。对于许多应用程序来说，几分钟的延迟已经足够；例如：
- en: Having a dashboard that refreshes you on key performance indicators of your
    website over the last few minutes
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有仪表板，可以在过去几分钟内刷新您网站的关键性能指标
- en: Extracting the most recent trending topics in a social network
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取社交网络中最近的热门话题
- en: Computing the energy consumption trends of a group of households
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算一组家庭能耗趋势
- en: Introducing new media in a recommendation system
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推荐系统中引入新媒体
- en: Whereas Spark is an equal-opportunity processor and delays all data elements
    for (at most) one batch before acting on them, some other streaming engines exist
    that can fast-track some elements that have priority, ensuring a faster responsivity
    for them. If your response time is essential for these specific elements, alternative
    stream processors like Apache Flink or Apache Storm might be a better fit. But
    if you’re just interested in fast processing *on average*, such as when monitoring
    a system, Spark makes an interesting proposition.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Spark是一个平等机会的处理器，并延迟了所有数据元素（最多）一个批次才对其进行操作，但还存在一些其他流处理引擎，可以快速处理具有优先级的某些元素，确保它们的响应速度更快。如果对这些特定元素的响应时间至关重要，那么像Apache
    Flink或Apache Storm这样的替代流处理器可能更合适。但如果您只对*平均*快速处理感兴趣，比如监控系统时，Spark提供了一个有趣的选择。
- en: Throughput-Oriented Processing
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面向吞吐量的处理
- en: All in all, where Spark truly excels at stream processing is with throughput-oriented
    data analytics.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，Spark 在流处理方面真正擅长的是面向吞吐量的数据分析。
- en: 'We can compare the microbatch approach to a train: it arrives at the station,
    waits for passengers for a given period of time and then transports all passengers
    that boarded to their destination. Although taking a car or a taxi for the same
    trajectory might allow one passenger to travel faster door to door, the batch
    of passengers in the train ensures that far more travelers arrive at their destination.
    The train offers higher throughput for the same trajectory, at the expense that
    some passengers must wait until the train departs.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将微批处理方法比作火车：它到达车站，等待一段时间的乘客，然后将所有上车的乘客一起运送到他们的目的地。尽管对于同样的行程，乘客可能选择驾车或出租车可以更快地从门到门出发，但火车的乘客批量确保更多旅客抵达他们的目的地。火车为相同的行程提供更高的吞吐量，但有些乘客必须等到火车启程。
- en: The Spark core engine is optimized for distributed batch processing. Its application
    in a streaming context ensures that large amounts of data can be processed per
    unit of time. Spark amortizes the overhead of distributed task scheduling by having
    many elements to process at once and, as we saw earlier in this chapter, it utilizes
    in-memory techniques, query optimizations, caching, and even code generation to
    speed up the transformational process of a dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Spark核心引擎经过优化，适用于分布式批处理。在流处理上的应用确保了每单位时间可以处理大量数据。Spark通过一次处理多个元素摊销了分布式任务调度的开销，并且正如我们在本章早期看到的那样，它利用内存技术、查询优化、缓存甚至代码生成来加速数据集的转换过程。
- en: When using Spark in an end-to-end application, an important constraint is that
    downstream systems receiving the processed data must also be able to accept the
    full output provided by the streaming process. Otherwise, we risk creating application
    bottlenecks that might cause cascading failures when faced with sudden load peaks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Spark进行端到端应用时，一个重要的约束是接收处理数据的下游系统必须能够接受流处理过程提供的完整输出。否则，在突然的负载高峰面前，我们可能会出现应用程序瓶颈，从而导致级联故障。
- en: Spark’s Polyglot API
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的多语言API
- en: 'We have now outlined the main design foundations of Apache Spark as they affect
    stream processing, namely a rich API and an in-memory processing model, defined
    within the model of an execution engine. We have explored the specific streaming
    modes of Apache Spark, and still at a high level, we have determined that the
    predominance of microbatching makes us think of Spark as more adapted to throughput-oriented
    tasks, for which more data yields more quality. We now want to bring our attention
    to one additional aspect where Spark shines: its programming ecosystem.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经概述了Apache Spark的主要设计基础，这些基础对流处理有影响，即丰富的API和内存处理模型，定义在执行引擎的模型内。我们已经探讨了Apache
    Spark的特定流处理模式，从较高的角度来看，我们确定了微批处理的主导性使我们认为Spark更适合面向吞吐量导向的任务，其中更多的数据意味着更高的质量。现在，我们希望把注意力放在Spark突出的另一个方面：其编程生态系统。
- en: Spark was first coded as a Scala-only project. As its interest and adoption
    widened, so did the need to support different user profiles, with different backgrounds
    and programming language skills. In the world of scientific data analysis, Python
    and R are arguably the predominant languages of choice, whereas in the enterprise
    environment, Java has a dominant position.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Spark最初是作为一个仅限于Scala的项目编写的。随着兴趣和采纳的扩展，支持不同用户档案的需求也随之增加，这些用户具有不同的背景和编程语言技能。在科学数据分析领域，Python和R可以说是首选语言，而在企业环境中，Java占据主导地位。
- en: Spark, far from being just a library for distributing computation, has become
    a polyglot framework that the user can interface with using Scala, Java, Python,
    or the R language. The development language is still Scala, and this is where
    the main innovations come in.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Spark不仅仅是一个用于分布式计算的库，它已经成为一个多语言框架，用户可以使用Scala、Java、Python或R语言进行接口交互。开发语言仍然是Scala，这也是主要创新的地方。
- en: Caution
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The coverage of the Java API has for a long time been fairly synchronized with
    Scala, owing to the excellent Java compatibility offered by the Scala language.
    And although in Spark 1.3 and earlier versions Python was lagging behind in terms
    of functionalities, it is now mostly caught up. The newest addition is R, for
    which feature-completeness is an enthusiastic work in progress.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 长期以来，Java API的覆盖范围与Scala相当同步，这要归功于Scala语言提供的出色Java兼容性。尽管在Spark 1.3及更早版本中，Python在功能上落后，但现在大部分已经迎头赶上。最新增加的是R，其功能完备性正在积极进行中。
- en: This versatile interface has let programmers of various levels and backgrounds
    flock to Spark for implementing their own data analytics needs. The amazing and
    growing richness of the contributions to the Spark open source project are a testimony
    to the strength of Spark as a federating framework.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个多才多艺的接口让各种水平和背景的程序员都涌向 Spark，以满足他们自己的数据分析需求。对 Spark 开源项目的贡献的惊人且不断增长的丰富性证明了
    Spark 作为一个联合框架的强大性。
- en: Nevertheless, Spark’s approach to best catering to its users’ computing goes
    beyond letting them use their favorite programming language.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Spark 最好为其用户提供计算服务的方法不仅仅是让他们使用自己喜欢的编程语言。
- en: Fast Implementation of Data Analysis
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速实现数据分析
- en: Spark’s advantages in developing a streaming data analytics pipeline go beyond
    offering a concise, high-level API in Scala and compatible APIs in Java and Python.
    It also offers the simple model of Spark as a practical shortcut throughout the
    development process.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 在开发流数据分析管道方面的优势不仅在于提供 Scala 中简洁的高级 API，还在于 Java 和 Python 中的兼容 API。它还提供了作为开发过程中的实用快捷方式的
    Spark 的简单模型。
- en: Component reuse with Spark is a valuable asset, as is access to the Java ecosystem
    of libraries for machine learning and many other fields. As an example, Spark
    lets users benefit from, for instance, the [Stanford CoreNLP](http://bit.ly/2UMALSk)
    library with ease, letting you avoid the painful task of writing a tokenizer.
    All in all, this lets you quickly prototype your streaming data pipeline solution,
    getting first results quickly enough to choose the right components at every step
    of the pipeline development.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中实现组件复用是一个有价值的资产，如同使用 Java 生态系统中用于机器学习和许多其他领域的库。例如，Spark 让用户可以轻松地从[斯坦福
    CoreNLP](http://bit.ly/2UMALSk) 库中受益，从而避免编写分词器这样的痛苦任务。总之，这使您可以快速原型化您的流数据管道解决方案，快速获得第一批结果，以便在管道开发的每个步骤中选择合适的组件。
- en: Finally, stream processing with Spark lets you benefit from its model of fault
    tolerance, leaving you with the confidence that faulty machines are not going
    to bring the streaming application to its knees. If you have enjoyed the automatic
    restart of failed Spark jobs, you will doubly appreciate that resiliency when
    running a 24/7 streaming operation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过 Spark 进行流处理让您受益于其容错模型，使您有信心故障机器不会使流应用程序陷入困境。如果您喜欢失败的 Spark 作业的自动重启，那么在运行
    24/7 的流式操作时，您将会双倍欣赏这种弹性。
- en: 'In conclusion, Spark is a framework that, while making trade-offs in latency,
    optimizes for building a data analytics pipeline with agility: fast prototyping
    in a rich environment and stable runtime performance under adverse conditions
    are problems it recognizes and tackles head-on, offering users significant advantages.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，Spark 是一个框架，尽管在延迟方面做出了一些妥协，但在构建数据分析管道时进行了优化：在丰富的环境中进行快速原型设计，并在不利条件下保持稳定的运行时性能是它认可并直接解决的问题，为用户提供了显著的优势。
- en: To Learn More About Spark
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欲了解更多关于 Spark 的信息
- en: This book is focused on streaming. As such, we move quickly through the Spark-centric
    concepts, in particular about batch processing. The most detailed references are
    [[Karau2015]](app01.xhtml#Karau2015) and [[Chambers2018]](app01.xhtml#Chambers2018).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本书专注于流处理。因此，我们迅速介绍了与 Spark 相关的概念，特别是关于批处理的部分。最详细的参考资料分别是[[Karau2015]](app01.xhtml#Karau2015)
    和 [[Chambers2018]](app01.xhtml#Chambers2018)。
- en: On a more low-level approach, the official documentation in the [Spark Programming
    guide](http://bit.ly/1Bns1XZ) is another accessible must-read.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在更低层次的方法上，[Spark 编程指南](http://bit.ly/1Bns1XZ) 的官方文档也是另一种可访问的必读资料。
- en: Summary
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about Spark and where it came from.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了 Spark 及其来源。
- en: You saw how Spark extends that model with key performance improvements, notably
    in-memory computing, as well as how it expands on the API with new higher-order
    functions.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您已经看到了 Spark 如何通过关键的性能改进来扩展该模型，特别是在内存计算方面，以及如何通过新的高阶函数扩展 API。
- en: We also considered how Spark integrates into the modern ecosystem of big data
    solutions, including the smaller footprint it focuses on, when compared to its
    older brother, Hadoop.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还考虑了 Spark 如何集成到大数据解决方案的现代生态系统中，包括其与其老大哥 Hadoop 相比，专注于更小的占用空间。
- en: We focused on the streaming APIs and, in particular, on the meaning of their
    microbatching approach, what uses they are appropriate for, as well as the applications
    they would not serve well.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们专注于流式 API，特别是它们的微批处理方法的含义，适合的用途以及它们不适用的应用程序。
- en: Finally, we considered stream processing in the context of Spark, and how building
    a pipeline with agility, along with a reliable, fault-tolerant deployment is its
    best use case.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们考虑了在Spark环境中的流处理，以及如何通过敏捷地构建流水线，加上可靠、容错的部署来实现最佳使用案例。
