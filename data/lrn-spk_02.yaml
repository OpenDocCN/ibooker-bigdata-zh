- en: 'Chapter 1\. Introduction to Apache Spark: A Unified Analytics Engine'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 1 章：Apache Spark 简介：统一分析引擎
- en: This chapter lays out the origins of Apache Spark and its underlying philosophy.
    It also surveys the main components of the project and its distributed architecture.
    If you are familiar with Spark’s history and the high-level concepts, you can
    skip this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了 Apache Spark 的起源及其基本理念。它还介绍了项目的主要组件及其分布式架构。如果您熟悉 Spark 的历史和高级概念，可以跳过本章。
- en: The Genesis of Spark
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 的起源
- en: 'In this section, we’ll chart the course of Apache Spark’s short evolution:
    its genesis, inspiration, and adoption in the community as a de facto big data
    unified processing engine.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将梳理 Apache Spark 短暂发展的历程：它的起源、灵感以及在社区中作为事实上的大数据统一处理引擎的采用。
- en: Big Data and Distributed Computing at Google
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谷歌的大数据与分布式计算
- en: 'When we think of scale, we can’t help but think of the ability of Google’s
    search engine to index and search the world’s data on the internet at lightning
    speed. The name Google is synonymous with scale. In fact, Google is a deliberate
    misspelling of the mathematical term *googol*: that’s 1 plus 100 zeros!'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论规模时，我们不禁会想到谷歌搜索引擎在互联网上索引和搜索数据的闪电般速度。谷歌这个名字与规模是同义词。事实上，谷歌是对数学术语 *googol*
    的故意拼写错误：那是 1 加上 100 个零！
- en: Neither traditional storage systems such as relational database management systems
    (RDBMSs) nor imperative ways of programming were able to handle the scale at which
    Google wanted to build and search the internet’s indexed documents. The resulting
    need for new approaches led to the creation of the [*Google File System* (GFS)](https://oreil.ly/-6H9D),
    [*MapReduce* (MR)](https://oreil.ly/08zaO), and [*Bigtable*](https://oreil.ly/KfS8C).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的存储系统如关系数据库管理系统（RDBMS）和命令式编程方式都无法处理谷歌希望构建和搜索互联网索引文档的规模。由此产生的需求推动了 [*Google
    File System*（GFS）](https://oreil.ly/-6H9D)、[*MapReduce*（MR）](https://oreil.ly/08zaO)
    和 [*Bigtable*](https://oreil.ly/KfS8C) 的创建。
- en: While GFS provided a fault-tolerant and distributed filesystem across many commodity
    hardware servers in a cluster farm, Bigtable offered scalable storage of structured
    data across GFS. MR introduced a new parallel programming paradigm, based on functional
    programming, for large-scale processing of data distributed over GFS and Bigtable.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 GFS 在集群农场的许多廉价硬件服务器上提供了容错和分布式文件系统，但 Bigtable 在 GFS 上提供了结构化数据的可扩展存储。MR 引入了一种基于函数式编程的新并行编程范式，用于在分布在
    GFS 和 Bigtable 上的数据上进行大规模处理。
- en: In essence, your MR applications interact with the [MapReduce system](https://oreil.ly/T0f8r)
    that sends computation code (map and reduce functions) to where the data resides,
    favoring data locality and cluster rack affinity rather than bringing data to
    your application.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，您的 MR 应用与 [MapReduce 系统](https://oreil.ly/T0f8r) 交互，将计算代码（映射和减少函数）发送到数据所在的位置，支持数据局部性和集群机架亲和性，而不是将数据带到应用程序。
- en: The workers in the cluster aggregate and reduce the intermediate computations
    and produce a final appended output from the reduce function, which is then written
    to a distributed storage where it is accessible to your application. This approach
    significantly reduces network traffic and keeps most of the input/output (I/O)
    local to disk rather than distributing it over the network.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点聚合和减少中间计算，通过减少函数生成最终附加输出，然后将其写入分布式存储中，供应用程序访问。这种方法显著减少了网络流量，并使大部分输入/输出（I/O）保持在本地磁盘而非分布在网络上。
- en: Most of the work Google did was proprietary, but the ideas expressed in the
    aforementioned [three papers](https://oreil.ly/HokID) spurred innovative ideas
    elsewhere in the open source community—especially at Yahoo!, which was dealing
    with similar big data challenges of scale for its search engine.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然谷歌的大部分工作是专有的，但上述三篇论文中表达的思想在开源社区中激发了创新思想，尤其是在雅虎等面对类似大规模数据挑战的地方。
- en: Hadoop at Yahoo!
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 雅虎的 Hadoop！
- en: 'The computational challenges and solutions expressed in Google’s GFS paper
    provided a blueprint for the [Hadoop File System (HDFS)](https://oreil.ly/JfsBd),
    including the MapReduce implementation as a framework for distributed computing.
    Donated to the [Apache Software Foundation (ASF)](https://www.apache.org/), a
    vendor-neutral non-profit organization, in April 2006, it became part of the [Apache
    Hadoop](https://oreil.ly/twL6R) framework of related modules: Hadoop Common, MapReduce,
    HDFS, and Apache Hadoop YARN.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的GFS论文中表达的计算挑战和解决方案为[Hadoop文件系统（HDFS）](https://oreil.ly/JfsBd)提供了一个蓝图，包括作为分布式计算框架的MapReduce实现。2006年4月捐赠给[Apache软件基金会（ASF）](https://www.apache.org/)，这成为了相关模块的Apache
    Hadoop框架的一部分：Hadoop Common、MapReduce、HDFS和Apache Hadoop YARN。
- en: Although Apache Hadoop had garnered widespread adoption outside Yahoo!, inspiring
    a large open source community of contributors and two open source–based commercial
    companies (Cloudera and Hortonworks, now merged), the MapReduce framework on HDFS
    had a few shortcomings.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Apache Hadoop在Yahoo!之外获得了广泛应用，激发了一个庞大的开源社区，吸引了许多贡献者和两家基于开源的商业公司（Cloudera和Hortonworks，现已合并），但在HDFS上的MapReduce框架存在一些不足之处。
- en: 'First, it was hard to manage and administer, with cumbersome operational complexity.
    Second, its general batch-processing MapReduce API was verbose and required a
    lot of boilerplate setup code, with brittle fault tolerance. Third, with large
    batches of data jobs with many pairs of MR tasks, each pair’s intermediate computed
    result is written to the local disk for the subsequent stage of its operation
    (see [Figure 1-1](#intermittent_iteration_of_reads_and_writ)). This repeated performance
    of disk I/O took its toll: large MR jobs could run for hours on end, or even days.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它很难管理和管理，操作复杂性令人生畏。其次，其一般批处理MapReduce API冗长，需要大量样板设置代码，容错性脆弱。第三，对于具有许多MR任务对的大批量数据作业，每对任务的中间计算结果都写入本地磁盘以供后续阶段操作使用（参见[图1-1](#intermittent_iteration_of_reads_and_writ)）。这种重复的磁盘I/O操作付出了代价：大规模MR作业可能运行数小时甚至数天。
- en: '![Intermittent iteration of reads and writes between map and reduce computations](assets/lesp_0101.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![在映射和减少计算之间间歇性迭代的读写](assets/lesp_0101.png)'
- en: Figure 1-1\. Intermittent iteration of reads and writes between map and reduce
    computations
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1\. 在映射和减少计算之间间歇性迭代的读写
- en: And finally, even though Hadoop MR was conducive to large-scale jobs for general
    batch processing, it fell short for combining other workloads such as machine
    learning, streaming, or interactive SQL-like queries.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管Hadoop MR有助于一般批处理的大规模作业，但它在结合其他工作负载（如机器学习、流处理或交互式SQL查询）方面表现不佳。
- en: To handle these new workloads, engineers developed bespoke systems (Apache Hive,
    Apache Storm, Apache Impala, Apache Giraph, Apache Drill, Apache Mahout, etc.),
    each with their own APIs and cluster configurations, further adding to the operational
    complexity of Hadoop and the steep learning curve for developers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这些新的工作负载，工程师们开发了定制系统（如Apache Hive、Apache Storm、Apache Impala、Apache Giraph、Apache
    Drill、Apache Mahout等），每个系统都有自己的API和集群配置，进一步增加了Hadoop的操作复杂性和开发人员的陡峭学习曲线。
- en: The question then became (bearing in mind Alan Kay’s adage, “Simple things should
    be simple, complex things should be possible”), was there a way to make Hadoop
    and MR simpler and faster?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当时的问题是（牢记Alan Kay的格言：“简单的事情应该简单，复杂的事情应该可能”），是否有办法使Hadoop和MR更简单更快？
- en: Spark’s Early Years at AMPLab
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark在AMPLab的早期发展阶段
- en: Researchers at UC Berkeley who had previously worked on Hadoop MapReduce took
    on this challenge with a project they called *Spark*. They acknowledged that MR
    was inefficient (or intractable) for interactive or iterative computing jobs and
    a complex framework to learn, so from the onset they embraced the idea of making
    Spark simpler, faster, and easier. This endeavor started in 2009 at the RAD Lab,
    which later became the AMPLab (and now is known as the RISELab).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 加州大学伯克利分校的研究人员曾参与过Hadoop MapReduce项目，他们接受了这一挑战，并启动了一个名为*Spark*的项目。他们意识到MR在交互式或迭代计算作业中效率低下（或难以解决），且是一个复杂的学习框架，因此从一开始就采纳了使Spark更简单、更快、更易用的理念。这项努力始于2009年的RAD实验室，后来成为AMPLab（现在被称为RISELab）。
- en: '[Early papers](https://oreil.ly/RFY2w) published on Spark demonstrated that
    it was 10 to 20 times faster than Hadoop MapReduce for certain jobs. Today, it’s
    [many orders of magnitude faster](https://spark.apache.org). The central thrust
    of the Spark project was to bring in ideas borrowed from Hadoop MapReduce, but
    to enhance the system: make it highly fault tolerant and embarrassingly parallel,
    support in-memory storage for intermediate results between iterative and interactive
    map and reduce computations, offer easy and composable APIs in multiple languages
    as a programming model, and support other workloads in a unified manner. We’ll
    come back to this idea of unification shortly, as it’s an important theme in Spark.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 早期关于 Spark 的论文表明，对于某些工作，它比 Hadoop MapReduce 快 10 到 20 倍。今天，它比前者快几个数量级。Spark
    项目的核心目标是借鉴 Hadoop MapReduce 的思想，但增强系统：使其高度容错和尴尬并行，支持在迭代和交互式映射和减少计算之间的中间结果内存存储，提供多语言作为编程模型的简单和可组合的
    API，并统一支持其他工作负载。我们很快会回到这个统一的概念，因为这是 Spark 中的重要主题。
- en: By 2013 Spark had gained widespread use, and some of its original creators and
    researchers—Matei Zaharia, Ali Ghodsi, Reynold Xin, Patrick Wendell, Ion Stoica,
    and Andy Konwinski—donated the Spark project to the ASF and formed a company called
    Databricks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 到了 2013 年，Spark 已被广泛使用，其原始创作者和研究人员之一 —— Matei Zaharia、Ali Ghodsi、Reynold Xin、Patrick
    Wendell、Ion Stoica 和 Andy Konwinski —— 将 Spark 项目捐赠给 ASF，并成立了名为 Databricks 的公司。
- en: Databricks and the community of open source developers worked to release [Apache
    Spark 1.0](https://oreil.ly/Pq11v) in May 2014, under the governance of the ASF.
    This first major release established the momentum for frequent future releases
    and contributions of notable features to Apache Spark from Databricks and over
    100 commercial vendors.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 和开源社区的开发者们在 Apache Spark 1.0 发布于 2014 年 5 月，由 ASF 管理。这个首个重要版本为未来频繁发布和
    Databricks 以及超过 100 家商业供应商贡献显著特性奠定了基础。
- en: What Is Apache Spark?
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Apache Spark？
- en: '[Apache Spark](https://spark.apache.org) is a unified engine designed for large-scale
    distributed data processing, on premises in data centers or in the cloud.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个统一引擎，专为大规模分布式数据处理设计，可以在数据中心或云中进行。
- en: Spark provides in-memory storage for intermediate computations, making it much
    faster than Hadoop MapReduce. It incorporates libraries with composable APIs for
    machine learning (MLlib), SQL for interactive queries (Spark SQL), stream processing
    (Structured Streaming) for interacting with real-time data, and graph processing
    (GraphX).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供内存存储以进行中间计算，使其比 Hadoop MapReduce 快得多。它包含具有可组合 API 的库，用于机器学习（MLlib）、交互式查询的
    SQL（Spark SQL）、与实时数据交互的流处理（Structured Streaming）以及图处理（GraphX）。
- en: 'Spark’s design philosophy centers around four key characteristics:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的设计理念围绕着四个关键特性展开：
- en: Speed
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度
- en: Ease of use
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用便捷性
- en: Modularity
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模块化
- en: Extensibility
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展性
- en: Let’s take a look at what this means for the framework.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这对框架意味着什么。
- en: Speed
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速度
- en: Spark has pursued the goal of speed in several ways. First, its internal implementation
    benefits immensely from the hardware industry’s recent huge strides in improving
    the price and performance of CPUs and memory. Today’s commodity servers come cheap,
    with hundreds of gigabytes of memory, multiple cores, and the underlying Unix-based
    operating system taking advantage of efficient multithreading and parallel processing.
    The framework is optimized to take advantage of all of these factors.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 在多个方面追求速度目标。首先，其内部实现受益于硬件行业近年来在提高 CPU 和内存价格性能方面的巨大进展。如今的商品服务器价格便宜，拥有数百
    GB 的内存、多个核心，并且基于 Unix 的操作系统利用高效的多线程和并行处理。框架被优化以充分利用所有这些因素。
- en: Second, Spark builds its query computations as a directed acyclic graph (DAG);
    its DAG scheduler and query optimizer construct an efficient computational graph
    that can usually be decomposed into tasks that are executed in parallel across
    workers on the cluster. And third, its physical execution engine, Tungsten, uses
    whole-stage code generation to generate compact code for execution (we will cover
    SQL optimization and whole-stage code generation in [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，Spark将其查询计算构建为有向无环图（DAG）；其DAG调度器和查询优化器构建了一个高效的计算图，通常可以分解为在集群上的工作节点上并行执行的任务。第三，其物理执行引擎Tungsten使用整体阶段代码生成来生成紧凑的执行代码（我们将在[第三章](ch03.html#apache_sparkapostrophes_structured_apis)中讨论SQL优化和整体阶段代码生成）。
- en: With all the intermediate results retained in memory and its limited disk I/O,
    this gives it a huge performance boost.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所有中间结果保留在内存中，并且其有限的磁盘I/O使其性能大幅提升。
- en: Ease of Use
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用简便性
- en: Spark achieves simplicity by providing a fundamental abstraction of a simple
    logical data structure called a Resilient Distributed Dataset (RDD) upon which
    all other higher-level structured data abstractions, such as DataFrames and Datasets,
    are constructed. By providing a set of *transformations* and *actions* as *operations*,
    Spark offers a simple programming model that you can use to build big data applications
    in familiar languages.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Spark通过提供一个名为Resilient Distributed Dataset（RDD）的简单逻辑数据结构的基本抽象来实现简化。所有其他高级结构化数据抽象（如DataFrames和Datasets）都是在此基础上构建的。通过提供一组*转换*和*操作*作为*操作*，Spark提供了一个简单的编程模型，您可以在熟悉的语言中使用它来构建大数据应用程序。
- en: Modularity
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模块化
- en: 'Spark operations can be applied across many types of workloads and expressed
    in any of the supported programming languages: Scala, Java, Python, SQL, and R.
    Spark offers unified libraries with well-documented APIs that include the following
    modules as core components: Spark SQL, Spark Structured Streaming, Spark MLlib,
    and GraphX, combining all the workloads running under one engine. We’ll take a
    closer look at all of these in the next section.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Spark操作可应用于多种工作负载类型，并在支持的编程语言（Scala、Java、Python、SQL和R）中表达。Spark提供了统一的库，具有良好文档化的API，包括以下核心组件模块：Spark
    SQL、Spark Structured Streaming、Spark MLlib和GraphX，将所有工作负载组合在一个引擎下运行。我们将在下一节详细介绍所有这些内容。
- en: You can write a single Spark application that can do it all—no need for distinct
    engines for disparate workloads, no need to learn separate APIs. With Spark, you
    get a unified processing engine for your workloads.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以编写一个单一的Spark应用程序，可以处理所有内容——无需为不同的工作负载使用不同的引擎，也无需学习单独的API。使用Spark，您可以获得一个统一的处理引擎来处理您的工作负载。
- en: Extensibility
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性
- en: Spark focuses on its fast, parallel computation engine rather than on storage.
    Unlike Apache Hadoop, which included both storage and compute, Spark decouples
    the two. That means you can use Spark to read data stored in myriad sources—Apache
    Hadoop, Apache Cassandra, Apache HBase, MongoDB, Apache Hive, RDBMSs, and more—and
    process it all in memory. Spark’s `DataFrameReader`s and `DataFrameWriter`s can
    also be extended to read data from other sources, such as Apache Kafka, Kinesis,
    Azure Storage, and Amazon S3, into its logical data abstraction, on which it can
    operate.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Spark专注于其快速的并行计算引擎，而不是存储。与同时包括存储和计算的Apache Hadoop不同，Spark将这两者解耦。这意味着您可以使用Spark来读取存储在多种来源中的数据——如Apache
    Hadoop、Apache Cassandra、Apache HBase、MongoDB、Apache Hive、RDBMS等，并在内存中处理。Spark的`DataFrameReader`和`DataFrameWriter`还可以扩展到从其他来源（如Apache
    Kafka、Kinesis、Azure Storage和Amazon S3）读取数据，将其读入其逻辑数据抽象中，并对其进行操作。
- en: The community of Spark developers maintains a list of [third-party Spark packages](https://oreil.ly/2tIVP)
    as part of the growing ecosystem (see [Figure 1-2](#apache_sparkapostrophes_ecosystem)).
    This rich ecosystem of packages includes Spark connectors for a variety of external
    data sources, performance monitors, and more.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Spark开发者社区维护着一个[第三方Spark包](https://oreil.ly/2tIVP)列表，作为不断增长的生态系统的一部分（见[图1-2](#apache_sparkapostrophes_ecosystem)）。这个丰富的软件包生态系统包括用于各种外部数据源的Spark连接器、性能监视器等。
- en: '![Apache Spark’s ecosystem of connectors](assets/lesp_0102.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark生态系统的连接器](assets/lesp_0102.png)'
- en: Figure 1-2\. Apache Spark’s ecosystem of connectors
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2\. Apache Spark生态系统的连接器
- en: Unified Analytics
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统一分析
- en: While the notion of unification is not unique to Spark, it is a core component
    of its design philosophy and evolution. In November 2016, the Association for
    Computing Machinery (ACM) recognized Apache Spark and conferred upon its original
    creators the prestigious ACM Award for their [paper](https://oreil.ly/eak-T) describing
    Apache Spark as a “Unified Engine for Big Data Processing.” The award-winning
    paper notes that Spark replaces all the separate batch processing, graph, stream,
    and query engines like Storm, Impala, Dremel, Pregel, etc. with a unified stack
    of components that addresses diverse workloads under a single distributed fast
    engine.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管统一化的概念并不是Spark独有的，但它是其设计哲学和演变的核心组成部分。2016年11月，计算机协会（ACM）认可了Apache Spark，并授予其原始创作者的杰出ACM奖，以表彰他们关于Apache
    Spark作为“大数据处理统一引擎”的[论文](https://oreil.ly/eak-T)。获奖论文指出，Spark取代了所有独立的批处理、图形、流和查询引擎，如Storm、Impala、Dremel、Pregel等，使用一个统一的组件堆栈来处理各种工作负载。
- en: Apache Spark Components as a Unified Stack
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Spark组件作为统一堆栈
- en: 'As shown in [Figure 1-3](#apache_spark_components_and_api_stack), Spark offers
    four distinct components as libraries for diverse workloads: Spark SQL, Spark
    MLlib, Spark Structured Streaming, and GraphX. Each of these components is separate
    from Spark’s core fault-tolerant engine, in that you use APIs to write your Spark
    application and Spark converts this into a DAG that is executed by the core engine.
    So whether you write your Spark code using the provided Structured APIs (which
    we will cover in [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis))
    in Java, R, Scala, SQL, or Python, the underlying code is decomposed into highly
    compact bytecode that is executed in the workers’ JVMs across the cluster.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[1-3](#apache_spark_components_and_api_stack)所示，Spark提供了四个不同的组件作为用于各种工作负载的库：Spark
    SQL，Spark MLlib，Spark Structured Streaming和GraphX。每个组件都与Spark核心的容错引擎分开，您可以使用API编写您的Spark应用程序，Spark将其转换为由核心引擎执行的DAG。因此，无论您是使用提供的结构化API（我们将在[第3章](ch03.html#apache_sparkapostrophes_structured_apis)中介绍）还是在Java、R、Scala、SQL或Python中编写Spark代码，底层代码都会被分解为高度紧凑的字节码，在集群中的工作节点的JVM中执行。
- en: '![Apache Spark components and API stack](assets/lesp_0103.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark组件和API堆栈](assets/lesp_0103.png)'
- en: Figure 1-3\. Apache Spark components and API stack
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3\. Apache Spark组件和API堆栈
- en: Let’s look at each of these components in more detail.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下这些组件。
- en: Spark SQL
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark SQL
- en: This module works well with structured data. You can read data stored in an
    RDBMS table or from file formats with structured data (CSV, text, JSON, Avro,
    ORC, Parquet, etc.) and then construct permanent or temporary tables in Spark.
    Also, when using Spark’s Structured APIs in Java, Python, Scala, or R, you can
    combine SQL-like queries to query the data just read into a Spark DataFrame. To
    date, Spark SQL is [ANSI SQL:2003-compliant](https://oreil.ly/pJq1C) and it also
    functions as a pure SQL engine.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块适用于结构化数据。您可以读取存储在关系数据库表中的数据或从带有结构化数据的文件格式（CSV、文本、JSON、Avro、ORC、Parquet等）中读取数据，然后在Spark中构建永久或临时表。此外，当使用Java、Python、Scala或R中的Spark结构化API时，您可以组合类似SQL的查询来查询刚刚读入Spark
    DataFrame中的数据。迄今为止，Spark SQL符合[ANSI SQL:2003标准](https://oreil.ly/pJq1C)，它也可以作为纯SQL引擎运行。
- en: 'For example, in this Scala code snippet, you can read from a JSON file stored
    on Amazon S3, create a temporary table, and issue a SQL-like query on the results
    read into memory as a Spark DataFrame:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在这个Scala代码片段中，您可以从存储在Amazon S3上的JSON文件中读取数据，创建一个临时表，并对读入内存的Spark DataFrame执行类似SQL的查询：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can write similar code snippets in Python, R, or Java, and the generated
    bytecode will be identical, resulting in the same performance.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Python、R或Java中编写类似的代码片段，生成的字节码将是相同的，从而获得相同的性能。
- en: Spark MLlib
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark MLlib
- en: Spark comes with a library containing common machine learning (ML) algorithms
    called MLlib. Since Spark’s first release, the performance of this library component
    has improved significantly because of Spark 2.x’s underlying engine enhancements.
    MLlib provides many popular machine learning algorithms built atop high-level
    DataFrame-based APIs to build models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Spark附带一个包含常见机器学习（ML）算法的库，称为MLlib。自Spark首次发布以来，由于Spark 2.x底层引擎的增强，此库组件的性能显著提高。MLlib提供许多流行的机器学习算法，构建在基于高级DataFrame的API之上，用于构建模型。
- en: Note
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Starting with Apache Spark 1.6, the [MLlib project](https://oreil.ly/cyc1c)
    is split between two packages: `spark.mllib` and `spark.ml`. The DataFrame-based
    API is the latter while the former contains the RDD-based APIs, which are now
    in maintenance mode. All new features go into `spark.ml`. This book refers to
    “MLlib” as the umbrella library for machine learning in Apache Spark.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 自 Apache Spark 1.6 起，[MLlib 项目](https://oreil.ly/cyc1c) 分为两个包：`spark.mllib`
    和 `spark.ml`。基于 DataFrame 的 API 属于后者，而前者包含基于 RDD 的 API，现已处于维护模式。所有新功能都集中在 `spark.ml`
    中。本书将“MLlib”称为 Apache Spark 中机器学习的总体库。
- en: 'These APIs allow you to extract or transform features, build pipelines (for
    training and evaluating), and persist models (for saving and reloading them) during
    deployment. Additional utilities include the use of common linear algebra operations
    and statistics. MLlib includes other low-level ML primitives, including a generic
    gradient descent optimization. The following Python code snippet encapsulates
    the basic operations a data scientist may do when building a model (more extensive
    examples will be discussed in Chapters [10](ch10.html#machine_learning_with_mllib)
    and [11](ch11.html#managingcomma_deployingcomma_and_scaling)):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 API 允许您提取或转换特征、构建流水线（用于训练和评估），以及持久化模型（用于保存和重新加载）。附加工具包括使用常见线性代数运算和统计学。MLlib
    还包括其他低级别的 ML 原语，包括通用梯度下降优化。以下 Python 代码片段封装了数据科学家在构建模型时可能进行的基本操作（更详细的示例将在第 [10](ch10.html#machine_learning_with_mllib)
    和第 [11](ch11.html#managingcomma_deployingcomma_and_scaling) 章中讨论）：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Spark Structured Streaming
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark 结构化流处理
- en: Apache Spark 2.0 introduced an experimental [Continuous Streaming model](https://oreil.ly/YJSEq)
    and [Structured Streaming APIs](https://oreil.ly/NYYsJ), built atop the Spark
    SQL engine and DataFrame-based APIs. By Spark 2.2, Structured Streaming was generally
    available, meaning that developers could use it in their production environments.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 2.0 引入了一个实验性的[持续流处理模型](https://oreil.ly/YJSEq)和基于 DataFrame 的 [结构化流处理
    API](https://oreil.ly/NYYsJ)，构建在 Spark SQL 引擎之上。到 Spark 2.2，结构化流处理已经普遍可用，意味着开发者可以在生产环境中使用它。
- en: Necessary for big data developers to combine and react in real time to both
    static data and streaming data from engines like Apache Kafka and other streaming
    sources, the new model views a stream as a continually growing table, with new
    rows of data appended at the end. Developers can merely treat this as a structured
    table and issue queries against it as they would a static table.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大数据开发者而言，需要实时结合和响应来自 Apache Kafka 等引擎及其他流数据源的静态数据和流数据。新模型将流视为持续增长的表，新的数据行追加到末尾。开发者可以将其简单地视为结构化表，并像对待静态表一样进行查询。
- en: Underneath the Structured Streaming model, the Spark SQL core engine handles
    all aspects of fault tolerance and late-data semantics, allowing developers to
    focus on writing streaming applications with relative ease. This new model obviated
    the old DStreams model in Spark’s 1.x series, which we will discuss in more detail
    in [Chapter 8](ch08.html#structured_streaming). Furthermore, Spark 2.x and Spark
    3.0 extended the range of streaming data sources to include Apache Kafka, Kinesis,
    and HDFS-based or cloud storage.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化流处理模型下，Spark SQL 核心引擎处理所有容错和延迟数据语义的方面，使开发者能够相对轻松地编写流处理应用程序。这种新模型取代了 Spark
    1.x 系列中的旧 DStreams 模型，我们将在第 [Chapter 8](ch08.html#structured_streaming) 中详细讨论。此外，Spark
    2.x 和 Spark 3.0 扩展了流数据源的范围，包括 Apache Kafka、Kinesis 和基于 HDFS 或云存储的数据源。
- en: 'The following code snippet shows the typical anatomy of a Structured Streaming
    application. It reads from a localhost socket and writes the word count results
    to an Apache Kafka topic:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了结构化流处理应用程序的典型结构。它从本地主机的套接字读取，并将词频统计结果写入 Apache Kafka 主题：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: GraphX
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GraphX
- en: 'As the name suggests, GraphX is a library for manipulating graphs (e.g., social
    network graphs, routes and connection points, or network topology graphs) and
    performing graph-parallel computations. It offers the standard graph algorithms
    for analysis, connections, and traversals, contributed by users in the community:
    the available algorithms include PageRank, Connected Components, and Triangle
    Counting.^([1](ch01.html#ch01fn1))'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名，GraphX 是一个用于操作图形的库（例如社交网络图、路由和连接点或网络拓扑图），并执行图形并行计算。它提供了标准的图形分析、连接和遍历算法，社区用户贡献的算法包括
    PageRank、连通组件和三角形计数。^([1](ch01.html#ch01fn1))
- en: 'This code snippet shows a simple example of how to join two graphs using the
    GraphX APIs:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码片段展示了如何使用GraphX API简单地连接两个图形：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Apache Spark’s Distributed Execution
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Spark的分布式执行
- en: If you have read this far, you already know that Spark is a distributed data
    processing engine with its components working collaboratively on a cluster of
    machines. Before we explore programming with Spark in the following chapters of
    this book, you need to understand how all the components of Spark’s distributed
    architecture work together and communicate, and what deployment modes are available.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经阅读到这里，你已经知道Spark是一个分布式数据处理引擎，其组件在机群上协作工作。在我们探索本书后续章节中关于Spark编程之前，你需要理解Spark分布式架构的所有组件如何协同工作和通信，以及可用的部署模式。
- en: Let’s start by looking at each of the individual components shown in [Figure 1-4](#apache_spark_components_and_architecture)
    and how they fit into the architecture. At a high level in the Spark architecture,
    a Spark application consists of a driver program that is responsible for orchestrating
    parallel operations on the Spark cluster. The driver accesses the distributed
    components in the cluster—the Spark executors and cluster manager—through a `SparkSession`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从查看[图1-4](#apache_spark_components_and_architecture)中显示的每个个体组件及其如何适应架构开始。在Spark架构的高层次上，一个Spark应用程序由负责在Spark集群上协调并行操作的驱动程序组成。驱动程序通过`SparkSession`访问集群中的分布式组件。
- en: '![Apache Spark components and architecture](assets/lesp_0104.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark的组件和架构](assets/lesp_0104.png)'
- en: Figure 1-4\. Apache Spark components and architecture
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4\. Apache Spark的组件和架构
- en: Spark driver
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark驱动程序
- en: 'As the part of the Spark application responsible for instantiating a `SparkSession`,
    the Spark driver has multiple roles: it communicates with the cluster manager;
    it requests resources (CPU, memory, etc.) from the cluster manager for Spark’s
    executors (JVMs); and it transforms all the Spark operations into DAG computations,
    schedules them, and distributes their execution as tasks across the Spark executors.
    Once the resources are allocated, it communicates directly with the executors.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 作为负责实例化`SparkSession`的Spark应用程序的一部分，Spark驱动程序具有多重角色：它与集群管理器通信；它请求来自集群管理器的资源（CPU、内存等）给Spark的执行器（JVMs）；它将所有Spark操作转换为DAG计算，安排它们，并将它们作为任务分发到Spark执行器中。一旦资源分配完成，它直接与执行器通信。
- en: SparkSession
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SparkSession
- en: In Spark 2.0, the `SparkSession` became a unified conduit to all Spark operations
    and data. Not only did it [subsume previous entry points to Spark](https://oreil.ly/Ap0Pq)
    like the `SparkContext`, `SQLContext`, `HiveContext`, `SparkConf`, and `StreamingContext`,
    but it also made working with Spark simpler and easier.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0中，`SparkSession`成为了所有Spark操作和数据的统一通道。它不仅[取代了以前的Spark入口点](https://oreil.ly/Ap0Pq)，如`SparkContext`、`SQLContext`、`HiveContext`、`SparkConf`和`StreamingContext`，而且使得使用Spark变得更简单更容易。
- en: Note
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although in Spark 2.x the `SparkSession` subsumes all other contexts, you can
    still access the individual contexts and their respective methods. In this way,
    the community maintained backward compatibility. That is, your old 1.x code with
    `SparkContext` or `SQLContext` will still work.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在Spark 2.x中`SparkSession`取代了所有其他上下文，你仍然可以访问各个上下文及其各自的方法。这样做是为了保持向后兼容性。也就是说，你的旧1.x版本的代码中使用`SparkContext`或`SQLContext`仍然可以工作。
- en: Through this one conduit, you can create JVM runtime parameters, define DataFrames
    and Datasets, read from data sources, access catalog metadata, and issue Spark
    SQL queries. `SparkSession` provides a single unified entry point to all of Spark’s
    functionality.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个统一通道，你可以创建JVM运行时参数，定义DataFrames和Datasets，从数据源读取数据，访问目录元数据，并执行Spark SQL查询。`SparkSession`为所有Spark功能提供了单一统一的入口点。
- en: In a standalone Spark application, you can create a `SparkSession` using one
    of the high-level APIs in the programming language of your choice. In the Spark
    shell (more on this in the next chapter) the `SparkSession` is created for you,
    and you can access it via a global variable called `spark` or `sc`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在独立的Spark应用程序中，你可以使用所选编程语言中的一个高级API来创建`SparkSession`。在Spark shell中（更多内容请参阅下一章节），`SparkSession`会为你创建，并且可以通过全局变量`spark`或`sc`访问它。
- en: Whereas in Spark 1.x you would have had to create individual contexts (for streaming,
    SQL, etc.), introducing extra boilerplate code, in a Spark 2.x application you
    can create a `SparkSession` per JVM and use it to perform a number of Spark operations.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 1.x 中，您必须创建各种上下文（用于流处理、SQL 等），引入额外的样板代码。而在 Spark 2.x 应用程序中，您可以为每个 JVM
    创建一个 `SparkSession`，并使用它执行多个 Spark 操作。
- en: 'Let’s take a look at an example:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一个例子：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Cluster manager
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群管理器
- en: 'The cluster manager is responsible for managing and allocating resources for
    the cluster of nodes on which your Spark application runs. Currently, Spark supports
    four cluster managers: the built-in standalone cluster manager, Apache Hadoop
    YARN, Apache Mesos, and Kubernetes.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理器负责管理和分配运行 Spark 应用程序的节点集群的资源。目前，Spark 支持四种集群管理器：内置独立集群管理器、Apache Hadoop
    YARN、Apache Mesos 和 Kubernetes。
- en: Spark executor
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark 执行器
- en: A Spark executor runs on each worker node in the cluster. The executors communicate
    with the driver program and are responsible for executing tasks on the workers.
    In most deployments modes, only a single executor runs per node.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 执行器在集群中的每个工作节点上运行。执行器与驱动程序通信，并负责在工作节点上执行任务。在大多数部署模式中，每个节点上只运行一个执行器。
- en: Deployment modes
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署模式
- en: An attractive feature of Spark is its support for myriad deployment modes, enabling
    Spark to run in different configurations and environments. Because the cluster
    manager is agnostic to where it runs (as long as it can manage Spark’s executors
    and fulfill resource requests), Spark can be deployed in some of the most popular
    environments—such as Apache Hadoop YARN and Kubernetes—and can operate in different
    modes. [Table 1-1](#cheat_sheet_for_spark_deployment_modes) summarizes the available
    deployment modes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的一个吸引人的特点是其支持多种部署模式，使得 Spark 能够在不同的配置和环境中运行。由于集群管理器对其运行位置不加区分（只要能管理 Spark
    的执行器并满足资源请求），Spark 可以部署在一些最流行的环境中，比如 Apache Hadoop YARN 和 Kubernetes，并且可以以不同的模式运行。[表 1-1](#cheat_sheet_for_spark_deployment_modes)
    总结了可用的部署模式。
- en: Table 1-1\. Cheat sheet for Spark deployment modes
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1-1\. Spark 部署模式速查表
- en: '| Mode | Spark driver | Spark executor | Cluster manager |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Mode | Spark 驱动程序 | Spark 执行器 | 集群管理器 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Local | Runs on a single JVM, like a laptop or single node | Runs on the
    same JVM as the driver | Runs on the same host |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Local | 在单个 JVM 上运行，比如笔记本电脑或单节点 | 在与驱动程序相同的 JVM 上运行 | 在相同的主机上运行 |'
- en: '| Standalone | Can run on any node in the cluster | Each node in the cluster
    will launch its own executor JVM | Can be allocated arbitrarily to any host in
    the cluster |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Standalone | 可以在集群中的任何节点上运行 | 集群中的每个节点将启动自己的执行器 JVM | 可以随意分配到集群中的任何主机上 |'
- en: '| YARN (client) | Runs on a client, not part of the cluster | YARN’s NodeManager’s
    container | YARN’s Resource Manager works with YARN’s Application Master to allocate
    the containers on NodeManagers for executors |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| YARN (client) | 运行在客户端，不是集群的一部分 | YARN 的 NodeManager 容器 | YARN 的资源管理器与 YARN
    的应用程序主管一起为执行器在 NodeManagers 上分配容器 |'
- en: '| YARN (cluster) | Runs with the YARN Application Master | Same as YARN client
    mode | Same as YARN client mode |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| YARN (cluster) | 与 YARN Application Master 一起运行 | 与 YARN 客户端模式相同 | 与 YARN
    客户端模式相同 |'
- en: '| Kubernetes | Runs in a Kubernetes pod | Each worker runs within its own pod
    | Kubernetes Master |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Kubernetes | 运行在 Kubernetes 的 pod 中 | 每个 worker 在自己的 pod 中运行 | Kubernetes
    Master |'
- en: Distributed data and partitions
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式数据和分区
- en: Actual physical data is distributed across storage as partitions residing in
    either HDFS or cloud storage (see [Figure 1-5](#data_is_distributed_across_physical_mach)).
    While the data is distributed as partitions across the physical cluster, Spark
    treats each partition as a high-level logical data abstraction—as a DataFrame
    in memory. Though this is not always possible, each Spark executor is preferably
    allocated a task that requires it to read the partition closest to it in the network,
    observing data locality.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的物理数据分布在存储中，作为驻留在 HDFS 或云存储中的分区（见[图 1-5](#data_is_distributed_across_physical_mach)）。虽然数据作为分区分布在物理集群中，但
    Spark 将每个分区视为高级逻辑数据抽象——即内存中的 DataFrame。尽管这并非总是可能，但每个 Spark 执行器最好被分配一个需要它读取最接近它的网络中分区的任务，以保证数据本地性。
- en: '![Data is distributed across physical machines](assets/lesp_0105.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![数据分布在物理机器上](assets/lesp_0105.png)'
- en: Figure 1-5\. Data is distributed across physical machines
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-5\. 数据分布在物理机器上
- en: Partitioning allows for efficient parallelism. A distributed scheme of breaking
    up data into chunks or partitions allows Spark executors to process only data
    that is close to them, minimizing network bandwidth. That is, each executor’s
    core is assigned its own data partition to work on (see [Figure 1-6](#each_executorapostrophes_core_gets_a_par)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 分区允许有效的并行处理。将数据分解成块或分区的分布方案允许 Spark 执行器仅处理靠近它们的数据，从而最小化网络带宽。也就是说，每个执行器的核心被分配了自己的数据分区来处理（见[图
    1-6](#each_executorapostrophes_core_gets_a_par)）。
- en: '![Each executor’s core gets a partition of data to work on](assets/lesp_0106.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![每个执行器的核心获得一个数据分区来处理](assets/lesp_0106.png)'
- en: Figure 1-6\. Each executor’s core gets a partition of data to work on
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-6\. 每个执行器的核心获得一个数据分区来处理
- en: 'For example, this code snippet will break up the physical data stored across
    clusters into eight partitions, and each executor will get one or more partitions
    to read into its memory:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这段代码片段将把存储在集群中的物理数据分割成八个分区，每个执行器将获取一个或多个分区读入其内存：
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And this code will create a DataFrame of 10,000 integers distributed over eight
    partitions in memory:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将在内存中创建一个包含 10,000 个整数的 DataFrame，分布在八个分区中：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Both code snippets will print out `8`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个代码片段都将打印出`8`。
- en: In Chapters [3](ch03.html#apache_sparkapostrophes_structured_apis) and [7](ch07.html#optimizing_and_tuning_spark_applications),
    we will discuss how to tune and change partitioning configuration for maximum
    parallelism based on how many cores you have on your executors.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[3](ch03.html#apache_sparkapostrophes_structured_apis)章和第[7](ch07.html#optimizing_and_tuning_spark_applications)章中，我们将讨论如何调整和更改分区配置，以实现基于执行器核心数量的最大并行性。
- en: The Developer’s Experience
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发者的体验
- en: 'Of all the developers’ delights, none is more attractive than a set of composable
    APIs that increase productivity and are easy to use, intuitive, and expressive.
    One of Apache Spark’s principal appeals to developers has been its [easy-to-use
    APIs](https://oreil.ly/80dKh) for operating on small to large data sets, across
    languages: Scala, Java, Python, SQL, and R.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有开发者的喜悦中，没有比一组可组合的 API 更吸引人的了，它们可以提高生产力，易于使用，直观且表达力强。Apache Spark 最主要吸引开发者的一个特点是其[易于使用的
    API](https://oreil.ly/80dKh)，可用于操作小到大的数据集，跨语言：Scala、Java、Python、SQL 和 R。
- en: One primary motivation behind Spark 2.x was to unify and simplify the framework
    by limiting the number of concepts that developers have to grapple with. Spark
    2.x introduced higher-level abstraction APIs as domain-specific language constructs,
    which made programming Spark highly expressive and a pleasant developer experience.
    You express what you want the task or operation to compute, not how to compute
    it, and let Spark ascertain how best to do it for you. We will cover these Structured
    APIs in [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis), but first
    let’s take a look at who the Spark developers are.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.x 背后的一个主要动机是通过限制开发者需要处理的概念数量来统一和简化框架。Spark 2.x 引入了更高级别的抽象 API，作为特定领域语言构造，这使得编写
    Spark 程序变得高度表达性和开发体验愉快。您只需表达您希望任务或操作计算什么，而不是如何计算它，并让 Spark 来确定如何为您最好地执行。我们将在[第
    3 章](ch03.html#apache_sparkapostrophes_structured_apis)介绍这些结构化 API，但首先让我们看看 Spark
    开发者是谁。
- en: Who Uses Spark, and for What?
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谁在使用 Spark，以及用途是什么？
- en: Not surprisingly, most developers who grapple with big data are data engineers,
    data scientists, or machine learning engineers. They are drawn to Spark because
    it allows them to build a range of applications using a single engine, with familiar
    programming languages.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，大多数处理大数据的开发者是数据工程师、数据科学家或机器学习工程师。他们被吸引到 Spark，因为它允许他们使用单一引擎构建各种应用程序，并使用熟悉的编程语言。
- en: Of course, developers may wear many hats and sometimes do both data science
    and data engineering tasks, especially in startup companies or smaller engineering
    groups. Among all these tasks, however, data—massive amounts of data—is the foundation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，开发者可能会穿很多帽子，有时会同时执行数据科学和数据工程任务，特别是在初创公司或较小的工程团队中。然而，在所有这些任务中，数据——大量的数据——是基础。
- en: Data science tasks
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据科学任务
- en: As a discipline that has come to prominence in the era of big data, data science
    is about using data to tell stories. But before they can narrate the stories,
    data scientists have to cleanse the data, explore it to discover patterns, and
    build models to predict or suggest outcomes. Some of these tasks require knowledge
    of statistics, mathematics, computer science, and programming.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 作为在大数据时代崭露头角的学科，数据科学是利用数据讲述故事的过程。但在他们能够叙述故事之前，数据科学家必须清洗数据，探索数据以发现模式，并建立模型以预测或建议结果。其中一些任务需要统计学、数学、计算机科学和编程的知识。
- en: Most data scientists are proficient in using analytical tools like SQL, comfortable
    with libraries like NumPy and pandas, and conversant in programming languages
    like R and Python. But they must also know how to *wrangle* or *transform* data,
    and how to use established classification, regression, or clustering algorithms
    for building models. Often their tasks are iterative, interactive or ad hoc, or
    experimental to assert their hypotheses.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据科学家擅长使用SQL等分析工具，熟悉NumPy和pandas等库，能够使用R和Python等编程语言。但他们还必须知道如何*处理*或*转换*数据，以及如何使用成熟的分类、回归或聚类算法来构建模型。他们的任务通常是迭代的、交互式的或临时的，或者是为了验证他们的假设进行实验性的。
- en: Fortunately, Spark supports these different tools. Spark’s MLlib offers a common
    set of machine learning algorithms to build model pipelines, using high-level
    estimators, transformers, and data featurizers. Spark SQL and the Spark shell
    facilitate interactive and ad hoc exploration of data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Spark支持这些不同的工具。Spark的MLlib提供了一套通用的机器学习算法来构建模型管道，使用高级估计器、转换器和数据特征化工具。Spark
    SQL和Spark Shell支持对数据的交互式和临时探索。
- en: Additionally, Spark enables data scientists to tackle large data sets and scale
    their model training and evaluation. Apache Spark 2.4 introduced a new gang scheduler,
    as part of [Project Hydrogen](https://oreil.ly/8h3wr), to accommodate the fault-tolerant
    needs of training and scheduling deep learning models in a distributed manner,
    and Spark 3.0 has introduced the ability to support GPU resource collection in
    the standalone, YARN, and Kubernetes deployment modes. This means developers whose
    tasks demand deep learning techniques can use Spark.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark使数据科学家能够处理大数据集并扩展其模型训练和评估。Apache Spark 2.4作为[Project Hydrogen](https://oreil.ly/8h3wr)的一部分引入了新的集群调度程序，以适应分布式训练和调度深度学习模型的容错需求，而Spark
    3.0引入了在独立、YARN和Kubernetes部署模式下支持GPU资源收集的能力。这意味着那些需要深度学习技术的开发人员可以使用Spark。
- en: Data engineering tasks
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据工程任务
- en: After building their models, data scientists often need to work with other team
    members, who may be responsible for deploying the models. Or they may need to
    work closely with others to build and transform raw, dirty data into clean data
    that is easily consumable or usable by other data scientists. For example, a classification
    or clustering model does not exist in isolation; it works in conjunction with
    other components like a web application or a streaming engine such as Apache Kafka,
    or as part of a larger data pipeline. This pipeline is often built by data engineers.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立模型之后，数据科学家经常需要与其他团队成员合作，这些成员可能负责部署模型。或者他们可能需要与其他人密切合作，将原始的肮脏数据转换为干净的数据，以便其他数据科学家轻松消费或使用。例如，分类或聚类模型并不孤立存在；它们与诸如Web应用程序或Apache
    Kafka等流引擎之类的其他组件一起工作，或作为更大数据管道的一部分。这种管道通常由数据工程师构建。
- en: Data engineers have a strong understanding of software engineering principles
    and methodologies, and possess skills for building scalable data pipelines for
    a stated business use case. Data pipelines enable end-to-end transformations of
    raw data coming from myriad sources—data is cleansed so that it can be consumed
    downstream by developers, stored in the cloud or in NoSQL or RDBMSs for report
    generation, or made accessible to data analysts via business intelligence tools.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师对软件工程原则和方法论有很强的理解，并具备构建可扩展数据管道以支持业务用例的技能。数据管道使得从多种来源获取的原始数据能够进行端到端的转换——数据被清洗，以便下游开发者使用，存储在云中或NoSQL或RDBMS中进行报告生成，或通过商业智能工具对数据分析师可访问。
- en: Spark 2.x introduced an evolutionary streaming model called [*continuous applications*](https://oreil.ly/p0_fC)
    with Structured Streaming (discussed in detail in [Chapter 8](ch08.html#structured_streaming)).
    With Structured Streaming APIs, data engineers can build complex data pipelines
    that enable them to ETL data from both real-time and static data sources.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.x引入了一种称为[*continuous applications*](https://oreil.ly/p0_fC)的进化流处理模型，结构化流处理（在[第8章](ch08.html#structured_streaming)中详细讨论）。通过结构化流处理API，数据工程师可以构建复杂的数据管道，从实时和静态数据源进行ETL数据处理。
- en: Data engineers use Spark because it provides a simple way to parallelize computations
    and hides all the complexity of distribution and fault tolerance. This leaves
    them free to focus on using high-level DataFrame-based APIs and domain-specific
    language (DSL) queries to do ETL, reading and combining data from multiple sources.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师使用Spark，因为它提供了并行化计算的简便方式，并隐藏了分布和容错的所有复杂性。这使他们能够专注于使用基于高级DataFrame的API和领域特定语言（DSL）查询来进行ETL，读取和合并来自多个源的数据。
- en: The performance improvements in Spark 2.x and Spark 3.0, due to the [Catalyst
    optimizer](https://oreil.ly/pAHKJ) for SQL and [Tungsten](https://oreil.ly/nIE6h)
    for compact code generation, have made life for data engineers much easier. They
    can choose to use any of the three [Spark APIs](https://oreil.ly/c1sf8)—RDDs,
    DataFrames, or Datasets—that suit the task at hand, and reap the benefits of Spark.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于[Catalyst优化器](https://oreil.ly/pAHKJ)用于SQL和[Tungsten](https://oreil.ly/nIE6h)用于紧凑代码生成，Spark
    2.x和Spark 3.0中的性能提升大大简化了数据工程师的生活。他们可以选择使用适合手头任务的三种[Spark API](https://oreil.ly/c1sf8)之一——RDDs、DataFrames或Datasets，并享受Spark带来的益处。
- en: Popular Spark use cases
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 热门的Spark使用案例
- en: 'Whether you are a data engineer, data scientist, or machine learning engineer,
    you’ll find Spark useful for the following use cases:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您是数据工程师、数据科学家还是机器学习工程师，您都会发现Spark在以下用例中非常有用：
- en: Processing in parallel large data sets distributed across a cluster
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行处理分布在集群中的大数据集
- en: Performing ad hoc or interactive queries to explore and visualize data sets
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行自由查询或交互式查询以探索和可视化数据集
- en: Building, training, and evaluating machine learning models using MLlib
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MLlib构建、训练和评估机器学习模型
- en: Implementing end-to-end data pipelines from myriad streams of data
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从多种数据流实现端到端数据管道
- en: Analyzing graph data sets and social networks
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析图数据集和社交网络
- en: Community Adoption and Expansion
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社区采纳与扩展
- en: Not surprisingly, Apache Spark struck a chord in the open source community,
    especially among data engineers and data scientists. Its design philosophy and
    its inclusion as an Apache Software Foundation project have fostered immense interest
    among the developer community.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，Apache Spark在开源社区中引起了共鸣，尤其是在数据工程师和数据科学家中间。其设计理念及其作为Apache软件基金会项目的纳入，引发了开发者社区的极大兴趣。
- en: Today, there are over 600 [Apache Spark Meetup groups](https://oreil.ly/XjqQN)
    globally with close to half a million members. Every week, someone in the world
    is giving a talk at a meetup or conference or sharing a blog post on how to use
    Spark to build data pipelines. The [Spark + AI Summit](https://oreil.ly/G9vYT)
    is the largest conference dedicated to the use of Spark for machine learning,
    data engineering, and data science across many verticals.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 全球有超过600个[Apache Spark Meetup小组](https://oreil.ly/XjqQN)，近50万名成员。每周，世界各地都有人在Meetup或会议上发表演讲或分享博客文章，介绍如何使用Spark构建数据流水线。[Spark
    + AI Summit](https://oreil.ly/G9vYT)是专注于Spark在机器学习、数据工程和数据科学等多个垂直领域应用的最大会议。
- en: Since Spark’s first 1.0 release in 2014 there have been many minor and major
    releases, with the most recent major release of Spark 3.0 coming in 2020\. This
    book will cover aspects of Spark 2.x and Spark 3.0\. By the time of its publication
    the community will have released Spark 3.0, and most of the code in this book
    has been tested with Spark 3.0-preview2.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 自2014年Spark首次发布1.0版本以来，已发布了许多次次要和主要版本，其中最近的主要版本是2020年的Spark 3.0。本书将涵盖Spark 2.x和Spark
    3.0的各个方面。到出版时，社区将已发布Spark 3.0，并且本书中的大部分代码已经在Spark 3.0-preview2上进行了测试。
- en: Over the course of its releases, Spark has continued to attract contributors
    from across the globe and from numerous organizations. Today, Spark has close
    to 1,500 contributors, well over 100 releases, 21,000 forks, and some 27,000 commits
    on GitHub, as [Figure 1-7](#the_state_of_apache_spark_on_github_left) shows. And
    we hope that when you finish this book, you will feel compelled to contribute
    too.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在其发布过程中，Spark 一直吸引着来自全球和多个组织的贡献者。目前，Spark 拥有近 1,500 名贡献者，超过 100 次发布，21,000 个分支，以及约
    27,000 次提交，正如[图 1-7](#the_state_of_apache_spark_on_github_left) 所示。我们希望当你阅读完这本书后，也会产生贡献的冲动。
- en: '![The state of Apache Spark on GitHub (source: https://github.com/apache/spark)](assets/lesp_0107.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark 在 GitHub 上的状态（来源：https://github.com/apache/spark）](assets/lesp_0107.png)'
- en: 'Figure 1-7\. The state of Apache Spark on GitHub (source: [*https://github.com/apache/spark*](https://github.com/apache/spark))'
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-7\. Apache Spark 在 GitHub 上的状态（来源：[https://github.com/apache/spark](https://github.com/apache/spark)）
- en: Now we can turn our attention to the fun of learning—where and how to start
    using Spark. In the next chapter, we’ll show you how to get up and running with
    Spark in three simple steps.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始关注学习的乐趣——在哪里以及如何开始使用 Spark。下一章，我们将向你展示如何通过三个简单的步骤快速启动 Spark。
- en: ^([1](ch01.html#ch01fn1-marker)) Contributed to the community by Databricks
    as an open source project, [GraphFrames](https://oreil.ly/_JGxi) is a general
    graph processing library that is similar to Apache Spark’s GraphX but uses DataFrame-based
    APIs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.html#ch01fn1-marker)) 作为开源项目由 Databricks 贡献给社区，[GraphFrames](https://oreil.ly/_JGxi)
    是一个通用的图处理库，类似于 Apache Spark 的 GraphX，但使用基于 DataFrame 的 API。
