- en: Chapter 3\. Setting Up Your Data Models and Ingesting Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 设置您的数据模型和摄入数据
- en: Now that you have set up your Amazon Redshift data warehouse, let’s consider
    a data management strategy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经设置好了Amazon Redshift数据仓库，让我们考虑一个数据管理策略。
- en: In this chapter, we will discuss a few options for your data management strategy
    and whether you should employ a [“Data Lake First Versus Data Warehouse First
    Strategy”](#datalake-vs-data-warehouse). Next, we’ll go into [“Defining Your Data
    Model”](#defining-your-data_model) and use the [“Student Information Learning
    Analytics Dataset”](#sis_dataset) to illustrate how to create tables and [“Load
    Batch Data into Amazon Redshift”](#loading-data) using a sample of this data in
    Amazon S3\. However, in today’s world, where speed to insights is critical to
    maintaining your competetive edge, we’ll also show you how to [“Load Real-Time
    and Near Real-Time Data”](#loading-realtime-data). Lastly, we’ll cover how you
    can [“Optimize Your Data Structures”](#optimizing-data-structures).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论几种数据管理策略选项，以及是否应该采用[“数据湖优先与数据仓库优先策略”](#datalake-vs-data-warehouse)。接下来，我们将进入[“定义您的数据模型”](#defining-your-data_model)，并使用[“学生信息学习分析数据集”](#sis_dataset)来说明如何在Amazon
    S3中创建表格和[“批量加载数据到Amazon Redshift”](#loading-data)。然而，在今天的竞争激烈的世界中，洞察力的速度至关重要，我们还将向您展示如何[“加载实时和准实时数据”](#loading-realtime-data)。最后，我们将介绍如何[“优化您的数据结构”](#optimizing-data-structures)。
- en: Data Lake First Versus Data Warehouse First Strategy
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据湖优先与数据仓库优先策略
- en: In today’s digital age, organizations are constantly collecting and generating
    large amounts of data. This data can come from various sources such as user interactions,
    sensor readings, and social media activity. Managing this data effectively is
    crucial for organizations to gain insights and make informed business decisions.
    One of the key challenges in managing this data is deciding on the appropriate
    data management strategy. Two popular strategies that organizations use are the
    *Data Lake first* strategy and the *Data Warehouse first* strategy. When you are
    considering your cloud-based data management strategy, whether you are migrating
    an on-premises data warehouse or loading new data, a question you should consider
    is whether you take a Data Lake first or a Data Warehouse first strategy.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的数字时代，组织不断收集和生成大量数据。这些数据可以来自各种来源，如用户互动、传感器读数和社交媒体活动。有效管理这些数据对于组织获取见解并做出明智的业务决策至关重要。在管理这些数据时面临的关键挑战之一是决定适当的数据管理策略。组织常用的两种流行策略是*数据湖优先*策略和*数据仓库优先*策略。当您考虑您的基于云的数据管理策略时，无论是迁移本地数据仓库还是加载新数据，您应该考虑的一个问题是是否采用数据湖优先策略还是数据仓库优先策略。
- en: Data Lake First Strategy
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据湖优先策略
- en: The Data Lake first strategy involves creating a centralized repository for
    all raw data, regardless of its structure or format. This data lake is typically
    built on a scalable storage platform, such as Amazon S3, and is designed to handle
    large volumes of data. The data is then ingested into the data lake in its raw
    form, and data scientists, analysts, and other stakeholders can use various data
    processing and analytics tools to extract insights from the data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖优先策略涉及创建一个集中的存储库，用于存放所有原始数据，无论其结构或格式如何。这个数据湖通常建立在可扩展的存储平台上，例如亚马逊S3，并且设计用于处理大量数据。然后将数据以其原始形式摄入到数据湖中，数据科学家、分析师和其他利益相关者可以使用各种数据处理和分析工具从数据中提取见解。
- en: One of the main advantages of the Data Lake first strategy is that it allows
    for flexibility and scalability. Organizations can easily ingest new data sources,
    and the data lake can scale to handle large amounts of data. Additionally, maintaining
    the raw data in an untransformed format allows for more accurate insights and
    preserves the data integrity and data lineage.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖优先策略的主要优势之一是它允许灵活性和可伸缩性。组织可以轻松摄入新的数据源，并且数据湖可以扩展以处理大量数据。此外，保持原始数据未经转换的格式可以提供更精确的洞察，并保持数据的完整性和数据血统。
- en: However, one of the main disadvantages of the Data Lake first strategy is that
    it can be difficult to manage and govern the data effectively. You have to organize
    and maintain the files in buckets and partition appropriately for performance.
    Additionally, data scientists and analysts may have to spend a significant amount
    of time and resources preparing the data before they can extract insights.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据湖优先策略的主要缺点之一是，管理和有效治理数据可能会很困难。您必须适当地将文件组织和维护在存储桶中，进行分区以获得良好的性能。此外，数据科学家和分析师可能需要花费大量时间和资源准备数据，然后才能提取见解。
- en: Data Warehouse First Strategy
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据仓库优先策略
- en: The Data Warehouse first strategy involves creating a centralized database for
    data that is optimized for querying and reporting. The data is extracted from
    various sources, transformed to fit a predefined schema, and loaded into the data
    warehouse. Data scientists, analysts, and other stakeholders can then use SQL
    or other query languages to extract insights from the data. This approach is often
    preferred when the primary focus is on analytics and BI, and this central data
    store is used to share data with other services or users.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库优先策略涉及创建为查询和报告优化的集中式数据库。数据从各种来源提取，经过转换以适应预定义的架构，然后加载到数据仓库中。数据科学家、分析师和其他利益相关者随后可以使用SQL或其他查询语言从数据中提取见解。当主要关注点是分析和商业智能时，通常偏好这种方法，并且这种中心化的数据存储用于与其他服务或用户共享数据。
- en: One of the main advantages of the Data Warehouse first strategy is that it allows
    for better data governance and management. The structured data is easy to understand
    and find, making it easier for stakeholders to extract insights. Business analysts
    can analyze the data using ANSI SQL query language that is easy to use. The data
    is ready for consumption by decision makers as it is transformed and cleaned during
    the loading process, reducing the amount of time and resources needed to prepare
    the data for analysis.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库优先策略的主要优势之一是，它允许更好的数据治理和管理。结构化数据易于理解和查找，这使得利益相关者更容易提取见解。业务分析师可以使用易于使用的ANSI
    SQL查询语言分析数据。数据在加载过程中经过转换和清理，因此减少了为分析准备数据所需的时间和资源。
- en: However, one of the main disadvantages of the Data Warehouse first strategy
    is that organizations may have to spend more time and resources to extract and
    transform data from new data sources before providing access to business decision
    makers. Additionally, the transformation and cleaning process may lead to data
    loss or inaccuracies, which needs to be taken into consideration during the ETL
    design process. New innovations with zero-ETL strategy alleviates some of this
    problem in replicating data from source to target.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据仓库优先策略的主要缺点之一是，组织可能需要花费更多的时间和资源，从新数据源中提取和转换数据，然后才能提供给业务决策者访问。此外，转换和清理过程可能导致数据丢失或不准确，在ETL设计过程中需要考虑这些问题。零ETL策略的新创新减轻了从源到目标复制数据的一些问题。
- en: Deciding On a Strategy
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 制定策略
- en: Both the Data Lake first strategy and the Data Warehouse first strategy have
    their advantages and disadvantages. Organizations must consider their specific
    needs and goals when choosing a strategy. The Data Lake first strategy is best
    suited for organizations that need to handle large volumes of structured and unstructured
    data and require more flexibility and scalability to enable access to the data
    through a broader set of tools. On the other hand, the Data Warehouse first strategy
    is best suited for organizations that want to use SQL-based data management and
    data governance for structured and semistructured data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖优先策略和数据仓库优先策略都有各自的优缺点。组织在选择策略时必须考虑其特定需求和目标。数据湖优先策略最适合需要处理大量结构化和非结构化数据，并需要更灵活性和可扩展性以通过更广泛的工具访问数据的组织。另一方面，数据仓库优先策略最适合希望使用基于SQL的数据管理和数据治理来处理结构化和半结构化数据的组织。
- en: The choice of a strategy could depend on the source of data and whether you
    want to replicate data from source to target at table level or at the application
    layer. When you extract data from a source system, typically you replicate at
    the physical layer at a table level. But, if the source system is a SaaS application
    like Salesforce, SAP, or ServiceNow, then you have the option of replicating data
    at the database table layer or at the application layer. Since these SaaS applications
    typically involve thousands of tables, they usually have extraction logic built
    in to apply the business rules for the native tables. For example, SAP has data
    extractors (SAP Business Warehouse extractors) that apply business logic to the
    source tables and extract data at a logical layer. A sales order transaction could
    be stored in 50 different tables within the SAP application, but the extractor
    will apply business logic to combine these tables to deliver a single denormalized
    sales data row that is easy to consume. If you want to centrally store this data
    for various type of workloads, such as big data processing, machine learning,
    or analytics for native and non-native AWS Analytics services, then building a
    data lake would make sense. If the workload is purely for analytics and BI, then
    it would be better to take a Data Warehouse first approach.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 选择策略可能取决于数据来源，以及您是希望在表级别还是应用程序层级复制数据从源到目标。当您从源系统提取数据时，通常会在物理层面以表级别复制。但是，如果源系统是像Salesforce、SAP或ServiceNow这样的SaaS应用程序，则可以选择在数据库表层级或应用程序层级复制数据。由于这些SaaS应用程序通常涉及数千个表，它们通常内置提取逻辑来应用本地表的业务规则。例如，SAP拥有数据提取器（SAP
    Business Warehouse提取器），用于将业务逻辑应用到源表并在逻辑层面提取数据。例如，销售订单交易可能存储在SAP应用程序内的50个不同表中，但提取器将应用业务逻辑来合并这些表，以提供一个易于使用的单个去规范化的销售数据行。如果您希望将这些数据集中存储用于各种工作负载，例如大数据处理、机器学习或用于本地和非本地AWS分析服务的分析，则构建数据湖是有意义的。如果工作负载纯粹用于分析和BI，那么最好采用数据仓库优先的方法。
- en: If the requirements dictate a table level replication, then you can either bring
    the raw data into the data lake or ingest data directly into a data warehouse
    such as Amazon Redshift. The approach you take with this scenario will depend
    on your business and technical requirements. Within the AWS cloud, if you want
    to share the raw data in the Amazon S3 data lake with other services like EMR,
    Athena, or SageMaker for business users and data scientists to consume, then taking
    a Data Lake first approach would make sense. With a lake first approach, you have
    the flexibility of keeping the data in its raw format and establishing governance
    on top of that data and sharing without having to go through another service.
    This comes with the additional complexity of having to maintain the raw files
    in Amazon S3 and partitioning for optimal storage and performance by storing and
    managing the buckets and partitions. Updates for these files can be done using
    Apache Hudi.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需求要求在表级别进行复制，则可以将原始数据带入数据湖，或直接将数据摄入到数据仓库（例如Amazon Redshift）。您在这种情况下采取的方法将取决于您的业务和技术要求。在AWS云中，如果您希望将原始数据与Amazon
    S3数据湖中的其他服务（如EMR、Athena或SageMaker）共享，以供业务用户和数据科学家使用，则采用数据湖优先的方法是有意义的。采用湖优先的方法，您可以灵活地保留数据的原始格式，并在数据上建立治理，无需通过其他服务即可共享数据。这增加了维护Amazon
    S3中原始文件的复杂性，并通过存储和管理存储桶和分区来实现最佳的存储和性能分区。可以使用Apache Hudi来更新这些文件。
- en: You will need to first evaluate the skills in your organization and your long-term
    strategy before you choose an approach. Amazon Redshift now supports native Spark
    integration to run EMR or AWS Glue code on Amazon Redshift. This integration enables
    you to write Python or Scala code and Amazon Redshift will take care of converting
    the code to native SQL code and run it within your Amazon Redshift data warehouse.
    With Amazon Redshift ML, you can run machine learning using SQL syntax and not
    have to code in Python or R languages. With these new features like data sharing,
    Amazon Redshift ML, native Spark integration, and AWS Data Exchange (ADX) integration,
    the need to build a data lake just to share data with other services could diminish
    further.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择方法之前，您需要首先评估组织中的技能和长期战略。Amazon Redshift现在支持本地Spark集成，可以在Amazon Redshift上运行EMR或AWS
    Glue代码。此集成使您可以编写Python或Scala代码，Amazon Redshift将负责将代码转换为本地SQL代码，并在Amazon Redshift数据仓库内运行。借助Amazon
    Redshift ML，您可以使用SQL语法运行机器学习，而无需使用Python或R语言进行编码。通过像数据共享、Amazon Redshift ML、本地Spark集成和AWS数据交换（ADX）集成这样的新功能，构建数据湖仅用于与其他服务共享数据的需求可能进一步减少。
- en: In the Data Warehouse first approach, you ingest the raw data directly into
    the Amazon Redshift data warehouse. This is done at the database table level using
    either the AWS Database Migration Service (DMS) or a data integration tool of
    your choice. This will be in line with the ELT approach, where you then read the
    raw data from within the data warehouse to transform and load for further analysis.
    With this approach, when you need to share data to other AWS native services,
    you can use the Amazon Redshift data sharing feature, and to share with non-native
    services for other use cases, you can use the `UNLOAD` command to offload data
    from Amazon Redshift to Amazon S3.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在“数据仓库优先”方法中，您将原始数据直接导入到Amazon Redshift数据仓库中。这是通过使用AWS数据库迁移服务（DMS）或您选择的数据集成工具在数据库表级别完成的。这将与ELT方法一致，其中您随后从数据仓库内部读取原始数据进行转换和加载，以供进一步分析使用。通过这种方法，当您需要将数据共享到其他AWS本地服务时，您可以使用Amazon
    Redshift数据共享功能；而对于非本地服务的其他用例，您可以使用`UNLOAD`命令将数据从Amazon Redshift卸载到Amazon S3。
- en: To summarize, the data lake versus Data Warehouse first strategy, a Data Lake
    first approach is useful when you want to maintain raw data to meet known and
    unknown requirements to future-proof your analytics application. This is to meet
    requirements of organizational-level decision makers, where it may be difficult
    to provide business value initially. Data lake is just a repository of data, and
    needs additional compute services to drive value. A Data Warehouse first approach
    is to store processed data in a purpose-built data store, and it usually is for
    a particular domain and scope of requirements. It is ideal for analysis of large
    data because it is well structured, easy to use, and easy to understand. Organizations
    often need both. Data warehouses have been around for years. Data lakes were born
    out of the need to harness big data and benefit from raw, granular structured
    and unstructured data. But there is still a need to create data warehouses for
    analytics use by business users.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 综上所述，数据湖与数据仓库优先策略中的“数据湖优先”方法在您希望保留原始数据以满足已知和未知未来需求、为您的分析应用未雨绸缪时非常有用。这是为了满足组织层面决策者的需求，最初可能难以提供业务价值。数据湖只是数据的存储库，需要额外的计算服务来产生价值。而“数据仓库优先”方法则是将处理后的数据存储在专门构建的数据存储中，通常适用于特定领域和需求范围。它非常适合分析大数据，因为它结构良好、易于使用和理解。组织通常需要两者兼而有之。数据仓库已存在多年。数据湖则诞生于需要利用大数据并从原始、粒度化的结构化和非结构化数据中获益的需求。但仍然需要创建数据仓库，以供业务用户进行分析使用。
- en: Defining Your Data Model
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义您的数据模型
- en: When you launch Amazon Redshift, a default database is created. You can create
    your database objects under the default database or create additional databases
    to organize the objects under different databases. How you organize the database
    objects will depend on multiple factors, including application ownership, security
    requirements, ease of management, and cross-database query requirements.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动Amazon Redshift时，将创建一个默认数据库。您可以在默认数据库下创建您的数据库对象，或创建额外的数据库以便在不同数据库下组织对象。如何组织数据库对象将取决于多个因素，包括应用程序所有权、安全需求、易于管理以及跨数据库查询需求。
- en: This section describes how your Amazon Redshift data warehouse is organized
    and provides a starting point for understanding where to create your database
    objects. We will also cover the common data modeling strategies used to manage
    data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了Amazon Redshift数据仓库的组织结构，并提供了理解在哪里创建数据库对象的起点。我们还将涵盖用于管理数据的常见数据建模策略。
- en: Database Schemas, Users, and Groups
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据库架构、用户和组
- en: For Amazon Redshift, the database is the highest level in the hierarchy of database
    objects, and you can create multiple databases within a data warehouse. Within
    each database, you can have one or more schemas. Within each schema you create
    tables that store the data in a structured format, and other objects including
    views, procedures, and functions. By default, a database has a single schema,
    which is named “public.” You can use schemas to group database objects under a
    common name.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Amazon Redshift，数据库是数据库对象层次结构中的最高级别，您可以在数据仓库中创建多个数据库。在每个数据库中，您可以有一个或多个模式。在每个模式中，您可以创建以结构化格式存储数据的表，以及包括视图、过程和函数在内的其他对象。默认情况下，数据库有一个名为“public”的单个模式。您可以使用模式将数据库对象分组到一个公共名称下。
- en: Amazon Redshift supports both cross-schema and cross-database queries, and you
    could choose to organize database objects related to each application in separate
    schemas or separate databases. When organizing the database, consider if your
    queries have to go across databases. Cross-schema queries happen within the same
    database, and do not have any limitations. But cross-database queries have some
    [limitations](https://oreil.ly/2zPZV) that you should consider. For performance,
    note that cross-database queries do not support result cache, and hence repeated
    queries have to go against the database. Concurrency scaling is also not supported
    for cross-database queries. Consider what percentage of queries have to go across
    databases before making a decision to organize objects in different databases.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift支持跨模式和跨数据库查询，您可以选择将与每个应用程序相关的数据库对象组织在单独的模式或单独的数据库中。在组织数据库时，请考虑您的查询是否必须跨数据库。跨模式查询发生在同一个数据库中，并且没有任何限制。但是跨数据库查询有一些[限制](https://oreil.ly/2zPZV)，您应考虑这些限制。就性能而言，注意跨数据库查询不支持结果缓存，因此重复查询必须针对数据库进行。并发缩放也不支持跨数据库查询。在决定将对象组织到不同数据库之前，请考虑多少百分比的查询必须跨数据库。
- en: The other factor to consider is the number of schemas or tables for your workloads
    or business area. If the number of schemas or table objects required exceeds the
    [quotas or limits](https://oreil.ly/wtKal), then you might want to organize the
    data in separate databases.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个要考虑的因素是工作负载或业务领域中模式或表对象的数量。如果所需的模式或表对象数量超过[配额或限制](https://oreil.ly/wtKal)，那么您可能希望将数据组织在单独的数据库中。
- en: For cross-database queries you use a two-dot (`database.schema.table`) notation
    to access a table outside of the current database. You can also create an external
    schema to reference using a one-dot (`externalschema.table`) notation. For more
    details on cross-database queries, you can refer to [querying data across databases](https://oreil.ly/-03Yn).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于跨数据库查询，您可以使用两点(`database.schema.table`)表示法访问当前数据库之外的表。您还可以创建外部模式以使用一点(`externalschema.table`)表示法进行引用。有关跨数据库查询的更多详细信息，请参考[跨数据库查询数据](https://oreil.ly/-03Yn)。
- en: When you have a multi-tenant architecture, you can refer to [this blog](https://oreil.ly/gaFz7)
    to organize your database objects.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当您拥有多租户架构时，您可以参考[此博客](https://oreil.ly/gaFz7)来组织您的数据库对象。
- en: Star Schema, Denormalized, Normalized
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 星型模式、去规范化、规范化
- en: Data warehouses have been built using the *star schema*, where there are one
    or more central fact tables that contain the measures to be analyzed, surrounded
    by dimension tables that provide the business context to the analysis. This layout
    looks like a star, thus the name. The star schema dimensions can be either denormalized
    as a single table with columns for every attribute of dimension or further normalized
    into separate tables, which makes the schema diagram look like a snowflake instead
    of a star and thus referred to as a snowflake schema ([Figure 3-1](#star_snowflake)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库使用*星型模式*构建，其中有一个或多个中心事实表包含要分析的度量，周围是提供分析业务背景的维度表。这种布局看起来像一个星星，因此得名。星型模式维度可以是去规范化的单个表，每个维度的属性都有列，也可以进一步规范化为单独的表，这使得模式图看起来像雪花而不是星星，因此称为雪花模式([图 3-1](#star_snowflake))。
- en: '![Star schema objects compared with snowflake schema objects](assets/ardg_0301.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![星型模式对象与雪花模式对象的比较](assets/ardg_0301.png)'
- en: Figure 3-1\. Star schema objects compared with snowflake schema objects
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. 星型模式对象与雪花模式对象的比较
- en: A star schema stores redundant data in dimension tables, whereas in a *snowflake
    schema*, the dimension tables avoid redundancy. However, this leads to increased
    query complexity and can impact query performance since more tables need to be
    joined for the analysis. A star schema data model or a denormalized data model
    is recommended when you create a data warehouse on Amazon Redshift.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 星型模式数据模型在维表中存储冗余数据，而在*雪花模式*中，维表避免了冗余。然而，这会增加查询复杂性，并可能影响查询性能，因为分析时需要连接更多的表。在创建Amazon
    Redshift的数据仓库时，推荐使用星型模式数据模型或去规范化数据模型。
- en: On the other hand, storage requirements of a snowflake schema are lower as there
    is no redundancy and there is less risk of data integrity issues.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，雪花模式的存储需求较低，因为没有冗余，数据完整性问题的风险也较低。
- en: With advancements in storage media and dropping prices per TB, the storage aspect
    is less of a concern over query simplicity, and query execution often takes higher
    priority for modern data warehouses. This makes star schema a popular architecture
    for running analytics of very large datasets.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随着存储介质的进步和每TB价格的下降，存储方面不再是问题，查询简单性通常更受现代数据仓库的重视。这使得星型模式成为运行非常大数据集分析的流行架构。
- en: Let’s look at a simple data model to understand the difference between normalized
    and a denormalized star schema. Sales orders are stored in a relational database
    using a normalized model. As you see in [Figure 3-2](#oltp_schema), SalesHeader
    and SalesLineItem are stored in separate tables with a one-to-many relationship
    along with the master data tables Customer, Product, and Currency.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的数据模型，以了解标准化和去规范化星型模式之间的区别。销售订单使用标准化模型存储在关系数据库中。如您在[图 3-2](#oltp_schema)中所见，销售头和销售行项目存储在单独的表中，并且与客户、产品和货币的主数据表具有一对多的关系。
- en: '![OLTP Schema](assets/ardg_0302.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![OLTP Schema](assets/ardg_0302.png)'
- en: Figure 3-2\. Denormalized OLTP schema data model in source database
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. 源数据库中的去规范化 OLTP 模式数据模型
- en: For the star schema model in [Figure 3-3](#star_schema), the SalesHeader and
    SalesLineItem tables are combined into one SalesFact table, and the data is also
    aggregated at the order level.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[图 3-3](#star_schema)中的星型模式模型，销售头和销售行项目表合并为一个销售事实表，并且数据还在订单级别进行了聚合。
- en: '![Star schema](assets/ardg_0303.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![Star schema](assets/ardg_0303.png)'
- en: Figure 3-3\. Star schema data model for data warehouse
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. 数据仓库的星型模式数据模型
- en: Student Information Learning Analytics Dataset
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学生信息学习分析数据集
- en: In the previous chapter, you learned how you can create an Amazon Redshift serverless
    data warehouse and use the query editor to query the sample data. Now let’s see
    how you can create new data models, ingest data into Amazon Redshift, and analyze
    it using the native query editor.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，您学习了如何创建Amazon Redshift无服务器数据仓库，并使用查询编辑器查询示例数据。现在让我们看看如何创建新的数据模型，将数据导入Amazon
    Redshift，并使用本地查询编辑器进行分析。
- en: For this, we chose a [student learning analytics dataset](https://oreil.ly/TxkDS)
    to help you understand how to build a star schema data model and ingest data into
    Amazon Redshift to analyze, predict, and improve student outcomes.^([1](ch03.html#id1443))
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们选择了一个帮助你了解如何构建星型模式数据模型并将数据导入Amazon Redshift以分析、预测和改善学生结果的[学生学习分析数据集](https://oreil.ly/TxkDS)^([1](ch03.html#id1443))。
- en: The Open University Learning Analytics Dataset (OULAD) contains data about courses,
    students, and their interactions with an online Virtual Learning Environment (VLE)
    for seven selected courses, called modules. The dataset assumes that there are
    two semesters yearly, and courses start in February and October every year. The
    course semester is identified by the code_presentation column in the courses table,
    and the code modules are suffixed with letter “B” and “J,” respectively, with
    a four-digit year as a prefix. The dataset consists of tables connected using
    unique identifiers. All tables are stored in the CSV format.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Open University 学习分析数据集（OULAD）包含关于课程、学生及其在在线虚拟学习环境（VLE）中互动的数据，适用于七个选定的课程（称为模块）。数据集假设每年有两个学期，课程在每年的二月和十月开始。课程学期由课程表中的
    code_presentation 列标识，而代码模块则以字母“B”和“J”后缀，并以四位数年份作为前缀。数据集由使用唯一标识符连接的表组成，所有表均以CSV格式存储。
- en: The data model consists of seven tables, with data related to student, modules,
    and activities, as shown in [Figure 3-4](#sis_oulad_datamodel), which shows the
    entity relationship. For the purposes of this book, we have modified this data
    model to store data for multiple schools instead of just one school. You can use
    the Data Definition Language (DDL) scripts shown in [Example 3-1](#creating_sis_data_model)
    to create the schema and database tables.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据模型包括七个表，其中包含与学生、模块和活动相关的数据，如 [图 3-4](#sis_oulad_datamodel) 所示，显示了实体关系。为了本书的目的，我们修改了此数据模型，以存储多个学校的数据，而不仅仅是一个学校。您可以使用所示的数据定义语言（DDL）脚本（参见
    [示例 3-1](#creating_sis_data_model)）来创建架构和数据库表。
- en: The sample anonymized dataset is available in the link [OULAD dataset](https://oreil.ly/TxkDS)
    to learn more about the dataset, and you can download and store it in an S3 bucket
    of your choice. We stored it in an S3 bucket [`arn:aws:s3:::openlearn-redshift`](https://oreil.ly/Fdjqm),
    and you can use this S3 location to ingest data into Amazon Redshift using the
    `COPY` command. You can view the S3 dataset as shown in [Figure 3-5](#view_s3_raw_data).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 匿名样本数据集可在链接 [OULAD 数据集](https://oreil.ly/TxkDS) 中获取，了解更多数据集信息，并可以下载并存储在您选择的
    S3 存储桶中。我们将其存储在 S3 存储桶 [`arn:aws:s3:::openlearn-redshift`](https://oreil.ly/Fdjqm)
    中，您可以使用此 S3 位置使用 `COPY` 命令将数据导入 Amazon Redshift。您可以查看 S3 数据集，如 [图 3-5](#view_s3_raw_data)
    所示。
- en: '![Student Information System Dataset](assets/ardg_0304.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![学生信息系统数据集](assets/ardg_0304.png)'
- en: Figure 3-4\. Student information system dataset
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-4\. 学生信息系统数据集
- en: You can view the S3 dataset as shown in [Figure 3-5](#view_s3_raw_data), and
    you will use this as a source to ingest the sample dataset into Amazon Redshift
    using the `COPY` command. Similarly, you can find other publicly available datasets
    and create data models for these datasets to explore Amazon Redshift features.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看 S3 数据集，如 [图 3-5](#view_s3_raw_data) 所示，并将其用作源，使用 `COPY` 命令将样本数据集导入 Amazon
    Redshift。同样，您可以找到其他公开可用的数据集，并为这些数据集创建数据模型，以探索 Amazon Redshift 的功能。
- en: '![Review raw data in Amazon S3](assets/ardg_0305.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![在 Amazon S3 中查看原始数据](assets/ardg_0305.png)'
- en: Figure 3-5\. Review raw data in Amazon S3
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-5\. 在 Amazon S3 中查看原始数据
- en: Create Data Models for Student Information Learning Analytics Dataset
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为学生信息学习分析数据集创建数据模型
- en: Next, let’s create database tables to load data into Amazon Redshift. Connect
    to your Amazon Redshift data warehouse. Use the following script to create the
    schema and tables for your sample student information dataset (see [Example 3-1](#creating_sis_data_model)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建数据库表，将数据加载到 Amazon Redshift 中。连接到您的 Amazon Redshift 数据仓库。使用以下脚本为您的样本学生信息数据集创建架构和表（参见
    [示例 3-1](#creating_sis_data_model)）。
- en: Example 3-1\. Load student information data
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-1\. 加载学生信息数据
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When creating tables in Amazon Redshift, while there are many options to choose
    the distribution, sorting, and encoding for each table, in the previous example
    we did not specify these options, and the default of `AUTO` was used. In most
    cases, using `AUTO` will indicate that the Amazon Redshift service will monitor
    the actual usage of the table and automatically tune the table for you.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 Amazon Redshift 表时，虽然有多种选项可供选择分发、排序和编码每个表，但在上一个示例中，我们没有指定这些选项，并且使用了 `AUTO`
    的默认设置。在大多数情况下，使用 `AUTO` 将指示 Amazon Redshift 服务监视表的实际使用情况，并自动调整表。
- en: Load Batch Data into Amazon Redshift
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将批量数据加载到 Amazon Redshift
- en: Now that you created the data tables and have the data files available in Amazon
    S3, you can load the data into Amazon Redshift. There are multiple options to
    load data into Amazon Redshift.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已创建数据表并在 Amazon S3 中有数据文件可用，您可以将数据加载到 Amazon Redshift 中。有多种选项可用于将数据加载到 Amazon
    Redshift 中。
- en: Using the `COPY` command
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `COPY` 命令
- en: Using AWS Glue or third-party ETL tools
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS Glue 或第三方 ETL 工具
- en: Manual loading using SQL commands
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SQL 命令进行手动加载
- en: Using the Query Editor V2
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用查询编辑器 V2
- en: Using the COPY Command
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `COPY` 命令
- en: The `COPY` command is the simplest and most efficient way to load data into
    Amazon Redshift. It allows you to load data directly from Amazon S3, Amazon DynamoDB,
    and Amazon EMR, as well as from external data sources such as CSV and JSON files.
    The `COPY` command automatically parallelizes the data load and can handle large
    amounts of data quickly and easily. This command reads multiple data files and
    can also split the files as necessary based on the number of slices in the target
    data warehouse to allocate the workload to all nodes and slices. It will also
    sort the rows and distribute data across node slices. A best practice is to compress
    the files when you store in Amazon S3 for faster reads. For ingestion of data,
    please note the differences between loading compressed files versus uncompressed
    files.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`COPY`命令是将数据加载到Amazon Redshift中最简单和最有效的方式。它允许直接从Amazon S3、Amazon DynamoDB和Amazon
    EMR加载数据，以及从CSV和JSON文件等外部数据源加载。`COPY`命令自动并行加载数据，并可以快速轻松地处理大量数据。该命令读取多个数据文件，并根据目标数据仓库中的切片数量必要时分割文件，以将工作负载分配给所有节点和切片。它还将行排序并分发数据到节点切片之间。存储在Amazon
    S3中时，推荐的最佳做法是压缩文件以提高读取速度。在数据摄入方面，请注意加载压缩文件与未压缩文件之间的差异。'
- en: When you load compressed data as a single large file, Amazon Redshift serializes
    the load. But if you split the file into smaller files, the `COPY` command loads
    multiple files in parallel. This increases parallelism by dividing the workload
    among the nodes in your data warehouse. We recommend that you split your data
    into smaller files that are about equal size, from 1 MB to 1 GB after compression.
    For optimum parallelism, as a rule of thumb, make the number of the files a multiple
    of the number of slices in your data warehouse with the size between 1 and 125
    MB after compression. For example, if you are loading a 1 GB file into a two-node
    ra3.4xlarge data warehouse that has 4 slices per node, divide the file into multiples
    of 8, so you can split the file into 8 files of 125 MB each for efficient load.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将压缩数据作为单个大文件加载时，Amazon Redshift会串行加载数据。但是，如果您将文件分割成较小的文件，`COPY`命令会并行加载多个文件。这通过将工作负载分割到数据仓库中的节点之间增加了并行性。我们建议您将数据分割成大约相等大小的较小文件，压缩后大小从1
    MB到1 GB。为了实现最佳并行性，根据您的数据仓库中切片的数量将文件数设为1到125 MB之间的倍数。例如，如果您将1 GB文件加载到具有每个节点4个切片的双节点ra3.4xlarge数据仓库中，则可以将文件分割为8个125
    MB大小的文件，以实现高效加载。
- en: When you load all the data from a single large compressed file, Amazon Redshift
    is forced to perform a serialized load, which is much slower.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当您从单个大压缩文件加载所有数据时，Amazon Redshift被迫执行串行加载，速度要慢得多。
- en: In contrast, when you load delimited data from a large uncompressed file, Amazon
    Redshift makes use of multiple slices. These slices work in parallel automatically.
    This provides fast load performance. Specifically, when Amazon Redshift loads
    uncompressed, delimited data, data is split into ranges and handled by slices
    in each node.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，当您从大型未压缩文件中加载分隔数据时，Amazon Redshift会利用多个切片。这些切片会自动并行工作。这提供了快速的加载性能。具体来说，当Amazon
    Redshift加载未压缩的分隔数据时，数据会分割成范围，并由每个节点中的切片处理。
- en: When loading compressed files, a good practice is to split your data into smaller
    files that are about equal size, from 1 MB to 1 GB after compression. For optimum
    parallelism, the ideal file size is 1 to 125 MB after compression.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当加载压缩文件时，一个好的做法是将数据分割成大约相等大小的较小文件，压缩后大小从1 MB到1 GB。为了实现最佳并行性，理想的文件大小在压缩后为1到125
    MB。
- en: Ingest Data for the Student Learning Analytics Dataset
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摄入学生学习分析数据集
- en: To ingest the sample student learning analytics dataset, we use the recommended
    `COPY` command with the Amazon S3 bucket where we stored the sample data. The
    list of commands are as shown in [Example 3-2](#load_sis_data_model), and you
    can use these commands and replace the S3 location and the region with appropriate
    values.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了摄入示例学生学习分析数据集，我们使用推荐的`COPY`命令，将样本数据存储在Amazon S3桶中。命令列表如[示例 3-2](#load_sis_data_model)所示，您可以使用这些命令，并替换适当值的S3位置和地区。
- en: Example 3-2\. Create schema and tables for student information data
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-2\. 创建学生信息数据的模式和表
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We are using the `default` keyword to use the default IAM role associated with
    the data warehouse. Amazon Redshift uses the IAM role that is [set as the default](https://oreil.ly/lRd6z)
    and associated with the data warehouse when the command runs. You can run the
    `DEFAULT_IAM_ROLE` command to check the current default IAM role that is attached
    to the data warehouse.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`default`关键字来使用与数据仓库关联的默认 IAM 角色。当命令运行时，Amazon Redshift 使用被设定为默认并与数据仓库关联的
    IAM 角色。您可以运行`DEFAULT_IAM_ROLE`命令来检查当前附加到数据仓库的默认 IAM 角色。详细信息请参见 [此处设定为默认](https://oreil.ly/lRd6z)。
- en: Amazon Redshift sorts each batch of records in a single load based on sort key
    order. However, it does not resort existing records already stored for each `COPY`
    execution. If each batch of new data follows the existing rows in your table,
    your data is properly stored in sort order, and you don’t need to run a vacuum.
    You don’t need to presort the rows in each load because `COPY` sorts each batch
    of incoming data as it loads.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 根据排序键顺序对每批记录进行排序。然而，它不会对已存储的每个`COPY`执行中的现有记录重新排序。如果每批新数据遵循表中现有行的顺序，则您的数据将按排序顺序正确存储，您无需运行真空操作。您不需要在每次加载中对行进行预排序，因为`COPY`在加载每批传入数据时会对数据进行排序。
- en: Building a Star Schema
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建星型模式
- en: You just ingested the data into a normalized student information data model
    to store the transactional records for course selections, grades, outcomes, and
    registrations for students. However, the business requirement is to enable school
    administrators and faculty to be able to measure course outcomes. As discussed
    in the previous chapter, a star schema model consisting of fact and dimension
    tables is the recommended data model for a data warehouse. The tables `course_registration`,
    `course_outcome`, and `course_schedule` have the data necessary to measure outcomes,
    so these tables could form the basis for the fact table.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您刚刚将数据导入了一个规范化的学生信息数据模型，用于存储学生的课程选择、成绩、结果和注册的事务记录。然而，业务需求是使学校管理员和教师能够衡量课程成果。如前一章所讨论的，一个由事实和维度表组成的星型模式模型是数据仓库的推荐数据模型。表`course_registration`、`course_outcome`和`course_schedule`包含了测量结果所需的数据，因此这些表可以形成事实表的基础。
- en: There are many approaches to transform the data into your denormalized fact
    table. You can use an *extract-transform-load* (ETL) approach, which reads the
    source data, processes the transformations in an external application, and loads
    the results, or you can use an *extract-load-transform* (ELT) approach, which
    uses the data you just loaded and transforms the data in place using the power
    of the Amazon Redshift compute. In [Chapter 4, “Data Transformation Strategies”](ch04.html#AR_TGD_CH4),
    we’ll go into more details on deciding between these strategies. However, to complete
    this example, we will show how to use an ELT approach with the data that you just
    loaded.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以将数据转换为您的去规范化事实表。您可以使用 *提取-转换-加载* (ETL) 方法，该方法读取源数据，在外部应用程序中处理转换，并加载结果；或者您可以使用
    *提取-加载-转换* (ELT) 方法，该方法使用您刚刚加载的数据，并利用 Amazon Redshift 计算的能力在原地转换数据。在 [第四章，“数据转换策略”](ch04.html#AR_TGD_CH4)
    中，我们将更详细地讨论如何在这些策略之间进行选择。然而，为了完成此示例，我们将展示如何使用您刚刚加载的数据来使用 ELT 方法。
- en: '[Example 3-3](#mv_course_outcomes_fact) reads the normalized source tables
    and builds the `mv_course_outcomes_fact` materialized view. The advantage of creating
    a materialized view is that it can be set to incrementally refresh data when the
    underlying tables are updated.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-3](#mv_course_outcomes_fact) 读取规范化的源表，并构建`mv_course_outcomes_fact`物化视图。创建物化视图的优点是，当底层表更新时，可以设置增量刷新数据。'
- en: Example 3-3\. Create a materialized view to denormalize
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-3\. 创建一个物化视图来去规范化
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The student and faculty dimension tables can now be joined to the materialized
    view, which is the fact table to create a start schema model. Now when you look
    at the full star schema model, it will be as shown in [Figure 3-6](#full_star_schema_model).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以将学生和教师维度表连接到物化视图，这是事实表，用于创建一个星型模式模型。现在，当您查看完整的星型模式模型时，如 [图 3-6](#full_star_schema_model)
    所示。
- en: '![Complete start schema model](assets/ardg_0306.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![完整的星型模式模型](assets/ardg_0306.png)'
- en: Figure 3-6\. Full star schema model
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-6\. 完整的星型模式模型
- en: Once you ingest the data, you can test the results by selecting data from the
    tables. Since we just created a materialized view to consolidate data from multiple
    tables, we can query this materialized view for efficient query performance. Let’s
    test using a couple of queries (Examples [3-4](#ex_students) and [3-5](#ex_lecture)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您摄取数据，您可以通过从表格中选择数据来测试结果。由于我们刚刚创建了一个从多个表中整合数据的物化视图，因此可以查询此物化视图以实现高效的查询性能。让我们使用一些查询进行测试（示例
    [3-4](#ex_students) 和 [3-5](#ex_lecture)）。
- en: Example 3-4\. Find number of students who secured each grade
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-4\. 查找获得每个等级的学生人数
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Example 3-5\. Determine if there is a correlation between lecture duration and
    grade
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-5\. 确定讲座持续时间与成绩之间是否存在相关性
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To correlate the student engagement on the learning management system to the
    outcome, you can join the `student_lms` table with the `student_assessment` to
    derive insights. Next you’ll see a materialized view, `mv_student_lmsactivites_and_score`,
    created in [Example 3-6](#mv_student_lmsactivites_and_score).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将学生在学习管理系统上的参与与结果相关联，您可以将 `student_lms` 表与 `student_assessment` 进行连接以获取洞察。接下来您将看到一个已物化的视图，`mv_student_lmsactivites_and_score`，创建于
    [示例 3-6](#mv_student_lmsactivites_and_score)。
- en: Example 3-6\. Student activities `total_score mean_score`
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-6\. 学生活动 `total_score mean_score`
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With this materialized view, you can gain many insights into student performance,
    as in [Example 3-7](#clicks_vs_result), to analyze the impact of the student engagement
    on the result. Here you analyze the number of clicks for a student using the online
    learning management compared to the result or grade.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个物化视图，您可以获得许多关于学生表现的洞察，如 [示例 3-7](#clicks_vs_result)，分析学生在在线学习管理中的点击次数与结果或成绩的影响。在这里，您分析了学生使用在线学习管理与结果或成绩之间的点击次数。
- en: Example 3-7\. Clicks versus result
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-7\. 点击次数与结果
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In [Example 3-8](#percentage_results), you see a query to analyze the percentage
    results by module to understand which modules students are scoring higher or lower,
    so the schools can proactively set up programs to increase student engagement
    for better outcomes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 3-8](#percentage_results) 中，您看到一个查询以分析按模块的百分比结果，以了解哪些模块学生得分较高或较低，因此学校可以积极设置计划，以增加学生参与度，从而获得更好的结果。
- en: Example 3-8\. Percentage results by MODULE
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-8\. 按模块的百分比结果
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can also query the tables directly to gain insights. [Example 3-9](#courses_completed)
    shows a query to find out the number of students who completed a course by performing
    any type of assessment but exams. You can try running this query.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以直接查询表格以获得洞察。[示例 3-9](#courses_completed) 展示了一条查询，以查找完成课程但未通过考试进行任何评估的学生人数。您可以尝试运行此查询。
- en: Example 3-9\. Students who finished a course performing any assessment but exams
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-9\. 完成课程进行任何评估但未通过考试的学生
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Continuous File Ingestion from Amazon S3
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Amazon S3进行连续文件摄取
- en: Continuous ingestion of files landing in an Amazon S3 bucket into Amazon Redshift
    tables allows users to simplify their transformation pipeline. When you set up
    a `COPY JOB`, Amazon Redshift detects when new Amazon S3 files are added to the
    path specified in your `COPY` command. A `COPY` command is then automatically
    triggered, and the system keeps track of which files have been loaded and also
    determines the number of files batched together per `COPY` command. You can use
    this feature to automate the ingestion process without having to create an external
    data ingestion pipeline. For more details on continuous ingestion, you can refer
    to the [online documentation](https://oreil.ly/FNea8).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从Amazon S3存储桶连续摄取文件到Amazon Redshift表允许用户简化其转换管道。当您设置 `COPY JOB` 时，Amazon Redshift
    检测到新的Amazon S3文件添加到您 `COPY` 命令指定的路径时。然后会自动触发 `COPY` 命令，并且系统会跟踪已加载的文件，并确定每个 `COPY`
    命令批处理在一起的文件数量。您可以使用此功能自动化摄取过程，而无需创建外部数据摄取管道。有关连续摄取的更多详细信息，请参阅 [在线文档](https://oreil.ly/FNea8)。
- en: The `COPY` job execution details (as in [Example 3-10](#copy_continuous)) are
    stored in the system tables for you to monitor the loads, and you can also use
    this to review historical job executions and load details. The `sys_copy_job`
    system table contains a row for each `COPY JOB` currently defined.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`COPY` 作业的执行细节（如 [示例 3-10](#copy_continuous)）存储在系统表中，供您监视加载，并且您还可以使用此信息来审查历史作业执行和加载细节。`sys_copy_job`
    系统表包含当前定义的每个 `COPY JOB` 的行。'
- en: Example 3-10\. Create a `COPY` job
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-10\. 创建 `COPY` 作业
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As shown in [Example 3-11](#copy_view_listoffiles), to view the list of files
    loaded by a `COPY JOB`, you can run the following sample query replacing `job_id`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [示例 3-11](#copy_view_listoffiles) 所示，要查看由 `COPY JOB` 加载的文件列表，您可以运行以下示例查询，并替换
    `job_id`：
- en: Example 3-11\. View a `COPY` job
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-11\. 查看 `COPY` 作业
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using AWS Glue for Transformations
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于转换的 AWS Glue
- en: AWS Glue is one of the native serverless data integration services, commonly
    used to transform data using Python or Scala language and run on a data processing
    engine. AWS Glue makes it easier to discover, prepare, move, and integrate data
    from multiple sources for analytics, ML, and application development. It offers
    multiple data integration engines, which include AWS Glue for Apache Spark, AWS
    Glue for Ray, and AWS Glue for Python Shell. You can use the appropriate engine
    for your workload based on the characteristics of your workload and the preferences
    of your developers and analysts. Amazon Redshift supports Spark integration, which
    allows you to push down the Python or Scala transformation logic execution to
    Amazon Redshift layer by translating the Spark code to SQL code without moving
    the data out of the data warehouse.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue 是一种原生的无服务器数据集成服务，通常用于使用 Python 或 Scala 语言进行数据转换并在数据处理引擎上运行。AWS Glue
    使得从多个来源发现、准备、移动和集成数据变得更加简单，用于分析、ML 和应用开发。它提供多个数据集成引擎，包括 AWS Glue for Apache Spark、AWS
    Glue for Ray 和 AWS Glue for Python Shell。您可以根据工作负载的特征以及开发人员和分析师的偏好，选择适合您工作负载的引擎。Amazon
    Redshift 支持 Spark 集成，允许您将 Python 或 Scala 转换逻辑的执行推送到 Amazon Redshift 层，通过将 Spark
    代码转换为 SQL 代码，而无需移动数据出数据仓库。
- en: Since AWS Glue V4, there’s now a Amazon Redshift Spark connector with a new
    JDBC driver featured with AWS Glue ETL jobs. You can use it to build Apache Spark
    applications that read from and write to data in Amazon Redshift as part of your
    data ingestion and transformation pipelines. With the new connector and driver,
    these applications maintain their performance and transactional consistency of
    the data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 自 AWS Glue V4 起，现在有一个带有新 JDBC 驱动程序的 Amazon Redshift Spark 连接器，特色是与 AWS Glue
    ETL 作业一起使用。您可以使用它来构建 Apache Spark 应用程序，读取和写入 Amazon Redshift 中的数据，作为数据摄取和转换流水线的一部分。有了新的连接器和驱动程序，这些应用程序可以保持其数据的性能和事务一致性。
- en: With AWS Glue ([Figure 3-7](#AWS_Glue_intro)), you can crawl an Amazon S3 data
    source to create a catalog, apply transformations, and ingest data into an Amazon
    Redshift data warehouse.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AWS Glue ([图 3-7](#AWS_Glue_intro))，您可以爬取 Amazon S3 数据源以创建目录，应用转换，并将数据摄入
    Amazon Redshift 数据仓库。
- en: '![ETL integration using AWS Glue](assets/ardg_0307.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![使用 AWS Glue 进行 ETL 集成](assets/ardg_0307.png)'
- en: Figure 3-7\. ETL integration using AWS Glue
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-7\. 使用 AWS Glue 进行 ETL 集成
- en: Amazon Redshift integration for Apache Spark makes it easy to build and run
    Spark applications on Amazon Redshift using AWS Glue. This integration feature
    for Apache Spark adds pushdown capabilities for operations such as sort, aggregate,
    limit, join, and scalar functions so only the relevant data is moved from the
    Amazon Redshift data warehouse to the consuming Spark application in AWS for better
    performance. You can refer to this [blog for more information](https://oreil.ly/A309N).
    In [Chapter 4, “Data Transformation Strategies”](ch04.html#AR_TGD_CH4), you will
    learn how you can use AWS Glue Studio to create ETL transformations using a visual
    interface.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 与 Apache Spark 的集成使得在 Amazon Redshift 上使用 AWS Glue 构建和运行 Spark
    应用程序变得更加简单。这种 Apache Spark 的集成功能增加了对排序、聚合、限制、连接和标量函数等操作的推送能力，因此只有相关数据从 Amazon
    Redshift 数据仓库移动到 AWS 中消费的 Spark 应用程序，以获得更好的性能。您可以参考此 [博客获取更多信息](https://oreil.ly/A309N)。在
    [第 4 章，“数据转换策略”](ch04.html#AR_TGD_CH4) 中，您将学习如何使用 AWS Glue Studio 创建 ETL 转换，使用可视化界面。
- en: Manual Loading Using SQL Commands
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SQL 命令手动加载数据
- en: Manually loading data into Amazon Redshift using SQL commands is a viable option,
    but it is generally not recommended for large datasets because it is time-consuming
    and error-prone. However, it can be useful for small datasets or for testing purposes.
    You can use SQL commands such as `INSERT` and `CREATE TABLE` to load data if using
    a `COPY` command is not an option. A multirow insert or bulk insert operation
    is recommended instead of a single `INSERT` statement.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SQL 命令手动将数据加载到 Amazon Redshift 是一种可行的选择，但通常不建议用于大型数据集，因为这是耗时且容易出错的。但是，对于小型数据集或测试目的，它可能很有用。如果不能使用
    `COPY` 命令，则可以使用诸如 `INSERT` 和 `CREATE TABLE` 等 SQL 命令来加载数据。建议使用多行插入或批量插入操作，而不是单个
    `INSERT` 语句。
- en: Multirow inserts improve performance by batching up a series of inserts. The
    following [Example 3-12](#multi_row_insert) inserts two rows into a five-column
    table using a single `INSERT` statement. This is still a small insert, shown simply
    to illustrate the syntax of a multirow insert.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 多行插入通过批量插入一系列插入来提高性能。以下是 [示例 3-12](#multi_row_insert) 使用单个 `INSERT` 语句将两行插入到具有五列的表中。这仍然是一个小插入，简单地展示了多行插入的语法。
- en: Example 3-12\. Multirow insert
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-12\. 多行插入
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: When you need to move data or a subset of data from one table into another,
    you can use a bulk insert operation with a `SELECT` clause, as in [Example 3-13](#move_data_table_to_table),
    for high-performance data insertions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要将数据或数据子集从一张表移动到另一张表时，可以使用带有 `SELECT` 子句的批量插入操作，例如 [示例 3-13](#move_data_table_to_table)，用于高性能数据插入。
- en: Example 3-13\. Create table as statement (CTAS) with data
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-13\. 使用数据创建表语句（CTAS）
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If you want to incrementally add to the table, create the table first and insert
    records based on a criteria as in [Example 3-14](#move_without_data).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要逐步添加到表中，请先创建表，然后按照条件插入记录，如 [示例 3-14](#move_without_data) 所示。
- en: Example 3-14\. Create table as statement (CTAS) without data
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-14\. 使用数据创建表语句（CTAS）但不包含数据
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Using the Query Editor V2
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用查询编辑器 V2
- en: For simple and quick data loads into Amazon Redshift, you can use the Query
    Editor V2 load data feature. You can upload a file directly from your desktop
    folder or upload the data file to an Amazon S3 location, and then within the Query
    Editor V2, choose the load data option as shown in [Figure 3-8](#load_s3_data_from_qev2).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单快速的数据加载到 Amazon Redshift，您可以使用查询编辑器 V2 的加载数据功能。您可以直接从桌面文件夹上传文件，或将数据文件上传到
    Amazon S3 位置，然后在查询编辑器 V2 中选择加载数据选项，如 [图 3-8](#load_s3_data_from_qev2) 所示。
- en: The Query Editor V2 will use the `COPY` command behind the scenes to load data
    from the Amazon S3 location you specify. The `COPY` command generated and used
    in the Query Editor V2 load data wizard supports all the parameters available
    to the `COPY` command syntax to copy from Amazon S3 through the wizard. You can
    set up data conversion parameters to accept invalid characters, set up date and
    time format, truncate data, handle exceptions like blank lines, missing columns,
    trailing white spaces, and other parameters. Choose the “Data conversion parameters”
    option under the “Advanced settings” section of the screen, shown in [Figure 3-8](#load_s3_data_from_qev2).
    In addition, you can also set up load data operations like number of rows to analyze
    for compression analysis, auto update option for compression encoding, error handling,
    and statistics update options.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 查询编辑器 V2 将在后台使用 `COPY` 命令从您指定的 Amazon S3 位置加载数据。在查询编辑器 V2 的加载数据向导中生成和使用的 `COPY`
    命令支持所有 `COPY` 命令语法可用的参数，用于通过向导从 Amazon S3 复制。您可以设置数据转换参数以接受无效字符，设置日期和时间格式，截断数据，处理空白行、缺少列、尾随空格等异常情况。在屏幕的“高级设置”部分下选择“数据转换参数”选项，如
    [图 3-8](#load_s3_data_from_qev2) 所示。此外，您还可以设置加载数据操作，例如分析用于压缩分析的行数、自动更新压缩编码选项、错误处理和统计更新选项。
- en: '![load data using Query Editor V2](assets/ardg_0308.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![使用查询编辑器 V2 加载数据](assets/ardg_0308.png)'
- en: Figure 3-8\. Upload the CSV files to Amazon S3 and load using Query Editor V2
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-8\. 将 CSV 文件上传到 Amazon S3 并使用查询编辑器 V2 加载
- en: When you ingest data using the `COPY` command into an empty table, Amazon Redshift
    can analyze the data and optimize the compression type for each column. The `COMPUPDATE`
    parameter in the `COPY` command determines the action for compression.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用 `COPY` 命令将数据输入空表时，Amazon Redshift 可以分析数据并为每列优化压缩类型。`COPY` 命令中的 `COMPUPDATE`
    参数确定压缩的操作方式。
- en: Load Real-Time and Near Real-Time Data
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载实时和准实时数据
- en: '*Real-time data* refers to data that is processed and analyzed as soon as it
    is generated. This type of data is critical for time-sensitive applications such
    as financial trading, transportation, and logistics. *Near real-time data* is
    similar to real-time data, but with a slight delay in processing and analysis,
    usually a few minutes or less.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*实时数据*指的是生成后立即处理和分析的数据。这种类型的数据对于诸如金融交易、运输和物流等时间敏感的应用至关重要。*准实时数据*与实时数据类似，但在处理和分析上稍有延迟，通常为几分钟或更少。'
- en: Loading real-time and near real-time data into a data warehouse or BI system
    is a challenging task that requires efficient data ingestion, processing, and
    storage capabilities. The process of loading real-time and near real-time data
    involves several steps, including data extraction, data transformation, and data
    loading.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 将实时和准实时数据加载到数据仓库或 BI 系统是一项具有挑战性的任务，需要高效的数据摄取、处理和存储能力。加载实时和准实时数据的过程涉及多个步骤，包括数据提取、数据转换和数据加载。
- en: '*Data extraction* is the process of obtaining data from various sources, such
    as sensors, log files, and streaming data platforms. *Data transformation* is
    the process of cleaning, validating, and normalizing the data before loading it
    into the data warehouse or BI system. *Data loading* is the process of importing
    the data into the target system and making it available for analysis and reporting.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据提取* 是从各种来源（如传感器、日志文件和流数据平台）获取数据的过程。*数据转换* 是在加载到数据仓库或 BI 系统之前清理、验证和规范化数据的过程。*数据加载*
    是将数据导入目标系统并使其可用于分析和报告的过程。'
- en: There are several approaches to loading real-time and near real-time data, including
    batch loading, incremental loading, and stream processing. *Batch loading* is
    the process of loading data in large chunks at regular intervals. *Incremental
    loading* is the process of loading only new or changed data. *Stream processing*
    is the process of continuously processing and analyzing data as it is generated.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 处理实时和准实时数据的几种方法包括批量加载、增量加载和流处理。*批量加载* 是定期以大块加载数据的过程。*增量加载* 是仅加载新数据或更改的过程。*流处理*
    是连续处理和分析生成的数据的过程。
- en: In order to handle the high volume, velocity, and variety of real-time and near
    real-time data, various big data technologies such as Apache Kafka, Apache Storm,
    Apache Spark, and Apache Flink are widely adopted.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理实时和准实时数据的高容量、高速率和多样性，广泛采用各种大数据技术，如 Apache Kafka、Apache Storm、Apache Spark
    和 Apache Flink。
- en: Near Real-Time Replication Using AWS Database Migration Service
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AWS 数据库迁移服务进行近实时复制
- en: AWS DMS is a fully managed service that makes it easy to migrate databases to
    AWS. DMS can migrate your data to and from most widely used commercial and open
    source databases such as Oracle, MySQL, MariaDB, PostgreSQL (pgSQL), Microsoft
    SQL Server, and many more. One of the common use cases of DMS is migrating data
    to Amazon Redshift.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: AWS DMS 是一项完全托管的服务，使得将数据库迁移到 AWS 变得轻松。DMS 可以将您的数据迁移到和从多数广泛使用的商业和开源数据库，如 Oracle、MySQL、MariaDB、PostgreSQL（pgSQL）、Microsoft
    SQL Server 等。DMS 的常见用例之一是将数据迁移到 Amazon Redshift。
- en: Before you begin your migration, it is important to plan and prepare your migration.
    This includes identifying the source and target databases, the amount of data
    to be migrated, and any specific requirements or constraints that need to be considered.
    You should also test your migration in a nonproduction environment to ensure that
    everything is working as expected.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始迁移之前，重要的是计划和准备好迁移工作。这包括识别源数据库和目标数据库、需要迁移的数据量以及需要考虑的任何特定要求或约束条件。您还应在非生产环境中测试迁移过程，以确保一切按预期运行。
- en: Once you have planned and prepared for your migration, you can create a DMS
    replication instance. A replication instance is a DMS resource that you use to
    perform the actual migration. It is responsible for connecting to the source and
    target databases and for moving the data from one to the other.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您计划并准备好迁移工作，您可以创建 DMS 复制实例。复制实例是一个 DMS 资源，用于执行实际的迁移工作。它负责连接源数据库和目标数据库，并将数据从一处移动到另一处。
- en: After you create a replication instance, you can create a migration task. A
    migration task is a DMS resource that defines the specific details of the migration,
    such as the source and target databases, the data to be migrated, and any specific
    settings or options.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 创建复制实例后，您可以创建迁移任务。迁移任务是 DMS 资源，定义了迁移的具体细节，如源数据库和目标数据库、要迁移的数据以及任何特定的设置或选项。
- en: When you create a migration task, you can choose to perform a full load or a
    change data capture (CDC) migration. A full load migration will copy all of the
    data from the source database to the target, while a CDC migration will copy only
    the changes made to the source database since the last migration.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 创建迁移任务时，您可以选择执行完整加载或变更数据捕获（CDC）迁移。完整加载迁移将所有数据从源数据库复制到目标数据库，而 CDC 迁移仅复制自上次迁移以来对源数据库所做的更改。
- en: Once you have created a migration task, you can start the migration. DMS will
    begin moving data from the source database to the target. You can monitor the
    progress of the migration using the DMS console or the AWS CLI. For more details,
    refer to [documentation on using Amazon Redshift data warehouse as a target](https://oreil.ly/LN6gM).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了迁移任务，您可以开始迁移。DMS将开始从源数据库向目标数据库移动数据。您可以使用DMS控制台或AWS CLI监视迁移进度。有关更多详细信息，请参阅[使用Amazon
    Redshift数据仓库作为目标的文档](https://oreil.ly/LN6gM)。
- en: When the migration is complete, you can perform any necessary postmigration
    tasks, such as creating indexes or loading data into additional tables. You should
    also test the target database to ensure that all the data has been migrated correctly
    and that the target database is working as expected.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移完成后，您可以执行任何必要的迁移后任务，例如创建索引或将数据加载到其他表中。您还应该测试目标数据库，确保所有数据已正确迁移，并且目标数据库按预期工作。
- en: Amazon DMS provides a simple and easy way to migrate data to Amazon Redshift.
    By following the steps outlined in this chapter, you can plan, prepare, and perform
    your migration with confidence, knowing that your data will be moved quickly and
    securely to your new data warehouse.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon DMS为将数据迁移到Amazon Redshift提供了一种简单且易用的方法。通过遵循本章中概述的步骤，您可以计划、准备和执行迁移，放心地知道您的数据将快速且安全地移动到您的新数据仓库中。
- en: Amazon Aurora Zero-ETL Integration with Amazon Redshift
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Aurora与Amazon Redshift的零ETL集成
- en: '*Zero-ETL integration* with Amazon Redshift enables an architecture pattern
    that eliminates the need for complex ETL jobs to move data for analysis. Zero-ETL
    integration with Amazon Redshift is available from Amazon Aurora to enable near
    real-time analytics and ML on petabytes of transactional data, whether that data
    is in the same account or a different account. Within seconds of transactional
    data being written into an Aurora database, the data is available in Amazon Redshift,
    so you don’t have to build and maintain complex data pipelines to perform extract
    and load operations.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*零ETL集成*与Amazon Redshift结合，实现了一种架构模式，消除了为分析移动数据而需要复杂ETL作业的需要。从Amazon Aurora到Amazon
    Redshift的零ETL集成可以实现几乎实时的分析和ML处理，无论数据是否在同一账户或不同账户中。在将事务数据写入Aurora数据库几秒钟内，数据就可用于Amazon
    Redshift，因此您无需构建和维护复杂的数据流水线来执行抽取和加载操作。'
- en: This [zero-ETL integration](https://oreil.ly/QNwc-) also enables you to analyze
    data from multiple Amazon Aurora database clusters in the same new or existing
    Amazon Redshift data warehouse to derive insights across many applications or
    partitions. With near real-time access to transactional data, you can leverage
    Amazon Redshift’s analytics capabilities such as built-in ML, materialized views,
    data sharing, and federated access to multiple data stores and data lakes to derive
    insights from transactional and other data. This architecture allows for faster
    time-to-insight and reduced costs, as data does not need to be loaded and transformed
    before being analyzed. Additionally, it allows for near real-time analysis of
    data in the data warehouse without impacting the workloads in your transactional
    system.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这种[零ETL集成](https://oreil.ly/QNwc-)还使您能够在同一个新的或现有的Amazon Redshift数据仓库中分析来自多个Amazon
    Aurora数据库集群的数据，从而跨多个应用程序或分区获取洞见。通过接近实时访问事务数据，您可以利用Amazon Redshift的分析能力，如内置ML、物化视图、数据共享和对多个数据存储和数据湖的联合访问，从事务数据和其他数据中获取洞见。此架构允许更快的洞察时间和降低成本，因为数据在分析之前不需要加载和转换。此外，它允许在数据仓库中接近实时地分析数据，而不会影响您的事务系统的工作负载。
- en: To get started with zero-ETL, you will need to first ensure your Amazon Aurora
    database and Amazon Redshift data warehouse are configured correctly. For example,
    for Aurora you will need to be sure you are using the latest version and have
    logging enabled. Similarly, for Amazon Redshift, you will need to ensure you’re
    on the latest version and have the required parameters set. See the [online documentation](https://oreil.ly/meMEy)
    for more details on the required configuration parameters.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用零ETL，您首先需要确保Amazon Aurora数据库和Amazon Redshift数据仓库已正确配置。例如，对于Aurora，您需要确保使用最新版本并启用了日志记录。同样，对于Amazon
    Redshift，您需要确保使用最新版本并设置了必需的参数。有关所需配置参数的详细信息，请参阅[在线文档](https://oreil.ly/meMEy)。
- en: Next, you’ll set up the required permissions that enable your Amazon Aurora
    database to load your Amazon Redshift data warehouse. To complete this activity,
    you will need to ensure that your user has the `redshift:CreateInboundIntegration`
    permission.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将设置所需的权限，以使您的Amazon Aurora数据库能够加载Amazon Redshift数据仓库的数据。要完成此操作，您需要确保您的用户具有`redshift:CreateInboundIntegration`权限。
- en: Navigate to the Amazon Redshift console in your data warehouse resource policy
    and use the “Add authorized integration sources” option to specify the Amazon
    Resource Name (ARN) of the Amazon Aurora database (see [Figure 3-9](#auth_int_src)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 转到Amazon Redshift控制台中的数据仓库资源策略，并使用“添加授权集成源”选项指定Amazon Aurora数据库的Amazon资源名称（ARN）（参见[图3-9](#auth_int_src)）。
- en: '![Edit authorized integration sources](assets/ardg_0309.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![编辑授权集成源](assets/ardg_0309.png)'
- en: Figure 3-9\. Edit authorized integration sources
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9\. 编辑授权集成源
- en: Now, you’re ready to create the zero-ETL integration. To complete this activity,
    you will need to ensure your user has the `rds:CreateIntegration` and `rds:DescribeIntegration`
    permission. Additionally, you may need the `rds:DeleteIntegration` permission
    if you ever need to delete the integration.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经准备好创建零ETL集成。要完成此操作，您需要确保您的用户具有`rds:CreateIntegration`和`rds:DescribeIntegration`权限。此外，如果您需要删除集成，可能还需要`rds:DeleteIntegration`权限。
- en: Navigate to the Amazon Relational Database Service (RDS) console and click on
    the “Zero-ETL integrations” menu item. Next, click on “Create zero-ETL integration”
    (see [Figure 3-10](#create_int)). When creating the integration in the same account,
    select the Amazon Aurora database and the Amazon Redshift data warehouse from
    the prepopulated list and submit your request. You can monitor the creation of
    your zero-ETL integration by inspecting the status field in the Amazon RDS console.
    Your integration is ready when the status changes from Creating to Active.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 转到Amazon关系数据库服务（RDS）控制台，点击“零ETL集成”菜单项。接下来，点击“创建零ETL集成”（参见[图3-10](#create_int)）。在同一账户中创建集成时，选择预填充列表中的Amazon
    Aurora数据库和Amazon Redshift数据仓库，然后提交请求。您可以通过检查Amazon RDS控制台中的状态字段来监视零ETL集成的创建情况。当状态从“Creating”变为“Active”时，您的集成就准备就绪了。
- en: '![Create zero-ETL integration](assets/ardg_0310.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![创建零ETL集成](assets/ardg_0310.png)'
- en: Figure 3-10\. Create zero-ETL integration
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-10\. 创建零ETL集成
- en: 'Finally, you can start querying the data that has been loaded to Amazon Redshift.
    First, capture the `integration_id` for your zero-ETL integration from the Amazon
    RDS console or by executing the following SQL statement in Amazon Redshift:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以开始查询加载到Amazon Redshift的数据。首先，从Amazon RDS控制台或通过在Amazon Redshift中执行以下SQL语句捕获您的零ETL集成的`integration_id`：
- en: '[PRE14]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, create a local database referencing the `integration_id`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建一个引用`integration_id`的本地数据库：
- en: '[PRE15]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When complete, you’ll be able to navigate and query all the objects that have
    been synchronized from your Amazon Aurora database in near real time. Each Amazon
    Aurora database/schema in your source will manifest as a different schema in the
    target Amazon Redshift data warehouse database.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，您可以浏览和查询从您的Amazon Aurora数据库同步到Amazon Redshift的所有对象，几乎实时显示。源Amazon Aurora数据库/模式将作为目标Amazon
    Redshift数据仓库数据库中的不同模式呈现。
- en: To further process the data, you can consider materialized views and scripts
    or stored procedures that can be scheduled to run using the Amazon Redshift scheduler
    or by using external orchestration tools.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 要进一步处理数据，您可以考虑物化视图、脚本或存储过程，并可以使用Amazon Redshift调度程序或外部编排工具定期运行。
- en: If your Amazon Aurora database is in a different account from your Redshift
    data warehouse, you will need to perform additional configuration steps such as
    setting up an authorized principal and enabling cross-account access. For details
    on how to set up a cross-account integration, see the [online documentation](https://oreil.ly/QNwc-).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的Amazon Aurora数据库与您的Redshift数据仓库位于不同的账户中，则需要执行额外的配置步骤，如设置授权主体和启用跨账户访问。有关如何设置跨账户集成的详细信息，请参阅[在线文档](https://oreil.ly/QNwc-)。
- en: Using Amazon AppFlow
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Amazon AppFlow
- en: Many organizations today use SaaS applications to run their business operations.
    Some SaaS applications, such as [SAP](https://www.sap.com) and [Infor](https://www.infor.com),
    provide comprehensive ERP modules, while others such as Salesforce, Google Analytics,
    Facebook Ads, and ServiceNow provide best-of-breed features to run certain functions
    of your business. To provide business insights to your users, you might have to
    combine data from multiple SaaS applications, for example opportunities from [Salesforce](https://www.salesforce.com)
    and actual sales from SAP. These SaaS applications provide APIs or extractors
    to extract data from the applications at a transactional level or an application
    level.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当今许多组织使用 SaaS 应用来运行其业务操作。一些 SaaS 应用，如 [SAP](https://www.sap.com) 和 [Infor](https://www.infor.com)，提供全面的
    ERP 模块，而其他一些如 Salesforce、Google Analytics、Facebook Ads 和 ServiceNow 则提供最佳功能来运行业务的特定功能。为了向用户提供业务见解，您可能需要结合来自多个
    SaaS 应用的数据，例如来自 [Salesforce](https://www.salesforce.com) 的机会和来自 SAP 的实际销售。这些 SaaS
    应用提供 API 或提取器，以从应用程序的事务级或应用级提取数据。
- en: Amazon AppFlow is a fully managed integration service that helps you securely
    transfer data between SaaS applications such as Salesforce, SAP, Google Analytics,
    Facebook Ads, and ServiceNow and AWS services such as Amazon S3 and Amazon Redshift
    in just a few clicks.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon AppFlow 是一种完全托管的集成服务，帮助您安全地在多个点击之间传输数据，例如 Salesforce、SAP、Google Analytics、Facebook
    Ads 和 ServiceNow 等 SaaS 应用与 AWS 服务，如 Amazon S3 和 Amazon Redshift。
- en: With Amazon AppFlow ([Figure 3-11](#Amazon_Appflow)), you can perform transformations
    and enrich the data through filters and validations. It supports data connectivity
    to 50 connectors, and you can move data bidirectionally to AWS services like Amazon
    S3 and Amazon Redshift. You can also create your own custom connector that can
    read from any API source to stream data into Amazon Redshift. To transfer data
    to Amazon Redshift from any source application, [create an Amazon AppFlow flow](https://oreil.ly/AQs8T),
    and choose Amazon Redshift as the data destination. For detailed steps to connect
    Amazon Appflow to your Amazon Redshift data warehouse, refer to this [documentation](https://oreil.ly/ythnj).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Amazon AppFlow（[图 3-11](#Amazon_Appflow)），您可以通过过滤器和验证执行转换和数据增强。它支持与 50 个连接器的数据连接，并可以双向移动数据到
    AWS 服务如 Amazon S3 和 Amazon Redshift。您还可以创建自定义连接器，从任何 API 源读取数据流入 Amazon Redshift。要从任何源应用程序将数据传输到
    Amazon Redshift，请[创建一个 Amazon AppFlow 流](https://oreil.ly/AQs8T)，并选择 Amazon Redshift
    作为数据目标。有关连接 Amazon Appflow 到您的 Amazon Redshift 数据仓库的详细步骤，请参阅此[文档](https://oreil.ly/ythnj)。
- en: '![ETL integration using Amazon Appflow ](assets/ardg_0311.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Amazon Appflow 进行 ETL 集成](assets/ardg_0311.png)'
- en: Figure 3-11\. ETL integration using Amazon AppFlow
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-11\. 使用 Amazon AppFlow 进行 ETL 集成
- en: Before you begin your data ingestion, it is important to plan and prepare your
    data flow. This includes identifying the source and target apps and services,
    the data you want to transfer, and any specific requirements or constraints that
    need to be considered. You should also test your data flow in a nonproduction
    environment to ensure that everything is working as expected.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始数据摄入之前，重要的是规划和准备您的数据流。这包括识别要传输的源和目标应用程序和服务、要传输的数据以及需要考虑的任何特定要求或约束。您还应在非生产环境中测试数据流，以确保一切按预期工作。
- en: Once you have planned and prepared for your data flow, you can create a new
    flow in AppFlow. To do this, you will need to specify the source app and service,
    the target app and service, and the data you want to transfer. AppFlow supports
    many popular apps and services, including Salesforce, ServiceNow, Slack, and many
    more.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您规划并准备好数据流，您可以在 AppFlow 中创建新的流。为此，您需要指定源应用程序和服务、目标应用程序和服务以及要传输的数据。AppFlow
    支持许多热门的应用程序和服务，包括 Salesforce、ServiceNow、Slack 等等。
- en: Next, you will need to configure the settings for your flow. This includes specifying
    the schedule for the flow, such as how often the data should be transferred and
    any specific options or settings for the source and target apps and services.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要配置流的设置。这包括指定流的计划，例如数据传输的频率以及源应用程序和目标应用程序和服务的任何特定选项或设置。
- en: After you have configured the settings for your flow, you can create the flow.
    When you create a flow, AppFlow will create all the necessary resources, such
    as connectors and triggers, to move the data between the source and target apps
    and services.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 完成流的设置后，您可以创建流。创建流时，AppFlow 将创建所有必要的资源，如连接器和触发器，以在源应用程序和目标应用程序和服务之间传输数据。
- en: When the flow is created, AppFlow will automatically begin transferring data
    from the source app and service to the target. You can monitor the progress of
    the flow using the AppFlow console or the AWS CLI.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 创建流程后，AppFlow将自动开始从源应用程序和服务传输数据到目标位置。您可以使用AppFlow控制台或AWS CLI监视流程的进度。
- en: When the data is transferred, it will be ingested into your Amazon Redshift
    data warehouse, where it can be queried and analyzed using standard SQL. You can
    then use your existing BI tools to create reports and visualizations based on
    the data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据传输完成后，数据将被摄入到您的亚马逊Redshift数据仓库中，您可以使用标准SQL对其进行查询和分析。然后，您可以使用现有的BI工具基于数据创建报告和可视化。
- en: Amazon AppFlow provides a simple and easy way to ingest data into Amazon Redshift.
    By following the steps outlined in this chapter, you can plan, prepare, and perform
    your data ingestion with confidence, knowing that your data will be transferred
    quickly and securely to your data warehouse. AppFlow enables you to automate the
    data flow process between different apps and services and make it more efficient.
    For a real use case using Amazon AppFlow to pull data from Salesforce into Amazon
    Redshift, refer to this [blog](https://oreil.ly/Zhrpk).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊AppFlow提供了一种简单易行的方法将数据摄入到亚马逊Redshift中。通过按照本章节中概述的步骤进行规划、准备和执行数据摄取，您可以放心地知道您的数据将快速安全地传输到数据仓库中。AppFlow使您能够自动化不同应用程序和服务之间的数据流程，并提高其效率。要了解使用亚马逊AppFlow从Salesforce拉取数据到亚马逊Redshift的实际用例，请参阅此[博客](https://oreil.ly/Zhrpk)。
- en: Streaming Ingestion
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式摄取
- en: Streaming ingestion is the process of continuously loading data into a data
    warehouse or BI system in real time or near real time. This allows for real time
    analysis and reporting of the data, which is critical for time-sensitive applications
    such as financial trading, transportation, and logistics. Streaming ingestion
    typically uses a streaming data platform, such as Apache Kafka or Amazon Kinesis,
    to collect and manage the data streams. The data streams are then processed and
    loaded into the target data warehouse or BI systems.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 流式摄取是将数据连续加载到数据仓库或BI系统中的过程，实时或准实时进行。这允许对数据进行实时分析和报告，对于金融交易、交通运输和物流等时间敏感应用程序至关重要。流式摄取通常使用流式数据平台（如Apache
    Kafka或亚马逊Kinesis）来收集和管理数据流。然后处理并将数据加载到目标数据仓库或BI系统中。
- en: There are several benefits to using streaming ingestion, including the ability
    to handle high-velocity and high-volume data, perform real-time analytics and
    reporting, and detect and respond to events in real time. However, streaming ingestion
    also poses some challenges, such as the need for efficient data processing and
    storage capabilities, robust data integration and management tools, and specialized
    skills and expertise.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用流式数据摄取有几个好处，包括处理高速和大容量数据的能力，进行实时分析和报告，并实时检测和响应事件。然而，流式数据摄取也带来了一些挑战，如需要高效的数据处理和存储能力、强大的数据集成和管理工具，以及专业的技能和专业知识。
- en: 'Use cases for Amazon Redshift streaming ingestion center around working with
    data that is generated continually (streamed) and needs to be processed within
    a short period (latency) of its generation. Sources of data can vary, from IoT
    devices to system telemetry, utility service usage, geolocation of devices, and
    more. There can be multiple use cases for streaming ingestion,including the following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊Redshift流式摄取的用例主要围绕持续生成的数据（流式数据）进行工作，需要在其生成后的短时间内（延迟）进行处理。数据来源可以多种多样，从物联网设备到系统遥测、公用服务使用、设备地理位置等。流式摄取可以有多个用例，包括以下内容：
- en: Monitoring equipment in real time for alerts
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 实时监控设备以获取警报
- en: Consider a fleet of vehicles equipped with sensors that collect data on various
    metrics such as speed, temperature, and fuel consumption. The sensor data needs
    to be analyzed in real time to provide alerts on any outliers to be able to proactively
    address issues.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一组装有传感器的车辆队列，这些传感器收集各种指标数据，如速度、温度和燃料消耗。需要实时分析传感器数据，以提供有关任何异常值的警报，以便能够及时解决问题。
- en: Real-time ad placements on websites
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 网站上的实时广告投放
- en: Analyzing social media data from multiple platforms, such as Twitter and Facebook,
    for real-time ad placements or preventing misinformation or obscenity.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 分析来自多个平台（如Twitter和Facebook）的社交媒体数据，用于实时广告投放或防止虚假信息或淫秽内容。
- en: Improve the gaming experience
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 改善游戏体验
- en: You can focus on in-game conversions, player retention, and optimizing the gaming
    experience by analyzing real-time data from gamers.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过分析游戏玩家的实时数据，专注于游戏内转化率、玩家留存率和优化游戏体验。
- en: Real-time retail analytics on streaming POS data
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 流式POS数据的实时零售分析
- en: You can access and visualize all your global point of sale (POS) retail sales
    transaction data for real-time analytics, reporting, and visualization.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以实时访问和可视化所有全球销售点（POS）零售销售交易数据，进行实时分析、报告和可视化。
- en: 'Amazon Redshift supports loading real-time data using streaming services. You
    can use either Amazon Kinesis Data Firehose or Kinesis Data Streams independently
    or use the native integration with Amazon Redshift:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift支持使用流式服务加载实时数据。您可以独立使用Amazon Kinesis数据Firehose或Kinesis数据流，或使用与Amazon
    Redshift的本地集成：
- en: The first option is to use Kinesis Firehose or Kinesis Data Streams. This involves
    connecting the stream to Amazon Kinesis Data Firehose and waiting for Kinesis
    Data Firehose to stage the data in Amazon S3, using various-sized batches at varying-length
    buffer intervals. After this, Kinesis Data Firehose initiates a `COPY` command
    to load the data from Amazon S3\. Previously, this usually involved latency in
    the order of minutes and needed data pipelines on top of the data loaded from
    the stream. Now, you can ingest data directly from the data stream.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一选项是使用Kinesis Firehose或Kinesis数据流。这涉及将数据流连接到Amazon Kinesis数据Firehose，并等待Kinesis数据Firehose将数据暂存在Amazon
    S3中，使用各种大小的批处理和不同长度的缓冲间隔。之后，Kinesis数据Firehose启动`COPY`命令，从Amazon S3加载数据。以前，这通常涉及几分钟的延迟，并且需要在从数据流加载的数据之上构建数据管道。现在，您可以直接从数据流摄入数据。
- en: The second option is native integration with Amazon Kinesis Data Streams or
    Amazon Managed Streaming for Apache Kafka (MSK) data streams. Natively integrating
    with Amazon streaming engines, Amazon Redshift streaming ingestion ingests hundreds
    of MB of data per second so you can query data in near real time. With Amazon
    Redshift streaming ingestion, you can connect to multiple Amazon Kinesis Data
    Streams or Amazon MSK data streams and pull data directly to Amazon Redshift without
    staging data in Amazon S3\. Define a scheme or choose to ingest semistructured
    data with `SUPER` data type; you can also set up and manage ELT pipelines with
    SQL.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个选项是与Amazon Kinesis数据流或Amazon托管的Apache Kafka（MSK）数据流的本地集成。通过与Amazon流引擎的本地集成，Amazon
    Redshift流摄入每秒摄入数百MB的数据，因此您可以几乎实时地查询数据。使用Amazon Redshift流摄入，您可以连接到多个Amazon Kinesis数据流或Amazon
    MSK数据流，并直接将数据拉入Amazon Redshift，无需将数据暂存在Amazon S3中。定义方案或选择使用`SUPER`数据类型摄入半结构化数据；您还可以使用SQL设置和管理ELT管道。
- en: The native Amazon Redshift streaming ingestion capability allows you to connect
    to Kinesis Data Streams directly, without the latency and complexity associated
    with staging the data in Amazon S3 and loading it into the data warehouse. You
    can now connect to and access the data from the stream using SQL and simplify
    your data pipelines by creating materialized views directly on top of the stream.
    The materialized views can also include SQL transforms as part of your ELT pipeline.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 本地的Amazon Redshift流式摄入功能允许您直接连接到Kinesis数据流，无需通过将数据暂存到Amazon S3并加载到数据仓库中的延迟和复杂性。您现在可以使用SQL连接并访问数据流，并通过直接在数据流之上创建物化视图简化数据流水线。物化视图还可以包括作为ELT管道的一部分的SQL转换。
- en: After you define the materialized views, you can refresh them to query the most
    recent stream data. This means that you can perform downstream processing and
    transformations of streaming data using SQL at no additional cost and use your
    existing BI and analytics tools for real-time analytics.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 定义物化视图后，您可以刷新它们以查询最新的流数据。这意味着您可以使用SQL对流数据进行下游处理和转换，而不会产生额外的费用，并可以使用现有的BI和分析工具进行实时分析。
- en: Amazon Redshift streaming ingestion works by acting as a stream consumer. A
    materialized view is the landing area for data that is consumed from the stream.
    When the materialized view is refreshed, Amazon Redshift compute nodes allocate
    each data shard to a compute slice. Each slice consumes data from the allocated
    shards until the materialized view attains parity with the stream. The very first
    refresh of the materialized view fetches data from the `TRIM_HORIZON` of the stream.
    Subsequent refreshes read data from the last `SEQUENCE_NUMBER` of the previous
    refresh until it reaches parity with the stream data. [Figure 3-12](#streaming-ingestion-workflow)
    illustrates this workflow.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 流式摄入通过充当流消费者的方式工作。材料化视图是从流中消费的数据的着陆区域。当材料化视图被刷新时，Amazon Redshift
    计算节点会将每个数据分片分配给计算片段。每个片段从分配的分片中消费数据，直到材料化视图与流达到一致。材料化视图的第一次刷新从流的`TRIM_HORIZON`获取数据。后续刷新从前一次刷新的`SEQUENCE_NUMBER`开始读取数据，直到与流数据达到一致。[图 3-12](#streaming-ingestion-workflow)说明了这个工作流程。
- en: '![Streaming ingestion workflow](assets/ardg_0312.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![流摄入工作流](assets/ardg_0312.png)'
- en: Figure 3-12\. Streaming ingestion workflow
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-12\. 流摄入工作流
- en: Steps to get started with streaming ingestion
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开始使用流摄入的步骤
- en: The first step is to set up a Kinesis Data Stream as the source for the streaming
    ingestion pipeline. You can use Kinesis Streams data generator to set up test
    data as discussed in the blog [“Testing Your Streaming Data Solution with the
    New Amazon Kinesis Data Generator”](https://oreil.ly/Iz7xC).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是设置 Kinesis Data Stream 作为流摄入管道的源。您可以使用 Kinesis Streams 数据生成器设置测试数据，如博客中所述[“使用新的Amazon
    Kinesis数据生成器测试您的流数据解决方案”](https://oreil.ly/Iz7xC)。
- en: After setting up the data stream, you define a schema in Amazon Redshift with
    `CREATE EXTERNAL SCHEMA` to reference a Kinesis Data Streams resource.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 设置数据流后，您可以在 Amazon Redshift 中使用`CREATE EXTERNAL SCHEMA`定义模式，以引用 Kinesis Data
    Streams 资源。
- en: '[PRE16]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Create an IAM role as in [Example 3-15](#kds_iam_role) with a trust policy that
    allows your Amazon Redshift data warehouse to assume the role.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 IAM 角色，如[示例 3-15](#kds_iam_role)，其中信任策略允许您的 Amazon Redshift 数据仓库扮演该角色。
- en: Example 3-15\. IAM policy that grants access to your stream
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-15\. 授予访问流的 IAM 策略
- en: '[PRE17]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To access data in the stream, you create a materialized view with a select
    from the stream. When you query the materialized view, the returned records are
    a point-in-time view of the stream:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问流中的数据，您可以创建一个带有流选择的材料化视图。当您查询材料化视图时，返回的记录是流的某一时刻视图：
- en: '[PRE18]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, you refresh the view to do an initial load of data from streams to the
    materialized view:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您刷新视图以从流到材料化视图进行初始数据加载：
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now you can query the data from the materialized view using standard SQL statements:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以使用标准 SQL 语句从材料化视图查询数据：
- en: '[PRE20]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You can store stream records in the semistructured SUPER format or define a
    schema that results in data converted to Amazon Redshift data types.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将流记录存储在半结构化的 SUPER 格式中，或者定义一个模式，将数据转换为 Amazon Redshift 数据类型。
- en: For detailed steps to set up a streaming ingestion from Amazon Kinesis Data
    Streams, refer to [“Getting Started with Streaming Ingestion from Amazon Kinesis
    Data Streams.”](https://oreil.ly/2rhbT)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 有关从 Amazon Kinesis Data Streams 设置流摄入的详细步骤，请参阅[“开始使用Amazon Kinesis Data Streams进行流摄入”](https://oreil.ly/2rhbT)。
- en: Important considerations and best practices
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重要考虑因素和最佳实践
- en: The following are important considerations and best practices for performance
    and billing as you set up your streaming ingestion environment.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在设置流摄入环境时性能和计费的重要考虑因素和最佳实践。
- en: Auto refresh queries for a materialized view or views are treated as any other
    user workload. Auto refresh loads data from the stream as it arrives.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 自动刷新查询材料化视图或视图被视为任何其他用户工作负载。自动刷新会在数据流到达时加载数据。
- en: Auto refresh can be turned on explicitly for a materialized view created for
    streaming ingestion. To do this, specify `AUTO REFRESH` in the materialized view
    definition. Manual refresh is the default. To specify auto refresh for an existing
    materialized view for streaming ingestion, you can run `ALTER MATERIALIZED VIEW`
    to turn it on. For more information, see [`CREATE MATERIALIZED VIEW`](https://oreil.ly/syJfX)
    or [`ALTER MATERIALIZED VIEW`](https://oreil.ly/UBWgM).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 自动刷新可以明确地打开用于流入站点创建的物化视图。要做到这一点，请在物化视图定义中指定 `AUTO REFRESH`。手动刷新是默认设置。要为现有的用于流入站点的物化视图指定自动刷新，可以运行
    `ALTER MATERIALIZED VIEW` 来打开它。有关更多信息，请参阅 [`CREATE MATERIALIZED VIEW`](https://oreil.ly/syJfX)
    或 [`ALTER MATERIALIZED VIEW`](https://oreil.ly/UBWgM)。
- en: For Amazon Redshift serverless, the setup and configuration instructions are
    the same as setting up streaming ingestion on a provisioned cluster. It is important
    to size Amazon Redshift serverless with the necessary level of RPUs to support
    streaming ingestion with auto refresh and other workloads.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于亚马逊 Redshift 无服务器，设置和配置说明与在预置集群上设置流入站点相同。非常重要的是，要使用适当级别的 RPUs 大小亚马逊 Redshift
    无服务器，以支持带有自动刷新和其他工作负载的流入站点。
- en: When you configure streaming ingestion, Amazon Redshift attempts to connect
    to an Amazon MSK cluster in the same Availability Zone (AZ), if rack awareness
    is enabled for Amazon MSK. If all of your nodes are in different AZs from your
    Amazon Redshift data warehouse, you can incur cross-AZ data-transfer cost. To
    avoid this, keep at least one Amazon MSK broker cluster node in the same AZ as
    your Amazon Redshift data warehouse.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置流入站点时，如果 Amazon MSK 的机架感知已启用，则 Amazon Redshift 尝试连接到同一可用区（AZ）中的 Amazon MSK
    集群。如果您所有的节点都与 Amazon Redshift 数据仓库的不同 AZ 中，您可能会产生跨 AZ 数据传输成本。为了避免这种情况，请至少在 Amazon
    Redshift 数据仓库相同 AZ 中保留一个 Amazon MSK broker 集群节点。
- en: After creating a materialized view, its initial refresh starts from the `TRIM_HORIZON`
    of a Kinesis stream, or from offset 0 of an Amazon MSK topic.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 创建物化视图后，其初始刷新从 Kinesis 流的 `TRIM_HORIZON` 或 Amazon MSK 主题的偏移 0 开始。
- en: Supported data formats are limited to those that can be converted from `VARBYTE`.
    For more information, see [`VARBYTE` type](https://oreil.ly/MyZiJ) and [`VARBYTE`
    operators](https://oreil.ly/XPn3v).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的数据格式仅限于可以从 `VARBYTE` 转换的格式。有关更多信息，请参阅 [`VARBYTE` 类型](https://oreil.ly/MyZiJ)
    和 [`VARBYTE` 运算符](https://oreil.ly/XPn3v)。
- en: It is possible to ingest a stream and land the data into multiple materialized
    views. For instance, a use case where you ingest a stream containing sports data,
    but you organize data for each sport into separate materialized views. However,
    when you ingest data into and refresh multiple materialized views, there can be
    higher egress costs, as well as bandwidth, throughput, and performance limitations
    for your streaming provider. Additionally, consider how higher resource use for
    reading into more than one materialized view can impact other workloads. For these
    reasons, we recommend you land the data for each stream in a single materialized
    view. For more information about pricing for data streams, see [Kinesis Data Streams
    Pricing](https://oreil.ly/wUws2) and [Amazon MSK Pricing](https://oreil.ly/Lt7KM).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将流入的数据流落入多个物化视图中。例如，一种用例是您接收包含体育数据的流，但将每种体育的数据组织到单独的物化视图中。然而，当您将数据流入并刷新多个物化视图时，可能会导致更高的出站成本，以及对您的流媒体提供商的带宽、吞吐量和性能限制。此外，请考虑读取更多物化视图的高资源使用如何影响其他工作负载。因此，我们建议您将每个流的数据落入单个物化视图中。有关数据流定价的更多信息，请参阅
    [Kinesis 数据流定价](https://oreil.ly/wUws2) 和 [Amazon MSK 定价](https://oreil.ly/Lt7KM)。
- en: Optimize Your Data Structures
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化您的数据结构
- en: Traditionally, databases have been built on Symmetric Multiprocessing (SMP)
    architecture where multiple CPUs access shared memory and disks. This tightly
    coupled multiprocessor system is unable to scale linearly to meet data growth
    and keep up with query execution throughput requirements.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，数据库建立在对称多处理（SMP）架构上，多个 CPU 访问共享内存和磁盘。这种紧密耦合的多处理器系统无法线性扩展以满足数据增长和查询执行吞吐量要求。
- en: 'These challenges are overcome by MPP architecture. MPP architectures are of
    two types:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战通过 MPP 架构得以克服。MPP 架构有两种类型：
- en: 'Shared-disk architectures: here CPU and memory are parallel processing, but
    the disk is shared.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享磁盘架构：在此，CPU 和内存是并行处理的，但磁盘是共享的。
- en: 'Shared-nothing architecture: here CPU, memory as well as disk, all are processing
    in parallel.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享无事务架构：在此，CPU、内存以及磁盘都是并行处理的。
- en: As mentioned earlier in this book, Amazon Redshift is an MPP shared-nothing
    architecture that achieves linear scalability by processing data on each node
    using the memory and CPU attached to the node. This architecture achieves massive
    scale as there is no single executor bottleneck to slow down the system, and adding
    or removing nodes provides linear scalability. The physical storage of data of
    a single object or table on individual nodes means MPP systems have distributed
    or replicated tables and the distribution style plays a crucial role in query
    performance. Michael Stonebraker, from the University of California, Berkeley,
    covers [The Case for Shared Nothing](https://oreil.ly/eMiar) in his paper.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书前面提到的，Amazon Redshift是一种MPP共享无共享体系结构，通过在每个节点上使用节点附加的内存和CPU来处理数据，实现线性可伸缩性。这种架构通过在每个节点上处理数据，实现了大规模，因为没有单个执行器瓶颈来减慢系统，并且添加或删除节点提供了线性可伸缩性。在单个对象或表的物理存储数据中，意味着MPP系统具有分布式或复制表，分布式风格在查询性能中起着关键作用。迈克尔·斯通布雷克（Michael
    Stonebraker）在他的论文中探讨了[共享无共享的情况](https://oreil.ly/eMiar)。
- en: When you create your database objects, certain key table design decisions influence
    overall query performance. These design choices also have a significant effect
    on storage requirements, which in turn could affect query performance. The key
    goal is to reduce the number of I/O operations and minimize the memory required
    to process queries.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建数据库对象时，某些关键的表设计决策会影响整体查询性能。这些设计选择还会对存储需求产生重大影响，进而可能影响查询性能。关键目标是减少I/O操作的数量并最小化处理查询所需的内存。
- en: Amazon Redshift automates many of these decisions for you through a combination
    of [“Automatic Table Optimization and Autonomics”](#ato_autonomics), however there
    may be times when you want to fine-tune your environment and set your own [“Distribution
    Style”](#dist_style), [“Sort Key”](#sort_key), or [“Compression Encoding”](#encoding).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift通过[“自动表优化和自动化”](#ato_autonomics)的结合为您自动化了许多决策，但是在您希望优化您的环境并设置自己的[“分布式风格”](#dist_style)、[“排序键”](#sort_key)或[“压缩编码”](#encoding)时，可能需要微调。
- en: Automatic Table Optimization and Autonomics
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动表优化和自动化
- en: Amazon Redshift uses automatic table optimization (ATO) to choose the right
    distribution style, sort keys, and encoding when you create a table with `AUTO`
    options. Hence it is a good practice to take advantage of the auto feature and
    create tables with `DISTSTYLE AUTO`, `SORTKEY AUTO`, and `ENCODING AUTO`. When
    tables are created with `AUTO` options, Amazon Redshift initially creates tables
    with optimal keys for the best first-time query performance possible using information
    such as the primary key and data types. In addition, Amazon Redshift analyzes
    the data volume and query usage patterns to evolve the distribution strategy and
    sort keys to optimize performance over time. Finally, Amazon Redshift will perform
    table maintenance activities on your tables that reduce fragmentation and ensure
    statistics are up-to-date. We discuss more on the topic of ATO in [Chapter 5,
    “Scaling and Performance Optimizations”](ch05.html#AR_TGD_CH5).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift在创建带有`AUTO`选项的表时使用自动表优化（ATO）来选择正确的分布式风格、排序键和编码。因此，利用自动功能并使用`DISTSTYLE
    AUTO`、`SORTKEY AUTO`和`ENCODING AUTO`创建表是一个良好的实践。当使用`AUTO`选项创建表时，Amazon Redshift最初会根据主键和数据类型等信息创建具有最佳首次查询性能的表。此外，Amazon
    Redshift分析数据量和查询使用模式，随着时间的推移演变分布策略和排序键以优化性能。最后，Amazon Redshift将在您的表上执行减少碎片和确保统计数据最新的表维护活动。我们在[第5章，“扩展和性能优化”](ch05.html#AR_TGD_CH5)中进一步讨论了ATO主题。
- en: Distribution Style
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式风格
- en: 'Since Amazon Redshift is an MPP data warehouse based on compute nodes, the
    way you distribute the data is important to ensure you utilize the resources optimally
    for a given workload. When you execute a query, the query optimizer may need to
    redistribute the rows to the compute nodes to perform any joins. The goal in selecting
    a table distribution style is to minimize the impact of the redistribution step
    by locating the data where it needs to be before the query is run to join two
    or more tables. There are four data distribution strategies: `AUTO`, `EVEN`, `ALL`,
    and `KEY` (see [Figure 3-13](#distribution_style_2_2_4_1)).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 由于亚马逊Redshift是基于计算节点的MPP数据仓库，因此数据分布的方式对于确保在给定工作负载下充分利用资源至关重要。当执行查询时，查询优化器可能需要将行重新分发到计算节点以执行任何连接操作。在选择表分布样式时的目标是通过将数据定位在查询运行之前的位置来最小化重新分发步骤的影响，以便连接两个或多个表。有四种数据分布策略：`AUTO`、`EVEN`、`ALL`和`KEY`（见[图 3-13](#distribution_style_2_2_4_1)）。
- en: '![Data distribution between compute nodes slices](assets/ardg_0313.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![计算节点切片之间的数据分布](assets/ardg_0313.png)'
- en: Figure 3-13\. Data distribution between compute nodes slices
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-13\. 计算节点切片之间的数据分布
- en: '`AUTO` distribution'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`AUTO` 分布'
- en: As mentioned earlier, using the `AUTO` keyword means Amazon Redshift uses its
    built-in AI and ML capabilities, and based on the data volume and query patterns,
    it performs the best data distribution automatically. This is the preferred distribution
    for many personas, especially those who might not be well versed with architecture
    but still need to analyze datasets to obtain business insights.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，使用 `AUTO` 关键字意味着亚马逊Redshift利用其内置的AI和ML能力，并根据数据量和查询模式自动执行最佳数据分布。这是许多角色的首选分布策略，特别是那些可能对架构不熟悉但仍需要分析数据集以获取业务洞见的人。
- en: '`EVEN` distribution'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`EVEN` 分布'
- en: In this distribution strategy, the data is stored in round-robin fashion to
    each slice of your Amazon Redshift data warehouse, so the data is evenly distributed
    with very minimal skew or imbalance between data volume across slices and nodes.
    `EVEN` distribution is best suited for large fact tables, which do not typically
    participate in joins with other dimension tables. Another good candidate is materialized
    views, where all the joins have already been executed and results have been captured,
    and queries mostly only filter rows.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种分布策略中，数据以轮询方式存储到亚马逊Redshift数据仓库的每个切片中，因此数据在切片和节点之间的数据量分布非常均匀，几乎没有倾斜或不平衡。`EVEN`
    分布最适合大型事实表，通常不参与与其他维度表的连接。另一个很好的候选对象是已经执行了所有连接并捕获了结果的物化视图，查询主要只是过滤行。
- en: '`ALL` distribution'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`ALL` 分布'
- en: In this distribution strategy, a full copy of the entire table is stored on
    the first slice of each compute node of your Amazon Redshift data warehouse. It
    is best suited for dimension tables to facilitate joins with fact tables without
    requiring data movement across nodes. As you can see, this facilitates faster
    query execution at the expense of increased storage costs.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种分布策略中，整个表的完整副本存储在亚马逊Redshift数据仓库每个计算节点的第一个切片上。这最适合维度表，以便在不需要跨节点移动数据的情况下与事实表进行连接。正如您所见，这有助于更快地执行查询，但增加了存储成本。
- en: '`KEY` distribution'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`KEY` 分布'
- en: In this distribution strategy, the fact table is distributed based off of hash
    generated on the designated column value, such that all values that result in
    the same hash are stored on the same slice of your Amazon Redshift data warehouse.
    This strategy is applied when joining two tables on the same key distributed column.
    The two tables can be two fact tables, a fact and a dimension table, or a staging
    and a target table.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种分布策略中，事实表基于指定列值生成的哈希进行分布，使得所有产生相同哈希值的值存储在亚马逊Redshift数据仓库的同一切片上。当在相同键分布列上连接两个表时应用此策略。这两个表可以是两个事实表，一个事实表和一个维度表，或一个暂存表和一个目标表。
- en: '`KEY` distribution allows for co-located join execution as the fact table and
    corresponding rows of the dimension table are always available on the very same
    slice. To ensure there is minimal skew between slices, it is required to use high
    cardinality columns for key distribution. Also note that Amazon Redshift allows
    only one column to be defined as a distribution key, so if your queries do a multiple
    column, join then you should create a new column by concatenating those individual
    column values as part of your data loading routines.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`KEY`分发允许在联接执行时共同位于同一片的事实表和对应的维度表行。为确保片之间的倾斜最小化，需要使用高基数列作为关键分发。还要注意，Amazon
    Redshift仅允许将一列定义为分发键，因此如果您的查询进行多列联接，则应在数据加载过程中通过串联这些单独的列值来创建新列。'
- en: You will notice that in all three distribution styles the focus is on reducing
    data movement when executing queries. This allows Amazon Redshift to perform at
    maximum throughput where each slice operates in parallel by dividing the total
    work by the number of nodes and slices.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，在所有三种分发样式中，重点是在执行查询时减少数据移动。这允许Amazon Redshift通过将总工作量按节点和片数分割，每个片并行运行，以实现最大吞吐量。
- en: 'Let’s take a scenario where you have a sales data warehouse, the `fact_sales`
    is the largest fact with billions of rows, which is joined very frequently with
    `d⁠i⁠m⁠_​c⁠u⁠s⁠t⁠o⁠m⁠e⁠r`, which has tens or hundreds of millions of rows. You
    then have `d⁠i⁠m⁠_​c⁠a⁠l⁠e⁠n⁠d⁠a⁠r`, `dim_product`, and `dim_region`, which have
    comparatively lower number of records. You also have materialized views for `sales_annual_mv`,
    `sales_qtrly_mv`, and `sales_monthly_mv` where various measures from the sales
    fact are preaggregated for dashboards and reports. After analyzing the data in
    this data warehouse, here are some recommendations for distribution:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一个销售数据仓库，`fact_sales`是最大的事实表，拥有数十亿行，经常与`dim_customer`（拥有数千万或数亿行）进行联接。然后您还有`dim_calendar`、`dim_product`和`dim_region`，这些表的记录数较少。您还为`sales_annual_mv`、`sales_qtrly_mv`和`sales_monthly_mv`创建了物化视图，用于仪表板和报告的预聚合度量。在分析此数据仓库中的数据后，以下是分发的一些建议：
- en: '`fact_sales` and `dim_customer` are good candidates for `KEY` distribution
    on `customer_key` but not on `calendar_key` or `product_key`.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fact_sales`和`dim_customer`适合使用`KEY`分发，但不适合在`calendar_key`或`product_key`上使用。'
- en: Other dimensions are good candidates for `ALL` distribution.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他维度适合使用`ALL`分发。
- en: The materialized views are good candidate for `EVEN` distribution.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物化视图适合使用`EVEN`分发。
- en: Since with Amazon Redshift you can choose only one column as your distribution
    key, if you import data from a source that has a multicolumn primary/foreign key,
    you may see that tables are being joined on more than one column. In this scenario,
    consider creating a new column in your tables with a concatenation of the join
    columns and use that new column as the distribution key.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Amazon Redshift只能选择一个列作为分发键，如果您从具有多列主键/外键的源导入数据，可能会看到表在多个列上进行联接。在这种情况下，考虑在您的表中创建一个新列，该列是联接列的串联，并将该新列用作分发键。
- en: Sort Key
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排序键
- en: Along with how data is distributed among compute node slices, the next aspect
    is how the data is physically sorted on disk. Amazon Redshift does not have indexes,
    but sort keys—along with zone maps—provide a similar functionality. Data is stored
    on disk in sorted order according to the sort key and the query optimizer uses
    sort order when it determines optimal query plans. Queries will be efficient with
    fewer I/O operations because they can skip entire blocks that fall outside the
    time range. The columns that are used as predicates to filter data are good candidates
    for defining as sort key columns.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 除了如何在计算节点片之间分发数据之外，下一个方面是数据在磁盘上的物理排序方式。Amazon Redshift没有索引，但排序键和区域映射提供了类似的功能。数据按照排序键的顺序存储在磁盘上，并且查询优化器在确定最佳查询计划时使用排序顺序。由于可以跳过时间范围之外的整个块，因此使用作为数据过滤谓词的列作为排序键列是高效的。
- en: Zone maps are the high and low values for each 1 MB block of storage for each
    column in the table. If the column is already sorted on, then you get nonoverlapping
    zone maps. These zone maps further reduce I/O by allowing Amazon Redshift to target
    only those blocks that a query needs to access. Refer to table attributes of the
    [`Create Table` command](https://oreil.ly/t2Pp8) for setting up sort keys.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 区域映射是每个表中每个列的每个 1 MB 存储块的高和低值。如果列已经排序，则获得非重叠的区域映射。这些区域映射通过允许 Amazon Redshift
    仅针对查询需要访问的那些块来进一步减少 I/O。设置排序键时，请参考[`Create Table`命令](https://oreil.ly/t2Pp8)中的表属性。
- en: '[Figure 3-14](#zone_maps) illustrates how the blocks of a sales table may be
    stored in your Amazon Redshift data warehouse. It also shows the zone map for
    the first 1 MB block of the sales_dt column where the minimum value is 20010523
    and the maximum value is 20010527\. This header record is scanned first by the
    query processor to determine if the data in the predicate clause or filter could
    exist in the block. If the predicate value is outside of the range in the zone
    map, then Amazon Redshift will skip the entire block and move to the next block.
    This minimizes the I/O and improves performance of the query.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-14](#zone_maps)说明了销售表的块如何存储在您的 Amazon Redshift 数据仓库中。它还显示了销售_dt列的第一个 1
    MB 块的区域映射，其中最小值为 20010523，最大值为 20010527。查询处理器首先扫描此头记录，以确定谓词子句或过滤器中的数据是否可能存在于该块中。如果谓词值超出区域映射中的范围，则
    Amazon Redshift 将跳过整个块并移动到下一个块。这最小化了 I/O 并提高了查询的性能。'
- en: '![Zone Maps](assets/ardg_0314.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![区域映射](assets/ardg_0314.png)'
- en: Figure 3-14\. Zone maps
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-14\. 区域映射
- en: 'In another example, consider a query on the `course_outcomes_fact` table. You
    may filter this table on a particular date, say, `Jun-09-2012`:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，考虑对`course_outcomes_fact`表的查询。您可以在特定日期上过滤该表，比如`Jun-09-2012`：
- en: '[PRE21]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The query processor is able to skip blocks as shown in [Figure 3-15](#sort_key_zone_maps)
    irrespective of whether the table is sorted or unsorted. On the unsorted table,
    the processor skips one block and has to scan three out of four blocks based on
    the MIN/MAX value in the zone map. However, for the sorted table, the processor
    has to scan only one out of four blocks.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 3-15](#sort_key_zone_maps)所示，无论表是排序还是未排序，查询处理器都能跳过块。对于未排序表，处理器跳过一个块并根据区域映射中的最小/最大值扫描四个块中的三个。但是对于排序表，处理器只需扫描四个块中的一个。
- en: '![Sort Key and Zone Maps](assets/ardg_0315.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![排序键和区域映射](assets/ardg_0315.png)'
- en: Figure 3-15\. Sort key and zone maps
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-15\. 排序键和区域映射
- en: As mentioned previously, we recommend that you create your tables with `SORTKEY
    AUTO` to let Amazon Redshift ATO choose the best sort key. In the case of `SORTKEY
    AUTO`, Amazon Redshift will analyze the query pattern for your table and apply
    a sort key, which is used most often in query predicates. By using automation
    to tune the design of your tables, you can get started more easily and get the
    fastest performance quickly without needing to invest time to manually tune and
    implement table optimizations.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们建议您使用`SORTKEY AUTO`创建表，以便让 Amazon Redshift ATO选择最佳排序键。对于`SORTKEY AUTO`，Amazon
    Redshift 将分析您表的查询模式并应用最常用于查询谓词中的排序键。通过使用自动化来调整表设计，您可以更轻松地开始并快速获得最快的性能，而无需投入时间手动调整和实施表优化。
- en: When choosing your own sort key, refer to [this query](https://oreil.ly/DIkwX)
    as a way to identify predicate columns. It is recommended to have no more than
    five columns in your sort key. Also, it is recommended to not apply any compression
    to the first column of the sort key to be able to effectively filter data blocks
    quickly as it reduces decompression overhead. The compound sort keys are highly
    effective in filtering data when leading columns are used instead of filtering
    only on the trailing columns. If you see that a different leading sort key column
    is equally popular in user queries, then leverage Amazon Redshift materialized
    views (MV). MVs provide automatic query rewrite, and the query optimizer will
    pick the appropriate MV instead of a base table.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择自己的排序键时，请参考[此查询](https://oreil.ly/DIkwX)来识别谓词列。建议在排序键中不超过五列，并且建议不对排序键的第一列应用任何压缩，以便能够快速有效地过滤数据块，因为这会减少解压缩的开销。在使用前导列而不是仅在尾随列上进行过滤时，复合排序键非常有效。如果发现用户查询中有不同的前导排序键列同样受欢迎，则利用
    Amazon Redshift 材料化视图（MV）。MV 提供自动查询重写，查询优化器将选择适当的 MV 而不是基础表。
- en: If you perform frequent range filtering or equality filtering on one column,
    specify that column as the sort key. In analytics use cases, it is common that
    data is queried based on components of date and time, and it’s a good idea to
    create the table with a date or timestamp column as the leading column for the
    sort key.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您频繁执行范围过滤或等值过滤于一列，请将该列指定为排序键。在分析使用情况下，基于日期和时间组件查询数据是常见的，建议将日期或时间戳列作为排序键的主导列创建表格是个不错的主意。
- en: If you frequently join a table and you don’t typically filter on either of the
    join tables, you can specify the join column as both the sort key and the distribution
    key. Doing this enables the query optimizer to choose a sort merge join instead
    of a slower hash join. Because the data is already sorted on the join key, the
    query optimizer can bypass the sort phase of the sort merge join.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您经常连接表格，且通常不会在连接表格上进行过滤，则可以将连接列同时指定为排序键和分布键。这样做可以使查询优化器选择排序合并连接而不是较慢的哈希连接。因为数据已经在连接键上排序，查询优化器可以绕过排序阶段的排序合并连接。
- en: Compression Encoding
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压缩编码
- en: Amazon Redshift applies column level compression, also known as encoding, to
    achieve three to four times compression compared to raw data. This also reduces
    the I/O requirements when accessing the data.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift应用列级压缩，也称为编码，以实现比原始数据三到四倍的压缩。这也减少了访问数据时的I/O需求。
- en: As mentioned earlier, the easiest way to manage encoding on your Amazon Redshift
    tables is to leverage the `ENCODING AUTO` option in your `create table` statement.
    When enabled, the encoding will be determined by the data type of the column and
    by the heuristics of the data being loaded.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在您的Amazon Redshift表上管理编码的最简单方法是利用`ENCODING AUTO`选项在您的`create table`语句中。启用后，编码将由列的数据类型和加载数据的启发式确定。
- en: Another option for setting the encoding on your tables is when you first ingest
    data using a `COPY` command into an empty table. By default, Amazon Redshift will
    choose the appropriate compression type by either sampling the incoming data or
    by using the data type of the column. This can be controlled in the `COPY` command
    using the `COMPUPDATE` parameter. With the `PRESET` option, the compression type
    will be determined based on data type, `ON` option will sample the dataset, and
    `OFF` option will not change the compression type. If you reload the same table
    over and over, you don’t have to analyze compression every time. `PRESET` is fast
    and works well in most scenarios. These options give you control over when and
    how the compression is determined and can ensure the properties of the table do
    not change if you are happy with the performance.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种设置表格编码的选择是首次使用`COPY`命令将数据加载到空表中时。默认情况下，Amazon Redshift将通过对传入数据进行抽样或使用列的数据类型选择适当的压缩类型。可以在`COPY`命令中使用`COMPUPDATE`参数进行控制。使用`PRESET`选项时，压缩类型将基于数据类型确定，`ON`选项将对数据集进行抽样，`OFF`选项将不更改压缩类型。如果您一遍又一遍地重新加载同一表格，您无需每次分析压缩。`PRESET`快速且在大多数情况下运行良好。这些选项使您能够控制何时以及如何确定压缩，并且可以确保表格的属性在性能满意时不会更改。
- en: In cases where your data profile has changed, it is a good idea to analyze if
    the compression settings in the table are optimal. You can do this using the `ANALYZE
    COMPRESSION` command (see [Example 3-16](#analyze_compression)). Note that the
    command can be executed on the entire table or a specific set of columns in the
    table.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据概况发生变化的情况下，分析表格中的压缩设置是否最优是个好主意。您可以使用`ANALYZE COMPRESSION`命令来执行此操作（见[示例 3-16](#analyze_compression)）。请注意，该命令可在整个表格或表格中的特定列集上执行。
- en: Example 3-16\. `ANALYZE COMPRESSION` command
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-16\. `ANALYZE COMPRESSION`命令
- en: '[PRE22]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'There are several best practices, if implemented, that ensure the most compression:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个最佳实践，如果实施，可以确保最大的压缩：
- en: If not using `AUTO`, use the appropriate encodings for your data types. For
    example, use run-length encoding (RLE) for columns with a high degree of repetition
    and delta encoding for columns with a high degree of similarity between rows.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不使用`AUTO`，请为您的数据类型使用适当的编码。例如，对于具有高重复度的列，请使用游程编码（RLE），对于行之间具有高相似性的列，请使用增量编码。
- en: Use the `COPY` command to load data as it automatically applies encoding parameters.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`COPY`命令加载数据，因为它会自动应用编码参数。
- en: Use the `VACUUM` command to increase compression by reducing fragmentation.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`VACUUM`命令通过减少碎片来增加压缩。
- en: Monitor the size of your tables and the amount of disk space they are using
    for opportunities to apply additional compression.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控表格大小及其使用的磁盘空间，以寻找应用额外压缩的机会。
- en: Avoid encoding small dimension tables because a 1 MB block (per column) can
    hold a large quantity of data, and in those cases compression will not yield an
    I/O benefit.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免对小维度表进行编码，因为每列的 1 MB 块可以容纳大量数据，在这些情况下，压缩不会带来 I/O 效益。
- en: Use compression for frequently accessed columns.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对频繁访问的列使用压缩。
- en: Summary
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter discussed key differences between a Data Lake first approach and
    a Data Warehouse first approach and scenarios where you can consider either approach.
    In addition, you created the sample data model and various types of transformation
    tools and strategies.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了数据湖优先方法与数据仓库优先方法之间的关键差异，以及可以考虑使用哪种方法的场景。此外，您还创建了样本数据模型和各种类型的转换工具和策略。
- en: The next chapter dives deeper into in-database transformation (ELT) and external
    transformations (ETL) for data in Amazon Redshift as well as how to query and
    transform all your data even when the data is not loaded in your data warehouse.
    We will also discuss strategies for orchestrating your data loads.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将更深入地探讨在 Amazon Redshift 中针对数据库内转换（ELT）和外部转换（ETL）的数据，以及如何在数据仓库中未加载数据的情况下查询和转换所有数据。我们还将讨论编排数据加载的策略。
- en: ^([1](ch03.html#id1443-marker)) Kuzilek J., Hlosta M., and Zdrahal Z. [Open
    University Learning Analytics dataset](https://doi.org/10.1038/sdata.2017.171),
    Sci. Data 4:170171 (2017).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.html#id1443-marker)) Kuzilek J., Hlosta M., 和 Zdrahal Z. [开放大学学习分析数据集](https://doi.org/10.1038/sdata.2017.171)，Sci.
    Data 4:170171 (2017)。
