- en: Chapter 9\. Setting Up Flink for Streaming Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 9 章\. 设置 Flink 用于流应用程序
- en: Today’s data infrastructures are diverse. Distributed data processing frameworks
    like Apache Flink need to be set up to interact with several components such as
    resource managers, filesystems, and services for distributed coordination.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当今的数据基础设施多种多样。像 Apache Flink 这样的分布式数据处理框架需要设置与多个组件交互，例如资源管理器、文件系统和用于分布式协调的服务。
- en: In this chapter, we discuss the different ways to deploy Flink clusters and
    how to configure them securely and make them highly available. We explain Flink
    setups for different Hadoop versions and filesystems and discuss the most important
    configuration parameters of Flink’s master and worker processes. After reading
    this chapter, you will know how to set up and configure a Flink cluster.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了部署 Flink 集群的不同方法以及如何安全配置和使其高可用。我们解释了不同 Hadoop 版本和文件系统的 Flink 设置，并讨论了
    Flink 主节点和工作节点进程的最重要配置参数。阅读完本章后，您将了解如何设置和配置 Flink 集群。
- en: Deployment Modes
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署模式
- en: 'Flink can be deployed in different environments, such as a local machine, a
    bare-metal cluster, a Hadoop YARN cluster, or a Kubernetes cluster. In [“Components
    of a Flink Setup”](ch03.html#chap-3-setup-components), we introduced the different
    components of a Flink setup: the JobManager, TaskManager, ResourceManager, and
    Dispatcher. In this section, we explain how to configure and start Flink in different
    environments—including standalone clusters, Docker, Apache Hadoop YARN, and Kubernetes—and
    how Flink’s components are assembled in each setup.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 可以在不同的环境中部署，例如本地机器、裸机集群、Hadoop YARN 集群或 Kubernetes 集群。在[“Flink 设置的组件”](ch03.html#chap-3-setup-components)中，我们介绍了
    Flink 设置的不同组件：JobManager、TaskManager、ResourceManager 和 Dispatcher。本节中，我们解释了如何在不同的环境中配置和启动
    Flink，包括独立集群、Docker、Apache Hadoop YARN 和 Kubernetes，以及在每种设置中如何组装 Flink 的组件。
- en: Standalone Cluster
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独立集群
- en: A standalone Flink cluster consists of at least one master process and at least
    one TaskManager process that run on one or more machines. All processes run as
    regular Java JVM processes. [Figure 9-1](#fig_standalone1) shows a standalone
    Flink setup.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 独立的 Flink 集群至少包括一个主进程和至少一个 TaskManager 进程，它们在一个或多个机器上运行。所有进程均作为常规 Java JVM 进程运行。[图 9-1](#fig_standalone1)
    展示了独立 Flink 设置。
- en: '![](assets/spaf_0901.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0901.png)'
- en: Figure 9-1\. Starting a standalone Flink cluster
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1\. 启动独立 Flink 集群
- en: The master process runs a Dispatcher and a ResourceManager in separate threads.
    Once they start running, the TaskManagers register themselves at the ResourceManager.
    [Figure 9-2](#fig_standalone2) shows how a job is submitted to a standalone cluster.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 主进程在单独的线程中运行调度器（Dispatcher）和资源管理器（ResourceManager）。一旦它们开始运行，任务管理器（TaskManagers）会在资源管理器注册自己。[图 9-2](#fig_standalone2)
    展示了如何将作业提交到独立集群。
- en: '![](assets/spaf_0902.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0902.png)'
- en: Figure 9-2\. Submitting an application to a Flink standalone cluster
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-2\. 将应用程序提交到 Flink 独立集群
- en: A client submits a job to the Dispatcher, which internally starts a JobManager
    thread and provides the JobGraph for execution. The JobManager requests the necessary
    processing slots from the ResourceManager and deploys the job for execution once
    the requested slots have been received.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端将作业提交给调度器（Dispatcher），调度器内部启动一个 JobManager 线程并提供执行的 JobGraph。JobManager 从资源管理器请求必要的处理槽，并在收到请求的槽后部署作业以执行。
- en: In a standalone deployment, the master and workers are not automatically restarted
    in the case of a failure. A job can recover from a worker failure if a sufficient
    number of processing slots is available. This can be ensured by running one or
    more standby workers. Job recovery from a master failure requires a highly available
    setup as discussed later in this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在独立部署中，主节点和工作节点在发生故障时不会自动重新启动。如果有足够数量的处理槽可用，作业可以从工作节点故障中恢复。这可以通过运行一个或多个备用工作节点来保证。从主节点故障中恢复作业需要一个高可用设置，后面在本章中讨论。
- en: 'In order to set up a standalone Flink cluster, download a binary distribution
    from the Apache Flink website and extract the `tar` archive with the command:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置独立的 Flink 集群，请从 Apache Flink 网站下载二进制分发版，并使用以下命令解压 `tar` 存档：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The extracted directory includes a *./bin* folder with bash scripts^([1](ch09.html#idm45498993250232))
    to start and stop Flink processes. The *./bin/start-cluster.sh* script starts
    a master process on the local machine and one or more TaskManagers on the local
    or remote machines.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的目录包括一个 *./bin* 文件夹，其中包含用于启动和停止 Flink 进程的 bash 脚本^([1](ch09.html#idm45498993250232))。*./bin/start-cluster.sh*
    脚本在本地主机上启动一个主进程，以及一个或多个本地或远程机器上的 TaskManager 进程。
- en: Flink is preconfigured to run a local setup and start a single master and a
    single TaskManager on the local machine. The start scripts must be able to start
    a Java process. If the `java` binary is not on the `PATH`, the base folder of
    a Java installation can be specified by exporting the `JAVA_HOME` environment
    variable or setting the `env.java.home` parameter in *./conf/flink-conf.yaml*.
    A local Flink cluster is started by calling  `./bin/start-cluster.sh`. You can
    visit Flink’s Web UI at *http://localhost:8081* and check the number of connected
    TaskManagers and available slots.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 预配置为在本地设置上运行，并在本地机器上启动单个主进程和单个 TaskManager。启动脚本必须能够启动 Java 进程。如果 `java`
    二进制文件不在 `PATH` 上，可以通过导出 `JAVA_HOME` 环境变量或在 *./conf/flink-conf.yaml* 中设置 `env.java.home`
    参数来指定 Java 安装的基本文件夹。通过调用 `./bin/start-cluster.sh` 可以启动本地 Flink 集群。可以访问 *http://localhost:8081*
    查看 Flink 的 Web UI，并检查连接的 TaskManagers 数量和可用的插槽数量。
- en: In order to start a distributed Flink cluster that runs on multiple machines,
    you need to adjust the default configuration and complete a few more steps.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动运行在多台机器上的分布式 Flink 集群，需要调整默认配置并完成几个额外的步骤。
- en: The hostnames (or IP addresses) of all machines that should run TaskManagers
    need to be listed in the *./conf/slaves* file.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有应该运行 TaskManager 的机器的主机名（或IP地址）需要列在 *./conf/slaves* 文件中。
- en: The *start-cluster.sh* script requires a passwordless SSH configuration on all
    machines to be able to start the TaskManager processes.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*start-cluster.sh* 脚本要求所有机器上都配置了无密码 SSH 配置，以便能够启动 TaskManager 进程。'
- en: The Flink distribution folder must be located on all machines at the same path.
    A common approach is to mount a network-shared directory with the Flink distribution
    on each machine.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flink 分发文件夹必须在所有机器上位于相同的路径上。常见的方法是在每台机器上挂载一个网络共享的目录，其中包含 Flink 分发。
- en: The hostname (or IP address) of the machine that runs the master process needs
    to be configured in the *./conf/flink-conf.yaml* file with the config key `jobmanager.rpc.address`.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行主进程的机器的主机名（或IP地址）需要在 *./conf/flink-conf.yaml* 文件中配置，使用配置键 `jobmanager.rpc.address`。
- en: Once everything has been set up, you can start the Flink cluster by calling
    `./bin/start-cluster.sh`. The script will start a local JobManager and start one
    TaskManager for each entry in the *slaves* file. You can check if the master process
    was started and all TaskManager were successfully registered by accessing the
    Web UI on the machine that runs the master process. A local or distributed standalone
    cluster is stopped by calling `./bin/stop-cluster.sh`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一切设置完成后，可以通过调用 `./bin/start-cluster.sh` 启动 Flink 集群。该脚本将在本地启动一个 JobManager，并为
    *slaves* 文件中的每个条目启动一个 TaskManager。可以通过访问运行主进程的机器上的 Web UI 来检查主进程是否已启动，并且所有 TaskManager
    是否成功注册。本地或分布式独立集群可以通过调用 `./bin/stop-cluster.sh` 停止。
- en: Docker
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker
- en: Docker is a popular platform used to package and run applications in containers.
    Docker containers are run by the operating system kernel of the host system and
    are therefore more lightweight than virtual machines. Moreover, they are isolated
    and communicate only through well-defined channels. A container is started from
    an image that defines the software in the container.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 是一个流行的平台，用于将应用程序打包和运行在容器中。Docker 容器由主机系统的操作系统内核运行，因此比虚拟机更轻量级。此外，它们是隔离的，并且仅通过定义良好的通道进行通信。容器是从定义容器中软件的镜像启动的。
- en: Members of the Flink community configure and build Docker images for Apache
    Flink and upload them to Docker Hub, a public repository for Docker images.^([2](ch09.html#idm45498993223160))
    The repository hosts Docker images for the most recent Flink versions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 社区成员配置并构建 Apache Flink 的 Docker 镜像，并上传到 Docker Hub，这是一个公共 Docker 镜像仓库。^([2](ch09.html#idm45498993223160))
    该仓库存储了最新版本的 Flink 镜像。
- en: Running Flink in Docker is an easy way to set up a Flink cluster on your local
    machine. For a local Docker setup you have to start two types of containers, a
    master container that runs the Dispatcher and ResourceManager, and one or more
    worker containers that run the TaskManagers. The containers work together like
    a standalone deployment (see [“Standalone Cluster”](#standalone-cluster)). After
    starting, a TaskManager registers itself at the ResourceManager. When a job is
    submitted to the Dispatcher, it spawns a JobManager thread, which requests processing
    slots from the ResourceManager. The ResourceManager assigns TaskManagers to the
    JobManager, which deploys the job once all required resources are available.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在Docker中运行Flink是在本地机器上设置Flink集群的简便方法。对于本地Docker设置，您需要启动两种类型的容器：运行调度器和资源管理器的主容器，以及运行任务管理器的一个或多个工作容器。这些容器共同像独立部署一样工作（见[“独立集群”](#standalone-cluster)）。启动后，任务管理器会在资源管理器上注册自己。当将作业提交给调度器时，它会生成一个作业管理器线程，该线程从资源管理器请求处理插槽。资源管理器将任务管理器分配给作业管理器，在所有必需的资源可用后，作业管理器部署作业。
- en: Master and worker containers are started from the same Docker image with different
    parameters as shown in [Example 9-1](#code_docker1).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 主容器和工作容器使用相同的Docker镜像，但使用不同的参数启动，如[示例 9-1](#code_docker1)所示。
- en: Example 9-1\. Starting a master and a worker container in Docker
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-1\. 在Docker中启动主容器和工作容器
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Docker will download the requested image and its dependencies from Docker Hub
    and start the containers running Flink. The Docker internal hostname of the JobManager
    is passed to the containers via the `JOB_MANAGER_RPC_ADDRESS` variable, which
    is used in the entry point of the container to adjust Flink’s configuration.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Docker将从Docker Hub下载请求的镜像及其依赖项，并启动运行Flink的容器。作业管理器的Docker内部主机名通过`JOB_MANAGER_RPC_ADDRESS`变量传递给容器，在容器的入口点中用于调整Flink的配置。
- en: The `-p 8081:8081` parameter of the first command maps port 8081 of the master
    container to port 8081 of the host machine to make the Web UI accessible from
    the host. You can access the Web UI by opening *http://localhost:8081* in your
    browser. The Web UI can be used to upload application JAR files and run the application.
    The port also exposes Flink’s REST API. Hence, you can also submit applications
    using Flink’s CLI client at *./bin/flink*, manage running applications, or request
    information about the cluster or running applications.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令的`-p 8081:8081`参数将主容器的8081端口映射到主机的8081端口，以便从主机访问Web UI。您可以在浏览器中打开*http://localhost:8081*来访问Web
    UI。Web UI可用于上传应用程序JAR文件并运行应用程序。该端口还公开了Flink的REST API。因此，您还可以使用Flink的CLI客户端在*./bin/flink*处提交应用程序，管理正在运行的应用程序，或请求有关集群或正在运行的应用程序的信息。
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that it is currently not possible to pass a custom configuration into the
    Flink Docker images. You need to build your own Docker image if you want to adjust
    some of the parameters. The build scripts of the available Docker Flink images
    are a good starting point for customized images.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，目前无法向Flink Docker镜像传递自定义配置。如果要调整某些参数，您需要构建自己的Docker镜像。可用的Docker Flink镜像的构建脚本是自定义镜像的良好起点。
- en: Instead of manually starting two (or more) containers, you can also create a
    Docker Compose configuration script, which automatically starts and configures
    a Flink cluster running in Docker containers and possibly other services such
    as ZooKeeper and Kafka. We will not go into the details of this mode, but among
    other things, a Docker Compose configuration needs to specify the network configuration
    so that Flink processes that run in isolated containers can communicate with each
    other. We refer you to Apache Flink’s documentation for details.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以创建一个Docker Compose配置脚本，而不是手动启动两个（或更多）容器，该脚本会自动启动和配置运行在Docker容器中的Flink集群，可能还包括其他服务，如ZooKeeper和Kafka。我们不会详细介绍这种模式的细节，但是Docker
    Compose配置需要指定网络配置，以便隔离容器中运行的Flink进程可以相互通信。有关详细信息，请参阅Apache Flink的文档。
- en: Apache Hadoop YARN
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Hadoop YARN
- en: YARN is the resource manager component of Apache Hadoop. It manages compute
    resources of a cluster environment—CPU and memory of the cluster’s machines—and
    provides them to applications that request resources. YARN grants resources as
    containers^([3](ch09.html#idm45498993140616)) that are distributed in the cluster
    and in which applications run their processes. Due to its origin in the Hadoop
    ecosystem, YARN is typically used by data processing frameworks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 是 Apache Hadoop 的资源管理组件。它管理集群环境的计算资源——集群机器的 CPU 和内存，并向请求资源的应用程序提供这些资源。YARN
    分配的资源为分布在集群中的容器^([3](ch09.html#idm45498993140616))，应用程序在这些容器中运行它们的进程。由于其起源于 Hadoop
    生态系统，YARN 通常由数据处理框架使用。
- en: 'Flink can run on YARN in two modes: the job mode and the session mode. In job
    mode, a Flink cluster is started to run a single job. Once the job terminates,
    the Flink cluster is stopped and all resources are returned. [Figure 9-3](#fig_yarn_jobmode)
    shows how a Flink job is submitted to a YARN cluster.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 可以在 YARN 上以两种模式运行：作业模式和会话模式。在作业模式下，启动一个 Flink 集群以运行单个作业。作业终止后，停止 Flink
    集群并归还所有资源。[图 9-3](#fig_yarn_jobmode) 显示了如何向 YARN 集群提交 Flink 作业。
- en: '![](assets/spaf_0903.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0903.png)'
- en: Figure 9-3\. Starting a Flink cluster on YARN in job mode
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-3\. 在 YARN 上以作业模式启动 Flink 集群
- en: When the client submits a job for execution, it connects to the YARN ResourceManager
    to start a new YARN application master process that consists of a JobManager thread
    and a ResourceManager. The JobManager requests the required slots from the ResourceManager
    to run the Flink job. Subsequently, Flink’s ResourceManager requests containers
    from YARN’s ResourceManager and starts TaskManager processes. Once started, the
    TaskManagers register their slots at Flink’s ResourceManager, which provides them
    to the JobManager. Finally, the JobManager submits the job’s tasks to the TaskManagers
    for execution.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当客户端提交作业以执行时，它会连接到 YARN 的资源管理器，启动一个新的 YARN 应用主节点进程，包括一个 JobManager 线程和一个资源管理器。JobManager
    请求所需的插槽以运行 Flink 作业。随后，Flink 的资源管理器从 YARN 的资源管理器请求容器并启动 TaskManager 进程。一旦启动，TaskManagers
    在 Flink 的资源管理器上注册其插槽，后者提供给 JobManager。最后，JobManager 向 TaskManagers 提交作业的任务以执行。
- en: The session mode starts a long-running Flink cluster that can run multiple jobs
    and needs to be manually stopped. If started in session mode, Flink connects to
    YARN’s ResourceManager to start an application master that runs a Dispatcher thread
    and a Flink ResourceManager thread. [Figure 9-4](#fig_yarn_sessionmode) shows
    an idle Flink YARN session setup.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 会话模式启动一个长时间运行的 Flink 集群，可以运行多个作业，需要手动停止。如果在会话模式下启动，Flink 会连接到 YARN 的资源管理器，启动一个应用主节点，其中包括一个分派器线程和一个
    Flink 资源管理器线程。[图 9-4](#fig_yarn_sessionmode) 显示了一个空闲的 Flink YARN 会话设置。
- en: '![](assets/spaf_0904.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0904.png)'
- en: Figure 9-4\. Starting a Flink cluster on YARN in session mode
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-4\. 在 YARN 上以会话模式启动 Flink 集群
- en: When a job is submitted for execution, the Dispatcher starts a JobManager thread,
    which requests slots from Flink’s ResourceManager. If not enough slots are available,
    Flink’s ResourceManager requests additional containers from the YARN ResourceManager
    to start TaskManager processes, which register themselves at the Flink ResourceManager.
    Once enough slots are available, Flink’s ResourceManager assigns them to the JobManager
    and the job execution starts. [Figure 9-5](#fig_yarn_sessionjob) shows how a job
    is executed in Flink’s YARN session mode.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当提交作业以执行时，分派器启动一个 JobManager 线程，后者从 Flink 的资源管理器请求插槽。如果没有足够的插槽可用，Flink 的资源管理器会请求从
    YARN 的资源管理器获取额外的容器，以启动 TaskManager 进程，这些进程在 Flink 的资源管理器上注册。一旦有足够的插槽可用，Flink 的资源管理器分配它们给
    JobManager，并开始作业执行。[图 9-5](#fig_yarn_sessionjob) 显示了作业在 Flink 的 YARN 会话模式下的执行方式。
- en: '![](assets/spaf_0905.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0905.png)'
- en: Figure 9-5\. Submitting a job to a Flink YARN session cluster
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-5\. 向 Flink YARN 会话集群提交作业
- en: For both setups—job and session mode—failed TaskManagers will be automatically
    restarted by Flink’s ResourceManager. There are a few parameters in the *./conf/flink-conf.yaml*
    configuration file you can use to control Flink’s recovery behavior on YARN. For
    example, you can configure the maximum number of failed containers until an application
    is terminated. In order to recover from master failures, a highly available setup
    needs to be configured as described in a later section.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于作业模式和会话模式下的两种设置，Flink 的 ResourceManager 将自动重新启动失败的 TaskManagers。在 *./conf/flink-conf.yaml*
    配置文件中，有几个参数可以用来控制 Flink 在 YARN 上的恢复行为。例如，您可以配置最大的失败容器数量，直到应用程序被终止。为了从主节点故障中恢复，需要配置一个高可用的设置，如后面章节所述。
- en: Regardless of whether you run Flink in job or session mode on YARN, it needs
    to have access to Hadoop dependencies in the correct version and the path to the
    Hadoop configuration. [“Integration with Hadoop Components”](#integration-with-hadoop)
    describes the required configuration in detail.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您是在 YARN 上的作业模式还是会话模式下运行 Flink，它都需要访问正确版本的 Hadoop 依赖项和 Hadoop 配置路径。["与 Hadoop
    组件集成"](#integration-with-hadoop) 详细描述了所需的配置。
- en: 'Given a working and well-configured YARN and HDFS setup, a Flink job can be
    submitted to be executed on YARN using Flink’s command-line client with the following
    command:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个工作正常且配置良好的 YARN 和 HDFS 设置的情况下，可以使用以下命令将 Flink 作业提交到 YARN 上执行，使用 Flink 的命令行客户端：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The parameter `-m` defines the host to which the job is submitted. If set to
    the keyword `yarn-cluster`, the client submits the job to the YARN cluster as
    identified by the Hadoop configuration. Flink’s CLI client supports many more
    parameters, such as the ability to control the memory of TaskManager containers.
    Refer to the documentation for a reference of available parameters. The Web UI
    of the started Flink cluster is served by the master process running on some node
    in the YARN cluster. You can access it via YARN’s Web UI, which provides a link
    on the Application Overview page under “Tracking URL: ApplicationMaster.”'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '参数 `-m` 定义了要提交作业的主机。如果设置为关键字 `yarn-cluster`，则客户端将作业提交到由 Hadoop 配置标识的 YARN 集群。Flink
    的 CLI 客户端支持许多其他参数，例如控制 TaskManager 容器的内存。请参考文档以获取可用参数的参考。启动的 Flink 集群的 Web UI
    由运行在 YARN 集群中某个节点上的主进程提供。您可以通过 YARN 的 Web UI 访问它，在应用概述页面的“跟踪 URL: ApplicationMaster”下提供了一个链接。'
- en: A Flink YARN session is started with the `./bin/yarn-session.sh` script, which
    also uses various parameters to control the size of containers, the name of the
    YARN application, or provide dynamic properties. By default, the script prints
    the connection information of the session cluster and does not return. The session
    is stopped and all resources are freed when the script is terminated. It is also
    possible to start a YARN session in detached mode using the `-d` flag. A detached
    Flink session can be terminated using YARN’s application utilities.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `./bin/yarn-session.sh` 脚本可以启动一个 Flink YARN 会话，该脚本还使用各种参数来控制容器的大小、YARN 应用程序的名称或提供动态属性。默认情况下，该脚本会打印会话集群的连接信息并且不会返回。当脚本终止时，会话会停止并释放所有资源。还可以使用
    `-d` 标志在后台模式下启动一个 YARN 会话。可以使用 YARN 的应用程序工具终止一个分离的 Flink 会话。
- en: Once a Flink YARN session is running, you can submit jobs to the session with
    the command `./bin/flink run ./path/to/job.jar`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Flink YARN 会话运行起来，您可以使用命令 `./bin/flink run ./path/to/job.jar` 向会话提交作业。
- en: Note
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that you do not need to provide connection information, as Flink memorized
    the connection details of the Flink session running on YARN. Similar to job mode,
    Flink’s Web UI is linked from the Application Overview of YARN’s Web UI.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您无需提供连接信息，因为 Flink 已经记住了在 YARN 上运行的 Flink 会话的连接详细信息。与作业模式类似，Flink 的 Web
    UI 是从 YARN 的 Web UI 的应用概述中链接过来的。
- en: Kubernetes
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Kubernetes is an open source platform that enables users to deploy and scale
    containerized applications in a distributed environment. Given a Kubernetes cluster
    and an application that is packaged into a container image, you can create a deployment
    of the application that tells Kubernetes how many instances of the application
    to start. Kubernetes will run the requested number of containers anywhere on its
    resources and restart them in the case of a failure. Kubernetes can also take
    care of opening network ports for internal and external communication and can
    provide services for process discovery and load balancing. Kubernetes runs on-premise,
    in cloud environments, or on hybrid infrastructure.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一个开源平台，允许用户在分布式环境中部署和扩展容器化应用程序。给定一个 Kubernetes 集群和打包成容器镜像的应用程序，您可以创建应用程序的部署，告诉
    Kubernetes 启动多少个应用程序实例。Kubernetes 将在其资源的任何地方运行请求的容器，并在发生故障时重新启动它们。Kubernetes 还可以负责打开用于内部和外部通信的网络端口，并提供进程发现和负载均衡的服务。Kubernetes
    可以在本地、云环境或混合基础设施上运行。
- en: 'Deploying data processing frameworks and applications on Kubernetes has become
    very popular. Apache Flink can be deployed on Kubernetes as well. Before diving
    into the details of how to set up Flink on Kubernetes, we need to briefly explain
    a few Kubernetes terms:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 部署数据处理框架和应用程序在 Kubernetes 上变得非常流行。Apache Flink 也可以在 Kubernetes 上部署。在深入探讨如何在
    Kubernetes 上设置 Flink 之前，我们需要简要解释一些 Kubernetes 术语：
- en: A *pod* is a container that is started and managed by Kubernetes.^([4](ch09.html#idm45498993010952))
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *pod* 是由 Kubernetes 启动和管理的容器。^([4](ch09.html#idm45498993010952))
- en: A *deployment* defines a specific number of pods, or containers, to run. Kubernetes
    ensures the requested number of pods is continuously running, and automatically
    restarts failed pods. Deployments can be scaled up or down.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *部署* 定义了运行的特定数量的 pod 或容器。Kubernetes 确保请求的 pod 数量持续运行，并自动重新启动失败的 pod。部署可以进行水平扩展或缩减。
- en: Kubernetes may run a pod anywhere on its cluster. When a pod is restarted after
    a failure or when deployments are scaled up or down, the IP address can change.
    This is obviously a problem if pods need to communicate with each other. Kubernetes
    provides services to overcome the issue. A *service* defines a policy for how
    a certain group of pods can be accessed. It takes care of updating the routing
    when a pod is started on a different node in the cluster.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 可以在集群的任何地方运行 pod。当 pod 因故障重新启动或部署进行水平扩展或缩减时，IP 地址可能会变化。如果 pod 需要彼此通信，则这显然是一个问题。Kubernetes
    提供了服务来解决这个问题。一个 *服务* 定义了如何访问某一组 pod 的策略。它负责在集群中的不同节点上启动 pod 时更新路由。
- en: Running Kubernetes on a Local Machine
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地机器上运行 Kubernetes
- en: Kubernetes is designed for cluster operations. However, the Kubernetes project
    provides Minikube, an environment to run a single-node Kubernetes cluster locally
    on a single machine for testing or daily development. We recommend setting up
    Minikube if you would like to try to run Flink on Kubernetes and do not have a
    Kubernetes cluster at hand.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 设计用于集群操作。然而，Kubernetes 项目提供了 Minikube，一个在单台机器上本地运行单节点 Kubernetes
    集群的环境，用于测试或日常开发。如果您想尝试在 Kubernetes 上运行 Flink 但又没有 Kubernetes 集群，我们建议设置 Minikube。
- en: 'In order to successfully run applications on a Flink cluster that is deployed
    on Minikube, you need to run the following command before deploying Flink: `minikube
    ssh ''sudo ip link set docker0 promisc on''`.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功在部署在 Minikube 上的 Flink 集群上运行应用程序，您需要在部署 Flink 之前运行以下命令：`minikube ssh 'sudo
    ip link set docker0 promisc on'`。
- en: A Flink setup for Kubernetes is defined with two deployments—one for the pod
    running the master process and the other for the worker process pods. There is
    also a service that exposes the ports of the master pod to the worker pods. The
    two types of pods—master and worker—behave just like the processes of a standalone
    or Docker deployment we described before. The master deployment configuration
    is shown in [Example 9-2](#code_k8s_masterdep).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 在 Kubernetes 上的设置定义了两个部署——一个用于运行主进程的 pod，另一个用于工作进程 pod。还有一个服务将主 pod 的端口暴露给工作
    pod。主和工作两种类型的 pod 的行为与之前描述的独立或 Docker 部署的进程相同。主部署配置如 [示例 9-2](#code_k8s_masterdep)
    所示。
- en: Example 9-2\. A Kubernetes deployment for a Flink master
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-2\. 用于 Flink 主节点的 Kubernetes 部署
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This deployment specifies that a single master container should be run (`replicas:
    1`). The master container is started from the Flink 1.7 Docker image (`image:
    flink:1.7`) with an argument that starts the master process (`args: - jobmanager`).
    Moreover, the deployment configures which ports of the container to open for RPC
    communication, the blob manager (to exchange large files), the queryable state
    server, and the Web UI and REST interface. [Example 9-3](#code_k8s_workerdep)
    shows the deployment for worker pods.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '此部署指定应运行一个单一的主容器（`replicas: 1`）。主容器从Flink 1.7 Docker镜像启动（`image: flink:1.7`），并使用一个参数启动主进程（`args:
    - jobmanager`）。此外，部署配置了要为RPC通信、blob管理器（用于交换大文件）、可查询状态服务器以及Web UI和REST接口打开容器的哪些端口。[示例 9-3](#code_k8s_workerdep)展示了工作Pod的部署。'
- en: Example 9-3\. A Kubernetes deployment for two Flink workers
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-3\. 两个Flink工作节点的Kubernetes部署
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The worker deployment looks almost identical to the master deployment with
    a few differences. First of all, the worker deployment specifies two replicas,
    which means that two worker containers are started. The worker containers are
    based on the same Flink Docker image but started with a different argument (`args:
    -taskmanager`). Moreover, the deployment also opens a few ports and passes the
    service name of the Flink master deployment so that the workers can access the
    master. The service definition that exposes the master process and makes it accessible
    to the worker containers is shown in [Example 9-4](#code_k8s_masterservice).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '工作部署与主节点部署几乎相同，但有几点不同之处。首先，工作部署指定了两个副本，即启动了两个工作容器。工作容器基于相同的Flink Docker镜像启动，但使用不同的参数（`args:
    -taskmanager`）。此外，部署还打开了几个端口，并传递了Flink主节点部署的服务名称，以便工作节点可以访问主节点。展示了公开主进程并使其对工作容器可访问的服务定义在[示例 9-4](#code_k8s_masterservice)中。'
- en: Example 9-4\. A Kubernetes service for the Flink master
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-4\. 用于Flink主节点的Kubernetes服务
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can create a Flink deployment for Kubernetes by storing each definition
    in a separate file, such as *master-deployment.yaml*, *worker-deployment.yaml*,
    or *master-service.yaml*. The files are also provided in our repository. Once
    you have the definition files, you can register them to Kubernetes using the `kubectl`
    command:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将每个定义存储在单独的文件中（例如*master-deployment.yaml*、*worker-deployment.yaml*或*master-service.yaml*）来为Kubernetes创建一个Flink部署。这些文件也提供在我们的代码仓库中。一旦有了定义文件，您可以使用`kubectl`命令将它们注册到Kubernetes：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When running these commands, Kubernetes starts to deploy the requested containers.
    You can show the status of all deployments by running the following command:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这些命令时，Kubernetes开始部署请求的容器。您可以通过运行以下命令显示所有部署的状态：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When you create the deployments for the first time, it will take a while until
    the Flink container image is downloaded. Once all pods are up, you will have a
    Flink cluster running on Kubernetes. However, with the given configuration, Kubernetes
    does not export any port to external environments. Hence, you cannot access the
    master container to submit an application or access the Web UI. You first need
    to tell Kubernetes to create a port forwarding from the master container to your
    local machine. This is done by running the following command:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当您首次创建部署时，将需要一些时间来下载Flink容器镜像。一旦所有Pod都启动，您将在Kubernetes上运行一个Flink集群。但是，根据给定的配置，Kubernetes不会将任何端口导出到外部环境。因此，您无法访问主容器来提交应用程序或访问Web
    UI。您首先需要告诉Kubernetes从主容器到本地机器创建端口转发。通过运行以下命令来完成这一操作：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When port forwarding is running, you can access the Web UI at *http://localhost:8081*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当端口转发运行时，您可以在*http://localhost:8081*访问Web UI。
- en: Now you can upload and submit jobs to the Flink cluster running on Kubernetes.
    Moreover, you can submit applications using the Flink CLI client (*./bin/flink*)
    and access the REST interface to request information about the Flink cluster or
    manage running applications.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以上传并提交作业到运行在Kubernetes上的Flink集群。此外，您可以使用Flink CLI客户端（*./bin/flink*）提交应用程序，并访问REST接口请求Flink集群的信息或管理运行中的应用程序。
- en: When a worker pod fails, Kubernetes will automatically restart the failed pod
    and the application will be recovered (given that checkpointing was activated
    and properly configured). In order to recover from a master pod failure, you need
    to configure a highly available setup.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当工作Pod失败时，Kubernetes将自动重新启动失败的Pod，并恢复应用程序（假设启用并正确配置了检查点）。为了从主Pod故障中恢复，您需要配置一个高可用设置。
- en: 'You can shut down a Flink cluster running on Kubernetes by running the following
    commands:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行以下命令关闭运行在 Kubernetes 上的 Flink 集群：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It is not possible to customize the configuration of the Flink deployment with
    the Flink Docker images we used in this section. You would need to build custom
    Docker images with an adjusted configuration. The build script for the provided
    image is a good starting point for a custom image.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中使用的 Flink Docker 映像不支持自定义 Flink 部署的配置。您需要使用调整过的配置构建自定义 Docker 映像。提供的映像构建脚本是开始自定义映像的良好起点。
- en: Highly Available Setups
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用设置
- en: Most streaming applications are ideally executed continuously with as little
    downtime as possible. Therefore, many applications must be able to automatically
    recover from failure of any process involved in the execution. While worker failures
    are handled by the ResourceManager, failures of the JobManager component require
    the configuration of a highly available (HA) setup.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数流处理应用程序理想情况下应持续执行，尽可能少地停机。因此，许多应用程序必须能够自动从执行过程中的任何故障中恢复。虽然 ResourceManager
    处理了工作节点的故障，但 JobManager 组件的故障需要配置高可用性（HA）设置。
- en: Flink’s JobManager holds metadata about an application and its execution, such
    as the application JAR file, the JobGraph, and pointers to completed checkpoints.
    This information needs to be recovered in case of a master failure. Flink’s HA
    mode relies on Apache ZooKeeper, a service for distributed coordination and consistent
    storage, and a persistent remote storage, such as HDFS, NFS, or S3\. The JobManager
    stores all relevant data in the persistent storage and writes a pointer to the
    information—the storage path—to ZooKeeper. In case of a failure, a new JobManager
    looks up the pointer from ZooKeeper and loads the metadata from the persistent
    storage. We presented the mode of operation and internals of Flink’s HA setup
    in more detail in [“Highly Available Setup”](ch03.html#chap-3-highly-available).
    In this section, we will configure this mode for different deployment options.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 的 JobManager 存储有关应用程序及其执行的元数据，例如应用程序 JAR 文件、JobGraph 和已完成检查点的指针。在主节点故障时，需要恢复这些信息。Flink
    的 HA 模式依赖于 Apache ZooKeeper，这是一个用于分布式协调和一致性存储的服务，并且依赖于持久的远程存储，例如 HDFS、NFS 或 S3\.
    JobManager 将所有相关数据存储在持久存储中，并向 ZooKeeper 写入信息的指针 — 存储路径。在发生故障时，新的 JobManager 会从
    ZooKeeper 查找指针，并从持久存储加载元数据。我们在 [“高可用设置”](ch03.html#chap-3-highly-available) 中更详细地介绍了
    Flink 的 HA 设置的操作模式和内部机制。在本节中，我们将为不同的部署选项配置此模式。
- en: A Flink HA setup requires a running Apache ZooKeeper cluster and a persistent
    remote storage, such as HDFS, NFS, or S3\. To help users start a ZooKeeper cluster
    quickly for testing purposes, Flink provides a helper script for bootstrapping.
    First, you need to configure the hosts and ports of all ZooKeeper processes involved
    in the cluster by adjusting the *./conf/zoo.cfg* file. Once that is done, you
    can call `./bin/start-zookeeper-quorum.sh` to start a ZooKeeper process on each
    configured node.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Flink 的 HA 设置需要运行的 Apache ZooKeeper 集群和持久的远程存储，例如 HDFS、NFS 或 S3\. 为了帮助用户快速启动一个用于测试目的的
    ZooKeeper 集群，Flink 提供了一个用于启动的辅助脚本。首先，您需要通过调整 *./conf/zoo.cfg* 文件来配置集群中所有 ZooKeeper
    进程的主机和端口。完成后，可以调用 `./bin/start-zookeeper-quorum.sh` 在每个配置的节点上启动一个 ZooKeeper 进程。
- en: Do Not Use start-zookeeper-quorum.sh for Production Setups
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 请勿将 start-zookeeper-quorum.sh 用于生产环境设置
- en: You should not use Flink’s ZooKeeper script for production environments but
    instead carefully configure and deploy a ZooKeeper cluster yourself.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，不应使用 Flink 的 ZooKeeper 脚本，而应仔细配置和部署一个 ZooKeeper 集群。
- en: The Flink HA mode is configured in the *./conf/flink-conf.yaml* file by setting
    the parameters as shown in [Example 9-5](#code_haflink_config).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*./conf/flink-conf.yaml* 文件中设置参数来配置 Flink 的 HA 模式，具体设置如 [示例 9-5](#code_haflink_config)
    所示。'
- en: Example 9-5\. Configuration of a HA Flink cluster
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-5\. HA Flink 集群的配置
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: HA Standalone Setup
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HA 独立设置
- en: A Flink standalone deployment does not rely on a resource provider, such as
    YARN or Kubernetes. All processes are manually started, and there is no component
    that monitors these processes and restarts them in case of a failure. Therefore,
    a standalone Flink cluster requires standby Dispatcher and TaskManager processes
    that can take over the work of failed processes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 独立部署不依赖于资源提供程序，如 YARN 或 Kubernetes。所有进程都需要手动启动，并且没有组件监视这些进程并在发生故障时重新启动它们。因此，独立的
    Flink 集群需要备用的 Dispatcher 和 TaskManager 进程，可以接管失败进程的工作。
- en: Besides starting standby TaskManagers, a standalone deployment does not need
    additional configuration to be able to recover from TaskManager failures. All
    started TaskManager processes register themselves at the active ResourceManager.
    An application can recover from a TaskManager failure as long as enough processing
    slots are on standby to compensate for the lost TaskManager. The ResourceManager
    hands out the previously idling processing slots and the application restarts.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 除了启动备用的 TaskManager 外，独立部署不需要额外的配置来从 TaskManager 失败中恢复。所有启动的 TaskManager 进程都会注册到活动
    ResourceManager。只要有足够的处理插槽处于待命状态以补偿丢失的 TaskManager，应用程序就可以从 TaskManager 失败中恢复。ResourceManager
    分配先前空闲的处理插槽，应用程序重新启动。
- en: If configured for HA, all Dispatchers of a standalone setup register at ZooKeeper.
    ZooKeeper elects a leader Dispatcher responsible for executing applications. When
    an application is submitted, the responsible Dispatcher starts a JobManager thread
    that stores its metadata in the configured persistent storage and a pointer in
    ZooKeeper as discussed before. If the master process that runs the active Dispatcher
    and JobManager fails, ZooKeeper elects a new Dispatcher as the leader. The leading
    Dispatcher recovers the failed application by starting a new JobManager thread
    that looks up the metadata pointer in ZooKeeper and loads the metadata from the
    persistent storage.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果配置了高可用性，独立部署的所有调度器都会注册到 ZooKeeper。ZooKeeper 选举一个负责执行应用程序的领导调度器。当提交应用程序时，负责的调度器会启动一个
    JobManager 线程，将其元数据存储在配置的持久存储中，并在 ZooKeeper 中存储指针，如前所述。如果运行活动调度器和 JobManager 的主进程失败，ZooKeeper
    将选举新的调度器作为领导者。领导调度器通过启动一个新的 JobManager 线程来恢复失败的应用程序，该线程从 ZooKeeper 中查找元数据指针并从持久存储加载元数据。
- en: 'In addition to the previously discussed configuration, an HA standalone deployment
    requires the following configuration changes. In *./conf/flink-conf.yaml* you
    need to set a cluster identifier for each running cluster. This is required if
    multiple Flink clusters rely on the same ZooKeeper instance for failure recovery:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前讨论的配置外，高可用性独立部署还需要进行以下配置更改。在 *./conf/flink-conf.yaml* 中，你需要为每个运行的集群设置一个集群标识符。如果多个
    Flink 集群依赖同一个 ZooKeeper 实例进行故障恢复，则这是必需的。
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you have a ZooKeeper quorum running and Flink properly configured, you can
    use the regular *./bin/start-cluster.sh* script to start a HA standalone cluster
    by adding additional hostnames and ports to the *./conf/masters* file.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经运行了一个 ZooKeeper 集群且正确配置了 Flink，你可以使用常规的 *./bin/start-cluster.sh* 脚本来启动一个高可用的独立集群，只需在
    *./conf/masters* 文件中添加额外的主机名和端口。
- en: HA YARN Setup
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高可用性 YARN 设置
- en: YARN is a cluster resource and container manager. By default, it automatically
    restarts failed master and TaskManager containers. Hence, you do not need to run
    standby processes in a YARN setup to achieve HA.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 是一个集群资源和容器管理器。默认情况下，它会自动重启失败的主节点和 TaskManager 容器。因此，在 YARN 设置中，你不需要运行备用进程来实现高可用。
- en: 'Flink’s master process is started as a YARN ApplicationMaster.^([5](ch09.html#idm45498992848888))
    YARN automatically restarts a failed ApplicationMaster but tracks and limits the
    number of restarts to prevent infinite recovery cycles. You need to configure
    the number of maximum ApplicationManager restarts in the YARN configuration file
    *yarn-site.xml* as shown:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 的主进程作为一个 YARN ApplicationMaster 启动。YARN 会自动重新启动失败的 ApplicationMaster，但会跟踪并限制重新启动的次数，以防止无限恢复循环。你需要在
    YARN 配置文件 *yarn-site.xml* 中配置最大 ApplicationManager 重启次数，如下所示：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Moreover, you need to adjust Flink’s configuration file *./conf/flink-conf.yaml*
    and configure the number of application restart attempts:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你需要调整 Flink 的配置文件 *./conf/flink-conf.yaml* 并配置应用程序重启尝试的次数：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: YARN only counts the number of restarts due to application failures—restarts
    due to preemption, hardware failures, or reboots are not taken into account for
    the number of application attempts. If you run Hadoop YARN version 2.6 or later,
    Flink automatically configures an attempt failure’s validity interval. This parameter
    specifies that an application is only completely canceled if it exceeds its restart
    attempts within the validity interval, meaning attempts that predate the interval
    are not taken into account. Flink configures the interval to the same value as
    the `akka.ask.timeout` parameter in *./conf/flink-conf.yaml* with a default value
    of 10 seconds.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 仅计算由于应用程序故障而导致的重启次数——不考虑由于抢占、硬件故障或重新启动而导致的重启。如果运行的是 Hadoop YARN 版本 2.6
    或更高版本，则 Flink 会自动配置尝试失败的有效间隔。此参数指定只有在有效间隔内超过重试尝试时，应用程序才完全取消，意味着不考虑间隔前的尝试。Flink
    会将该间隔配置为 *./conf/flink-conf.yaml* 中的 `akka.ask.timeout` 参数的相同值，默认为 10 秒。
- en: Given a running ZooKeeper cluster and properly configured YARN and Flink setups,
    you can start a Flink cluster in job mode or session mode as if HA were not enabled—by
    using `./bin/flink run -m yarn-cluster` and `./bin/yarn-session.sh`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 给定运行中的 ZooKeeper 集群和正确配置的 YARN 和 Flink 设置，您可以像没有启用 HA 一样启动作业模式或会话模式的 Flink 集群——通过
    `./bin/flink run -m yarn-cluster` 和 `./bin/yarn-session.sh`。
- en: Note
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that you must configure different cluster IDs for all Flink session clusters
    that connect to the same ZooKeeper cluster. When starting a Flink cluster in job
    mode, the cluster ID is automatically set to the ID of the started application
    and is therefore unique.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您必须为连接到同一 ZooKeeper 集群的所有 Flink 会话集群配置不同的集群 ID。在启动作业模式的 Flink 集群时，集群 ID
    自动设置为启动应用程序的 ID，因此是唯一的。
- en: HA Kubernetes Setup
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HA Kubernetes 设置
- en: When running Flink on Kubernetes with a master deployment and a worker deployment
    as described in [“Kubernetes”](#chap-9-kubernetes), Kubernetes will automatically
    restart failed containers to ensure the right number of pods is up and running.
    This is sufficient to recover from worker failures, which are handled by the ResourceManager.
    However, recovering from master failures requires additional configuration as
    discussed before.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用如 [“Kubernetes”](#chap-9-kubernetes) 中描述的主部署和工作部署在 Kubernetes 上运行 Flink 时，Kubernetes
    将自动重新启动失败的容器，以确保正确数量的 pod 正在运行。这足以从由 ResourceManager 处理的工作器故障中恢复。然而，从主节点故障中恢复需要额外的配置，如前所述。
- en: In order to enable Flink’s HA mode, you need to adjust Flink’s configuration
    and provide information such as the hostnames of the ZooKeeper quorum nodes, a
    path to a persistent storage, and a cluster ID for Flink. All of these parameters
    need to be added to Flink’s configuration file (*./conf/flink-conf.yaml*).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用 Flink 的 HA 模式，您需要调整 Flink 的配置并提供诸如 ZooKeeper quorum 节点的主机名、持久存储路径和 Flink
    的集群 ID 等信息。所有这些参数都需要添加到 Flink 的配置文件（*./conf/flink-conf.yaml*）中。
- en: Custom Configuration in Flink Images
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flink 镜像中的自定义配置
- en: Unfortunately, the Flink Docker image we used in the Docker and Kubernetes examples
    before does not support setting custom configuration parameters. Hence, the image
    cannot be used to set up a HA Flink cluster on Kubernetes. Instead, you need to
    build a custom image that either “hardcodes” the required parameters or is flexible
    enough to adjust the configuration dynamically through parameters or environment
    variables. The standard Flink Docker images are a good starting point to customize
    your own Flink images.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在之前 Docker 和 Kubernetes 示例中使用的 Flink Docker 镜像不支持设置自定义配置参数。因此，该镜像不能用于在
    Kubernetes 上设置 HA Flink 集群。相反，您需要构建一个自定义镜像，该镜像可以“硬编码”所需的参数，或者足够灵活，可以通过参数或环境变量动态调整配置。标准的
    Flink Docker 镜像是定制自己的 Flink 镜像的良好起点。
- en: Integration with Hadoop Components
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 Hadoop 组件集成
- en: Apache Flink can be easily integrated with Hadoop YARN and HDFS and other components
    of the Hadoop ecosystem, such as HBase. In all of these cases, Flink requires
    Hadoop dependencies on its classpath.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink 可以轻松集成到 Hadoop YARN 和 HDFS 以及 Hadoop 生态系统的其他组件，如 HBase。在所有这些情况下，Flink
    需要在其类路径上具有 Hadoop 依赖项。
- en: 'There are three ways to provide Flink with Hadoop dependencies:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 提供 Flink 与 Hadoop 依赖项的三种方法：
- en: Use a binary distribution of Flink that was built for a particular Hadoop version.
    Flink provides builds for the most commonly used vanilla Hadoop versions.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用为特定 Hadoop 版本构建的 Flink 二进制分发版本。Flink 提供了最常用的原生 Hadoop 版本的构建。
- en: Build Flink for a specific Hadoop version. This is useful if none of Flink’s
    binary distributions works with the Hadoop version deployed in your environment;
    for example, if you run a patched Hadoop version or a Hadoop version of a distributor,
    such as Cloudera, Hortonworks, or MapR.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为特定的 Hadoop 版本构建 Flink。如果 Flink 的二进制发行版都不适用于您环境中部署的 Hadoop 版本，例如运行补丁版的 Hadoop
    版本或发行商提供的 Hadoop 版本（如 Cloudera、Hortonworks 或 MapR），则此方法很有用。
- en: 'In order to build Flink for a specific Hadoop version, you need Flink’s source
    code, which can be obtained by downloading the source distribution from the website
    or cloning a stable release branch from the project’s Git repository, a Java JDK
    of at least version 8, and Apache Maven 3.2\. Enter the base folder of Flink’s
    source code and run one of the commands in the following:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要为特定的 Hadoop 版本构建 Flink，您需要 Flink 的源代码，可以从网站下载源码分发版或从项目的 Git 存储库克隆稳定的发行版分支，Java
    JDK 至少为版本 8，以及 Apache Maven 3.2。进入 Flink 源代码的基础文件夹，并在以下命令中运行一个：
- en: '[PRE14]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The completed build is located in the *./build-target* folder.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完成的构建位于 *./build-target* 文件夹中。
- en: 'Use the Hadoop-free distribution of Flink and manually configure the classpath
    for Hadoop’s dependencies. This approach is useful if none of the provided builds
    work for your setup. The classpath of the Hadoop dependencies must be declared
    in the `HADOOP_CLASSPATH` environment variable. If the variable is not configured,
    you can automatically set it with the following command if the `hadoop` command
    is accessible: `` export HADOOP_CLASSPATH=`hadoop classpath` ``.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用不带 Hadoop 的 Flink 分发版本，并手动配置 Hadoop 依赖的类路径。如果提供的构建都不适合您的设置，这种方法就很有用。必须在 `HADOOP_CLASSPATH`
    环境变量中声明 Hadoop 依赖的类路径。如果未配置此变量，可以通过以下命令自动设置它（如果 `hadoop` 命令可访问）：`` export HADOOP_CLASSPATH=`hadoop
    classpath` ``。
- en: The `classpath` option of the `hadoop` command prints its configured classpath.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`hadoop` 命令的 `classpath` 选项会打印其配置的类路径。'
- en: In addition to configuring the Hadoop dependencies, you need to provide the
    location of Hadoop’s configuration directory. This should be done by exporting
    either the `HADOOP_CONF_DIR` (preferred) or `HADOOP_CONF_PATH` environment variable.
    Once Flink knows about Hadoop’s configuration, it can connect to YARN’s ResourceManager
    and HDFS.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 除了配置 Hadoop 依赖项之外，还需要提供 Hadoop 配置目录的位置。应通过导出 `HADOOP_CONF_DIR`（优选）或 `HADOOP_CONF_PATH`
    环境变量来完成。一旦 Flink 知道了 Hadoop 的配置，它就可以连接到 YARN 的 ResourceManager 和 HDFS。
- en: Filesystem Configuration
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件系统配置
- en: Apache Flink uses filesystems for various tasks. Applications can read their
    input from and write their results to files (see [“Filesystem Source Connector”](ch08.html#chap-8-file-system-source)),
    application checkpoints and metadata are persisted in remote filesystems for recovery
    (see [“Checkpoints, Savepoints, and State Recovery”](ch03.html#chap-3-checkpoints)),
    and some internal components leverage filesystems to distribute data to tasks,
    such as application JAR files.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink 在各种任务中使用文件系统。应用程序可以从文件读取输入并将其结果写入文件（参见[“文件系统源连接器”](ch08.html#chap-8-file-system-source)），应用程序的检查点和元数据会持久化到远程文件系统以进行恢复（参见[“检查点、保存点和状态恢复”](ch03.html#chap-3-checkpoints)），某些内部组件利用文件系统将数据分发到任务中，例如应用程序的
    JAR 文件。
- en: Flink supports a wide variety of filesystems. Since Flink is a distributed system
    and runs processes on cluster or cloud environments, filesystems typically need
    to be globally accessible. Hence, Hadoop HDFS, S3, and NFS are commonly used filesystems.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 支持多种文件系统。由于 Flink 是一个分布式系统，运行在集群或云环境中，因此文件系统通常需要全局访问权限。因此，Hadoop HDFS、S3
    和 NFS 是常用的文件系统。
- en: Similar to other data processing systems, Flink looks at the URI scheme of a
    path to identify the filesystem the path refers to. For example, *file:///home/user/data.txt*
    points to a file in the local filesystem and *hdfs:///namenode:50010/home/user/data.txt*
    to a file in the specified HDFS cluster.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他数据处理系统类似，Flink 根据路径的 URI 方案来识别路径所指向的文件系统。例如，*file:///home/user/data.txt*
    指向本地文件系统中的文件，*hdfs:///namenode:50010/home/user/data.txt* 指向指定的 HDFS 集群中的文件。
- en: A filesystem is represented in Flink by an implementation of the `org.apache.flink.core.fs.FileSystem`
    class. A `FileSystem` class implements filesystem operations, such as reading
    from and writing to files, creating directories or files, and listing the contents
    of a directory. A Flink process (JobManager or TaskManager) instantiates one `FileSystem`
    object for each configured filesystem and shares it across all local tasks to
    guarantee that configured constraints such as limits on the number of open connections
    are enforced.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Flink 中，文件系统由 `org.apache.flink.core.fs.FileSystem` 类的实现来表示。`FileSystem` 类实现文件系统操作，如从文件中读取和写入、创建目录或文件以及列出目录的内容。Flink
    进程（JobManager 或 TaskManager）为每个配置的文件系统实例化一个 `FileSystem` 对象，并在所有本地任务之间共享，以确保强制执行诸如限制打开连接数等配置约束。
- en: 'Flink provides implementations for the most commonly used filesystems as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 提供了以下最常用文件系统的实现：
- en: Local filesystem
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本地文件系统
- en: Flink has built-in support for local filesystems, including locally mounted
    network filesystems, such as NFS or SAN, and does not require additional configuration.
    Local filesystems are referenced by the *file://* URI scheme.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 内置支持本地文件系统，包括本地挂载的网络文件系统，如 NFS 或 SAN，并且不需要额外的配置。本地文件系统使用 *file://* URI
    方案来引用。
- en: Hadoop HDFS
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop HDFS
- en: Flink’s connector for HDFS is always in the classpath of Flink. However, it
    requires Hadoop dependencies on the classpath in order to work. [“Integration
    with Hadoop Components”](#integration-with-hadoop) explains how to ensure Hadoop
    dependencies are loaded. HDFS paths are prefixed with the *hdfs://* scheme.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 对 HDFS 的连接器始终位于 Flink 的类路径中。但是，为了使其工作，需要在类路径中加载 Hadoop 依赖项。["与 Hadoop
    组件集成"](#integration-with-hadoop) 解释了如何确保加载 Hadoop 依赖项。HDFS 路径以 *hdfs://* 方案为前缀。
- en: Amazon S3
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon S3
- en: Flink provides two alternative filesystem connectors to connect to S3, which
    are based on Apache Hadoop and Presto. Both connectors are fully self-contained
    and do not expose any dependencies. To install either of both connectors, move
    the respective JAR file from the *./opt* folder into the *./lib* folder. The Flink
    documentation provides more details on the configuration of S3 filesystems. S3
    paths are specified with the *s3://* scheme.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 提供了两种连接到 S3 的替代文件系统连接器，基于 Apache Hadoop 和 Presto。这两个连接器都是完全自包含的，不会暴露任何依赖项。要安装这两个连接器之一，需要将相应的
    JAR 文件从 *./opt* 文件夹移动到 *./lib* 文件夹。Flink 文档提供了有关配置 S3 文件系统的更多详细信息。S3 路径使用 *s3://*
    方案来指定。
- en: OpenStack Swift FS
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack Swift FS
- en: Flink provides a connector to Swift FS, which is based on Apache Hadoop. The
    connector is fully self-contained and does not expose any dependencies. It is
    installed by moving the swift-connector JAR file from the *./opt* to the *./lib*
    folder. Swift FS paths are identified by the *swift://* scheme.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 提供了一个基于 Apache Hadoop 的 Swift FS 连接器。该连接器完全自包含，不会暴露任何依赖项。通过将 swift-connector
    JAR 文件从 *./opt* 移动到 *./lib* 文件夹来安装。Swift FS 路径通过 *swift://* 方案来标识。
- en: For filesystems for which Flink does not provide a dedicated connector, Flink
    can delegate to the Hadoop filesystem connector if it is correctly configured.
    This is why Flink is able to support all HCFSs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Flink 没有提供专用连接器的文件系统，如果正确配置，Flink 可以委托给 Hadoop 文件系统连接器。这就是为什么 Flink 能够支持所有
    HCFSs。
- en: 'Flink provides a few configuration options in *./conf/flink-conf.yaml* to specify
    a default filesystem and limit the number of filesystem connections. You can specify
    a default filesystem scheme (*fs.default-scheme*) that is automatically added
    as a prefix if a path does not provide a scheme. If you, for example, specify
    *fs.default-scheme: hdfs://nnode1:9000*, the path */result* will be extended to
    *hdfs://nnode1:9000/result*.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 'Flink 在 *./conf/flink-conf.yaml* 中提供了一些配置选项，用于指定默认文件系统和限制文件系统连接的数量。您可以指定一个默认的文件系统方案（*fs.default-scheme*），如果路径没有提供方案，将自动添加为前缀。例如，如果您指定
    *fs.default-scheme: hdfs://nnode1:9000*，路径 */result* 将扩展为 *hdfs://nnode1:9000/result*。'
- en: 'You can limit the number of connections that read from (input) and write to
    (output) a filesystem. The configuration can be defined per URI scheme. The relevant
    configuration keys are:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以限制从文件系统读取（输入）和写入（输出）的连接数。配置可以按 URI 方案定义。相关的配置键包括：
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The number of connections are tracked per TaskManager process and path authority—*hdfs://nnode1:50010*
    and *hdfs://nnode2:50010* are separately tracked. The connection limits can be
    configured either separately for input and output connections or as the total
    number of connections. When the filesystem reaches its connection limit and tries
    to open a new connection, it will block and wait for another connection to close.
    The timeout parameters define how long to wait until a connection request fails
    (`fs.<scheme>.limit.timeout`) and how long to wait until an idle connection is
    closed (`fs.<scheme>.limit.stream-timeout`).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 连接数根据 TaskManager 进程和路径权限进行跟踪，*hdfs://nnode1:50010* 和 *hdfs://nnode2:50010*
    分别进行跟踪。可以单独为输入和输出连接配置连接限制，也可以作为总连接数进行配置。当文件系统达到其连接限制并尝试打开新连接时，它将阻塞并等待另一个连接关闭。超时参数定义了等待连接请求失败的时间（`fs.<scheme>.limit.timeout`）以及等待空闲连接关闭的时间（`fs.<scheme>.limit.stream-timeout`）。
- en: You can also provide a custom filesystem connector. Take a look at the [Flink
    documentation](http://bit.ly/2HxINvi) to learn how to implement and register a
    custom filesystem.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以提供自定义的文件系统连接器。查看[Flink 文档](http://bit.ly/2HxINvi)了解如何实现和注册自定义文件系统。
- en: System Configuration
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统配置
- en: Apache Flink offers many parameters to configure its behavior and tweak its
    performance. All parameters can be defined in the *./conf/flink-conf.yaml* file,
    which is organized as a flat YAML file of key-value pairs. The configuration file
    is read by different components, such as the start scripts, the master and worker
    JVM processes, and the CLI client. For example, the start scripts, such as *./bin/start-cluster.sh*,
    parse the configuration file to extract JVM parameters and heap size settings,
    and the CLI client (*./bin/flink*) extracts the connection information to access
    the master process. Changes in the configuration file are not effective until
    Flink is restarted.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink 提供了许多参数来配置其行为和调整其性能。所有参数都可以在 *./conf/flink-conf.yaml* 文件中定义，该文件以键值对的形式组织为扁平的
    YAML 文件。配置文件由不同组件读取，例如启动脚本、主节点和工作节点的 JVM 进程以及 CLI 客户端。例如，启动脚本（例如 *./bin/start-cluster.sh*）解析配置文件以提取
    JVM 参数和堆大小设置，而 CLI 客户端（*./bin/flink*）提取连接信息以访问主进程。在修改配置文件后，需要重新启动 Flink 才能生效。
- en: To improve the out-of-the-box experience, Flink is preconfigured for a local
    setup. You need to adjust the configuration to successfully run Flink in distributed
    environments. In this section, we discuss different aspects that typically need
    to be configured when setting up a Flink cluster. We refer you to the [official
    documentation](http://bit.ly/2O4T7fv) for a comprehensive list and detailed descriptions
    of all parameters.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进开箱即用体验，Flink 预配置为本地设置。您需要调整配置以成功在分布式环境中运行 Flink。在本节中，我们讨论了设置 Flink 集群时通常需要配置的不同方面。我们建议您参考[官方文档](http://bit.ly/2O4T7fv)，以获取所有参数的详尽列表和详细描述。
- en: Java and Classloading
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Java 和类加载
- en: By default, Flink starts JVM processes using the Java executable linked by the
    `PATH` environment variable. If Java is not on the `PATH` or if you want to use
    a different Java version you can specify the root folder of a Java installation
    via the `JAVA_HOME` environment variable or the `env.java.home` key in the configuration
    file. Flink’s JVM processes can be started with custom Java options—for example,
    to fine-tune the garbage collector or to enable remote debugging, with the keys
    `env.java.opts`, `env.java.opts.jobmanager`, and `env.java.opts.taskmanager`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Flink 使用与 `PATH` 环境变量链接的 Java 可执行文件启动 JVM 进程。如果 Java 不在 `PATH` 中，或者您想使用不同的
    Java 版本，可以通过 `JAVA_HOME` 环境变量或配置文件中的 `env.java.home` 键指定 Java 安装的根文件夹。Flink 的
    JVM 进程可以使用自定义的 Java 选项启动，例如用于微调垃圾收集器或启用远程调试的键 `env.java.opts`、`env.java.opts.jobmanager`
    和 `env.java.opts.taskmanager`。
- en: Classloading issues are not uncommon when running jobs with external dependencies.
    In order to execute a Flink application, all classes in the application’s JAR
    file must be loaded by a classloader. Flink registers the classes of every job
    into a separate user-code classloader to ensure the dependencies of the job do
    not infer with Flink’s runtime dependencies or the dependencies of other jobs.
    User-code class loaders are disposed of when the corresponding job terminates.
    Flink’s system class loader loads all JAR files in the *./lib* folder and the
    user-code classloaders are derived from the system classloader.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 运行具有外部依赖的作业时常见类加载问题。为了执行 Flink 应用程序，必须由类加载器加载应用程序 JAR 文件中的所有类。Flink 将每个作业的类注册到单独的用户代码类加载器中，以确保作业的依赖不会干扰
    Flink 的运行时依赖项或其他作业的依赖项。用户代码类加载器在相应作业终止时被释放。Flink 的系统类加载器加载 *./lib* 文件夹中的所有 JAR
    文件，并且用户代码类加载器是从系统类加载器派生的。
- en: By default, Flink looks up user-code classes first in the child (user-code)
    classloader and then in parent (system) classloader to prevent version clashes
    in case a job uses the same dependency as Flink. However, you can also invert
    the lookup order with the `classloader.resolve-order` configuration key.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Flink 首先在子（用户代码）类加载器中查找用户代码类，然后在父（系统）类加载器中查找，以防止作业与 Flink 使用相同依赖版本冲突。但是，您也可以通过
    `classloader.resolve-order` 配置键来反转查找顺序。
- en: Note
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that some classes are always resolved first in the parent classloader (`classloader.parent-first-patterns.default`).
    You can extend the list by providing a whitelist of classname patterns that are
    first resolved from the parent classloader (`classloader.parent-first-patterns.additional`).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意一些类始终首先由父类加载器解析 (`classloader.parent-first-patterns.default`)。您可以通过提供一个类名模式的白名单来扩展列表，这些类名模式首先从父类加载器解析
    (`classloader.parent-first-patterns.additional`)。
- en: CPU
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPU
- en: Flink does not actively limit the amount of CPU resources it consumes. However,
    it uses processing slots (see [“Task Execution”](ch03.html#chap-3-task-execution)
    for a detailed discussion) to control the number of tasks that can be assigned
    to a worker process (TaskManager). A TaskManager provides a certain number of
    slots that are registered at and governed by the ResourceManager. A JobManager
    requests one or more slots to execute an application. Each slot can process one
    slice of an application, one parallel task of every operator of the application.
    Hence, the JobManager needs to acquire at least as many slots as the application’s
    maximum operator parallelism.^([6](ch09.html#idm45498992680568)) Tasks are executed
    as threads within the worker (TaskManager) process and take as much CPU resources
    as they need.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 不会主动限制其消耗的 CPU 资源量。然而，它使用处理插槽（详见 [“任务执行”](ch03.html#chap-3-task-execution)
    进行详细讨论）来控制可以分配给工作进程（TaskManager）的任务数量。一个 TaskManager 提供一定数量的插槽，这些插槽由 ResourceManager
    注册和管理。JobManager 请求一个或多个插槽来执行应用程序。每个插槽可以处理应用程序的一个切片，应用程序中每个操作符的一个并行任务。因此，JobManager
    需要至少获取与应用程序的最大操作符并行度相同数量的插槽。^([6](ch09.html#idm45498992680568)) 任务作为线程在工作进程（TaskManager）内执行，并且会占用它们需要的
    CPU 资源。
- en: The number of slots a TaskManager offers is controlled with the `taskmanager.numberOfTaskSlots`
    key in the configuration file. The default is one slot per TaskManager. The number
    of slots usually only needs to be configured for standalone setups as running
    Flink on a cluster resource manager (YARN, Kubernetes, Mesos) makes it easy to
    spin up multiple TaskManagers (each with one slot) per compute node.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: TaskManager 提供的插槽数量由配置文件中的 `taskmanager.numberOfTaskSlots` 键控制。默认为每个 TaskManager
    一个插槽。通常只需为独立设置配置插槽数量，因为在集群资源管理器（YARN、Kubernetes、Mesos）上运行 Flink 可以轻松地在每个计算节点上启动多个
    TaskManager（每个具有一个插槽）。
- en: Main Memory and Network Buffers
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主存储器和网络缓冲区
- en: Flink’s master and worker processes have different memory requirements. A master
    process mainly manages compute resources (ResourceManager) and coordinates the
    execution of applications (JobManager), while a worker process takes care of the
    heavy lifting and processes potentially large amounts of data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 的主控和工作进程具有不同的内存需求。主控进程主要管理计算资源（ResourceManager）并协调应用程序的执行（JobManager），而工作进程则负责处理重活和处理可能大量数据。
- en: Usually, the master process has moderate memory requirements. By default, it
    is started with 1 GB JVM heap memory. If the master process needs to manage several
    applications or an application with many operators, you might need to increase
    the JVM heap size with the `jobmanager.heap.size` configuration key.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，主进程的内存需求适中。默认情况下，它使用 1GB JVM 堆内存启动。如果主进程需要管理几个应用程序或具有许多运算符的应用程序，则可能需要使用`jobmanager.heap.size`配置键增加
    JVM 堆大小。
- en: Configuring the memory of a worker process is a bit more involved because there
    are multiple components that allocate different types of memory. The most important
    parameter is the size of the JVM heap memory, which is set with the key `taskmanager.heap.size`.
    The heap memory is used for all objects, including the TaskManager runtime, operators
    and functions of the application, and in-flight data. The state of an application
    that uses the in-memory or filesystem state backend is also stored on the JVM.
    Note that a single task can potentially consume the whole heap memory of the JVM
    that it is running on. Flink does not guarantee or grant heap memory per task
    or slot. Configurations with a single slot per TaskManager have better resource
    isolation and can prevent a misbehaving application from interfering with unrelated
    applications. If you run applications with many dependencies, the JVM’s nonheap
    memory can also grow significantly because it stores all TaskManager and user-code
    classes.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 配置工作进程的内存涉及到多个组件，因为有多个组件分配不同类型的内存。最重要的参数是 JVM 堆内存的大小，使用`taskmanager.heap.size`键进行设置。堆内存用于所有对象，包括任务管理器运行时、应用程序的操作符和函数以及飞行中的数据。使用内存或文件系统状态后端的应用程序状态也存储在
    JVM 上。请注意，单个任务可能会消耗其所在 JVM 的整个堆内存。Flink 不能保证或授予每个任务或插槽的堆内存。每个任务管理器的单个插槽配置具有更好的资源隔离性，并且可以防止行为异常的应用程序干扰不相关的应用程序。如果您运行具有许多依赖关系的应用程序，则
    JVM 的非堆内存也可能会显著增长，因为它存储所有任务管理器和用户代码类。
- en: In addition to the JVM, there are two other major memory consumers, Flink’s
    network stack and RocksDB, when it is used as a state backend. Flink’s network
    stack is based on the Netty library, which allocates network buffers from native
    (off-heap) memory. Flink requires a sufficient number of network buffers to be
    able to ship records from one worker process to the other. The number of buffers
    depends on the total number of network connections between operator tasks. For
    two operators connected by a partitioning or broadcasting connection the number
    of network buffers depends on the product of the sending and receiving operator
    parallelism. For applications with several partitioning steps, this quadratic
    dependency can quickly sum up to a significant amount of memory that is required
    for network transfer.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 JVM 外，另外两个主要的内存消耗者是 Flink 的网络堆栈和 RocksDB（用作状态后端时）。Flink 的网络堆栈基于 Netty 库，它从本地（堆外）内存中分配网络缓冲区。Flink
    需要足够数量的网络缓冲区，以便能够从一个工作进程向另一个工作进程发送记录。缓冲区的数量取决于操作符任务之间的网络连接总数。对于通过分区或广播连接连接的两个操作符，缓冲区的数量取决于发送和接收操作符并行度的乘积。对于具有多个分区步骤的应用程序，这种二次依赖关系可能很快累积到需要进行网络传输的大量内存。
- en: 'Flink’s default configuration is only suitable for a smaller scale distributed
    setup and needs to be adjusted for more serious scale. If the number of buffers
    is not appropriately configured, a job submission will fail with a `java.io.IOException:
    Insufficient number of network buffers`. In this case, you should provide more
    memory to the network stack.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 'Flink 的默认配置仅适用于较小规模的分布式设置，并且需要调整以适应更严重的规模。如果缓冲区的数量未适当配置，则作业提交将以`java.io.IOException:
    Insufficient number of network buffers`失败。在这种情况下，您应该为网络堆栈提供更多内存。'
- en: The amount of memory assigned for network buffers is configured with the `taskmanager.network.memory.fraction`
    key, which determines the fraction of the JVM size allocated for network buffers.
    By default, 10% of the JVM heap size is used. Since the buffers are allocated
    as off-heap memory, the JVM heap is reduced by that amount. The configuration
    key `taskmanager.memory.segment-size` determines the size of a network buffer,
    which is 32 KB by default. Reducing the size of a network buffer increases the
    number of buffers but can reduce the efficiency of the network stack. You can
    also specify a minimum (`taskmanager.network.memory.min`) and a maximum (`taskmanager.network.memory.max`)
    amount of memory that is used for network buffers (by default 64 MB and 1 GB,
    respectively) to set absolute limits for the relative configuration value.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 网络缓冲区分配的内存量由 `taskmanager.network.memory.fraction` 键配置，该键确定为网络缓冲区分配的 JVM 大小的分数。默认情况下，使用
    JVM 堆大小的 10%。由于缓冲区分配为堆外内存，因此 JVM 堆会减少相应的量。`taskmanager.memory.segment-size` 配置键确定网络缓冲区的大小，默认为
    32 KB。减少网络缓冲区的大小会增加缓冲区的数量，但可能会降低网络堆栈的效率。您还可以指定用于网络缓冲区的最小 (`taskmanager.network.memory.min`)
    和最大 (`taskmanager.network.memory.max`) 内存量（默认分别为 64 MB 和 1 GB），以设置相对配置值的绝对限制。
- en: RocksDB is another memory consumer that needs to be taken into consideration
    when configuring the memory of a worker process. Unfortunately, figuring out the
    memory consumption of RocksDB is not straightforward because it depends on the
    number of keyed states in an application. Flink creates a separate (embedded)
    RocksDB instance for each task of a keyed operator. Within each instance, every
    distinct state of the operator is stored in a separate column family (or table).
    With the default configuration, each column family requires about 200 MB to 240
    MB of off-heap memory. You can adjust RocksDB’s configuration and tweak its performance
    with many parameters.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: RocksDB 是配置工作进程内存时需要考虑的另一个内存消耗者。不幸的是，确定 RocksDB 的内存消耗并不直接，因为它取决于应用程序中键控状态的数量。对于每个键控操作符的任务，Flink
    创建一个单独的（嵌入式）RocksDB 实例。在每个实例中，操作符的每个不同状态存储在单独的列族（或表）中。在默认配置下，每个列族需要约 200 MB 到
    240 MB 的堆外内存。您可以通过多个参数调整 RocksDB 的配置并优化其性能。
- en: When configuring the memory setting of a TaskManager, you should size the JVM
    heap memory so there is enough memory left for the JVM nonheap memory (classes
    and metadata) and RocksDB if it is configured as a state backend. Network memory
    is automatically subtracted from the configured JVM heap size. Keep in mind that
    some resource managers, such as YARN, will immediately kill a container if it
    exceeds its memory budget.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 TaskManager 内存设置时，应调整 JVM 堆内存大小，以便为 JVM 非堆内存（类和元数据）和如果配置为状态后端的 RocksDB 留出足够的内存。网络内存会自动从配置的
    JVM 堆大小中减去。请记住，某些资源管理器（如 YARN）会在容器超出内存预算时立即终止该容器。
- en: Disk Storage
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 磁盘存储
- en: A Flink worker process stores data on the local filesystem for multiple reasons,
    including receiving application JAR files, writing log files, and maintaining
    application state if the RocksDB state backend is configured. With the `io.tmp.dirs`
    configuration key, you can specify one or more directories (separated by colons)
    that are used to store data in the local filesystem. By default, data is written
    to the default temporary directory as determined by the Java system property `java.io.tmpdir`,
    or `/tmp` on Linux and MacOS. The `io.tmp.dirs` parameter is used as the default
    value for the local storage path of most components of Flink. However, these paths
    can also be individually configured.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 工作进程出于多种原因将数据存储在本地文件系统中，包括接收应用程序 JAR 文件、写入日志文件以及在配置了 RocksDB 状态后端时维护应用程序状态。通过
    `io.tmp.dirs` 配置键，您可以指定一个或多个目录（用冒号分隔），用于在本地文件系统中存储数据。默认情况下，数据写入由 Java 系统属性 `java.io.tmpdir`
    决定的默认临时目录，Linux 和 MacOS 上为 `/tmp`。`io.tmp.dirs` 参数用作 Flink 大多数组件本地存储路径的默认值。但是，这些路径也可以单独配置。
- en: Ensure Temporary Directories Are Not Automatically Cleaned
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保不自动清理临时目录
- en: Some Linux distribution periodically clean the temporary directory /tmp. Make
    sure to disable this behavior or configure a different directory if you plan to
    run continuous Flink applications. Otherwise job recovery might miss metadata
    that was stored in the temporary directory and fail.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 Linux 发行版定期清理临时目录 `/tmp`。如果计划运行持续的 Flink 应用程序，请确保禁用此行为或配置不同的目录。否则，作业恢复可能会丢失存储在临时目录中的元数据而失败。
- en: The `blob.storage.directory` key configures the local storage directory of the
    blob server, which is used to exchange larger files such as the application JAR
    files. The `env.log.dir` key configures the directory into which a TaskManager
    writes its log files (by default, the *./log* directory in the Flink setup). Finally,
    the RocksDB state backend maintains application state in the local filesystem.
    The directory is configured using the `state.backend.rocksdb.localdir` key. If
    the storage directory is not explicitly configured, RocksDB uses the value of
    the `io.tmp.dirs` parameter.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`blob.storage.directory` 键配置了 blob 服务器的本地存储目录，用于交换诸如应用程序 JAR 文件之类的大文件。`env.log.dir`
    键配置了 TaskManager 写入其日志文件的目录（在 Flink 设置中，默认为 *./log* 目录）。最后，RocksDB 状态后端在本地文件系统中维护应用程序状态。该目录使用
    `state.backend.rocksdb.localdir` 键进行配置。如果未明确配置存储目录，则 RocksDB 使用 `io.tmp.dirs`
    参数的值。'
- en: Checkpointing and State Backends
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查点和状态后端
- en: Flink offers a few options to configure how state backends checkpoint their
    state. All parameters can be explicitly specified within the code of an application
    as described in [“Tuning Checkpointing and Recovery”](ch10.html#chap-10-checkpoint-tuning).
    However, you can also provide default settings for a Flink cluster through Flink’s
    configuration file, which are applied if job-specific options are not declared.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 提供了几种选项来配置状态后端如何检查其状态。可以在应用程序代码中显式指定所有参数，如 [“调整检查点和恢复”](ch10.html#chap-10-checkpoint-tuning)
    中所述。但是，您还可以通过 Flink 的配置文件为 Flink 集群提供默认设置，如果未声明特定于作业的选项，则会应用这些设置。
- en: An important choice that affects the performance of an application is the state
    backend that maintains its state. You can define the default state backend of
    a cluster with the `state.backend` key. Moreover, you can enable asynchronous
    checkpointing (`state.backend.async`) and incremental checkpointing (`state.backend.incremental`).
    Some backends do not support all options and might ignore them. You can also configure
    the root directories at the remote storage to which checkpoints (`state.checkpoints.dir`)
    and savepoints (`state.savepoints.dir`) are written.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 影响应用程序性能的重要选择之一是维护其状态的状态后端。您可以使用 `state.backend` 键定义集群的默认状态后端。此外，您可以启用异步检查点（`state.backend.async`）和增量检查点（`state.backend.incremental`）。一些后端不支持所有选项，可能会忽略它们。您还可以配置远程存储的根目录，用于写入检查点（`state.checkpoints.dir`）和保存点（`state.savepoints.dir`）。
- en: Some checkpointing options are backend specific. For the RocksDB state backend
    you can define one or more paths at which RocksDB stores its local files (`state.backend.rocksdb.localdir`)
    and whether timer state is stored on the heap (default) or in RocksDB (`state.backend.rocksdb.timer-service.factory`).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一些检查点选项是特定于后端的。对于 RocksDB 状态后端，您可以定义一个或多个路径，RocksDB 在这些路径上存储其本地文件（`state.backend.rocksdb.localdir`），以及定时器状态是存储在堆（默认）还是在
    RocksDB 中（`state.backend.rocksdb.timer-service.factory`）。
- en: Finally, you can enable and configure local recovery for a Flink cluster by
    default.^([7](ch09.html#idm45498992628904)) To enable local recovery, set the
    parameter `state.backend.local-recovery` to true. The storage location of the
    local state copy can be specified as well (`taskmanager.state.local.root-dirs`).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以默认启用并配置 Flink 集群的本地恢复。要启用本地恢复，请将参数 `state.backend.local-recovery` 设置为
    true。还可以指定本地状态副本的存储位置（`taskmanager.state.local.root-dirs`）。
- en: Security
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全性
- en: Data processing frameworks are sensitive components of a company’s IT infrastructure
    and need to be secured against unauthorized use and access to data. Apache Flink
    supports Kerberos authentication and can be configured to encrypt all network
    communication with SSL.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理框架是公司 IT 基础设施中敏感的组件，需要防止未经授权的使用和对数据的访问。Apache Flink 支持 Kerberos 认证，并可以配置为使用
    SSL 加密所有网络通信。
- en: Flink features Kerberos integration with Hadoop and its components (YARN, HDFS,
    HBase), ZooKeeper, and Kafka. You can enable and configure the Kerberos support
    for each service separately. Flink supports two authentication modes—keytabs and
    Hadoop delegation tokens. Keytabs are the preferred approach because tokens expire
    after some time, which can cause problems for long-running stream processing applications.
    Note that the credentials are tied to a Flink cluster and not to a running job;
    all applications that run on the same cluster use the same authentication token.
    If you need to work with different credentials, you should start a new cluster.
    Consult the [Flink documentation](http://bit.ly/2Fc4i3e) for detailed instructions
    on enabling and configuring Kerberos authentication.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持与Hadoop及其组件（YARN、HDFS、HBase）、ZooKeeper和Kafka的Kerberos集成。您可以为每个服务单独启用和配置Kerberos支持。Flink支持两种认证模式——Keytabs和Hadoop委托令牌。Keytabs是首选方法，因为令牌在一段时间后会过期，这可能会导致长时间运行的流处理应用程序出现问题。请注意，凭据与Flink集群绑定，而不是与运行的作业绑定；所有在同一集群上运行的应用程序使用相同的认证令牌。如果您需要使用不同的凭据工作，您应该启动一个新的集群。请参阅[Flink文档](http://bit.ly/2Fc4i3e)以获取关于启用和配置Kerberos认证的详细说明。
- en: Flink supports mutual authentication of communication partners and encryption
    of network communication with SSL for internal and external communication. For
    internal communication (RPC calls, data transfer, and blob service communication
    to distribute libraries or other artifacts) all Flink processes (Dispatcher, ResourceManager,
    JobManager, and TaskManager) perform mutual authentication— senders and receivers
    validate each other via an SSL certificate. The certificate acts as a shared secret
    and can be embedded into containers or attached to a YARN setup.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持通信双方的互相认证，并使用SSL对内部和外部通信进行网络通信加密。对于内部通信（RPC调用、数据传输和分发库或其他文件的blob服务通信），所有的Flink进程（调度器、资源管理器、作业管理器和任务管理器）都执行互相认证——发送方和接收方通过SSL证书进行验证。证书作为一个共享的秘密可以嵌入到容器中或者附加到YARN设置中。
- en: All external communication with Flink services—submitting and controlling applications
    and accessing the REST interface—happens over REST/HTTP endpoints.^([8](ch09.html#idm45498992614584))
    You can enable SSL encryption for these connections as well. Mutual authentication
    can also be enabled. However, the recommended approach is setting up and configuring
    a dedicated proxy service that controls access to the REST endpoint. The reason
    is that proxy services offer more authentication and configuration options than
    Flink. Encryption and authentication for communication to queryable state is not
    supported yet.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 所有与Flink服务的外部通信——提交和控制应用程序以及访问REST接口——都通过REST/HTTP端点进行。^([8](ch09.html#idm45498992614584))
    您也可以为这些连接启用SSL加密。还可以启用双向认证。然而，推荐的方法是设置和配置一个专用的代理服务来控制对REST端点的访问。原因是代理服务提供比Flink更多的认证和配置选项。目前尚不支持对查询状态通信的加密和认证。
- en: By default, SSL authentication and encryption is not enabled. Since the setup
    requires several steps, such as generating certificates, setting up TrustStores
    and KeyStores, and configuring cipher suites, we refer you to the official [Flink
    documentation](http://bit.ly/2Fc4i3e). The documentation also includes how-tos
    and tips for different environments, such as standalone clusters, Kubernetes,
    and YARN.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，未启用SSL认证和加密。由于设置涉及多个步骤，如生成证书、设置信任存储和密钥存储以及配置密码套件，我们建议您参阅官方的[Flink文档](http://bit.ly/2Fc4i3e)。文档还包括如何在独立集群、Kubernetes和YARN等不同环境中进行操作和技巧。
- en: Summary
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter we discussed how Flink is set up in different environments and
    how to configure HA setups. We explained how to enable support for various filesystems
    and how to integrate them with Hadoop and its components. Finally, we discussed
    the most important configuration options. We did not provide a comprehensive configuration
    guide; instead, we refer you to [the official documentation of Apache Flink](http://bit.ly/2O5ikGP)
    for a complete list and detailed descriptions of all configuration options.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了Flink在不同环境中的设置以及如何配置高可用设置。我们解释了如何启用对各种文件系统的支持以及如何与Hadoop及其组件集成。最后，我们讨论了最重要的配置选项。我们没有提供全面的配置指南；而是建议您参考[Apache
    Flink的官方文档](http://bit.ly/2O5ikGP)，以获取所有配置选项的完整列表和详细描述。
- en: ^([1](ch09.html#idm45498993250232-marker)) In order to run Flink on Windows,
    you can use a provided bat script or you can use the regular bash scripts on the
    Windows Subsystem for Linux (WSL) or Cygwin. All scripts only work for local setups.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.html#idm45498993250232-marker)) 要在 Windows 上运行 Flink，可以使用提供的批处理脚本，也可以在
    Windows Subsystem for Linux (WSL) 或 Cygwin 上使用常规的 bash 脚本。所有脚本仅适用于本地设置。
- en: ^([2](ch09.html#idm45498993223160-marker)) Flink Docker images are not part
    of the official Apache Flink release.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch09.html#idm45498993223160-marker)) Flink Docker 镜像不包含在官方 Apache Flink
    发行版中。
- en: ^([3](ch09.html#idm45498993140616-marker)) Note that the concept of a container
    in YARN is different from a container in Docker.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch09.html#idm45498993140616-marker)) 注意，YARN 中的容器概念与 Docker 中的容器不同。
- en: ^([4](ch09.html#idm45498993010952-marker)) Kubernetes also supports pods consisting
    of multiple tightly linked containers.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch09.html#idm45498993010952-marker)) Kubernetes 还支持由多个紧密链接的容器组成的 pod。
- en: ^([5](ch09.html#idm45498992848888-marker)) ApplicationMaster is YARN’s master
    process of an application.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch09.html#idm45498992848888-marker)) ApplicationMaster 是 YARN 应用的主进程。
- en: ^([6](ch09.html#idm45498992680568-marker)) It is possible to assign operators
    to different slot-sharing groups and thus assign their tasks to distinct slots.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch09.html#idm45498992680568-marker)) 可以将运算符分配给不同的槽共享组，从而将它们的任务分配给不同的槽。
- en: ^([7](ch09.html#idm45498992628904-marker)) See [“Configuring Recovery”](ch10.html#chap-10-recovery-tuning)
    for details on this feature.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch09.html#idm45498992628904-marker)) 详见[“配置恢复”](ch10.html#chap-10-recovery-tuning)了解此功能的详情。
- en: ^([8](ch09.html#idm45498992614584-marker)) [Chapter 10](ch10.html#chap-10) discusses
    job submission and the REST interface.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch09.html#idm45498992614584-marker)) [第 10 章](ch10.html#chap-10)讨论了作业提交和
    REST 接口。
