- en: Chapter 10\. Structured Streaming Sources
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 10 章\. 结构化流源
- en: The previous chapters provided a good overview of the Structured Streaming programming
    model and how you can apply it in a practical way. You also saw how sources are
    the starting point of each Structured Streaming program. In this chapter, we study
    the general characteristics of a source and review the available sources in greater
    detail, including their different configuration options and modes of operation.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章对结构化流编程模型提供了很好的概述，并展示了如何在实际中应用它。您还看到了源是每个结构化流程序的起点。在本章中，我们研究了源的一般特性，并更详细地审视了可用的源，包括它们的不同配置选项和操作模式。
- en: Understanding Sources
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解源
- en: In Structured Streaming, a source is an abstraction that represents streaming
    data providers. The concept behind the source interface is that streaming data
    is a continuous flow of events over time that can be seen as a sequence, indexed
    with a monotonously incrementing counter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化流中，源是表示流数据提供程序的抽象。源接口背后的概念是，流数据是随时间持续流动的事件序列，可以看作是一个按单调递增计数器索引的序列。
- en: '[Figure 10-1](#event-sequence) illustrates how each event in the stream is
    considered to have an ever-increasing offset.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-1](#event-sequence) 显示了流中每个事件被认为具有不断增加的偏移量。'
- en: '![spas 1001](Images/spas_1001.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![spas 1001](Images/spas_1001.png)'
- en: Figure 10-1\. A stream seen as an indexed sequence of events
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 流被视为事件索引序列
- en: Offsets, as shown in [Figure 10-2](#offset-process), are used to request data
    from the external source and to indicate what data has already been consumed.
    Structured Streaming knows when there is data to process by asking the current
    offset from the external system and comparing it to the last processed offset.
    The data to be processed is requested by getting a *batch* between two offsets
    `start` and `end`. The source is informed that data has been processed by committing
    a given offset. The source contract guarantees that all data with an offset less
    than or equal to the committed offset has been processed and that subsequent requests
    will stipulate only offsets greater than that committed offset. Given these guarantees,
    sources might opt to discard the processed data to free up system resources.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在 [图 10-2](#offset-process) 中所示的，偏移量用于从外部源请求数据，并指示已经消耗的数据。结构化流通过向外部系统请求当前偏移量并将其与上次处理的偏移量进行比较来知晓是否有数据需要处理。要处理的数据是通过获取
    `start` 和 `end` 之间的一个 *批次* 来请求的。通过提交给定的偏移量通知源已经处理了数据。源契约保证已处理的偏移量小于或等于提交的偏移量的所有数据，并且随后的请求只会规定大于该提交的偏移量。基于这些保证，源可以选择丢弃已处理的数据以释放系统资源。
- en: '![spas 1002](Images/spas_1002.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![spas 1002](Images/spas_1002.png)'
- en: Figure 10-2\. Offset process sequence
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 偏移量处理序列
- en: 'Let’s take a closer look at the dynamics of the offset-based processing shown
    in [Figure 10-2](#offset-process):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下在 [图 10-2](#offset-process) 中显示的基于偏移量处理的动态：
- en: At *t1*, the system calls `getOffset` and obtains the current offset for the
    *source*.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *t1* 时，系统调用 `getOffset` 并获取 *源* 的当前偏移量。
- en: At *t2*, the system obtains the batch up to the last known offset by calling
    `getBatch(start, end)`. Note that new data might have arrived in the meantime.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *t2* 时，系统通过调用 `getBatch(start, end)` 获取到最后已知偏移量的批次。请注意，此期间可能会有新数据到达。
- en: At *t3*, the system `commits` the offset and the source drops the corresponding
    records.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *t3* 时，系统 `提交` 偏移量，源删除相应记录。
- en: This process repeats constantly, ensuring the acquisition of streaming data.
    To recover from eventual failure, offsets are often *checkpointed* to external
    storage.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程不断重复，确保获取流数据。为了从可能的失败中恢复，偏移量通常会被 *检查点* 到外部存储。
- en: 'Besides the offset-based interaction, sources must fulfill two requirements:
    to be reliable, sources must be replayable in the same order; and sources must
    provide a schema.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于偏移量的交互，源必须满足两个要求：为了可靠性，源必须以相同顺序可重放；并且源必须提供模式。
- en: Reliable Sources Must Be Replayable
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠的源必须是可重播的
- en: In Structured Streaming, *replayability* is the capacity to request a part of
    the stream that had been already requested but not committed yet. Just like we
    can rewind that Netflix series we are watching to see a piece we just missed because
    of a distraction, sources must offer the capability to replay a piece of the stream
    that was already requested but not committed. This is done by calling `getBatch`
    with the offset range that we want to receive again.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化流处理中，*可重放性*是指能够请求已经请求但尚未提交的流的部分。就像我们可以倒回正在观看的 Netflix 系列节目，以查看因为分心而错过的片段一样，数据源必须提供重播已请求但未提交流的能力。通过调用
    `getBatch` 并指定我们想要重新接收的偏移范围来实现这一点。
- en: A source is considered to be reliable when it can produce an uncommitted offset
    range even after a total failure of the Structured Streaming process. In this
    failure recovery process, offsets are restored from their last known checkpoint
    and are requested again from the source. This requires the actual streaming system
    that is backing the source implementation to store data safely outside the `streaming`
    process. By requiring replayability from the source, Structured Streaming delegates
    recovery responsibility to the source. This implies that only reliable sources
    work with Structured Streaming to create strong end-to-end delivery guarantees.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当结构化流处理过程完全失败后，数据源仍然能够生成未提交的偏移范围时，源被认为是可靠的。在此失败恢复过程中，偏移量从其上次已知的检查点恢复，并从数据源重新请求。这要求支持数据安全存储在
    `streaming` 过程外的实际流系统。通过要求数据源具备可重放性，结构化流处理将恢复责任委托给数据源。这意味着只有可靠的数据源与结构化流处理一起工作，以创建强大的端到端交付保证。
- en: Sources Must Provide a Schema
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据源必须提供模式
- en: A defining characteristic of the structured APIs of Spark is that they rely
    on schema information to handle the data at different levels. As opposed to processing
    opaque Strings or Byte Array blobs, schema information provides insights on how
    the data is shaped in terms of fields and types. We can use schema information
    to drive optimizations at different levels in the stack, from query planning to
    the internal binary representation of data, storage, and access to it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 结构化 API 的一个定义特征是它们依赖于模式信息来处理不同层次的数据。与处理不透明的字符串或字节数组 blob 相对，模式信息提供了关于数据字段和类型的洞察力。我们可以使用模式信息来驱动从查询规划到数据的内部二进制表示、存储和访问的不同层次的优化。
- en: Sources must provide schema information that describes the data they produce.
    Some source implementations allow this schema to be configured and use this configuration
    information to automatically parse incoming data and transform it into valid records.
    In fact, many file-based streaming sources such as JSON or comma-separated values
    (CSV) files follow this model, in which the user must provide the schema used
    by the file format to ensure proper parsing. Some other sources use a fixed internal
    schema that expresses the metadata information of every record and leaves the
    parsing of the payload to the application.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据源必须提供描述其生成数据的模式信息。一些数据源实现允许配置此模式，并使用此配置信息自动解析传入数据并将其转换为有效记录。事实上，许多基于文件的流式数据源（如
    JSON 或逗号分隔值（CSV）文件）遵循这种模式，用户必须提供文件格式使用的模式以确保正确解析。其他一些数据源使用固定的内部模式来表达每条记录的元数据信息，并将有效载荷的解析留给应用程序。
- en: From an architectural perspective, creating schema-driven streaming applications
    is desirable because it facilitates the global understanding of how data flows
    through the system and drives the formalization of the different stages of a multiprocess
    streaming pipeline.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构角度来看，创建基于模式驱动的流处理应用程序是可取的，因为它有助于全局理解数据如何在系统中流动，并推动多进程流水线的不同阶段的正式化。
- en: Defining schemas
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义模式
- en: 'In Structured Streaming, we reuse the Spark SQL API for creating schema definitions.
    There are several different methods that we can use to define the schema that
    defines the content of the stream—programmatically, inferred from a `case class`
    definition, or loaded from an existing dataset:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化流处理中，我们重用 Spark SQL API 来创建模式定义。有几种不同的方法可以用来定义流内容的模式：通过编程方式、从 `case class`
    定义中推断，或者从现有数据集加载：
- en: Programmatically
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在编程方式中
- en: 'We use the `StructType` and `StructField` classes to build up a representation
    of the schema. For example, to represent a tracked vehicle with *id*, *type*,
    and *location coordinates*, we can construct the corresponding schema structure
    as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`StructType`和`StructField`类来构建模式的表示。例如，要表示具有*id*、*type*和*位置坐标*的跟踪车辆，我们可以构造以下相应的模式结构：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`StructField` can contain nested `StructType`s, making it possible to create
    schemas of arbitrary depth and complexity.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`StructField`可以包含嵌套的`StructType`，从而可以创建任意深度和复杂度的模式。'
- en: By inference
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过推断
- en: In Scala, the schema also can be represented using arbitrary combinations of
    `case class`es. Given a single `case class` or a `case class` hierarchy, the schema
    representation can be computed by creating an `Encoder` for the `case class` and
    obtaining the schema from that `encoder` instance.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中，模式也可以使用任意组合的`case class`来表示。给定单个`case class`或`case class`层次结构，可以通过为`case
    class`创建`Encoder`并从该`encoder`实例获取模式来计算模式表示。
- en: 'Using this method, the same schema definition used in the preceding example
    can be obtained like so:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此方法，可以像下面这样获取先前示例中使用的相同模式定义：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Extract from a dataset
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集中提取
- en: 'A practical method of obtaining a schema definition is by maintaining a sample
    data file in a schema-aware format, such as Parquet. To obtain our schema definition,
    we load the sample dataset and get the schema definition from the loaded `DataFrame`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 获取模式定义的一种实用方法是通过维护以Parquet等模式感知格式存储的样本数据文件。为了获取我们的模式定义，我们加载样本数据集，并从加载的`DataFrame`中获取模式定义：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The programmatic way of defining schemas is powerful but requires effort and
    is complex to maintain, often leading to errors. Loading a dataset might be practical
    at the prototyping stage, but it requires keeping the sample dataset up-to-date,
    which in some cases can lead to accidental complexity.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模式的编程方式功能强大，但需要付出努力，并且维护复杂，往往导致错误。在原型设计阶段加载数据集可能是实用的，但在某些情况下，需要保持样本数据集的最新状态，这可能会导致意外的复杂性。
- en: Although the best method to choose might be different from one use case to the
    other, in general, when working with Scala, we prefer to use the inference method,
    when possible.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管选择最佳方法可能因用例而异，但一般来说，在使用Scala时，我们更倾向于在可能的情况下使用推断方法。
- en: Available Sources
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可用的数据源
- en: 'The following are the sources currently available in the Spark distribution
    of Structured Streaming:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是当前在结构化流Spark分发中可用的源：
- en: File
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 文件
- en: 'Allows the ingestion of data stored as files. In most cases, the data is transformed
    in records that are further processed in streaming mode. This supports these formats:
    JSON, CSV, Parquet, ORC, and plain text.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 允许摄入存储为文件的数据。在大多数情况下，数据转换为进一步在流模式中处理的记录。支持这些格式：JSON、CSV、Parquet、ORC和纯文本。
- en: Kafka
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka
- en: Allows the consumption of streaming data from Apache Kafka.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 允许从Apache Kafka消费流数据。
- en: Socket
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 套接字
- en: A TCP socket client able to connect to a TCP server and consume a text-based
    data stream. The stream must be encoded in the UTF-8 character set.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个TCP套接字客户端能够连接到TCP服务器并消费文本数据流。流必须使用UTF-8字符集进行编码。
- en: Rate
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 速率
- en: Produces an internally generated stream of `(timestamp, value)` records with
    a configurable production rate. This is normally used for learning and testing
    purposes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 生成具有可配置生产率的内部生成的`(timestamp, value)`记录流。通常用于学习和测试目的。
- en: 'As we discussed in [“Understanding Sources”](#understanding_sources), sources
    are considered reliable when they provide replay capabilities from an offset,
    even when the structured streaming process fails. Using this criterion, we can
    classify the available sources as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[“理解数据源”](#understanding_sources)中讨论的那样，当结构化流处理失败时，源提供从偏移量回放的能力时，认为这些源是可靠的。根据此标准，我们可以将可用的源分类如下：
- en: Reliable
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠
- en: File source, Kafka source
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 文件源、Kafka源
- en: Unreliable
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不可靠
- en: Socket source, Rate source
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 套接字源、速率源
- en: The unreliable sources may be used in a production system only when the loss
    of data can be tolerated.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 不可靠的源仅在可以容忍数据丢失时才可用于生产系统。
- en: Warning
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The streaming source API is currently undergoing evolution. As of this writing,
    there is no stable public API to develop custom sources. This is expected to change
    in the near future.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 流源API目前正在不断发展中。截至目前为止，没有稳定的公共API来开发自定义源。预计在不久的将来会发生变化。
- en: In the next part of this chapter, we explore in detail the sources currently
    available. As production-ready sources, the File and the Kafka sources have many
    options that we discuss in detail. The Socket and the Rate source are limited
    in features, which will be evident by their concise coverage.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后续部分中，我们将详细探讨当前可用的数据来源。作为生产就绪的来源，文件和Kafka来源具有许多我们将详细讨论的选项。套接字和速率来源在功能上有所限制，这将通过其简明的覆盖来表现出来。
- en: The File Source
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件来源
- en: The File source is a simple streaming data source that reads files from a monitored
    directory in a filesystem. A file-based handover is a commonly used method to
    bridge a batch-based process with a streaming system. The batch process produces
    its output in a file format and drops it in a common directory where a suitable
    implementation of the File source can pick these files up and transform their
    contents into a stream of records for further processing in streaming mode.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 文件来源是一个简单的流数据来源，从监视的文件系统目录中读取文件。基于文件的交接是一种常用的方法，用于将基于批处理的过程与流式系统桥接起来。批处理过程以文件格式生成其输出，并将其放置在一个通用目录中，文件来源的适当实现可以捡起这些文件并将其内容转换为记录流，以进一步在流模式下处理。
- en: Specifying a File Format
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指定文件格式
- en: The files are read using a specified format, which is provided with the `.format(<format_name>)`
    method in the `readStream` builder, or by using the dedicated methods in the `DataStreamReader`
    that indicate the format to use; for example, `readStream.parquet('/path/to/dir/')`.
    When using the dedicated methods corresponding to each supported format, the method
    call should be done as the last call of the builder.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 文件使用指定的格式进行读取，该格式由`readStream`构建器的`.format(<format_name>)`方法提供，或者通过`DataStreamReader`的专用方法指定要使用的格式；例如，`readStream.parquet('/path/to/dir/')`。当使用每种支持格式对应的专用方法时，方法调用应作为构建器的最后一次调用。
- en: For example, the three forms in [Example 10-1](#file_source_builder) are equivalent.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[示例 10-1](#file_source_builder)中的三种形式是等效的。
- en: Example 10-1\. Building a FileStream
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-1\. 构建文件流
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As of Spark v2.3.0, the following file-based formats are supported by Structured
    Streaming. These are the same file formats supported by the static `DataFrame`,
    `Dataset`, and SQL APIs:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 截至Spark v2.3.0，结构化流支持以下基于文件的格式。这些是静态`DataFrame`、`Dataset`和SQL API支持的相同文件格式：
- en: CSV
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV
- en: JSON
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON
- en: Parquet
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet
- en: ORC
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ORC
- en: text
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本
- en: textFile
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: textFile
- en: Common Options
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见选项
- en: Regardless of the specific format, the general functionality of the File source
    is to monitor a directory in a shared filesystem identified by its specific URL.
    All file formats support a common set of options that control the file inflow
    and define the aging criteria of the files.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 无论具体格式如何，文件来源的一般功能是监视由其特定URL标识的共享文件系统中的目录。所有文件格式都支持一组通用选项，这些选项控制文件流入并定义文件的老化标准。
- en: Warning
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: As Apache Spark is a fast-evolving project, APIs and their options might change
    in future versions. Also, in this section, we cover only the most relevant options
    that apply to streaming workloads. For the most up-to-date information, always
    check the API [documentation corresponding to your Spark version](http://spark.apache.org/).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Apache Spark是一个快速发展的项目，API及其选项可能会在未来版本中发生变化。此外，在本节中，我们仅涵盖适用于流处理工作负载的最相关选项。要获取最新信息，请始终查看与您的Spark版本对应的API
    [文档](http://spark.apache.org/)。
- en: 'These options can be set for all file-based sources:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选项可以设置给所有基于文件的来源：
- en: '`maxFilesPerTrigger` (Default: unset)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`maxFilesPerTrigger`（默认：未设置）'
- en: Indicates how many files will be consumed at each query trigger. This setting
    limits the number of files processed at each trigger, and in doing so, it helps
    to control the data inflow in the system.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 指示每个查询触发器将消耗多少文件。此设置限制每个触发器处理的文件数量，从而帮助控制系统中的数据流入。
- en: '`latestFirst` (Default: `false`)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`latestFirst`（默认：`false`）'
- en: When this flag is set to `true`, newer files are elected for processing first.
    Use this option when the most recent data has higher priority over older data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当设置此标志为`true`时，较新的文件优先用于处理。当最新数据的优先级高于旧数据时，请使用此选项。
- en: '`maxFileAge` (Default: `7 days`)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`maxFileAge`（默认：`7 days`）'
- en: Defines an age threshold for the files in the directory. Files older than the
    threshold will not be eligible for processing and will be effectively ignored.
    This threshold is relative to the most recent file in the directory and not to
    the system clock. For example, if `maxFileAge` is `2 days` and the most recent
    file is from yesterday, the threshold to consider a file too old will be *older
    than three days ago*. This dynamic is similar to watermarks on event time.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 目录中为文件定义了一个年龄阈值。比阈值更老的文件将不符合处理条件，将被有效地忽略。这个阈值是相对于目录中最近的文件而不是系统时钟的。例如，如果`maxFileAge`是`2
    days`，并且最近的文件是昨天的，那么考虑文件是否太旧的阈值将会是*三天前之前的*。这种动态类似于事件时间上的水印。
- en: '`fileNameOnly` (Default: `false`)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`fileNameOnly`（默认为`false`）'
- en: When set to `true`, two files will be considered the same if they have the same
    name; otherwise, the full path will be considered.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当设置为`true`时，如果两个文件具有相同的名称，则它们将被视为相同；否则，将考虑完整路径。
- en: Note
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When `latestFirst` is set to `true` and the `maxFilesPerTrigger` option is configured,
    `maxFileAge` is ignored because there might be a condition in which files that
    are valid for processing become older than the threshold because the system gives
    priority to the most recent files found. In such cases, no aging policy can be
    set.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当设置`latestFirst`为`true`并配置了`maxFilesPerTrigger`选项时，将忽略`maxFileAge`，因为可能会出现这样一种情况，即为了处理而变得比阈值更老的文件。在这种情况下，无法设置老化策略。
- en: Common Text Parsing Options (CSV, JSON)
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见文本解析选项（CSV、JSON）
- en: Some file formats, such as CSV and JSON, use a configurable parser to transform
    the text data in each file into structured records. It’s possible for upstream
    processes to create records that do not fulfill the expected format. These records
    are considered corrupted.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一些文件格式，如CSV和JSON，使用可配置的解析器将每个文件中的文本数据转换为结构化记录。上游进程可能会创建不符合预期格式的记录，这些记录被视为损坏。
- en: Streaming systems are characterized by their continuous running. A streaming
    process should not fail when bad data is received. Depending on the business requirements,
    we can either drop the invalid records or route the data considered corrupted
    to a separate error-handling flow.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 流式系统的特点是其持续运行。当接收到坏数据时，流处理过程不应该失败。根据业务需求，我们可以丢弃无效记录或将被视为损坏的数据路由到单独的错误处理流程。
- en: Handing parsing errors
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理解析错误
- en: 'The following options allow for the configuration of the parser behavior to
    handle those records that are considered corrupted:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下选项允许配置解析器的行为，以处理那些被视为损坏的记录：
- en: '`mode` (default `PERMISSIVE`)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`mode`（默认为`PERMISSIVE`）'
- en: Controls the way that corrupted records are handled during parsing. Allowed
    values are `PERMISSIVE`, `DROPMALFORMED`, and `FAILFAST`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 控制解析过程中处理损坏记录的方式。允许的值包括`PERMISSIVE`、`DROPMALFORMED`和`FAILFAST`。
- en: '`PERMISSIVE`: The value of the corrupted record is inserted in a special field
    configured by the option `columnNameOfCorruptRecord` that must exist in the schema.
    All other fields are set to `null`. If the field does not exist, the record is
    dropped (same behavior as `DROPMALFORMED`).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PERMISSIVE`：损坏记录的值将插入由选项`columnNameOfCorruptRecord`配置的特殊字段中，该字段必须存在于模式中。所有其他字段将设置为`null`。如果字段不存在，则记录将被丢弃（与`DROPMALFORMED`的行为相同）。'
- en: '`DROPMALFORMED`: Corrupted records are dropped.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DROPMALFORMED`：丢弃损坏的记录。'
- en: '`FAILFAST`: An exception is thrown when a corrupted record is found. This method
    is not recommended in a streaming process, because the propagation of the exception
    will potentially make the streaming process fail and stop.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FAILFAST`：发现损坏记录时会抛出异常。在流处理中不推荐使用此方法，因为异常的传播可能会导致流处理失败并停止。'
- en: '`columnNameOfCorruptRecord` (default: “_corrupt_record”)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`columnNameOfCorruptRecord`（默认为“_corrupt_record”）'
- en: Permits the configuration of the special field that contains the string value
    of malformed records. This field can also be configured by setting `spark.sql.columnNameOfCorruptRecord`
    in the Spark configuration. If both `spark.sql.columnNameOfCorruptRecord` and
    this option are set, this option takes precedence.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 允许配置包含损坏记录的字符串值的特殊字段。还可以通过设置Spark配置中的`spark.sql.columnNameOfCorruptRecord`来配置此字段。如果同时设置了`spark.sql.columnNameOfCorruptRecord`和此选项，则此选项优先。
- en: Schema inference
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模式推断
- en: '`inferSchema` (default: `false`)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`inferSchema`（默认为`false`）'
- en: Schema inference is not supported. Setting this option is ignored. Providing
    a *schema* is mandatory.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 不支持模式推断。设置此选项将被忽略。提供*模式*是强制性的。
- en: Date and time formats
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日期和时间格式
- en: '`dateFormat` (default: `"yyyy-MM-dd"`)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`dateFormat` (默认："yyyy-MM-dd")'
- en: Configures the pattern used to parse `date` fields. Custom patterns follow the
    formats defined at [java.text.SimpleDateFormat](https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 配置用于解析 `date` 字段的模式。自定义模式遵循在[java.text.SimpleDateFormat](https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html)中定义的格式。
- en: '`timestampFormat` (default: `"yyyy-MM-dd''T''HH:mm:ss.SSSXXX"`)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`timestampFormat` (默认："yyyy-MM-dd''T''HH:mm:ss.SSSXXX")'
- en: Configures the pattern used to parse `timestamp` fields. Custom patterns follow
    the formats defined at [java.text.SimpleDateFormat](https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 配置用于解析 `timestamp` 字段的模式。自定义模式遵循在[java.text.SimpleDateFormat](https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html)中定义的格式。
- en: JSON File Source Format
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JSON 文件源格式
- en: The JSON format support for the File source lets us consume text files encoded
    as JSON, in which each line in the file is expected to be a valid JSON object.
    The JSON records are parsed using the provided schema. Records that do not follow
    the schema are considered invalid, and there are several options available to
    control the handling of invalid records.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 文件源的 JSON 格式支持我们消耗以 JSON 编码的文本文件，其中文件中的每一行都被期望是一个有效的 JSON 对象。使用提供的模式解析 JSON
    记录。不符合模式的记录被视为无效，有多个选项可用于控制无效记录的处理。
- en: JSON parsing options
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JSON 解析选项
- en: By default, the JSON File source expects the file contents to follow the [JSON
    Lines specification](http://jsonlines.org/). That is, each independent line in
    the file corresponds to a valid JSON document that complies with the specified
    schema. Each line should be separated by a newline (`\n`) character. A `CRLF`
    character (`\r\n`) is also supported because trailing white spaces are ignored.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，JSON 文件源期望文件内容遵循[JSON Lines 规范](http://jsonlines.org/)。即，文件中的每一行对应一个符合指定模式的有效
    JSON 文档。每行应以换行符（`\n`）分隔。还支持 `CRLF` 字符（`\r\n`），因为会忽略尾随空白。
- en: 'We can tweak the tolerance of the JSON parser to process data that does not
    fully comply with the standard. It’s also possible to change the behavior to handle
    those records that are considered corrupted. The following options allow for the
    configuration of the parser behavior:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调整 JSON 解析器的容错性，以处理不完全符合标准的数据。还可以更改处理被视为损坏的记录的行为。以下选项允许配置解析器的行为：
- en: '`allowComments` (default: `false`)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`allowComments` (默认："false")'
- en: 'When enabled, comments in Java/C++ style are allowed in the file and the corresponding
    line will be ignored; for example:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 启用后，允许文件中的 Java/C++ 风格的注释，并且将忽略相应的行；例如：
- en: '[PRE4]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Otherwise, comments in the JSON file are considered corrupted records and handled
    following the `mode` setting.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，在 JSON 文件中的注释将被视为损坏记录，并根据 `mode` 设置进行处理。
- en: '`allowNumericLeadingZeros` (default: `false`)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`allowNumericLeadingZeros` (默认："false")'
- en: When enabled, leading zeros in numbers are allowed (e.g., 00314). Otherwise,
    the leading zeros are considered invalid numeric values, the corresponding record
    is deemed corrupted, and it is handled following the `mode` setting.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 启用后，数字中的前导零将被允许（例如，00314）。否则，前导零将被视为无效的数值，相应的记录将被视为损坏，并且会根据 `mode` 设置进行处理。
- en: '`allowSingleQuotes` (default: `true`)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`allowSingleQuotes` (默认："true")'
- en: 'Allows the use of single quotes to demark fields. When enabled, both single
    quotes and double quotes are allowed. Regardless of this setting, quote characters
    cannot be nested and must be properly escaped when used within a value; for example:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 允许使用单引号作为字段的标记。启用后，单引号和双引号都被允许。无论此设置如何，引号字符不能嵌套，并且在值中使用时必须适当地转义；例如：
- en: '[PRE5]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`allowUnquotedFieldNames` (default: `false`)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`allowUnquotedFieldNames` (默认："false")'
- en: Allows unquoted JSON field names (e.g., `{firstname:"Alice"}`). Note that it’s
    not possible to have spaces in field names when using this option (e.g., `{first
    name:"Alice"}` is considered corrupted even when the field name matches the schema).
    Use with caution.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 允许未引用的 JSON 字段名（例如，`{firstname:"Alice"}`）。注意，当使用此选项时，字段名中不能包含空格（例如，`{first name:"Alice"}`
    被视为损坏，即使字段名与模式匹配）。请谨慎使用。
- en: '`multiLine` (default: `false`)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiLine` (默认："false")'
- en: When enabled, instead of parsing JSON Lines, the parser will consider the contents
    of each file as a single valid JSON document and will attempt to parse its contents
    as records following the defined schema.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 启用后，解析器将不再解析 JSON Lines，而是将每个文件的内容视为单个有效的 JSON 文档，并尝试将其内容解析为遵循定义模式的记录。
- en: Use this option when the producer of the file can output only complete JSON
    documents as files. In such cases, use a top-level array to group the records,
    as shown in [Example 10-2](#json_array).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当文件的生产者只能输出完整的JSON文档作为文件时，请使用此选项。在这种情况下，使用顶层数组来分组记录，如[示例 10-2](#json_array)所示。
- en: Example 10-2\. Using a top-level array to group records
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-2\. 使用顶层数组分组记录
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`primitivesAsString` (default `false`)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`primitivesAsString` (默认 `false`)'
- en: When enabled, primitive value types are considered strings. This allows you
    to read documents having fields of mixed types, but all values are read as a `String`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当启用时，原始值类型被视为字符串。这允许您读取具有混合类型字段的文档，但所有值都将作为`String`读取。
- en: In [Example 10-3](#primitives_as_strings) the resulting `age` field is of type
    `String`, containing values `age="15"` for *“Coraline”* and `age="unknown"` for
    *“Diana”*.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 10-3](#primitives_as_strings)中，生成的 `age` 字段的类型为 `String`，包含 *“Coraline”*
    的 `age="15"` 值和 *“Diana”* 的 `age="unknown"` 值。
- en: Example 10-3\. primitivesAsString usage
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-3\. `primitivesAsString` 的使用
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: CSV File Source Format
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CSV 文件源格式
- en: CSV is a popular tabular data storage and exchange format that is widely supported
    by enterprise applications. The `File Source` CSV format support allows us to
    ingest and process the output of such applications in a Structured Streaming application.
    Although the name “CSV” originally indicated that values are separated by commas,
    often the separation character can be freely configured. There are many configuration
    options available to control the way data is transformed from plain text to structured
    records.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 是一种流行的表格数据存储和交换格式，被企业应用广泛支持。`File Source` CSV 格式支持允许我们在结构化流应用程序中摄取和处理这些应用程序的输出。尽管“CSV”原来的名称表示值是用逗号分隔的，但分隔符通常可以自由配置。有许多配置选项可用于控制将数据从纯文本转换为结构化记录的方式。
- en: 'In the rest of this section, we cover the most common options and, in particular,
    those that are relevant for the streaming process. For formatting-related options,
    refer to the [latest documentation](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader%40csv.html).
    These are the most commonly used CSV parsing options:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们涵盖了最常见的选项，特别是那些与流处理相关的选项。关于格式相关的选项，请参考[最新文档](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader%40csv.html)。以下是最常用的CSV解析选项：
- en: CSV parsing options
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CSV 解析选项
- en: '`comment` (default: “” [disabled])'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`comment` (默认：“” [禁用])'
- en: 'Configures a character that marks lines considered as comments; for example,
    when using `option("comment","#")`, we can parse the following CSV with a comment
    in it:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 配置用于标记为注释行的字符；例如，当使用 `option("comment","#")` 时，我们可以解析以下包含注释的CSV：
- en: '[PRE8]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`header` (default: `false`)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`header` (默认：`false`)'
- en: Given that a schema must be provided, the header line is ignored and has no
    effect.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 需要提供模式，因此标题行将被忽略并且不起作用。
- en: '`multiline` (default: `false`)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiline` (默认：`false`)'
- en: Consider each file as one record spanning all of the lines in the file.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个文件视为一个跨越文件中所有行的记录。
- en: '`quote` (default: `"` [double quote])'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`quote` (默认：`"` [双引号])'
- en: Configures the character used to enclose values that contain a column separator.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 配置用于包围包含列分隔符的值的字符。
- en: '`sep` (default: `,` [comma])'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`sep` (默认：`,` [逗号])'
- en: Configures the character that separates the fields in each line.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 配置用于分隔每行中字段的字符。
- en: Parquet File Source Format
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquet 文件源格式
- en: Apache Parquet is a column-oriented, file-based data storage format. The internal
    representation splits original rows into chunks of columns that are stored using
    compression techniques. As a consequence, queries that require specific columns
    do not need to read the complete file. Instead, the relevant pieces can be independently
    addressed and retrieved. Parquet supports complex nested data structures and preserves
    the schema structure of the data. Due to its enhanced query capabilities, efficient
    use of storage space and the preservation of schema information, Parquet is a
    popular format for storing large, complex datasets.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet 是一种基于列的文件数据存储格式。内部表示将原始行拆分为列的块，并使用压缩技术进行存储。因此，需要特定列的查询无需读取完整文件，而是可以独立地访问和检索相关的片段。Parquet
    支持复杂的嵌套数据结构，并保留数据的模式结构。由于其增强的查询功能、有效利用存储空间和保留模式信息，Parquet 是存储大型复杂数据集的流行格式。
- en: Schema definition
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模式定义
- en: To create a streaming source from Parquet files, it is sufficient to provide
    the schema of the data and the directory location. The schema provided during
    the streaming declaration is fixed for the duration of the streaming source definition.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 Parquet 文件创建流源，只需提供数据的模式和目录位置。在流声明期间提供的模式对于流源定义的整个期间是固定的。
- en: '[Example 10-4](#parquet_file_source_builder) shows the creation of a Parquet-based
    File source from a folder in `hdfs://data/folder` using the provided schema.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 10-4](#parquet_file_source_builder) 展示了如何使用提供的模式从`hdfs://data/folder`文件夹创建基于
    Parquet 的文件源。'
- en: Example 10-4\. Building a Parquet source example
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-4\. 构建 Parquet 源示例
- en: '[PRE9]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Text File Source Format
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本文件源格式
- en: The text format for the File source supports the ingestion of plain-text files.
    Using configuration options, it’s possible to ingest the text either line by line
    or the entire file as a single text blob. The schema of the data produced by this
    source is naturally `StringType` and does not need to be specified. This is a
    generic format that we can use to ingest arbitrary text for further programmatic
    processing, from the famous word count to custom parsing of proprietary text formats.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 文件来源的文本格式支持纯文本文件的摄取。使用配置选项，可以逐行或整个文件作为单个文本块进行摄取。此来源产生的数据模式自然是`StringType`，无需指定。这是一个通用格式，我们可以用来摄取任意文本，从著名的词频统计到专有文本格式的自定义解析。
- en: Text ingestion options
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本摄取选项
- en: 'Next to the common options for the File source that we saw in [“Common Options”](#file_source_common_options),
    the text file format supports reading text files as a whole using the `wholetext`
    option:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们在 [“常见选项”](#file_source_common_options) 中看到的文件源的常见选项之外，文本文件格式支持使用`wholetext`选项将文本文件作为整体读取：
- en: '`wholetext` (default `false`)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`wholetext`（默认为 `false`）'
- en: If true, read the complete file as a single text blob. Otherwise, split the
    text into lines using the standard line separators (`\n`, `\r\n`, `\r`) and consider
    each line a record.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设置为 true，则将整个文件作为单个文本块读取。否则，使用标准的行分隔符（`\n`、`\r\n`、`\r`）将文本拆分成行，并将每行视为一个记录。
- en: text and textFile
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: text 和 textFile
- en: 'The text format supports two API alternatives:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 文本格式支持两种 API 替代方案：
- en: '`text`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`text`'
- en: Returns a dynamically typed `DataFrame` with a single `value` field of type
    `StringType`
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个动态类型的带有单个`value`字段的`DataFrame`，类型为`StringType`
- en: '`textFile`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`textFile`'
- en: Returns a statically typed `Dataset[String]`
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个静态类型的`Dataset[String]`
- en: We can use the `text` format specification as a terminating method call or as
    a `format` option. To obtain a statically typed `Dataset`, we must use `textFile`
    as the last call of the stream builder call. The examples in [Example 10-5](#text_file_source_builder)
    illustrate the specific API usage.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`text`格式规范作为终止方法调用或作为`format`选项。要获得静态类型的`Dataset`，必须将`textFile`作为流构建器调用的最后一步。[示例 10-5](#text_file_source_builder)
    中的示例展示了具体的 API 使用方式。
- en: Example 10-5\. Text format API usage
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-5\. 文本格式 API 使用
- en: '[PRE10]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The Kafka Source
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka 源
- en: Apache Kafka is a Publish/Subscribe (pub/sub) system based on the concept of
    a distributed log. Kafka is highly scalable and offers high-throughput, low-latency
    handling of the data at the consumer and producer sides. In Kafka, the unit of
    organization is a topic. Publishers send data to a topic and subscribers receive
    data from the topic to which they subscribed. This data delivery happens in a
    reliable way. Apache Kafka has become a popular choice of messaging infrastructure
    for a wide range of streaming use cases.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka 是基于分布式日志概念的发布/订阅（pub/sub）系统。Kafka 具有高度可扩展性，并且在消费者和生产者端处理数据时提供高吞吐量和低延迟的处理能力。在
    Kafka 中，组织的单位是主题。发布者将数据发送到主题，订阅者从他们订阅的主题接收数据。这种数据交付是可靠的。Apache Kafka 已成为广泛应用于各种流处理用例的消息基础设施的热门选择。
- en: The Structured Streaming source for Kafka implements the subscriber role and
    can consume data published to one or several topics. This is a reliable source.
    Recall from our discussion in [“Understanding Sources”](#understanding_sources),
    this means that data delivery semantics are guaranteed even in case of partial
    or total failure and restart of the streaming process.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 的结构化流源实现了订阅者角色，可以消费发布到一个或多个主题的数据。这是一个可靠的数据源。回顾我们在 [“理解数据源”](#understanding_sources)
    中的讨论，这意味着即使部分或完全失败并重新启动流处理过程，数据交付语义也是有保证的。
- en: Setting Up a Kafka Source
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 Kafka 源
- en: 'To create a Kafka source, we use the `format("kafka")` method with the `createStream`
    builder on the Spark Session. We need two mandatory parameters to connect to Kafka:
    the addresses of the Kafka brokers, and the topic(s) to which we want to connect.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个 Kafka 源，我们在 Spark 会话中使用 `format("kafka")` 方法和 `createStream` 构建器。连接到 Kafka
    需要两个必需的参数：Kafka 代理的地址和我们想要连接的主题。
- en: The address of the Kafka brokers to connect to is provided through the option
    `kafka.bootstrap.servers` as a `String` containing a comma-separated list of `host:port`
    pairs.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到的 Kafka 代理的地址通过选项 `kafka.bootstrap.servers` 提供，作为一个包含逗号分隔的 `host:port` 对列表的
    `String`。
- en: '[Example 10-6](#simple_kafka_source) presents a simple Kafka source definition.
    It subscribes to a single topic, `topic1`, by connecting to the brokers located
    at `host1:port1`, `host2:port2`, and `host3:port3`.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 10-6](#simple_kafka_source) 展示了一个简单的 Kafka 源定义。它通过连接位于 `host1:port1`、`host2:port2`
    和 `host3:port3` 的代理订阅单个主题 `topic1`。'
- en: Example 10-6\. Creating a Kafka source
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-6\. 创建一个 Kafka 源
- en: '[PRE11]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The result of that call is a `DataFrame` with five fields: `key`, `value`,
    `topic`, `partition`, `offset`, `timestamp`, `timestampType`. Having a schema
    consisting of these five fields is fixed the Kafka source. It provides the raw
    `key` and `values` from Kafka and the metadata of each consumed record.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 该调用的结果是一个包含五个字段的 `DataFrame`：`key`、`value`、`topic`、`partition`、`offset`、`timestamp`、`timestampType`。具有这些字段的模式固定了
    Kafka 源。它提供了从 Kafka 获取的原始 `key` 和 `values` 以及每个消费记录的元数据。
- en: Usually, we are interested in only the `key` and `value` of the message. Both
    the `key` and the `value` contain a binary payload, represented internally as
    a `Byte Array`. When the data is written to Kafka using a `String` serializer,
    we can read that data back by casting the values to `String` as we have done in
    the last expression in the example. Although text-based encoding is a common practice,
    it’s not the most space-efficient way of exchanging data. Other encodings, such
    as the schema-aware AVRO format, might offer a better space efficiency with the
    added benefit of embedding the schema information.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们只对消息的 `key` 和 `value` 感兴趣。`key` 和 `value` 都包含一个二进制负载，在内部表示为 `Byte Array`。当使用
    `String` 序列化器将数据写入 Kafka 时，我们可以通过将值强制转换为 `String` 来读取数据，如示例中的最后一个表达式所示。尽管基于文本的编码是一种常见做法，但这不是交换数据的最节省空间的方式。其他编码，如具有模式感知的
    AVRO 格式，可能提供更好的空间效率，并附带嵌入模式信息的好处。
- en: The additional metadata in the message, such as `topic`, `partition`, or `offset`
    can be used in more complex scenarios. For example, the topic field will contain
    the *topic* that produced the record and could be used as a label or discriminator,
    in case we subscribe to several topics at the same time.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 消息中的附加元数据，例如 `topic`、`partition` 或 `offset`，可以在更复杂的场景中使用。例如，`topic` 字段将包含生成记录的
    *主题*，可以用作标签或分辨器，以防我们同时订阅多个主题。
- en: Selecting a Topic Subscription Method
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择一个主题订阅方法
- en: 'There are three different ways to specify the topic or topics that we want
    to consume:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种不同的方法来指定我们想要消费的主题或主题列表：
- en: '`subscribe`'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subscribe`'
- en: '`subscribePattern`'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subscribePattern`'
- en: '`assign`'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assign`'
- en: 'The Kafka source setup must contain one and only one of these subscription
    options. They provide different levels of flexibility to select the topic(s) and
    even the partitions to subscribe to:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 源设置必须包含这些订阅选项中的一个且仅一个。它们提供不同的灵活性级别，以选择要订阅的主题和甚至分区：
- en: '`subscribe`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`subscribe`'
- en: 'Takes a single topic or a list of comma-separated topics: `topic1, topic2,
    ..., topicn`. This method subscribes to each topic and creates a single, unified
    stream with the data of the union of all the topics; for example, `.option("subscribe",
    "topic1,topic3")`.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接受一个单一主题或逗号分隔的主题列表：`topic1, topic2, ..., topicn`。此方法订阅每个主题，并创建一个统一的流，包含所有主题的数据的并集；例如，`.option("subscribe",
    "topic1,topic3")`。
- en: '`subscribePattern`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`subscribePattern`'
- en: This is similar to `subscribe` in behavior, but the topics are specified with
    a regular expression pattern. For example, if we have topics `'factory1Sensors',
    'factory2Sensors', 'street1Sensors', 'street2Sensors'`, we can subscribe to all
    “factory” sensors with the expression `.option("subscribePattern", "factory[\\d]+Sensors")`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于 `subscribe` 的行为，但主题使用正则表达式模式指定。例如，如果我们有主题 `'factory1Sensors'、'factory2Sensors'、'street1Sensors'、'street2Sensors'`，我们可以通过表达式
    `.option("subscribePattern", "factory[\\d]+Sensors")` 订阅所有“工厂”传感器。
- en: '`assign`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`assign`'
- en: Allows the fine-grained specification of specific partitions per topic to consume.
    This is known as `TopicPartition`s in the Kafka API. The partitions per topic
    are indicated using a JSON object, in which each key is a topic and its value
    is an array of partitions. For example, the option definition `.option("assign",
    """{"sensors":[0,1,3]}""")` would subscribe to the partitions 0, 1, and 3 of topic
    *sensors*. To use this method we need information about the topic partitioning.
    We can obtain the partition information programmatically by using the Kafka API
    or through configuration.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 允许针对特定主题的特定分区进行精细化的指定以进行消费。在Kafka API中称为`TopicPartition`。通过JSON对象指定每个主题的分区，其中每个键是一个主题，其值是分区数组。例如，选项定义`.option("assign",
    """{"sensors":[0,1,3]}""")`将订阅主题*sensors*的分区0、1和3。要使用此方法，我们需要关于主题分区的信息。我们可以通过Kafka
    API或配置获取分区信息。
- en: Configuring Kafka Source Options
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Kafka源选项
- en: 'There are two categories of configuration options for the Structured Streaming
    source for Kafka: dedicated source configuration, and pass-through options that
    are given directly to the underlying Kafka consumer.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理中Kafka源有两类配置选项：专用源配置和直接传递给底层Kafka消费者的选项。
- en: Kafka source-specific options
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kafka源特定选项
- en: 'The following options configure the behavior of the Kafka source. They relate
    in particular to how offsets are consumed:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 下列选项配置了Kafka源的行为，特别是关于如何消费偏移量的配置：
- en: '`startingOffsets` (default: `latest`)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`startingOffsets`（默认值：`latest`）'
- en: 'Accepted values are `earliest`, `latest`, or a JSON object representing an
    association between topics, their partitions, and a given offset. Actual offset
    values are always positive numbers. There are two special offset values: `-2`
    denotes `earliest` and `-1` means `latest`; for example, `""" {"topic1": { "0":
    -1, "1": -2, "2":1024 }} """`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '可接受的值为`earliest`、`latest`或表示主题、它们的分区及给定偏移量关联的JSON对象。实际偏移量值始终为正数。有两个特殊的偏移量值：`-2`表示`earliest`，`-1`表示`latest`；例如，`"""
    {"topic1": { "0": -1, "1": -2, "2":1024 }} """`'
- en: '`startingOffsets` are only used the first time a query is started. All subsequent
    restarts will use the *checkpoint* information stored. To restart a streaming
    job from a specific offset, we need to remove the contents of the *checkpoint*.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`startingOffsets`仅在首次启动查询时使用。所有后续重新启动将使用存储的*checkpoint*信息。要从特定偏移量重新启动流作业，需要删除*checkpoint*的内容。'
- en: '`failOnDataLoss` (default: `true`)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`failOnDataLoss`（默认值：`true`）'
- en: This flag indicates whether to fail the restart of a streaming query in case
    data might be lost. This is usually when offsets are out of range, topics are
    deleted, or topics are rebalanced. We recommend setting this option to `false`
    during the develop/test cycle because stop/restart of the query side with a continuous
    producer will often trigger a failure. Set this back to `true` for production
    deployment.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 此标志指示在可能会丢失数据的情况下是否失败重新启动流查询。通常是当偏移量超出范围、主题被删除或主题重新平衡时。我们建议在开发/测试周期中将此选项设置为`false`，因为使用连续生产者停止/重新启动查询端往往会触发失败。在生产部署时将其设置回`true`。
- en: '`kafkaConsumer.pollTimeoutMs` (default: `512`)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`kafkaConsumer.pollTimeoutMs`（默认值：`512`）'
- en: The poll timeout (in milliseconds) to wait for data from Kafka in the distributed
    consumers running on the Spark executors.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行在Spark执行器上的分布式消费者从Kafka获取数据时，等待数据的轮询超时（毫秒）。
- en: '`fetchOffset.numRetries` (default: `3`)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`fetchOffset.numRetries`（默认值：`3`）'
- en: The number of retries before failing the fetch of Kafka offsets.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 获取Kafka偏移量失败之前的重试次数。
- en: '`fetchOffset.retryIntervalMs` (default: `10`)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`fetchOffset.retryIntervalMs`（默认值：`10`）'
- en: The delay between offset fetch retries in milliseconds.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 偏移量获取重试之间的延迟（毫秒）。
- en: '`maxOffsetsPerTrigger` (default: not set)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`maxOffsetsPerTrigger`（默认值：未设置）'
- en: This option allows us to set a rate limit to the number of total records to
    be consumed at each query trigger. The limit configured will be equally distributed
    among the set of partitions of the subscribed topics.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 此选项允许我们为每个查询触发器设置总记录的速率限制。配置的限制将均匀分布在订阅主题的分区集合中。
- en: Kafka Consumer Options
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka消费者选项
- en: It’s possible to pass configuration options through to the underlying Kafka
    consumer of this source. We do this by adding a `'kafka.'` prefix to the configuration
    key that we want to set.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过向配置键添加 `'kafka.'` 前缀来将配置选项传递到此源的底层Kafka消费者。
- en: For example, to configure Transport Layer Security (TLS) options for the Kafka
    source, we can set the Kafka consumer configuration option `security.protocol`
    by setting `kafka.security.protocol` in the source configuration.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要为 Kafka 源配置传输层安全性（TLS）选项，可以通过在源配置中设置 Kafka 消费者配置选项 `security.protocol` 来设置
    `kafka.security.protocol`。
- en: '[Example 10-7](#tls-configuration-kafka) demonstrates how to configure TLS
    for the Kafka source using this method.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 10-7](#tls-configuration-kafka) 展示了如何使用此方法为 Kafka 源配置 TLS。'
- en: Example 10-7\. Kafka source TLS configuration example
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-7\. Kafka 源 TLS 配置示例
- en: '[PRE12]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For an exhaustive listing of the Kafka consumer configuration options, refer
    to the official [Kafka documentation](http://bit.ly/2HnFl63).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取 Kafka 消费者配置选项的详尽列表，请参阅官方的 [Kafka 文档](http://bit.ly/2HnFl63)。
- en: Banned configuration options
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 禁止的配置选项
- en: Not all options from the standard consumer configuration can be used because
    they conflict with the internal process of the source, which is controlled with
    the settings we saw in [“Kafka source-specific options”](#kafka_specific_options).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有标准消费者配置选项都可以使用，因为它们与源的内部处理过程冲突，该过程受我们在 [“Kafka 源特定选项”](#kafka_specific_options)
    中看到的设置控制。
- en: These options are prohibited, as shown in [Table 10-1](#tab1001). This means
    that attempting to use any of them will result in an `IllegalArgumentException`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选项被禁止使用，如 [表 10-1](#tab1001) 所示。这意味着尝试使用任何这些选项都将导致 `IllegalArgumentException`。
- en: Table 10-1\. Banned Kafka options
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-1\. 禁止的 Kafka 选项
- en: '| option | Reason | Alternative |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| option | 原因 | 备选方案 |'
- en: '| --- | --- | --- |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `auto.offset.reset` | Offsets are managed within Structured Streaming | use
    `startingOffsets` instead |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| `auto.offset.reset` | 在结构化流中管理偏移量 | 使用 `startingOffsets` 替代 |'
- en: '| `enable.auto.commit` | Offsets are managed within Structured Streaming |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| `enable.auto.commit` | 在结构化流中管理偏移量 |  |'
- en: '| `group.id` | A unique group ID is managed internally per query |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| `group.id` | 每个查询内部管理唯一的组 ID |  |'
- en: '| `key.deserializer` | Payload is always represented as a `Byte Array` | Deserialization
    into specific formats is done programmatically |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| `key.deserializer` | 负载始终表示为 `Byte Array` | 可以通过编程方式将其反序列化为特定格式 |'
- en: '| `value.deserializer` | Payload is always represented as a `Byte Array` |
    Deserialization into specific formats is done programmatically |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| `value.deserializer` | 负载始终表示为 `Byte Array` | 可以通过编程方式将其反序列化为特定格式 |'
- en: '| `interceptor.classes` | A consumer interceptor might break the internal data
    representation |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| `interceptor.classes` | 消费者拦截器可能会破坏内部数据表示 |  |'
- en: The Socket Source
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Socket 源
- en: The Transmission Control Protocol (or TCP) is a connection-oriented protocol
    that enables bidirectional communication between a client and a server. This protocol
    underpins many of the higher-level communication protocols over the internet,
    such as FTP, HTTP, MQTT, and many others. Although application layer protocols
    like HTTP add additional semantics on top of a TCP connection, there are many
    applications that offer a vanilla, text-based TCP connection over a UNIX socket
    to deliver data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 传输控制协议（TCP）是一种连接导向的协议，它使客户端和服务器之间能够进行双向通信。该协议支持互联网上许多高级通信协议，如 FTP、HTTP、MQTT
    等。虽然应用层协议如 HTTP 在 TCP 连接之上添加了额外的语义，但许多应用程序仍通过 UNIX 套接字提供纯文本的 TCP 连接以传输数据。
- en: The Socket source is a TCP socket client able to connect to a TCP server that
    offers a UTF-8 encoded text-based data stream. It connects to a TCP server using
    a `host` and `port` provided as mandatory `options`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Socket 源是一个 TCP 套接字客户端，能够连接到提供 UTF-8 编码的基于文本的数据流的 TCP 服务器。它使用提供的 `host` 和 `port`
    作为必选的 `options` 连接到 TCP 服务器。
- en: Configuration
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置
- en: To connect to a TCP server, we need the address of the host and a port number.
    It’s also possible to configure the Socket source to add the timestamp at which
    each line of data is received.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到 TCP 服务器，我们需要主机的地址和端口号。还可以配置 Socket 源以在接收到每行数据时添加时间戳。
- en: 'These are the configuration options:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是配置选项：
- en: '`host` (mandatory)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`host`（必选）'
- en: The DNS hostname or IP address of the TCP server to which to connect.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接的 TCP 服务器的 DNS 主机名或 IP 地址。
- en: '`port` (mandatory)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`port`（必选）'
- en: The port number of the TCP server to which to connect.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接的 TCP 服务器的端口号。
- en: '`includeTimestamp` (default: `false`)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`includeTimestamp`（默认：`false`）'
- en: When enabled, the Socket source adds the timestamp of arrival to each line of
    data. It also changes the schema produced by this *source*, adding the `timestamp`
    as an additional field.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 当启用时，Socket源会将到达时间戳添加到每行数据中。它还会更改此*源*生成的模式，添加`timestamp`作为附加字段。
- en: In [Example 10-8](#socket_source_examples), we observe the two modes of operation
    that this source offers. With the `host`, `port` configuration, the resulting
    streaming `DataFrame` has a single field named `value` of type `String`. When
    we add the flag `includeTimestamp` set to `true`, the schema of the resulting
    streaming `DataFrame` contains the fields `value` and `timestamp`, where `value`
    is of type `String` as before, and `timestamp` is of type `Timestamp`. Also, observe
    the log warning this source prints at creation.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 10-8](#socket_source_examples)中，我们观察到此源提供的两种操作模式。通过`host`、`port`配置，生成的流`DataFrame`只有一个名为`value`的字段，类型为`String`。当我们将`includeTimestamp`标志设置为`true`时，生成的流`DataFrame`的模式包含字段`value`和`timestamp`，其中`value`与之前相同为`String`类型，而`timestamp`为`Timestamp`类型。同时，请注意此源创建时打印的日志警告。
- en: Example 10-8\. Socket source examples
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-8\. Socket源示例
- en: '[PRE13]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Operations
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作
- en: The Socket source creates a TCP client that connects to a TCP server specified
    in the configuration. This client runs on the Spark driver. It keeps the incoming
    data in memory until it is consumed by the query and the corresponding offset
    is committed. The data from committed offsets is evicted, keeping the memory usage
    stable under normal circumstances.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Socket源创建一个TCP客户端，连接到配置中指定的TCP服务器。此客户端在Spark驱动程序上运行。它将传入数据保存在内存中，直到查询消耗该数据并提交相应的偏移量。已提交偏移量的数据将被驱逐，在正常情况下保持内存使用稳定。
- en: Recall from the discussion in [“Understanding Sources”](#understanding_sources),
    a source is considered reliable if it can replay an uncommitted offset even in
    the event of failure and restart of the streaming process. This source is not
    considered to be reliable, because a failure of the Spark driver will result in
    losing all uncommitted data in memory.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾[“理解数据源”](#understanding_sources)中的讨论，如果源能在失败和流程重启时重放未提交的偏移量，则认为该源是可靠的。但这个源不可靠，因为Spark驱动程序的失败将导致内存中的所有未提交数据丢失。
- en: This source should be used only for cases in which data loss is acceptable for
    the use case.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在对数据丢失可接受的情况下才应使用此源。
- en: Note
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A common architectural alternative to directly connecting to a TCP server using
    the Socket source is to use Kafka as a reliable intermediate storage. A robust
    microservice can be used to bridge the TCP server and Kafka. This microservice
    collects the data from the TCP server and delivers it atomically to Kafka. Then,
    we can use the reliable Kafka source to consume the data and further process it
    in Structured Streaming.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的架构替代方案是，使用Kafka作为可靠的中间存储而不是直接使用Socket源连接到TCP服务器。可以使用强大的微服务来桥接TCP服务器和Kafka。该微服务从TCP服务器收集数据，并以原子方式将其传送到Kafka。然后，我们可以使用可靠的Kafka源来消费数据，并在结构化流中进一步处理。
- en: The Rate Source
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rate源
- en: 'The Rate source is an internal stream generator that produces a sequence of
    records at a configurable frequency, given in `records/second`. The output is
    a stream of records `(timestamp, value)` where `timestamp` corresponds to the
    moment of generation of the record, and `value` is an ever-increasing counter:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Rate源是一个内部流生成器，以可配置的频率生成一系列记录，单位为`records/second`。输出是一系列记录`(timestamp, value)`，其中`timestamp`对应于记录生成时刻，而`value`是递增计数器：
- en: '[PRE14]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This is intended for benchmarks and for exploring Structured Streaming, given
    that it does not rely on external systems to work. As we can appreciate in the
    previous example, it is very easy to create and completely self-contained.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 此内容旨在用于基准测试和探索结构化流，因为它不依赖于外部系统的正常运行。正如前面的示例所示，这非常容易创建并完全自包含。
- en: 'The code in [Example 10-9](#rate_stream_example) creates a rate stream of 100
    rows per second with a ramp-up time of 60 seconds. The schema of the resulting
    `DataFrame` contains two fields: `timestamp` of type `Timestamp`, and `value`,
    which is a `BigInt` at schema level, and a `Long` in the internal representation.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 10-9](#rate_stream_example)中的代码创建了一个每秒100行的速率流，具有60秒的逐步增加时间。生成的`DataFrame`的模式包含两个字段：类型为`Timestamp`的`timestamp`和模式级别为`BigInt`、内部表示为`Long`的`value`。'
- en: Example 10-9\. Rate source example
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-9\. Rate源示例
- en: '[PRE15]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Options
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选项
- en: 'The Rate source supports a few options that control the throughput and level
    of parallelism:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Rate source支持几个选项，用于控制吞吐量和并行级别：
- en: '`rowsPerSecond` (default: 1)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`rowsPerSecond`（默认：1）'
- en: The number of rows to generate each second.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 每秒生成的行数。
- en: '`rampUpTime` (default: 0)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`rampUpTime`（默认：0）'
- en: At the start of the stream, the generation of records will increase progressively
    until this time has been reached. The increase is linear.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在流的开始阶段，记录的生成将逐渐增加，直到达到设定的时间。增长是线性的。
- en: '`numPartitions` (default: default spark parallelism level)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`numPartitions`（默认：默认的Spark并行级别）'
- en: The number of partitions to generate. More partitions increase the parallelism
    level of the record generation and downstream query processing.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成的分区数。更多的分区可以增加记录生成和下游查询处理的并行级别。
