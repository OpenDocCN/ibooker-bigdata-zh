- en: Chapter 5\. The DataStream API (v1.7)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。DataStream API（v1.7）
- en: This chapter introduces the basics of Flink’s DataStream API. We show the structure
    and components of a typical Flink streaming application, discuss Flink’s type
    systems and the supported data types, and present data and partitioning transformations.
    Window operators, time-based transformations, stateful operators, and connectors
    are discussed in the next chapters. After reading this chapter, you will know
    how to implement a stream processing application with basic functionality. Our
    code examples use Scala for conciseness, but the Java API is mostly analogous
    (exceptions or special cases will be pointed out). We also provide complete example
    applications implemented in Java and Scala in our [GitHub repositories](https://github.com/streaming-with-flink/).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了Flink的DataStream API的基础知识。我们展示了典型Flink流应用程序的结构和组件，讨论了Flink的类型系统和支持的数据类型，并展示了数据和分区转换。窗口操作符、基于时间的转换、有状态操作符和连接器将在接下来的章节中讨论。阅读完本章后，您将了解如何实现具有基本功能的流处理应用程序。我们的代码示例使用Scala以确保简洁性，但Java
    API在大多数情况下是类似的（会指出例外或特殊情况）。我们还在我们的[GitHub存储库](https://github.com/streaming-with-flink/)中提供了用Java和Scala实现的完整示例应用程序。
- en: Hello, Flink!
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hello, Flink!
- en: Let’s start with a simple example to get a first impression of what it is like
    to write streaming applications with the DataStream API. We will use this example
    to showcase the basic structure of a Flink program and introduce some important
    features of the DataStream API. Our example application ingests a stream of temperature
    measurements from multiple sensors.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的示例开始，以了解使用DataStream API编写流应用程序的基本结构和一些重要特性。我们将使用此示例展示Flink程序的基本结构，并介绍DataStream
    API的一些重要功能。我们的示例应用程序从多个传感器中摄取温度测量流。
- en: 'First, let’s have a look at the data type we will be using to represent sensor
    readings:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一下我们将用于表示传感器读数的数据类型：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The program in [Example 5-1](#convert-temps) converts the temperatures from
    Fahrenheit to Celsius and computes the average temperature every 5 seconds for
    each sensor.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 示例中的程序（[示例 5-1](#convert-temps)）将温度从华氏度转换为摄氏度，并每5秒计算一次每个传感器的平均温度。
- en: Example 5-1\. Compute the average temperature every 5 seconds for a stream of
    sensors
  id: totrans-7
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-1。每5秒计算一次传感器流的平均温度
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You have probably already noticed that Flink programs are defined and submitted
    for execution in regular Scala or Java methods. Most commonly, this is done in
    a static main method. In our example, we define the `AverageSensorReadings` object
    and include most of the application logic inside `main()`.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，Flink程序是在常规Scala或Java方法中定义并提交执行的。通常情况下，这是在一个静态的main方法中完成的。在我们的示例中，我们定义了`AverageSensorReadings`对象，并在`main()`内包含大部分应用逻辑。
- en: 'To structure a typical Flink streaming application:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建典型的Flink流应用程序：
- en: Set up the execution environment.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置执行环境。
- en: Read one or more streams from data sources.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据源中读取一个或多个流。
- en: Apply streaming transformations to implement the application logic.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用流处理转换以实现应用逻辑。
- en: Optionally output the result to one or more data sinks.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选地将结果输出到一个或多个数据汇。
- en: Execute the program.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行程序。
- en: We now look at these parts in detail.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们详细看看这些部分。
- en: Set Up the Execution Environment
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置执行环境
- en: The first thing a Flink application needs to do is set up its *execution environment*.
    The execution environment determines whether the program is running on a local
    machine or on a cluster. In the DataStream API, the execution environment of an
    application is represented by the `StreamExecutionEnvironment`. In our example,
    we retrieve the execution environment by calling the static `getExecutionEnvironment()`
    method. This method returns a local or remote environment, depending on the context
    in which the method is invoked. If the method is invoked from a submission client
    with a connection to a remote cluster, a remote execution environment is returned.
    Otherwise, it returns a local environment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Flink应用程序首先需要做的是设置它的*执行环境*。执行环境决定程序是在本地机器上运行还是在集群上运行。在DataStream API中，应用程序的执行环境由`StreamExecutionEnvironment`表示。在我们的示例中，我们通过调用静态的`getExecutionEnvironment()`方法来获取执行环境。此方法根据调用方法时的上下文返回本地或远程环境。如果该方法从具有到远程集群的连接的提交客户端调用，则返回远程执行环境。否则，返回本地环境。
- en: 'It is also possible to explicitly create local or remote execution environments
    as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以按如下方式显式创建本地或远程执行环境：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we use `env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)`
    to instruct our program to interpret time semantics using event time. The execution
    environment offers more configuration options, such as setting the program parallelism
    and enabling fault tolerance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)`指示程序使用事件时间语义解释时间。执行环境提供了更多的配置选项，例如设置程序并行度和启用容错性。
- en: Read an Input Stream
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取输入流
- en: Once the execution environment has been configured, it is time to do some actual
    work and start processing streams. The `StreamExecutionEnvironment` provides methods
    to create stream sources that ingest data streams into the application. Data streams
    can be ingested from sources such as message queues or files, or also be generated
    on the fly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行环境配置完成，就可以开始处理流并进行一些实际工作。`StreamExecutionEnvironment`提供了方法来创建流源，将数据流导入应用程序。数据流可以从消息队列或文件等源摄取，也可以动态生成。
- en: 'In our example, we use:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: to connect to the source of the sensor measurements and create an initial `DataStream`
    of type `SensorReading`. Flink supports many data types, which we describe in
    the next section. Here, we use a Scala case class as the data type that we defined
    before. A `SensorReading` contains the sensor ID, a timestamp denoting when the
    measurement was taken, and the measured temperature. The `assignTimestampsAndWatermarks(new
    SensorTimeAssigner)` method assigns the timestamps and watermarks that are required
    for event time. The implementation details of `SensorTimeAssigner` do not concern
    us right now.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到传感器测量源并创建初始类型为`SensorReading`的`DataStream`。Flink支持许多数据类型，我们将在下一节中描述。这里，我们使用Scala
    case class作为我们之前定义的数据类型。`SensorReading`包含传感器ID、表示测量时间的时间戳以及测量的温度。`assignTimestampsAndWatermarks(new
    SensorTimeAssigner)`方法分配事件时间所需的时间戳和水印。现在我们不需要关心`SensorTimeAssigner`的实现细节。
- en: Apply Transformations
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用转换
- en: Once we have a `DataStream`, we can apply a transformation on it. There are
    different types of transformations. Some transformations can produce a new `DataStream`,
    possibly of a different type, while other transformations do not modify the records
    of the `DataStream` but reorganize it by partitioning or grouping. The logic of
    an application is defined by chaining transformations.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了`DataStream`，我们可以对其进行转换。有不同类型的转换。一些转换可以产生新的`DataStream`，可能是不同类型的，而其他转换则不修改`DataStream`的记录，但重新组织它通过分区或分组。应用程序的逻辑通过链式转换来定义。
- en: 'In our example, we first apply a `map()` transformation that converts the temperature
    of each sensor reading to Celsius. Then, we use the `keyBy()` transformation to
    partition the sensor readings by their sensor ID. Next, we define a `timeWindow()` transformation,
    which groups the sensor readings of each sensor ID partition into tumbling windows
    of 5 seconds:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们首先应用了一个`map()`转换，将每个传感器读数的温度转换为摄氏度。然后，我们使用`keyBy()`转换来根据传感器ID对传感器读数进行分区。接下来，我们定义了一个`timeWindow()`转换，将每个传感器ID分区的传感器读数分组为5秒的滚动窗口：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Window transformations are described in detail in [“Window Operators”](ch06.html#chap-6-windows).
    Finally, we apply a user-defined function that computes the average temperature
    on each window. We discuss the implementation of a user-defined function in a
    later section of this chapter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口转换在[“窗口操作符”](ch06.html#chap-6-windows)中有详细描述。最后，我们应用一个用户定义的函数，在每个窗口上计算平均温度。我们将在本章后面的部分讨论用户定义函数的实现。
- en: Output the Result
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出结果
- en: Streaming applications usually emit their results to some external system, such
    as Apache Kafka, a filesystem, or a database. Flink provides a well-maintained
    collection of stream sinks that can be used to write data to different systems.
    It is also possible to implement your own streaming sinks. There are also applications
    that do not emit results but keep them internally to serve them via Flink’s queryable
    state feature.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 流应用通常将其结果发射到某个外部系统，例如Apache Kafka、文件系统或数据库。Flink提供了一组良好维护的流接收器集合，可以用于将数据写入不同的系统。还可以实现自己的流接收器。还有一些应用程序不会发射结果，而是保留在内部以通过Flink的可查询状态功能提供服务。
- en: 'In our example, the result is a `DataStream[SensorReading]` record. Every record
    contains an average temperature of a sensor over a period of 5 seconds. The result
    stream is written to the standard output by calling `print()`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，结果是一个`DataStream[SensorReading]`记录。每个记录包含传感器在5秒内的平均温度。调用`print()`方法将结果流写入标准输出：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the choice of a streaming sink affects the end-to-end consistency
    of an application, whether the result of the application is provided with at-least
    once or exactly-once semantics. The end-to-end consistency of the application
    depends on the integration of the chosen stream sinks with Flink’s checkpointing
    algorithm. We will discuss this topic in more detail in [“Application Consistency
    Guarantees”](ch08.html#chap-8-guarantees).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，流式汇聚器的选择会影响应用程序的端到端一致性，无论应用程序的结果是以至少一次还是精确一次语义提供的。应用程序的端到端一致性取决于所选流汇聚器与Flink的检查点算法的集成。我们将在[“应用程序一致性保证”](ch08.html#chap-8-guarantees)中更详细地讨论此主题。
- en: Execute
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行
- en: 'When the application has been completely defined, it can be executed by calling `StreamExecutionEnvironment.execute()`.
    This is the last call in our example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序完全定义后，可以通过调用`StreamExecutionEnvironment.execute()`来执行。这是我们示例中的最后一次调用：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Flink programs are executed lazily. That is, the API calls that create stream
    sources and transformations do not immediately trigger any data processing. Instead,
    the API calls construct an execution plan in the execution environment, which
    consists of the stream sources created from the environment and all transformations
    that were transitively applied to these sources. Only when `execute()` is called
    does the system trigger the execution of the program.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Flink程序是惰性执行的。也就是说，创建流源和转换的API调用不会立即触发任何数据处理。相反，API调用在执行环境中构建执行计划，该计划由从环境创建的流源和所有应用于这些源的传递转换组成。只有在调用`execute()`时，系统才会触发程序的执行。
- en: The constructed plan is translated into a JobGraph and submitted to a JobManager
    for execution. Depending on the type of execution environment, a JobManager is
    started as a local thread (local execution environment) or the JobGraph is sent
    to a remote JobManager. If the JobManager runs remotely, the JobGraph must be
    shipped together with a JAR file that contains all classes and required dependencies
    of the application.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 构建的计划被翻译成一个JobGraph，并提交给JobManager进行执行。根据执行环境的类型，JobManager可以作为本地线程（本地执行环境）启动，或者JobGraph被发送到远程JobManager。如果JobManager远程运行，则必须将JobGraph与包含应用程序所有类和所需依赖项的JAR文件一起发送。
- en: Transformations
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换
- en: In this section we give an overview of the basic transformations of the DataStream
    API. Time-related operators such as window operators and other specialized transformations
    are described in later chapters. A stream transformation is applied on one or
    more streams and converts them into one or more output streams. Writing a DataStream
    API program essentially boils down to combining such transformations to create
    a dataflow graph that implements the application logic.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了DataStream API的基本转换。像窗口操作符和其他专业转换这样的与时间相关的操作符在后续章节中描述。流转换应用于一个或多个流，并将它们转换为一个或多个输出流。编写DataStream
    API程序主要是将这些转换组合起来，以创建实现应用程序逻辑的数据流图。
- en: 'Most stream transformations are based on user-defined functions. The functions
    encapsulate the user application logic and define how the elements of the input
    stream are transformed into the elements of the output stream. Functions, such
    as `MapFunction` in the following, are defined as classes that implement a transformation-specific
    function interface:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数流转换都基于用户定义的函数。这些函数封装了用户应用逻辑，并定义了如何将输入流的元素转换为输出流的元素。例如，`MapFunction`等函数被定义为实现特定转换函数接口的类：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The function interface defines the transformation method that needs to be implemented
    by the user, such as the `map()` method in the example above.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 函数接口定义了需要用户实现的转换方法，例如上面示例中的`map()`方法。
- en: Most function interfaces are designed as SAM (single abstract method) interfaces
    and they can be implemented as Java 8 lambda functions. The Scala DataStream API
    also has built-in support for lambda functions. When presenting the transformations
    of the DataStream API, we show the interfaces for all function classes, but mostly
    use lambda functions instead of function classes in code examples for brevity.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数函数接口都设计为SAM（单一抽象方法）接口，可以作为Java 8 lambda函数实现。Scala DataStream API也内置了对lambda函数的支持。在展示DataStream
    API的转换时，我们展示了所有函数类的接口，但在代码示例中为简洁起见，大多数情况下使用lambda函数而不是函数类。
- en: 'The DataStream API provides transformations for the most common data transformation
    operations. If you are familiar with batch data processing APIs, functional programming
    languages, or SQL you will find the API concepts very easy to grasp. We present
    the transformations of the DataStream API in four categories:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: DataStream API 提供了最常见数据转换操作的转换方法。如果您熟悉批处理数据处理 API、函数式编程语言或 SQL，您会发现 API 的概念非常容易理解。我们将
    DataStream API 的转换分为四类：
- en: Basic transformations are transformations on individual events.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基本转换是对单个事件的转换。
- en: '`KeyedStream` transformations are transformations that are applied to events
    in the context of a key.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`KeyedStream` 转换是在键的上下文中应用于事件的转换。'
- en: Multistream transformations merge multiple streams into one stream or split
    one stream into multiple streams.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多流转换将多个流合并为一个流，或将一个流拆分为多个流。
- en: Distribution transformations reorganize stream events.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分布转换重新组织流事件。
- en: Basic Transformations
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本转换
- en: Basic transformations process individual events, meaning that each output record
    was produced from a single input record. Simple value conversions, splitting of
    records, or filtering of records are examples of common basic functions. We explain
    their semantics and show code examples.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 基本转换处理单个事件，这意味着每个输出记录都是从单个输入记录生成的。简单值转换、记录分割或记录过滤是常见的基本函数示例。我们解释它们的语义并展示代码示例。
- en: Map
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Map
- en: The map transformation is specified by calling the `DataStream.map()` method
    and produces a new `DataStream`. It passes each incoming event to a user-defined
    mapper that returns exactly one output event, possibly of a different type. [Figure 5-1](#map-operation)
    shows a map transformation that converts every square into a circle.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `DataStream.map()` 方法指定映射转换，并生成新的 `DataStream`。它将每个传入的事件传递给用户定义的映射器，后者返回一个完全相同类型的输出事件。[图 5-1](#map-operation)
    显示了将每个方块转换为圆形的映射转换。
- en: '![](assets/spaf_0501.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0501.png)'
- en: Figure 5-1\. A map operation that transforms every square into a circle of the
    same color
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 将每个方块转换为相同颜色圆形的映射操作
- en: 'The `MapFunction` is typed to the types of the input and output events and
    can be specified using the `MapFunction` interface. It defines the `map()` method
    that transforms an input event into exactly one output event:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`MapFunction` 的类型与输入和输出事件的类型相匹配，并可以使用 `MapFunction` 接口指定。它定义了 `map()` 方法，将输入事件转换为一个输出事件：'
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is a simple mapper that extracts the first field (`id`) of each
    `SensorReading` in the input stream:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的映射器，从输入流中提取每个 `SensorReading` 的第一个字段（`id`）：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When using the Scala API or Java 8, the mapper can also be expressed as a lambda
    function:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Scala API 或 Java 8 时，映射器也可以表示为 Lambda 函数：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Filter
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过滤
- en: The filter transformation drops or forwards events of a stream by evaluating
    a boolean condition on each input event. A return value of `true` preserves the
    input event and forwards it to the output, and `false` results in dropping the
    event. A filter transformation is specified by calling the `DataStream.filter()`
    method and produces a new `DataStream` of the same type as the input `DataStream`. [Figure 5-2](#filter-operation)
    shows a filter operation that only preserves white squares.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤转换通过对每个输入事件上的布尔条件进行评估来丢弃或转发流的事件。返回值为 `true` 保留输入事件并将其转发到输出，而 `false` 则会丢弃事件。调用
    `DataStream.filter()` 方法指定过滤转换，并生成与输入 `DataStream` 类型相同的新 `DataStream`。[图 5-2](#filter-operation)
    显示了仅保留白色方块的过滤操作。
- en: '![](assets/spaf_0502.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0502.png)'
- en: Figure 5-2\. A filter operation that only retains white values
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. 仅保留白色值的过滤操作
- en: 'The boolean condition is implemented as a function either using the `FilterFunction`
    interface or a lambda function. The `FilterFunction` interface is typed to the
    type of the input stream and defines the `filter()` method that is called with
    an input event and returns a boolean:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 布尔条件实现为一个函数，可以使用 `FilterFunction` 接口或 Lambda 函数。`FilterFunction` 接口与输入流的类型匹配，并定义了调用输入事件并返回布尔值的
    `filter()` 方法：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following example shows a filter that drops all sensor measurements with
    temperature below 25°F:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了一个过滤器，该过滤器丢弃所有温度低于 25°F 的传感器测量值：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: FlatMap
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FlatMap
- en: The flatMap transformation is similar to map, but it can produce zero, one,
    or more output events for each incoming event. In fact, the flatMap transformation
    is a generalization of filter and map and can be used to implement both those
    operations. [Figure 5-3](#flatmap-operation) shows a flatMap operation that differentiates
    its output based on the color of the incoming event. If the input is a white square,
    it outputs the event unmodified. Black squares are duplicated, and gray squares
    are filtered out.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: flatMap 转换类似于 map，但它可以为每个传入事件生成零个、一个或多个输出事件。事实上，flatMap 转换是 filter 和 map 的泛化，可以用来实现这两个操作。[图
    5-3](#flatmap-operation) 展示了一个 flatMap 操作，根据传入事件的颜色区分其输出。如果输入是白色方块，则输出未修改的事件。黑色方块被复制，灰色方块被过滤掉。
- en: '![A flatMap operation that outputs white squares, duplicates black squares,
    and drops gray squares](assets/spaf_0503.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![执行 flatMap 操作，输出白色方块、复制黑色方块和丢弃灰色方块](assets/spaf_0503.png)'
- en: Figure 5-3\. A flatMap operation that outputs white squares, duplicates black
    squares, and drops gray squares
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 执行 flatMap 操作，输出白色方块、复制黑色方块和丢弃灰色方块
- en: 'The flatMap transformation applies a function on each incoming event. The corresponding `FlatMapFunction`
    defines the `flatMap()` method, which may return zero, one, or more events as
    results by passing them to the `Collector` object:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: flatMap 转换在每个传入事件上应用一个函数。相应的 `FlatMapFunction` 定义了 `flatMap()` 方法，通过将结果传递给 `Collector`
    对象，可以返回零个、一个或多个事件：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This example shows a flatMap transformation commonly found in data processing
    tutorials. The function is applied on a stream of sentences, splits each sentence
    by the space character, and emits each resulting word as an individual record:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例展示了在数据处理教程中常见的 flatMap 转换。该函数应用于一系列句子的流，将每个句子按空格字符拆分，并将每个结果单词作为单独的记录发出。
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: KeyedStream Transformations
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 键控流转换
- en: A common requirement of many applications is to process groups of events that
    share a certain property together. The DataStream API features the abstraction
    of a `KeyedStream`, which is a `DataStream` that has been logically partitioned
    into disjoint substreams of events that share the same key.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用程序的常见需求是一起处理具有某种属性的事件组。DataStream API 提供了 `KeyedStream` 的抽象，它是一个被逻辑分成不相交子流的
    `DataStream`，这些子流共享相同的键。
- en: Stateful transformations that are applied on a `KeyedStream` read from and write
    to state in the context of the currently processed event’s key. This means that
    all events with the same key access the same state and thus can be processed together.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于 `KeyedStream` 上的有状态转换在当前处理事件的键的上下文中读取和写入状态。这意味着具有相同键的所有事件访问相同的状态，因此可以一起处理。
- en: Note
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that stateful transformations and keyed aggregates have to be used with
    care. If the key domain is continuously growing—for example, because the key is
    a unique transaction ID—you have to clean up state for keys that are no longer
    active to avoid memory problems. Refer to [“Implementing Stateful Functions”](ch07.html#chap-7-stateful-functions), which
    discusses stateful functions in detail.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，有状态的转换和键控聚合必须小心使用。如果键域不断增长，例如因为键是唯一的事务 ID，您必须清理不再活动的键的状态，以避免内存问题。详见 [“实现有状态函数”](ch07.html#chap-7-stateful-functions)，该部分详细讨论了有状态函数。
- en: A `KeyedStream` can be processed using the map, flatMap, and filter transformations
    that you saw earlier. In the following, we will use a keyBy transformation to
    convert a `DataStream` into a `KeyedStream` and keyed transformations such as
    rolling aggregations and `reduce`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`KeyedStream` 可以使用您之前看到的 map、flatMap 和 filter 转换进行处理。接下来，我们将使用 keyBy 转换将 `DataStream`
    转换为 `KeyedStream`，并使用键控转换，如滚动聚合和 reduce。'
- en: keyBy
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: keyBy
- en: The keyBy transformation converts a `DataStream` into a `KeyedStream` by specifying
    a key. Based on the key, the events of the stream are assigned to partitions,
    so that all events with the same key are processed by the same task of the subsequent
    operator. Events with different keys can be processed by the same task, but the
    keyed state of a task’s function is always accessed in the scope of the current
    event’s key.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: keyBy 转换通过指定键将 `DataStream` 转换为 `KeyedStream`。根据键，流的事件被分配到分区中，以便由后续操作符的同一任务处理具有相同键的所有事件。具有不同键的事件可以由同一任务处理，但任务函数的键控状态始终在当前事件键的范围内访问。
- en: Considering the color of the input event as the key, [Figure 5-4](#keyby-operation)
    assigns black events to one partition and all other events to another partition.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑输入事件的颜色作为键，[图 5-4](#keyby-operation) 将黑色事件分配给一个分区，将所有其他事件分配给另一个分区。
- en: '![A keyBy operation that partitions events based on color](assets/spaf_0504.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![基于颜色对事件进行分区的 keyBy 操作](assets/spaf_0504.png)'
- en: Figure 5-4\. A keyBy operation that partitions events based on color
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. 基于颜色对事件进行分区的 keyBy 操作
- en: 'The `keyBy()` method receives an argument that specifies the key (or keys)
    to group by and returns a `KeyedStream`. There are different ways to specify keys.
    We cover them in [“Defining Keys and Referencing Fields”](#defining-keys). The
    following code declares the `id` field as the key of a stream of `SensorReading`
    records:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`keyBy()` 方法接收一个指定要按其分组的键（或键）的参数，并返回一个 `KeyedStream`。有不同的方法指定键。我们在 [“定义键和引用字段”](#defining-keys)
    中进行了介绍。以下代码将 `id` 字段声明为 `SensorReading` 记录流的键：'
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The lambda function `r => r.id` extracts the `id` field of a sensor reading
    `r`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 函数 `r => r.id` 从传感器读数 `r` 中提取 `id` 字段。
- en: Rolling aggregations
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 滚动聚合
- en: 'Rolling aggregation transformations are applied on a `KeyedStream` and produce
    a `DataStream` of aggregates, such as sum, minimum, and maximum. A rolling aggregate
    operator keeps an aggregated value for every observed key. For each incoming event,
    the operator updates the corresponding aggregate value and emits an event with
    the updated value. A rolling aggregation does not require a user-defined function
    but receives an argument that specifies on which field the aggregate is computed.
    The DataStream API provides the following rolling aggregation methods:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动聚合转换应用于 `KeyedStream`，生成包括求和、最小值和最大值在内的 `DataStream` 聚合。滚动聚合运算符为每个观察到的键保留一个聚合值。对于每个传入的事件，运算符更新相应的聚合值并发出带有更新值的事件。滚动聚合不需要用户定义的函数，但接收一个指定计算聚合的字段的参数。DataStream
    API 提供以下滚动聚合方法：
- en: '`sum()`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`sum()`'
- en: A rolling sum of the input stream on the specified field.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 输入流的指定字段的滚动求和。
- en: '`min()`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`min()`'
- en: A rolling minimum of the input stream on the specified field.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 输入流的指定字段的滚动最小值。
- en: '`max()`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`max()`'
- en: A rolling maximum of the input stream on the specified field.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 输入流的指定字段的滚动最大值。
- en: '`minBy()`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`minBy()`'
- en: A rolling minimum of the input stream that returns the event with the lowest
    value observed so far.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 输入流的滚动最小值返回迄今为止观察到的最低值的事件。
- en: '`maxBy()`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`maxBy()`'
- en: A rolling maximum of the input stream that returns the event with the highest
    value observed so far.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 输入流的指定字段的滚动最大值。
- en: It is not possible to combine multiple rolling aggregation methods—only a single
    rolling aggregate can be computed at a time.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 无法组合多个滚动聚合方法，一次只能计算单个滚动聚合。
- en: 'Consider the following example of keying a stream of `Tuple3[Int, Int, Int]`
    on the first field and computing a rolling sum on the second field:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑下面的示例，对 `Tuple3[Int, Int, Int]` 流的第一个字段进行键控，并在第二个字段上计算滚动求和：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this example the tuple input stream is keyed by the first field and the rolling
    sum is computed on the second field. The output of the example is `(1,2,2)` followed
    by `(1,7,2)` for the key “1” and `(2,3,1)` followed by `(2,5,1)` for the key “2.”
    The first field is the common key, the second field is the sum, and the third
    field is not defined.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，元组输入流按第一个字段键控，并在第二个字段上计算滚动求和。示例的输出为键“1”时的 `(1,2,2)`，随后为 `(1,7,2)`，以及键“2”时的
    `(2,3,1)`，随后为 `(2,5,1)`。第一个字段是公共键，第二个字段是求和值，第三个字段未定义。
- en: Only Use Rolling Aggregations on Bounded Key Domains
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仅在有界键域上使用滚动聚合
- en: The rolling aggregate operator keeps a state for every key that is processed.
    Since this state is never cleaned up, you should only apply a rolling aggregations
    operator on a stream with a bounded key domain.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动聚合运算符为每个处理的键保留一个状态。由于此状态永远不会清除，应仅在具有有限键域的流上应用滚动聚合运算符。
- en: Reduce
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Reduce
- en: The reduce transformation is a generalization of the rolling aggregation. It
    applies a `ReduceFunction` on a `KeyedStream`, which combines each incoming event
    with the current reduced value, and produces a `DataStream`. A reduce transformation
    does not change the type of the stream. The type of the output stream is the same
    as the type of the input stream.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Reduce 转换是滚动聚合的一般化。它在 `KeyedStream` 上应用 `ReduceFunction`，将每个传入的事件与当前减少的值组合，并生成一个
    `DataStream`。Reduce 转换不会更改流的类型。输出流的类型与输入流的类型相同。
- en: 'The function can be specified with a class that implements the `ReduceFunction`
    interface. `ReduceFunction` defines the `reduce()` method, which takes two input
    events and returns an event of the same type:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 函数可以由实现 `ReduceFunction` 接口的类指定。`ReduceFunction` 定义了 `reduce()` 方法，该方法接受两个输入事件并返回相同类型的事件：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In the example below, the stream is keyed by language and the result is a continuously
    updated list of words per language:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，流以语言为键，结果是每种语言的持续更新单词列表：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The lambda reduce function forwards the first tuple field (the key field) and
    concatenates the `List[String]` values of the second tuple field.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 减少函数将第一个元组字段（键字段）转发，并连接第二个元组字段的`List[String]`值。
- en: Only Use Rolling Reduce on Bounded Key Domains
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 只在有界键域上使用滚动减少操作。
- en: The rolling reduce operator keeps a state for every key that is processed. Since
    this state is never cleaned up, you should only apply a rolling reduce operator
    on a stream with a bounded key domain.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动减少操作符会为处理的每个键保留状态。由于此状态从不清理，您应仅对具有有界键域的流应用滚动减少操作符。
- en: Multistream Transformations
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多流转换
- en: Many applications ingest multiple streams that need to be jointly processed
    or split a stream in order to apply different logic to different substreams. In
    the following, we discuss the DataStream API transformations that process multiple
    input streams or emit multiple output streams.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用程序接收需要共同处理的多个流或分割流以应用不同逻辑于不同子流。接下来我们讨论处理多个输入流或发出多个输出流的 DataStream API 转换。
- en: Union
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Union
- en: The `DataStream.union()` method merges two or more `DataStream`s of the same
    type and produces a new `DataStream` of the same type. Subsequent transformations
    process the elements of all input streams. [Figure 5-5](#union-operation) shows
    a union operation that merges black and gray events into a single output stream.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataStream.union()` 方法合并两个或更多相同类型的 `DataStream` 并生成相同类型的新 `DataStream`。后续转换处理所有输入流的元素。
    [图5-5](#union-operation) 显示将黑色和灰色事件合并为单个输出流的联合操作。'
- en: '![A union operation that merges two input streams into one](assets/spaf_0505.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![将两个输入流合并为一个联合操作](assets/spaf_0505.png)'
- en: Figure 5-5\. A union operation that merges two input streams into one
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5。将两个输入流合并为一个联合操作。
- en: The events are merged in a FIFO fashion—the operator does not produce a specific
    order of events. Moreover, the union operator does not perform duplication elimination.
    Every input event is emitted to the next operator.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 事件以 FIFO 方式合并 —— 操作符不生成特定顺序的事件。此外，联合操作符不执行重复消除。每个输入事件都发射到下一个操作符。
- en: 'The following shows how to union three streams of type `SensorReading` into
    a single stream:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下显示如何将三个 `SensorReading` 类型的流联合为单个流：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Connect, coMap, and coFlatMap
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Connect、coMap 和 coFlatMap
- en: Combining events of two streams is a very common requirement in stream processing.
    Consider an application that monitors a forest area and outputs an alert whenever
    there is a high risk of fire. The application receives the stream of temperature
    sensor readings you have seen previously and an additional stream of smoke level
    measurements. When the temperature is over a given threshold and the smoke level
    is high, the application emits a fire alert.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在流处理中，将两个流的事件结合起来是一个非常常见的需求。考虑一个监控森林区域并在存在火灾高风险时输出警报的应用程序。该应用程序接收先前见过的温度传感器读数流以及额外的烟雾水平测量流。当温度超过给定阈值且烟雾水平较高时，应用程序会发出火灾警报。
- en: 'The DataStream API provides the connect transformation to support such use
    cases.^([1](ch05.html#idm45499009375336)) The `DataStream.connect()` method receives
    a `DataStream` and returns a `ConnectedStreams` object, which represents the two
    connected streams:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: DataStream API 提供连接转换以支持这类用例。^([1](ch05.html#idm45499009375336)) `DataStream.connect()`
    方法接收一个 `DataStream` 并返回一个 `ConnectedStreams` 对象，表示这两个连接的流：
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `ConnectedStreams` object provides `map()` and `flatMap()` methods that
    expect a `CoMapFunction` and `CoFlatMapFunction` as argument respectively.^([2](ch05.html#idm45499009286248))
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConnectedStreams` 对象提供了期望 `CoMapFunction` 和 `CoFlatMapFunction` 作为参数的 `map()`
    和 `flatMap()` 方法。^([2](ch05.html#idm45499009286248))'
- en: 'Both functions are typed on the types of the first and second input stream
    and on the type of the output stream and define two methods—one for each input.
    `map1()` and `flatMap1()` are called to process an event of the first input and
    `map2()` and `flatMap2()` are invoked to process an event of the second input:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个功能在第一个和第二个输入流的类型以及输出流的类型上都有类型化，并定义了两个方法——分别用于每个输入的`map1()`和`flatMap1()`，以及用于处理第二个输入的`map2()`和`flatMap2()`：
- en: '[PRE21]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: A Function Cannot Choose Which ConnectedStreams to Read
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 函数不能选择读取哪些ConnectedStreams
- en: It is not possible to control the order in which the methods of a `CoMapFunction`
    or `CoFlatMapFunction` are called. Instead, a method is called as soon as an event
    has arrived via the corresponding input.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 无法控制`CoMapFunction`或`CoFlatMapFunction`方法的调用顺序。相反，一旦事件通过相应的输入到达，方法就会立即被调用。
- en: 'Joint processing of two streams usually requires that events of both streams
    are deterministically routed based on some condition to be processed by the same
    parallel instance of an operator. By default, `connect()` does not establish a
    relationship between the events of both streams so events of both streams are
    randomly assigned to operator instances. This behavior yields nondeterministic
    results and is usually undesirable. In order to achieve deterministic transformations
    on `ConnectedStreams`, `connect()` can be combined with `keyBy()` or `broadcast()`.
    We first show the `keyBy()` case:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对两个流的联合处理要求基于某些条件确定地路由两个流的事件，以便由操作符的同一个并行实例进行处理。默认情况下，`connect()`不会建立两个流事件之间的关系，因此两个流的事件会随机分配给操作符实例。这种行为会导致不确定的结果，通常是不希望的。为了在`ConnectedStreams`上实现确定性转换，`connect()`可以与`keyBy()`或`broadcast()`结合使用。首先我们展示了`keyBy()`的情况：
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Regardless of whether you `keyBy()` `ConnectedStreams` or you `connect()` two
    `KeyedStreams`, the `connect()` transformation will route all events from both
    streams with the same key to the same operator instance. Note that the keys of
    both streams should refer to the same class of entities, just like a join predicate
    in a SQL query. An operator that is applied on a connected and keyed stream has
    access to keyed state.^([3](ch05.html#idm45499009156168))
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您是通过`keyBy()` `ConnectedStreams`还是通过`connect()`连接两个`KeyedStreams`，`connect()`转换将会将来自两个流的具有相同键的所有事件路由到同一个操作符实例。请注意，两个流的键应该引用同一类实体，就像SQL查询中的连接谓词一样。应用于连接和键控流的操作符可以访问键控状态。^([3](ch05.html#idm45499009156168))
- en: 'The next example shows how to connect a (nonkeyed) `DataStream` with a broadcasted
    stream:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例展示了如何连接一个（非键控）`DataStream`和一个广播流：
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: All events of the broadcasted stream are replicated and sent to all parallel
    operator instances of the subsequent processing function. The events of the nonbroadcasted
    stream are simply forwarded. Hence, the elements of both input streams can be
    jointly processed.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 所有广播流的事件都会复制并发送到后续处理函数的所有并行操作符实例。非广播流的事件则会简单地转发。因此，两个输入流的元素可以共同处理。
- en: Note
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can use broadcast state to connect a keyed and a broadcast stream. Broadcast
    state is an improved version of the `broadcast()`-`connect()` transformation.
    It also supports connecting a keyed and a broadcasted stream and storing the broadcasted
    events in managed state. This allows you to implement operators that are dynamically
    configured via a data stream (e.g., to add or remove filtering rules or update
    machine-learning models). The broadcast state is discussed in detail in [“Using
    Connected Broadcast State”](ch07.html#chap-7-broadcast-state).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用广播状态来连接键控流和广播流。广播状态是`broadcast()`-`connect()`转换的改进版本。它还支持连接键控流和广播流，并将广播事件存储在受管理状态中。这允许您通过数据流动态配置操作符（例如添加或删除过滤规则或更新机器学习模型）。广播状态在[“使用连接广播状态”](ch07.html#chap-7-broadcast-state)中有详细讨论。
- en: Split and select
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分割和选择
- en: Split is the inverse transformation to the union transformation. It divides
    an input stream into two or more output streams of the same type as the input
    stream. Each incoming event can be routed to zero, one, or more output streams.
    Hence, split can also be used to filter or replicate events. [Figure 5-6](#split-operation)
    shows a split operator that routes all white events into a separate stream than
    the rest.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Split是Union变换的反向转换。它将输入流分成两个或多个与输入流类型相同的输出流。每个传入事件可以路由到零个、一个或多个输出流。因此，Split也可以用于过滤或复制事件。[图5-6](#split-operation)显示了一个将所有白色事件路由到一个单独流的Split操作符。
- en: '![A split operation that splits the input stream into a stream of white events
    and a stream of others](assets/spaf_0506.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![将输入流分成白色事件流和其他事件流的分裂操作](assets/spaf_0506.png)'
- en: Figure 5-6\. A split operation that splits the input stream into a stream of
    white events and a stream of others
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-6。将输入流分成白色事件流和其他事件流的分裂操作
- en: The `DataStream.split()` method receives an `OutputSelector` that defines how
    stream elements are assigned to named outputs. The `OutputSelector` defines the
    `select()` method that is called for each input event and returns a `java.lang.Iterable[String]`.
    The `String` values that are returned for a record specify the output streams
    to which the record is routed.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataStream.split()`方法接收一个`OutputSelector`，定义了如何将流元素分配给命名输出。`OutputSelector`定义了一个`select()`方法，该方法对每个输入事件调用并返回一个`java.lang.Iterable[String]`。返回的`String`值指定将记录路由到的输出流。'
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `DataStream.split()` method returns a `SplitStream`, which provides a `select()` method
    to select one or more streams from the `SplitStream` by specifying the output
    names.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataStream.split()`方法返回一个`SplitStream`，提供一个`select()`方法，通过指定输出名称从`SplitStream`中选择一个或多个流。'
- en: '[Example 5-2](#tuple-stream) splits a stream of numbers into a stream of large
    numbers and a stream of small numbers.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-2](#tuple-stream)将数字流分成大数流和小数流。'
- en: Example 5-2\. Split a tuple stream into a stream with large numbers and a stream
    with small numbers.
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-2。将元组流分成大数流和小数流。
- en: '[PRE26]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: One restriction of the split transformation is that all outgoing streams are
    of the same type as the input type. In [“Emitting to Side Outputs”](ch06.html#chap-6-side-output),
    we present the side-output feature of the process functions, which can emit multiple
    streams of different types from a function.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`split` 变换的一个限制是所有出站流都与输入类型相同。在[“发射到侧输出”](ch06.html#chap-6-side-output)中，我们介绍了处理函数的侧输出功能，它可以从一个函数中发射多个不同类型的流。'
- en: Distribution Transformations
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分发变换
- en: Partitioning transformations correspond to the data exchange strategies we introduced
    in [“Data Exchange Strategies”](ch02.html#chap-2-data-exchange). These operations
    define how events are assigned to tasks. When building applications with the DataStream
    API the system automatically chooses data partitioning strategies and routes data
    to the correct destination depending on the operation semantics and the configured
    parallelism. Sometimes it is necessary or desirable to control the partitioning
    strategies at the application level or define custom partitioners. For instance,
    if we know that the load of the parallel partitions of a `DataStream` is skewed,
    we might want to rebalance the data to evenly distribute the computation load
    of subsequent operators. Alternatively, the application logic might require that
    all tasks of an operation receive the same data or that events be distributed
    following a custom strategy. In this section, we present `DataStream` methods
    that enable users to control partitioning strategies or define their own.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 分区变换对应于我们在[“数据交换策略”](ch02.html#chap-2-data-exchange)中介绍的数据交换策略。这些操作定义了事件分配给任务的方式。使用DataStream
    API构建应用程序时，系统会自动选择数据分区策略，并根据操作语义和配置的并行性将数据路由到正确的目的地。有时需要或希望在应用程序级别控制分区策略或定义自定义分区器。例如，如果我们知道`DataStream`的并行分区负载不均衡，可能希望重新平衡数据，以均匀分配后续运算符的计算负载。或者，应用程序逻辑可能要求操作的所有任务接收相同的数据，或者事件根据自定义策略进行分发。在本节中，我们介绍了允许用户控制分区策略或定义自己的`DataStream`方法。
- en: Note
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that `keyBy()` is different from the distribution transformations discussed
    in this section. All transformations in this section produce a `DataStream` whereas
    `keyBy()` results in a `KeyedStream`, on which transformation with access to keyed
    state can be applied.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`keyBy()`与本节讨论的分发变换不同。本节中的所有变换都生成一个`DataStream`，而`keyBy()`会生成一个`KeyedStream`，可以在其中应用有关键状态访问的变换。
- en: Random
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 随机
- en: The random data exchange strategy is implemented by the `DataStream.shuffle()`
    method. The method distributes records randomly according to a uniform distribution
    to the parallel tasks of the following operator.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 随机数据交换策略由`DataStream.shuffle()`方法实现。该方法根据均匀分布随机分发记录给后续运算符的并行任务。
- en: Round-Robin
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 轮询
- en: The `rebalance()` method partitions the input stream so that events are evenly
    distributed to successor tasks in a round-robin fashion. [Figure 5-7](#rebalance-rescale)
    illustrates the round-robin distribution transformation.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`rebalance()` 方法将输入流分区，以便以轮询方式均匀分发事件给后续任务。[图5-7](#rebalance-rescale) 描述了轮询分发变换。'
- en: Rescale
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 重新缩放
- en: The `rescale()` method also distributes events in a round-robin fashion, but
    only to a subset of successor tasks. In essence, the rescale partitioning strategy
    offers a way to perform a lightweight load rebalance when the number of sender
    and receiver tasks is not the same. The rescale transformation is more efficient
    if the number of receiver tasks is a multitude of the number of sender tasks or
    vice versa.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`rescale()` 方法还以轮询方式分发事件，但仅分发给某些后续任务。从本质上讲，重新缩放分区策略提供了一种在发送任务数和接收任务数不相同时执行轻量级负载重新平衡的方法。如果接收任务数是发送任务数的倍数或反之，则重新缩放变换更有效率。'
- en: The fundamental difference between `rebalance()` and `rescale()` lies in the
    way task connections are formed. While `rebalance()` will create communication
    channels between all sending tasks to all receiving tasks, `rescale()` will only
    create channels from each task to some of the tasks of the downstream operator.
    The connection pattern of the rescale distribution transformation is shown in
    [Figure 5-7](#rebalance-rescale).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`rebalance()` 和 `rescale()` 之间的基本区别在于任务连接的形成方式。`rebalance()` 会在所有发送任务和所有接收任务之间创建通信通道，而
    `rescale()` 只会从每个任务到下游运算符的一些任务之间创建通道。重新缩放分布变换的连接模式如 [图5-7](#rebalance-rescale)
    所示。'
- en: '![](assets/spaf_0507.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/spaf_0507.png)'
- en: Figure 5-7\. Rebalance and rescale transformations
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-7\. 重新平衡和重新缩放变换
- en: Broadcast
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 广播
- en: The `broadcast()` method replicates the input data stream so that all events
    are sent to all parallel tasks of the downstream operator.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`broadcast()` 方法复制输入数据流，以便所有事件都发送到下游运算符的所有并行任务。'
- en: Global
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 全局
- en: The `global()` method sends all events of the input data stream to the first
    parallel task of the downstream operator. This partitioning strategy must be used
    with care, as routing all events to the same task might impact application performance.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`global()` 方法将输入数据流的所有事件发送到下游运算符的第一个并行任务。这种分区策略必须谨慎使用，因为将所有事件路由到同一个任务可能会影响应用程序的性能。'
- en: Custom
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义
- en: 'When none of the predefined partitioning strategies is suitable, you can define
    your own by using the `partitionCustom()` method. This method receives a `Partitioner` object
    that implements the partitioning logic and the field or key position on which
    the stream is to be partitioned. The following example partitions a stream of
    integers so that all negative numbers are sent to the first task and all other
    numbers are sent to a random task:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当预定义的分区策略都不适用时，可以使用 `partitionCustom()` 方法自定义分区策略。此方法接收一个实现分区逻辑并指定流应该分区的字段或键位置的
    `Partitioner` 对象。以下示例将整数流分区，使所有负数被发送到第一个任务，而其他数字则随机发送到一个任务：
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Setting the Parallelism
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置并行性
- en: Flink applications are executed in parallel in a distributed environment such
    as a cluster of machines. When a DataStream program is submitted to the JobManager
    for execution, the system creates a dataflow graph and prepares the operators
    for execution. Each operator is parallelized into one or multiple tasks. Each
    task will process a subset of the operator’s input stream. The number of parallel
    tasks of an operator is called the parallelism of the operator. It determines
    how much the operator’s processing effort can be distributed and also how much
    data it can process.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 应用在分布式环境中（如机器集群）并行执行。当一个 DataStream 程序提交给 JobManager 执行时，系统会创建数据流图并准备运算符进行执行。每个运算符会被并行化为一个或多个任务。每个任务会处理运算符输入流的一个子集。运算符的并行任务数称为运算符的并行度，它决定了运算符处理工作的分布程度和它能处理的数据量。
- en: The parallelism of an operator can be controlled at the level of the execution
    environment or per individual operator. By default, the parallelism of all operators
    of an application is set as the parallelism of the application’s execution environment.
    The parallelism of the environment (and thus also the default parallelism of all
    operators) is automatically initialized based on the context in which the application
    is started. If the application runs in a local execution environment the parallelism
    is set to match the number of CPU cores. When submitting an application to a running
    Flink cluster, the environment parallelism is set to the default parallelism of
    the cluster unless it is explicitly specified via the submission client (see [“Running
    and Managing Streaming Applications”](ch10.html#chap-10-app-deployment) for more
    details).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 操作符的并行性可以在执行环境或每个单独操作符的级别上进行控制。默认情况下，一个应用程序的所有操作符的并行性都设置为应用程序执行环境的并行性。环境的并行性（因此也是所有操作符的默认并行性）会根据应用程序启动的上下文自动初始化。如果应用程序在本地执行环境中运行，则并行性设置为匹配
    CPU 内核数。当将应用程序提交到正在运行的 Flink 集群时，环境并行性设置为集群的默认并行性，除非通过提交客户端明确指定（详见[“运行和管理流应用程序”](ch10.html#chap-10-app-deployment)获取更多详细信息）。
- en: 'In general, it is a good idea to define the parallelism of your operators relative
    to the default parallelism of the environment. This allows you to easily scale
    the application by adjusting its parallelism via the submission client. You can
    access the default parallelism of the environment as shown in the following example:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，定义操作符的并行性相对于环境的默认并行性是个好主意。这样可以通过提交客户端调整并行性轻松扩展应用程序。您可以通过以下示例访问环境的默认并行性：
- en: '[PRE28]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You can also override the default parallelism of the environment, but you will
    no longer be able to control the parallelism of your application via the submission
    client:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以覆盖环境的默认并行性，但将无法通过提交客户端控制应用程序的并行性：
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The default parallelism of an operator can be overridden by specifying it explicitly.
    In the following example, the source operator will be executed with the default
    parallelism of the environment, the map transformation has double as many tasks
    as the source, and the sink operation will always be executed by two parallel
    tasks:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 操作符的默认并行性可以通过显式指定来覆盖。在以下示例中，源操作符将以环境的默认并行性执行，映射转换将具有源的两倍任务数量，并且汇聚操作将始终由两个并行任务执行：
- en: '[PRE30]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: When you submit the application via a submission client and specify the parallelism
    to be 16, the source will run with a parallelism of 16, the mapper will run with
    32 tasks, and the sink will run with 2 tasks. If you run the application in a
    local environment—or example, from your IDE—on a machine with 8 cores, the source
    task will run with 8 tasks, the mapper with 16 tasks, and the sink with 2 tasks.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当您通过提交客户端提交应用程序并将并行性设置为 16 时，源将以并行性 16 运行，映射器将以 32 个任务运行，而汇聚将以 2 个任务运行。如果在本地环境中运行应用程序，例如在您的
    IDE 上，在具有 8 个核心的机器上，源任务将以 8 个任务运行，映射器将以 16 个任务运行，而汇聚将以 2 个任务运行。
- en: Types
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类型
- en: Flink DataStream applications process streams of events that are represented
    as data objects. Functions are called with data objects and emit data objects.
    Internally, Flink needs to be able to handle these objects. They need to be serialized
    and deserialized to ship them over the network or write them into or read them
    from state backends, checkpoints, and savepoints. In order to do this efficiently,
    Flink requires detailed knowledge of the types of data the application processes.
    Flink uses the concept of type information to represent data types and generate
    specific serializers, deserializers, and comparators for every data type.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Flink DataStream 应用程序处理表示为数据对象的事件流。函数使用数据对象调用并发出数据对象。在内部，Flink 需要能够处理这些对象。它们需要被序列化和反序列化以便通过网络传输或写入和读取状态后端、检查点和保存点。为了高效地完成这些操作，Flink
    需要详细了解应用程序处理的数据类型。Flink 使用类型信息的概念来表示数据类型，并为每种数据类型生成特定的序列化器、反序列化器和比较器。
- en: Flink also features a type extraction system that analyzes the input and return
    types of functions to automatically obtain type information and hence serializers
    and deserializers. However, in certain situations, such as lambda functions or
    generic types, it is necessary to explicitly provide type information to make
    an application work or improve its performance.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Flink还提供了一种类型提取系统，分析函数的输入和返回类型以自动获取类型信息、序列化器和反序列化器。然而，在某些情况下，如lambda函数或泛型类型，需要显式提供类型信息来使应用程序正常工作或提高其性能。
- en: In this section, we discuss the types supported by Flink, how to create type
    information for a data type, and how to help Flink’s type system with hints if
    it cannot automatically infer the return type of a function.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了Flink支持的类型，如何为数据类型创建类型信息，以及如何在Flink的类型系统无法自动推断函数的返回类型时提供提示。
- en: Supported Data Types
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持的数据类型
- en: 'Flink supports all common data types that are available in Java and Scala.
    The most widely used types can be grouped into the following categories:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持Java和Scala中所有常见的数据类型。最常用的类型可以分为以下几类：
- en: Primitives
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始类型
- en: Java and Scala tuples
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java和Scala元组
- en: Scala case classes
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala的case类
- en: POJOs, including classes generated by Apache Avro
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括由Apache Avro生成的类在内的POJOs
- en: Some special types
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些特殊类型
- en: Types that are not specially handled are treated as generic types and serialized
    using the [Kryo serialization framework](https://github.com/EsotericSoftware/kryo).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有特别处理的类型，将其视为通用类型，并使用[Kryo序列化框架](https://github.com/EsotericSoftware/kryo)对其进行序列化。
- en: Only Use Kryo as a Fallback Solution
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仅在备用解决方案中使用Kryo
- en: Note that you should avoid using Kryo if possible. Since Kryo is a general-purpose
    serializer it is usually not very efficient. Flink provides configuration options
    to improve the efficiency by preregistering classes to Kryo. Moreover, Kryo does
    not provide a good migration path to evolve data types.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果可能的话，应该避免使用Kryo。由于Kryo是一个通用序列化器，通常效率不是很高。Flink提供配置选项，通过预注册类到Kryo来提高效率。此外，Kryo没有提供良好的迁移路径来演变数据类型。
- en: Let’s look at each type category.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每个类型类别。
- en: Primitives
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 原始类型
- en: 'All Java and Scala primitive types, such as `Int` (or `Integer` for Java),
    `String`, and `Double`, are supported. Here is an example that processes a stream
    of `Long` values and increments each element:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 支持所有Java和Scala原始类型，例如`Int`（或Java中的`Integer`）、`String`和`Double`。以下是处理`Long`值流并递增每个元素的示例：
- en: '[PRE31]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Java and Scala tuples
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Java和Scala元组
- en: Tuples are composite data types that consist of a fixed number of typed fields.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 元组是由固定数量的类型字段组成的复合数据类型。
- en: 'The Scala DataStream API uses regular Scala tuples. The following example filters
    a `DataStream` of tuples with two fields:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Scala DataStream API使用常规的Scala元组。以下示例筛选具有两个字段的`DataStream`元组：
- en: '[PRE32]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Flink provides efficient implementations of Java tuples. Flink’s Java tuples
    can have up to 25 fields, with each length is implemented as a separate class—`Tuple1`,
    `Tuple2`, up to `Tuple25`. The tuple classes are strongly typed.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Flink提供了高效的Java元组实现。Flink的Java元组最多可以有25个字段，每个长度实现为一个单独的类——`Tuple1`、`Tuple2`，直到`Tuple25`。元组类是强类型的。
- en: 'We can rewrite the filtering example in the Java DataStream API as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将Java DataStream API中的过滤示例重写如下：
- en: '[PRE33]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Tuple fields can be accessed by the name of their public fields—`f0`, `f1`,
    `f2`, etc., as shown earlier—or by position using the `getField(int pos)` method,
    where indexes start at 0:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 元组字段可以通过它们的公共字段名称如`f0`、`f1`、`f2`等（如前所示）或者通过使用`getField(int pos)`方法按位置访问，索引从0开始：
- en: '[PRE34]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In contrast to their Scala counterparts, Flink’s Java tuples are mutable, so
    the values of fields can be reassigned. Functions can reuse Java tuples in order
    to reduce the pressure on the garbage collector. The following example shows how
    to update a field of a Java tuple:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 与它们的Scala对应物相比，Flink的Java元组是可变的，因此可以重新分配字段的值。函数可以重用Java元组以减少垃圾收集器的压力。以下示例展示了如何更新Java元组的字段：
- en: '[PRE35]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Scala case classes
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Scala的case类
- en: 'Flink supports Scala case classes. Case class fields are accessed by name.
    In the following, we define a case class `Person` with two fields: `name` and
    `age`. As for the tuples, we filter the `DataStream` by age:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持Scala的case类。案例类字段通过名称访问。以下是我们定义一个名为`Person`的case类，其中包含两个字段：`name`和`age`。至于元组，我们通过年龄筛选`DataStream`：
- en: '[PRE36]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: POJOs
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: POJOs
- en: 'Flink analyzes each type that does not fall into any category and checks to
    see if it can be identified and handled as a POJO type. Flink accepts a class
    as a POJO if it satisfies the following conditions:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 分析每个不属于任何类别的类型，并检查是否可以将其识别并作为 POJO 类型处理。如果满足以下条件，Flink 将接受一个类作为 POJO：
- en: It is a public class.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一个公共类。
- en: It has a public constructor without any arguments—a default constructor.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有一个公共构造函数，没有任何参数——默认构造函数。
- en: All fields are public or accessible through getters and setters. The getter
    and setter functions must follow the default naming scheme, which is `Y getX()`
    and `setX(Y x)` for a field `x` of type `Y`.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有字段都是公共的或可以通过 getter 和 setter 访问。getter 和 setter 函数必须遵循默认命名方案，即对于类型为 `Y` 的字段
    `x`，getter 为 `Y getX()`，setter 为 `setX(Y x)`。
- en: All fields have types that are supported by Flink.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有字段的类型都受 Flink 支持。
- en: 'For example, the following Java class will be identified as a POJO by Flink:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下的 Java 类将被 Flink 标识为一个 POJO：
- en: '[PRE37]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Avro-generated classes are automatically identified by Flink and handled as
    POJOs.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Avro 生成的类会被 Flink 自动识别并作为 POJO 处理。
- en: Arrays, Lists, Maps, Enums, and other special types
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 数组、列表、映射、枚举及其他特殊类型
- en: Flink supports several special-purpose types, such as primitive and object `Array`
    types; Java’s `ArrayList`, `HashMap`, and `Enum` types; and Hadoop `Writable`
    types. Moreover, it provides type information for Scala’s `Either`, `Option`,
    and `Try` types, and Flink’s Java version of the `Either` type.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 支持多种特定用途的类型，如基本和对象 `Array` 类型；Java 的 `ArrayList`、`HashMap` 和 `Enum` 类型；以及
    Hadoop 的 `Writable` 类型。此外，它还为 Scala 的 `Either`、`Option` 和 `Try` 类型提供类型信息，以及 Flink
    的 Java 版本的 `Either` 类型。
- en: Creating Type Information for Data Types
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为数据类型创建类型信息
- en: The central class in Flink’s type system is `TypeInformation`. It provides the
    system with the necessary information it needs to generate serialiazers and comparators.
    For instance, when you join or group by some key, `TypeInformation` allows Flink
    to perform the semantic check of whether the fields used as keys are valid.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 类型系统的核心类是 `TypeInformation`。它为系统提供了生成序列化器和比较器所需的必要信息。例如，当你按键进行连接或分组时，`TypeInformation`
    允许 Flink 执行语义检查，以确定用作键的字段是否有效。
- en: When an application is submitted for execution, Flink’s type system tries to
    automatically derive the `TypeInformation` for every data type that is processed
    by the framework. A so-called type extractor analyzes the generic types and return
    types of all functions to obtain the respective `TypeInformation` objects. Hence,
    you might use Flink for a while without ever needing to worry about `TypeInformation`
    for your data types. However, sometimes the type extractor fails or you might
    want to define your own types and tell Flink how to handle them efficiently. In
    such cases, you need to generate a `TypeInformation` for a specific data type.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序提交执行时，Flink 的类型系统尝试为框架处理的每种数据类型自动推导 `TypeInformation`。一个所谓的类型提取器分析所有函数的泛型类型和返回类型，以获取相应的
    `TypeInformation` 对象。因此，你可能在不需要担心数据类型的 `TypeInformation` 的情况下使用 Flink 一段时间。然而，有时类型提取器会失败，或者你可能希望定义自己的类型并告诉
    Flink 如何高效处理它们。在这种情况下，你需要为特定的数据类型生成 `TypeInformation`。
- en: 'Flink provides two utility classes for Java and Scala with static methods to
    generate a `TypeInformation`. For Java, the helper class is `org.apache.flink.api.common.typeinfo.Types`,
    and it is used as shown in the following examples:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 为 Java 和 Scala 提供了两个实用程序类，其中包含静态方法来生成 `TypeInformation`。对于 Java，辅助类是 `org.apache.flink.api.common.typeinfo.Types`，并且如以下示例所示使用：
- en: '[PRE38]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`TypeInformation`’s helper class is `org.apache.flink.api.scala.typeutils.Types`
    for the Scala API, and it is used as shown in the following:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Scala API，`TypeInformation` 的辅助类是 `org.apache.flink.api.scala.typeutils.Types`，并且使用如下所示：
- en: '[PRE39]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Type Information in the Scala API
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scala API 中的类型信息
- en: 'In the Scala API, Flink uses Scala compiler macros that generate `TypeInformation`
    objects for all data types at compile time. To access the `createTypeInformation`
    macro function, make sure to always add the following import statement to your
    Scala application:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scala API 中，Flink 使用 Scala 编译器宏，在编译时为所有数据类型生成 `TypeInformation` 对象。要访问 `createTypeInformation`
    宏函数，请确保始终向你的 Scala 应用程序添加以下导入语句：
- en: '[PRE40]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Explicitly Providing Type Information
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 明确提供类型信息
- en: In most cases, Flink can automatically infer types and generate the correct
    `TypeInformation`. Flink’s type extractor leverages reflection and analyzes function
    signatures and subclass information to derive the correct output type of a user-defined
    function. Sometimes, though, the necessary information cannot be extracted (e.g.,
    because of Java erasing generic type information). Moreover, in some cases Flink
    might not choose the `TypeInformation` that generates the most efficient serializers
    and deserializers. Hence, you might need to explicitly provide `TypeInformation`
    objects to Flink for some of the data types used in your application.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，Flink 可以自动推断类型并生成正确的`TypeInformation`。Flink 的类型提取器利用反射和分析函数签名和子类信息来推导用户定义函数的正确输出类型。不过，有时无法提取必要的信息（例如，由于
    Java 擦除泛型类型信息）。此外，在某些情况下，Flink 可能不会选择生成最高效的序列化程序和反序列化程序的`TypeInformation`。因此，您可能需要为应用程序中使用的某些数据类型显式提供`TypeInformation`对象给
    Flink。
- en: 'There are two ways to provide `TypeInformation`. First, you can extend a function
    class to explicitly provide the `TypeInformation` of its return type by implementing
    the `ResultTypeQueryable` interface. The following example shows a `MapFunction`
    that provides its return type:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种提供`TypeInformation`的方式。首先，您可以扩展函数类，通过实现`ResultTypeQueryable`接口来显式提供其返回类型的`TypeInformation`。以下示例显示了一个提供其返回类型的`MapFunction`：
- en: '[PRE41]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In the Java DataStream API, you can also use the `returns()` method to explicitly
    specify the return type of an operator when defining the dataflow as shown in
    the following:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Java DataStream API 中，当定义数据流时，您还可以使用`returns()`方法显式指定操作符的返回类型，如下所示：
- en: '[PRE42]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Defining Keys and Referencing Fields
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义键和引用字段
- en: Some of the transformations you saw in the previous section require a key specification
    or field reference on the input stream type. In Flink, keys are not predefined
    in the input types like in systems that work with key-value pairs. Instead, keys
    are defined as functions over the input data. Therefore, it is not necessary to
    define data types to hold keys and values, which avoids a lot of boilerplate code.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中看到的一些转换需要输入流类型上的关键规范或字段引用。在 Flink 中，键不像处理键值对的系统那样预定义在输入类型中。相反，键被定义为对输入数据的函数。因此，不需要定义用于保存键和值的数据类型，这避免了大量的样板代码。
- en: In the following, we discuss different methods to reference fields and define
    keys on data types.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们讨论了在数据类型上引用字段和定义键的不同方法。
- en: Field Positions
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字段位置
- en: 'If the data type is a tuple, keys can be defined by simply using the field
    position of the corresponding tuple element. The following example keys the input
    stream by the second field of the input tuple:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据类型是元组，可以通过简单地使用相应元组元素的字段位置来定义键。以下示例将输入流按输入元组的第二个字段键入：
- en: '[PRE43]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Composite keys consisting of more than one tuple field can also be defined.
    In this case, the positions are provided as a list, one after the other. We can
    key the input stream by the second and third fields as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以定义由多个元组字段组成的复合键。在这种情况下，位置将按顺序提供为列表。我们可以通过以下方式将输入流键入第二个和第三个字段：
- en: '[PRE44]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Field Expressions
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字段表达式
- en: Another way to define keys and select fields is by using `String`-based field
    expressions. Field expressions work for tuples, POJOs, and case classes. They
    also support the selection of nested fields.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种定义键和选择字段的方式是使用基于`String`的字段表达式。字段表达式适用于元组、POJO 和案例类。它们还支持选择嵌套字段。
- en: 'In the introductory example of this chapter, we defined the following case
    class:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的入门示例中，我们定义了以下案例类：
- en: '[PRE45]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'To key the stream by sensor ID we can pass the field name `id` to the `keyBy()` function:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过传递字段名`id`给`keyBy()`函数来键入传感器 ID 的流：
- en: '[PRE46]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'POJO or case class fields are selected by their field name like in the above
    example. Tuple fields are referenced either by their field name (1-offset for
    Scala tuples, 0-offset for Java tuples) or by their 0-offset field index:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: POJO 或案例类字段通过其字段名来选择，就像上面的示例中那样。元组字段则通过它们的字段名（Scala 元组为1偏移，Java 元组为0偏移）或它们的0偏移字段索引来引用：
- en: '[PRE47]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Nested fields in POJOs and tuples are selected by denoting the nesting level
    with a "`.`" (period character). Consider the following case classes:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在 POJO 和元组的嵌套字段中，通过使用“`.`”（句点字符）来表示嵌套级别来选择。考虑以下案例类：
- en: '[PRE49]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'If we want to reference a person’s ZIP code, we can use a field expression:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想引用一个人的邮政编码，可以使用字段表达式：
- en: '[PRE50]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'It is also possible to nest expressions on mixed types. The following expression
    accesses the field of a tuple nested in a POJO:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以在混合类型的表达式上进行嵌套表达式。以下表达式访问了嵌入在 POJO 中的元组字段：
- en: '[PRE51]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'A full data type can be selected using the wildcard field expression "`_`"
    (underscore character):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用通配符字段表达式 "`_`"（下划线字符）选择完整的数据类型：
- en: '[PRE52]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Key Selectors
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 键选择器
- en: 'A third option to specify keys are `KeySelector` functions. A `KeySelector`
    function extracts a key from an input event:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种指定键的选项是 `KeySelector` 函数。`KeySelector` 函数从输入事件中提取键：
- en: '[PRE53]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The introductory example actually uses a simple `KeySelector` function in the
    `keyBy()` method:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 入门示例实际上在 `keyBy()` 方法中使用了一个简单的 `KeySelector` 函数：
- en: '[PRE54]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'A `KeySelector` function receives an input item and returns a key. The key
    does not necessarily have to be a field of the input event but can be derived
    using arbitrary computations. In the following, the `KeySelector` function returns
    the maximum of the tuple fields as the key:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '`KeySelector` 函数接收输入项并返回一个键。该键不一定是输入事件的字段，可以使用任意计算派生。在下面的示例中，`KeySelector`
    函数返回元组字段的最大值作为键：'
- en: '[PRE55]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Compared to field positions and field expressions, an advantage of `KeySelector`
    functions is that the resulting key is strongly typed due to the generic types
    of the `KeySelector` class.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 与字段位置和字段表达式相比，`KeySelector` 函数的一个优点是由于 `KeySelector` 类的泛型类型，生成的键是强类型的。
- en: Implementing Functions
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现函数
- en: You’ve seen user-defined functions in action in the code examples of this chapter
    so far. In this section, we explain the different ways in which you can define
    and parametrize functions in the DataStream API in more detail.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的代码示例中，你已经看到了用户定义函数的实际操作。在本节中，我们将更详细地解释在 DataStream API 中定义和参数化函数的不同方式。
- en: Function Classes
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 函数类
- en: Flink exposes all interfaces for user-defined functions, such as `MapFunction`,
    `FilterFunction`, and `ProcessFunction`, as interfaces or abstract classes.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 为用户定义的所有函数接口（如 `MapFunction`、`FilterFunction` 和 `ProcessFunction`）公开为接口或抽象类。
- en: 'A function is implemented by implementing the interface or extending the abstract
    class. In the following example, we implement a `FilterFunction` that filters
    for strings that contain the word `"flink"`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实现接口或扩展抽象类来实现函数。在以下示例中，我们实现了一个 `FilterFunction`，用于过滤包含字符串 `"flink"` 的字符串：
- en: '[PRE56]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'An instance of the function class can then be passed as an argument to the
    filter transformation:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以将函数类的实例作为参数传递给过滤转换：
- en: '[PRE57]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Functions can also be implemented as anonymous classes:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 函数还可以实现为匿名类：
- en: '[PRE58]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Functions can receive parameters through their constructor. We can parametrize
    the above example and pass the `String` `"flink"` as a parameter to the `KeywordFilter`
    constructor as shown below:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 函数可以通过它们的构造函数接收参数。我们可以对上述示例进行参数化，并将 `String` `"flink"` 作为参数传递给 `KeywordFilter`
    构造函数，如下所示：
- en: '[PRE59]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: When a program is submitted for execution, all function objects are serialized
    using Java serialization and shipped to all parallel tasks of their corresponding
    operators. Therefore, all configuration values are preserved after the object
    is deserialized.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 当程序提交执行时，所有函数对象都使用 Java 序列化进行序列化，并发送到其对应操作符的所有并行任务。因此，所有配置值在对象反序列化后都得以保留。
- en: Functions Must Be Java Serializable
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 函数必须是 Java 可序列化的
- en: Flink serializes all function objects with Java serialization to ship them to
    the worker processes. Everything contained in a user function must be `Serializable`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 使用 Java 序列化序列化所有函数对象，以便将它们发送到工作进程。用户函数中包含的所有内容都必须是 `Serializable`。
- en: If your function requires a nonserializable object instance, you can either
    implement it as a rich function and initialize the nonserializable field in the
    `open()` method or override the Java serialization and deserialization methods.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的函数需要一个不可序列化的对象实例，你可以将其实现为富函数，并在 `open()` 方法中初始化不可序列化字段，或者重写 Java 的序列化和反序列化方法。
- en: Lambda Functions
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lambda 函数
- en: 'Most DataStream API methods accept lambda functions. Lambda functions are available
    for Scala and Java and offer a simple and concise way to implement application
    logic when no advanced operations such as accessing state and configuration are
    required. The following example show a lambda function that filters tweets containing
    the word “flink”:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 DataStream API 方法接受 Lambda 函数。Lambda 函数适用于 Scala 和 Java，并提供了一种简单而简洁的实现应用逻辑的方式，无需像访问状态和配置这样的高级操作。以下示例展示了一个过滤包含单词“flink”的
    Lambda 函数：
- en: '[PRE60]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Rich Functions
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 富函数
- en: Oftentimes there is a need to initialize a function before it processes the
    first record or to retrieve information about the context in which it is executed.
    The DataStream API provides rich functions that expose more functionality than
    the regular functions discussed until now.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 通常需要在函数处理第一条记录之前初始化函数或检索其执行上下文的信息。DataStream API 提供了富函数，它们比目前讨论的常规函数提供更多功能。
- en: There are rich versions of all the DataStream API transformation functions,
    and you can use them in the same places you can use a regular function or lambda
    function. Rich functions can be parameterized just like regular function classes.
    The name of a rich function starts with `Rich` followed by the transformation
    name—`RichMapFunction`, `RichFlatMapFunction`, and so on.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 DataStream API 转换函数都有富版本，并且您可以在可以使用常规函数或 lambda 函数的地方使用它们。富函数可以像常规函数类一样进行参数化。富函数的名称以
    `Rich` 开头，后跟转换名称—`RichMapFunction`、`RichFlatMapFunction` 等。
- en: 'When using a rich function, you can implement two additional methods to the
    function’s lifecycle:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用富函数时，可以实现两个额外的方法来扩展函数的生命周期：
- en: The `open()` method is an initialization method for the rich function. It is
    called once per task before a transformation method like filter or map is called.
    `open()` is typically used for setup work that needs to be done only once. Note
    that the `Configuration` parameter is only used by the DataSet API and not by
    the DataStream API. Hence, it should be ignored.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`open()` 方法是富函数的初始化方法。在像 filter 或 map 这样的转换方法被调用之前，每个任务会调用一次 `open()` 方法。`open()`
    通常用于只需执行一次的设置工作。请注意，`Configuration` 参数仅由 DataSet API 使用，而不由 DataStream API 使用。因此，应该忽略它。'
- en: The `close()` method is a finalization method for the function and it is called
    once per task after the last call of the transformation method. Thus, it is commonly
    used for cleanup and releasing resources.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`close()` 方法是函数的一个终结方法，它在转换方法的最后一次调用后每个任务调用一次。因此，通常用于清理和释放资源。'
- en: In addition, the method `getRuntimeContext()` provides access to the function’s
    `RuntimeContext`. The `RuntimeContext` can be used to retrieve information such
    as the function’s parallelism, its subtask index, and the name of the task that
    executes the function. Further, it includes methods for accessing partitioned
    state. Stateful stream processing in Flink is discussed in detail in [“Implementing
    Stateful Functions”](ch07.html#chap-7-stateful-functions). The following example
    code shows how to use the methods of a `RichFlatMapFunction`. [Example 5-3](#richflatmap-example)
    shows the methods of a RichFLatMapFunction
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`getRuntimeContext()` 方法提供对函数的 `RuntimeContext` 的访问。`RuntimeContext` 可用于检索信息，例如函数的并行性、其子任务索引以及执行函数的任务的名称。此外，它还包括用于访问分区状态的方法。在
    Flink 中详细讨论了有状态流处理，可参考[“实现有状态函数”](ch07.html#chap-7-stateful-functions)。以下示例代码展示了如何使用
    `RichFlatMapFunction` 的方法。[示例 5-3](#richflatmap-example) 展示了 `RichFlatMapFunction`
    的 open() 和 close() 方法。
- en: Example 5-3\. The open() and close() methods of a RichFlatMapFunction.
  id: totrans-310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-3\. `RichFlatMapFunction` 的 open() 和 close() 方法。
- en: '[PRE61]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Including External and Flink Dependencies
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 包括外部和 Flink 依赖
- en: Adding external dependencies is a common requirement when implementing Flink
    applications. There are many popular libraries out there, such as Apache Commons
    or Google Guava, for various use cases. Moreover, most Flink applications depend
    on one or more of Flink’s connectors to ingest data from or emit data to external
    systems, like Apache Kafka, filesystems, or Apache Cassandra. Some applications
    also leverage Flink’s domain-specific libraries, such as the Table API, SQL, or
    the CEP library. Consequently, most Flink applications do not only depend on Flink’s
    DataStream API dependency and the Java SDK but also on additional third-party
    and Flink-internal dependencies.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现 Flink 应用程序时，添加外部依赖是一个常见的需求。有许多流行的库可供选择，如 Apache Commons 或 Google Guava，用于各种用例。此外，大多数
    Flink 应用程序依赖于一个或多个 Flink 连接器，用于从外部系统中摄取数据或将数据发出，如 Apache Kafka、文件系统或 Apache Cassandra。一些应用程序还利用
    Flink 的领域特定库，如 Table API、SQL 或 CEP 库。因此，大多数 Flink 应用程序不仅依赖于 Flink 的 DataStream
    API 和 Java SDK，还依赖于额外的第三方和 Flink 内部依赖。
- en: When an application is executed, all of its dependencies must be available to
    the application. By default, only the core API dependencies (DataStream and DataSet
    APIs) are loaded by a Flink cluster. All other dependencies an application requires
    must be explicitly provided.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序执行时，必须将其所有依赖项可用于应用程序。默认情况下，Flink 集群仅加载核心 API 依赖项（DataStream 和 DataSet API）。应用程序需要的所有其他依赖项必须明确提供。
- en: The reason for this is to keep the number of default dependencies low.^([4](ch05.html#idm45499006554472))
    Most connectors and libraries rely on one or more libraries, which typically have
    several additional transitive dependencies. Often, these include frequently used
    libraries, such as Apache Commons or Google’s Guava. Many problems originate from
    incompatibilities among different versions of the same library that are pulled
    in from different connectors or directly from the user application.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的原因是为了保持默认依赖项的数量较少。^([4](ch05.html#idm45499006554472)) 大多数连接器和库依赖于一个或多个库，这些库通常有几个额外的传递依赖项。这些经常包括常用库，如
    Apache Commons 或 Google 的 Guava。许多问题源于从不同连接器或直接从用户应用程序中提取的同一库的不同版本之间的不兼容性。
- en: 'There are two ways to ensure all dependencies are available to an application
    when it is executed:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以确保应用程序在执行时可用所有依赖项：
- en: Bundle all dependencies into the application JAR file. This yields a self-contained,
    yet typically quite large, application JAR file.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有依赖项捆绑到应用程序的 JAR 文件中。这将产生一个自包含但通常相当大的应用程序 JAR 文件。
- en: The JAR file of a dependency can be added to the *./lib* folder of a Flink setup.
    In this case, the dependencies are loaded into the classpath when Flink processes
    are started. A dependency that is added to the classpath like this is available
    to (and might interfere with) all applications that run on the Flink setup.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个依赖的 JAR 文件可以添加到 Flink 设置的 *./lib* 文件夹中。在这种情况下，当 Flink 进程启动时，这些依赖项将加载到类路径中。像这样添加到类路径的依赖项可供运行在
    Flink 设置上的所有应用程序使用，并可能会相互干扰。
- en: Building a so-called fat JAR file is the preferred way to handle application
    dependencies. The Flink Maven archetypes we introduced in [“Bootstrap a Flink
    Maven Project”](ch04.html#chap-4-maven) generate Maven projects that are configured
    to produce application-fat JARs that include all required dependencies. Dependencies
    included in the classpath of Flink processes by default are automatically excluded
    from the JAR file. The *pom.xml* file of a generated Maven project contains comments
    that explain how to add additional dependencies.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 构建所谓的 fat JAR 文件是处理应用程序依赖项的首选方法。我们在 [“引导 Flink Maven 项目”](ch04.html#chap-4-maven)
    中介绍的 Flink Maven 原型生成的 Maven 项目配置为生成包含所有必需依赖项的应用程序 fat JAR 文件。默认情况下包含在 Flink 进程类路径中的依赖项将自动排除在
    JAR 文件之外。生成的 Maven 项目的 *pom.xml* 文件包含了说明如何添加额外依赖项的注释。
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter we introduced the basics of Flink’s DataStream API. We examined
    the structure of Flink programs and learned how to combine data and partitioning
    transformations to build streaming applications. We also looked into supported
    data types and different ways to specify keys and user-defined functions. If you
    take a step back and read the introductory example once more, you hopefully have
    a better idea about what is going on. In [Chapter 6](ch06.html#chap-6), things
    are going to get even more interesting—we will learn how to enrich our programs
    with window operators and time semantics.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 Flink 的 DataStream API 的基础知识。我们研究了 Flink 程序的结构，并学习了如何组合数据和分区转换来构建流式应用程序。我们还探讨了支持的数据类型以及指定键和用户定义函数的不同方法。如果您退后一步再次阅读介绍性的例子，您希望对正在发生的事情有更好的理解。在
    [第 6 章](ch06.html#chap-6) 中，事情将变得更加有趣——我们将学习如何通过窗口操作符和时间语义丰富我们的程序。
- en: ^([1](ch05.html#idm45499009375336-marker)) Flink features dedicated operators
    for time-based stream joins, which are discussed in [Chapter 6](ch06.html#chap-6).
    The connect transformation and the cofunctions discussed in this section are more
    generic.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.html#idm45499009375336-marker)) Flink 特有的基于时间的流连接操作符，已在 [第 6 章](ch06.html#chap-6)
    中讨论。本节讨论的连接转换和共函数更加通用。
- en: ^([2](ch05.html#idm45499009286248-marker)) You can also apply a `CoProcessFunction`
    to `ConnectedStreams`. We discuss `CoProcessFunction` in [Chapter 6](ch06.html#chap-6).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.html#idm45499009286248-marker)) 您还可以将 `CoProcessFunction` 应用于 `ConnectedStreams`。我们在
    [第 6 章](ch06.html#chap-6) 中讨论了 `CoProcessFunction`。
- en: ^([3](ch05.html#idm45499009156168-marker)) See [Chapter 8](ch08.html#chap-8)
    for details on keyed state.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.html#idm45499009156168-marker)) 详细内容请参见[第 8 章](ch08.html#chap-8)关于键控状态的细节。
- en: ^([4](ch05.html#idm45499006554472-marker)) Flink also aims to keep its own external
    dependencies to a minimum and hides most of them (including transitive dependencies)
    from user applications to prevent version conflicts.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch05.html#idm45499006554472-marker)) Flink 也旨在尽可能减少自己的外部依赖，并将大部分依赖（包括传递依赖）对用户应用程序隐藏起来，以防止版本冲突。
