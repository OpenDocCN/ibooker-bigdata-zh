- en: 25  PCA II
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 25  PCA II
- en: 原文：[https://ds100.org/course-notes/pca_2/pca_2.html](https://ds100.org/course-notes/pca_2/pca_2.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://ds100.org/course-notes/pca_2/pca_2.html](https://ds100.org/course-notes/pca_2/pca_2.html)
- en: '*Learning Outcomes* **   Develop a deeper understanding of how to interpret
    Principal Components Analysis (PCA).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习成果* **   深入理解如何解释主成分分析（PCA）。'
- en: See applications of PCA to some real-world contexts.*  *## 25.1 PCA Review
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看看PCA在一些现实世界的应用。*  *## 25.1 PCA回顾
- en: 25.1.1 PCA with SVD
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.1.1 使用SVD的PCA
- en: 'After finding the SVD of \(X\):'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在找到\(X\)的SVD之后：
- en: '![slide15](../Images/8a0b9a9130ee1ccdcedd6223361588a5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![slide15](../Images/8a0b9a9130ee1ccdcedd6223361588a5.png)'
- en: We can derive the principal components of the data. Specifically, the first
    \(n\) rows of \(V^{T}\) are directions for the \(n\) principal components.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以推导出数据的主成分。具体来说，\(V^{T}\)的前\(n\)行是\(n\)个主成分的方向。
- en: 25.1.2 Columns of V are the Directions
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.1.2 V的列是方向
- en: '![slide16](../Images/f3fcfc67389614d9a84d1d3015c59ae5.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![slide16](../Images/f3fcfc67389614d9a84d1d3015c59ae5.png)'
- en: The elements of each column of \(V\) (row of \(V^{T}\)) rotate the original
    feature vectors into a principal component.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \(V\)的每一列元素（\(V^{T}\)的每一行）将原始特征向量旋转成一个主成分。
- en: The first column of V indicates how each feature contributes (e.g. positive,
    negative, etc.) to principal component 1.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: V的第一列表示每个特征对第一个主成分的贡献（例如正面、负面等）。
- en: '![slide17](../Images/48a12283f85977e8b03a04eac89da86c.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![slide17](../Images/48a12283f85977e8b03a04eac89da86c.png)'
- en: 'Coupled together, this interpretation also allows us to understand that:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解释也让我们理解：
- en: The principal components are all **orthogonal** to each other because the columns
    of U are orthonormal.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主成分都是**正交**的，因为U的列是标准正交的。
- en: Principal components are **axis-aligned**. That is, if you plot two PCs on a
    2D plane, one will lie on the x-axis, the other on the y-axis.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主成分是**轴对齐**的。也就是说，如果你在二维平面上绘制两个主成分，一个会在x轴上，另一个会在y轴上。
- en: Principal components are **linear combinations** of columns in our data X
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主成分是我们数据X中列的**线性组合**
- en: 25.1.3 Using Principal Components
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.1.3 使用主成分
- en: 'Let’s summarize the steps to obtain Principal Components via SVD:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结通过SVD获取主成分的步骤：
- en: Center the data matrix by subtracting the mean of each attribute column.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过减去每个属性列的平均值来使数据矩阵居中。
- en: 'To find the \(k\) **principal components**:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要找到\(k\)个**主成分**：
- en: Compute the SVD of the data matrix (\(X = U{\Sigma}V^{T}\))
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算数据矩阵的SVD（\(X = U{\Sigma}V^{T}\)）
- en: The first \(k\) columns of \(U{\Sigma}\) (or equivalently, \(XV\)) contain the
    \(k\) **principal components** of \(X\).
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(U{\Sigma}\)的前\(k\)列（或者等价地，\(XV\)）包含了\(X\)的\(k\)个**主成分**。
- en: 25.2 Data Variance and Centering
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.2 数据方差和居中
- en: 'We define the total variance of a data matrix as the sum of variances of attributes.
    The principal components are a low-dimension representation that capture as much
    of the original data’s total variance as possible. Formally, the \(i\)-th singular
    value tells us the **component score**, i.e., how much of the data variance is
    captured by the \(i\)-th principal component. Supposing the number of datapoints
    is \(n\):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义数据矩阵的总方差为属性方差的总和。主成分是一个低维表示，尽可能多地捕捉原始数据的总方差。形式上，第\(i\)个奇异值告诉我们**成分得分**，即第\(i\)个主成分捕捉了多少数据方差。假设数据点的数量是\(n\)：
- en: \[\text{i-th component score} = \frac{(\text{i-th singular value}^2)}{n}\]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{i-th component score} = \frac{(\text{i-th singular value}^2)}{n}\]
- en: Summing up the component scores is equivalent to computing the total variance
    *if we center our data*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将成分得分相加等同于计算总方差*如果我们对数据进行了居中*。
- en: '**Data Centering**: PCA has a data centering step that precedes any singular
    value decomposition, where if implemented defines the component score as above.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据居中**：PCA有一个数据居中的步骤，它在任何奇异值分解之前进行，如果实施了，就会定义上面的成分得分。'
- en: If you want to dive deeper into PCA, [Steve Brunton’s SVD Video Series](https://www.youtube.com/playlist?list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv)
    is a great resource.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解PCA，[Steve Brunton的SVD视频系列](https://www.youtube.com/playlist?list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv)是一个很好的资源。
- en: 25.3 Interpreting PCA
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.3 解释PCA
- en: '25.3.1 Case Study: House of Representatives Voting'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.3.1 案例研究：众议院投票
- en: Let’s examine how the House of Representatives (of the 116th Congress, 1st session)
    voted in the month of September 2019.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看众议院（第116届国会，第1届会议）在2019年9月投票的情况。
- en: 'Specifically, we’ll look at the records of Roll call votes. From the U.S. Senate
    ([link](https://www.senate.gov/reference/Index/Votes.htm)): Roll call votes occur
    when a representative or senator votes “yea” or “nay” so that the names of members
    voting on each side are recorded. A voice vote is a vote in which those in favor
    or against a measure say “yea” or “nay,” respectively, without the names or tallies
    of members voting on each side being recorded.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将查看点名投票的记录。来自美国参议院（[链接](https://www.senate.gov/reference/Index/Votes.htm)）：点名投票发生在代表或参议员投票“赞成”或“反对”时，以便记录每一方投票的成员的名字。口头表决是指那些赞成或反对某项措施的人分别说“赞成”或“反对”，而不记录每一方投票的成员的名字或计数。
- en: '**Do legislators’ roll call votes show a relationship with their political
    party?**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**立法者的点名投票是否与他们的政党有关？**'
- en: Please visit this [link](https://data100.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2FDS-100%2Ffa23-student&urlpath=lab%2Ftree%2Ffa23-student%2Flecture%2Flec26%2Flec26-votes.ipynb&branch=main)
    to see the full Jupyter notebook demo.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请访问此[链接](https://data100.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2FDS-100%2Ffa23-student&urlpath=lab%2Ftree%2Ffa23-student%2Flecture%2Flec26%2Flec26-votes.ipynb&branch=main)查看完整的Jupyter笔记本演示。
- en: As shown in the demo, the primary goal of PCA is to transform observations from
    high-dimensional data down to low dimensions through linear transformations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如演示所示，PCA的主要目标是通过线性变换将高维数据的观测值转换为低维数据。
- en: A related goal of PCA connects back to the idea that a low-dimension representation
    of the data should capture the variability of the original data. For example,
    if the first two singular values are large and the others are relatively small,
    then two dimensions are probably enough to describe most of what distinguishes
    one observation from another. However, if this is not the case, then a PCA scatter
    plot is probably omitting lots of information.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的一个相关目标是将数据的低维表示捕捉原始数据的变异性。例如，如果前两个奇异值很大，而其他奇异值相对较小，那么两个维度可能足以描述大部分区分观测之间的差异。然而，如果情况并非如此，那么PCA散点图可能遗漏了大量信息。
- en: 'We can use the following formulas to quantify the amount each principal component
    contributes to the total variance:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下公式来量化每个主成分对总方差的贡献：
- en: \[ \text{component score} = \frac{\sigma_i^{2}}{N}\]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{成分得分} = \frac{\sigma_i^{2}}{N}\]
- en: \[ \text{total variance} = \text{sum of all the component scores} = \sum_{i=1}^k
    \frac{\sigma_i^{2}}{N} \]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{总方差} = \text{所有成分得分之和} = \sum_{i=1}^k \frac{\sigma_i^{2}}{N} \]
- en: \[ \text{variance ratio of principal component i} = \frac{\text{component score
    i}}{\text{total variance}} = \frac{\sigma_i^{2} / N}{\sum_{i=1}^k \sigma_i^{2}
    / N}\]
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{主成分i的方差比率} = \frac{\text{主成分得分i}}{\text{总方差}} = \frac{\sigma_i^{2}
    / N}{\sum_{i=1}^k \sigma_i^{2} / N}\]
- en: In Python, assuming you had a 1D `NumPy` array of singular values `s` returned
    by `np.linalg.svd`, you could compute the list of variances ratios with `s**2
    / sum(s**2)`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，假设您有一个由`np.linalg.svd`返回的奇异值`s`的一维`NumPy`数组，您可以使用`s**2 / sum(s**2)`计算方差比率的列表。
- en: 25.3.2 PCA Plot
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.3.2 PCA图
- en: We often plot the first two principal components using a scatter plot, with
    PC1 on the \(x\)-axis and PC2 on the \(y\)-axis. This is often called a PCA plot.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常使用散点图绘制前两个主成分，其中PC1在\(x\)-轴上，PC2在\(y\)-轴上。这通常被称为PCA图。
- en: If the first two singular values are large and all others are small, then two
    dimensions are enough to describe most of what distinguishes one observation from
    another. If not, then a PCA plot is omitting lots of information.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前两个奇异值很大，而其他奇异值很小，那么两个维度足以描述大部分区分观测之间的差异。如果不是这样，那么PCA图遗漏了大量信息。
- en: 'PCA plots help us assess similarities between our data points and if there
    are any clusters in our dataset. In the case study before, for example, we could
    create the following PCA plot:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: PCA图帮助我们评估数据点之间的相似性以及数据集中是否存在任何聚类。例如，在之前的案例研究中，我们可以创建以下PCA图：
- en: '![pca_plot](../Images/81b6d133c69079ebc0054408217eb1e2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![pca_plot](../Images/81b6d133c69079ebc0054408217eb1e2.png)'
- en: 25.3.3 Scree Plots
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.3.3 屏幕图
- en: A scree plot shows the **variance ratio** captured by each principal component,
    with the largest variance ratio first. They help us visually determine the number
    of dimensions needed to describe the data reasonably. The singular values that
    fall in the region of the plot after a large drop-off correspond to principal
    components that are **not** needed to describe the data since they explain a relatively
    low proportion of the total variance of the data. For example, in the below plot,
    we could use the “elbow method” just described to figure out that the first 2
    PCs capture the bulk of the information.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕图显示了每个主成分捕获的**方差比率**，首先是最大的方差比率。它们帮助我们直观地确定描述数据所需的维度数量。在图中大幅下降后的区域中的奇异值对应于**不**需要描述数据的主成分，因为它们解释了数据总方差的相对较低比例。例如，在下面的图中，我们可以使用刚才描述的“拐点法”来确定前2个PCs捕获了大部分信息。
- en: '![scree_plot](../Images/3e17ee50a58f2ba5ba3f53384f850db0.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![scree_plot](../Images/3e17ee50a58f2ba5ba3f53384f850db0.png)'
- en: 25.3.4 Biplots
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.3.4 双标图
- en: Biplots superimpose the directions onto the plot of PC2 vs PC1, where vector
    \(j\) corresponds to the direction for feature \(j\) (e.g. \(v_{1j}, v_{2j}\)).
    There are several ways to scale biplot vectors – in this course, we plot the direction
    itself. For other scalings, which can lead to more interpretable directions/loadings,
    see [SAS biplots](https://blogs.sas.com/content/iml/2019/11/06/what-are-biplots.html)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 双标图将方向叠加到PC2 vs PC1的图上，其中向量\(j\)对应于特征\(j\)的方向（例如\(v_{1j}, v_{2j}\)）。有几种方法可以缩放双标图向量-在本课程中，我们绘制方向本身。对于其他缩放，可以导致更可解释的方向/载荷，请参阅[SAS双标图](https://blogs.sas.com/content/iml/2019/11/06/what-are-biplots.html)
- en: 'Through biplots, we can interpret how features correlate with the principal
    components shown: positively, negatively, or not much at all.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过双标图，我们可以解释特征如何与所示的主成分相关联：正相关、负相关或几乎没有相关。
- en: '![slide17_2](../Images/52761bfeb5af6f409154528b15a57bac.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![slide17_2](../Images/52761bfeb5af6f409154528b15a57bac.png)'
- en: The directions of the arrow are (\(v_1\), \(v_2\)) where \(v_1\) and \(v_2\)
    are how that specific feature column contributes to PC1 and PC2, respectively.
    \(v_1\) and \(v_2\) are elements of the first and second columns of \(V\), respectively
    (i.e., the first two rows of \(V^T\)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 箭头的方向是(\(v_1\), \(v_2\))，其中\(v_1\)和\(v_2\)是特定特征列对PC1和PC2的贡献，\(v_1\)和\(v_2\)分别是\(V\)的第一列和第二列的元素（即\(V^T\)的前两行）。
- en: Say we were considering feature 3, and say that was the purple arrow labeled
    “520” here (pointing bottom right).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在考虑特征3，并且说这是标记为“520”的紫色箭头（指向右下角）。
- en: \(v_1\) and \(v_2\) are the third elements of the respective columns in \(V\).
    They are scale feature 3’s column vector in the linear transformation to PC1 and
    PC2, respectively.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(v_1\)和\(v_2\)是\(V\)中相应列的第三个元素。它们是特征3的列向量在线性变换到PC1和PC2时的比例。
- en: Here we would infer that \(v_1\) (in the \(x\)/PC1-direction) is positive, meaning
    that a linear increase in feature 3 would correspond to a linear increase of PC1,
    meaning feature 3 and PC1 are positively correlated.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，我们可以推断\(v_1\)（在\(x\)/PC1方向上）是正的，这意味着特征3的线性增加将对应于PC1的线性增加，这意味着特征3和PC1呈正相关。
- en: \(v_2\) (in the \(y\)/pc2-direction) is negative, meaning a linear increase
    in feature 3 would result correspond to a linear decrease in PC2, meaning feature
    3 and PC2 are negatively correlated.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(v_2\)（在\(y\)/pc2方向）为负，意味着特征3的线性增加将导致PC2的线性减少，这意味着特征3和PC2呈负相关。
- en: 25.4 Applications of PCA
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.4 PCA的应用
- en: 25.4.1 PCA in Biology
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.4.1 生物学中的PCA
- en: PCA is commonly used in biomedical contexts, which have many named variables!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: PCA通常用于生物医学背景中，其中有许多命名变量！
- en: To cluster data ([Paper 1](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2680-1),
    [Paper 2](https://www.science.org/doi/10.1126/scirobotics.abk2378))
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行聚类（[论文1](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2680-1)，[论文2](https://www.science.org/doi/10.1126/scirobotics.abk2378)）
- en: To identify correlated variables ([interpret](https://docs.google.com/presentation/d/1-aDu0ILCkPx3iCcJGB3YXci-L4g90Q6AarXU6wffLB8/edit#slide=id.g62cb86badb_0_1128)
    rows of \(V^{T}\) as linear coefficients) ([Paper 3](https://www.nature.com/articles/s41598-017-05714-1)).
    Uses [biplots](https://www.google.com/url?q=https://www.geo.fu-berlin.de/en/v/soga/Geodata-analysis/Principal-Component-Analysis/principal-components-basics/Interpretation-and-visualization/index.html%23:~:text%3DThe%2520biplot%2520is%2520a%2520very,in%2520a%2520single%2520biplot%2520display.%26text%3DThe%2520plot%2520shows%2520the%2520observations,principal%2520components%2520(synthetic%2520variables).&sa=D&source=editors&ust=1682131633152964&usg=AOvVaw2H9SOeMP5kUS890Fkhfthx).
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别相关变量（[解释](https://docs.google.com/presentation/d/1-aDu0ILCkPx3iCcJGB3YXci-L4g90Q6AarXU6wffLB8/edit#slide=id.g62cb86badb_0_1128)
    \(V^{T}\) 的行作为线性系数）（[论文3](https://www.nature.com/articles/s41598-017-05714-1)）。使用[双标图](https://www.google.com/url?q=https://www.geo.fu-berlin.de/en/v/soga/Geodata-analysis/Principal-Component-Analysis/principal-components-basics/Interpretation-and-visualization/index.html%23:~:text%3DThe%2520biplot%2520is%2520a%2520very,in%2520a%2520single%2520biplot%2520display.%26text%3DThe%2520plot%2520shows%2520the%2520observations,principal%2520components%2520(synthetic%2520variables).&sa=D&source=editors&ust=1682131633152964&usg=AOvVaw2H9SOeMP5kUS890Fkhfthx)。
- en: 25.4.2 Why Perform PCA
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.4.2 为什么执行PCA
- en: 'We often perform PCA during the Exploratory Data Analysis (EDA) stage of our
    data science lifecycle (if we already know what to model, we probably don’t need
    PCA); it helps us with:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常在数据科学生命周期的探索性数据分析（EDA）阶段执行PCA（如果我们已经知道要建模什么，我们可能不需要PCA）；它帮助我们：
- en: Visually identifying clusters of similar observations in high dimensions.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高维度中直观地识别相似观察的聚类。
- en: 'Removing irrelevant dimensions if we suspect that the dataset is inherently
    low rank. For example, if the columns are collinear: there are many attributes
    but only a few mostly determine the rest through linear associations.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果怀疑数据集本质上是低秩的，则移除不相关的维度。例如，如果列是共线的：属性很多，但只有少数属性通过线性关联决定了其余属性。
- en: Finding a small basis for representing variations in complex things, e.g., images,
    genes.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找表示复杂事物变化的小基础，例如图像、基因。
- en: Reducing the number of dimensions to make some computation cheaper.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少维度以使某些计算更便宜。
- en: 25.4.3 Image Classification
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.4.3 图像分类
- en: In machine learning, PCA is often used as a preprocessing step prior to training
    a supervised model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，PCA经常用作在训练监督模型之前的预处理步骤。
- en: See the following [demo](https://data100.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2FDS-100%2Ffa23-student&urlpath=lab%2Ftree%2Ffa23-student%2Flecture%2Flec26%2Flec26-fashion-mnist.ipynb&branch=main)
    to see how PCA is useful for building an image classification model based on the
    MNIST-Fashion dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下[演示](https://data100.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2FDS-100%2Ffa23-student&urlpath=lab%2Ftree%2Ffa23-student%2Flecture%2Flec26%2Flec26-fashion-mnist.ipynb&branch=main)以了解PCA如何有助于基于MNIST-Fashion数据集构建图像分类模型。
- en: '![slide21](../Images/24591be128f80f81fc0cababe5a90507.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![slide21](../Images/24591be128f80f81fc0cababe5a90507.png)'
- en: 'The demo shows how we can use PCA during the Exploratory Data Analysis stage
    of our data science lifecycle to: - visually identify clusters of similar observations
    in high dimensions. - find a small basis for representing variations in complex
    things. - reduce the number of dimensions to make some computations cheaper.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 演示显示了我们如何在数据科学生命周期的探索性数据分析阶段使用PCA来：- 在高维度中直观地识别相似观察的聚类。- 寻找表示复杂事物变化的小基础。- 减少维度以使某些计算更便宜。
- en: 25.4.4 Why PCA, then Model?
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.4.4 为什么先PCA，然后建模？
- en: Reduces dimensionality, allowing us to speed up training and reduce the number
    of features, etc.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 降低维度，使我们能够加快训练速度并减少特征数量等。
- en: Avoids multicollinearity in the new features created (i.e. the principal components)
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免新特征中的多重共线性（即主成分）
- en: '![slide21](../Images/2771d6084236722fb7a03b3e4ea177bc.png)*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![slide21](../Images/2771d6084236722fb7a03b3e4ea177bc.png)*'
