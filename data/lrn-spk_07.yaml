- en: Chapter 6\. Spark SQL and Datasets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章. Spark SQL 和 Datasets
- en: In Chapters [4](ch04.html#spark_sql_and_dataframes_introduction_to) and [5](ch05.html#spark_sql_and_dataframes_interacting_wit),
    we covered Spark SQL and the DataFrame API. We looked at how to connect to built-in
    and external data sources, took a peek at the Spark SQL engine, and explored topics
    such as the interoperability between SQL and DataFrames, creating and managing
    views and tables, and advanced DataFrame and SQL transformations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html#spark_sql_and_dataframes_introduction_to)和[第5章](ch05.html#spark_sql_and_dataframes_interacting_wit)中，我们介绍了
    Spark SQL 和 DataFrame API。我们探讨了如何连接内置和外部数据源，瞥见了 Spark SQL 引擎，并探索了 SQL 和 DataFrame
    之间的互操作性，创建和管理视图和表，以及高级 DataFrame 和 SQL 转换。
- en: Although we briefly introduced the Dataset API in [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis),
    we skimmed over the salient aspects of how Datasets—strongly typed distributed
    collections—are created, stored, and serialized and deserialized in Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在[第3章](ch03.html#apache_sparkapostrophes_structured_apis)中简要介绍了 Dataset
    API，但我们只是简略地涉及了如何创建、存储、序列化和反序列化 Datasets —— 强类型分布式集合的显著方面。
- en: 'In this chapter, we go under the hood to understand Datasets: we’ll explore
    working with Datasets in Java and Scala, how Spark manages memory to accommodate
    Dataset constructs as part of the high-level API, and the costs associated with
    using Datasets.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入了解 Datasets：我们将探讨在 Java 和 Scala 中使用 Datasets，Spark 如何管理内存以容纳 Dataset
    构造作为高级 API 的一部分，以及使用 Datasets 所涉及的成本。
- en: Single API for Java and Scala
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java 和 Scala 的单一 API
- en: As you may recall from [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis)
    ([Figure 3-1](ch03.html#structured_apis_in_apache_spark) and [Table 3-6](ch03.html#typed_and_untyped_objects_in_spark)),
    Datasets offer a unified and singular API for strongly typed objects. Among the
    languages supported by Spark, only Scala and Java are strongly typed; hence, Python
    and R support only the untyped DataFrame API.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能记得的那样，在[第3章](ch03.html#apache_sparkapostrophes_structured_apis)中（[图3-1](ch03.html#structured_apis_in_apache_spark)和[表3-6](ch03.html#typed_and_untyped_objects_in_spark)），Datasets
    提供了一个统一且独特的 API 用于强类型对象。在 Spark 支持的语言中，只有 Scala 和 Java 是强类型的；因此，Python 和 R 只支持无类型的
    DataFrame API。
- en: Datasets are domain-specific typed objects that can be operated on in parallel
    using functional programming or the DSL operators you’re familiar with from the
    DataFrame API.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Datasets 是特定于领域的强类型对象，可以使用函数式编程或来自 DataFrame API 的 DSL 操作符并行操作它们。
- en: Thanks to this singular API, Java developers no longer risk lagging behind.
    For example, any future interface or behavior changes to Scala’s `groupBy()`,
    `flatMap()`, `map()`, or `filter()` API will be the same for Java too, because
    it’s a singular interface that is common to both implementations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个统一的 API，Java 开发者不再面临落后的风险。例如，对于 Scala 的 `groupBy()`、`flatMap()`、`map()`
    或 `filter()` API 的任何未来接口或行为更改，对于 Java 也是相同的，因为它是一个公共的统一接口。
- en: Scala Case Classes and JavaBeans for Datasets
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scala 的 Case Classes 和 JavaBeans 用于 Datasets
- en: If you recall from [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis)
    ([Table 3-2](ch03.html#basic_scala_data_types_in_spark)), Spark has internal data
    types, such as `StringType`, `BinaryType`, `IntegerType`, `BooleanType`, and `MapType`,
    that it uses to map seamlessly to the language-specific data types in Scala and
    Java during Spark operations. This mapping is done via encoders, which we discuss
    later in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得从[第3章](ch03.html#apache_sparkapostrophes_structured_apis)中（[表3-2](ch03.html#basic_scala_data_types_in_spark)），Spark
    拥有内部数据类型，如 `StringType`、`BinaryType`、`IntegerType`、`BooleanType` 和 `MapType`，它在
    Spark 操作期间使用这些类型无缝映射到 Scala 和 Java 的语言特定数据类型。这种映射是通过编码器完成的，我们将在本章后面讨论它们。
- en: 'In order to create `Dataset[T]`, where `T` is your typed object in Scala, you
    need a [case class](https://oreil.ly/06xko) that defines the object. Using our
    example data from [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis)
    ([Table 3-1](ch03.html#the_table_like_format_of_a_dataframe)), say we have a JSON
    file with millions of entries about bloggers writing about Apache Spark in the
    following format:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建 `Dataset[T]`，其中 `T` 是你在 Scala 中定义的类型化对象，你需要一个[案例类](https://oreil.ly/06xko)来定义这个对象。使用我们在[第3章](ch03.html#apache_sparkapostrophes_structured_apis)中的示例数据（[表3-1](ch03.html#the_table_like_format_of_a_dataframe)），假设我们有一个
    JSON 文件，包含数百万条关于博客作者写作 Apache Spark 的条目，格式如下：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To create a distributed `Dataset[Bloggers]`, we must first define a Scala case
    class that defines each individual field that comprises a Scala object. This case
    class serves as a blueprint or schema for the typed object `Bloggers`:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建分布式的 `Dataset[Bloggers]`，我们必须先定义一个Scala案例类，该类定义了构成Scala对象的每个单独字段。这个案例类作为
    `Bloggers` 类型对象的蓝图或模式：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now read the file from the data source:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以从数据源读取文件：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Each row in the resulting distributed data collection is of type `Bloggers`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 结果分布式数据集中的每行都是类型为 `Bloggers` 的对象。
- en: 'Similarly, you can create a JavaBean class of type `Bloggers` in Java and then
    use encoders to create a `Dataset<Bloggers>`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，您可以在Java中创建一个名为 `Bloggers` 的JavaBean类，然后使用编码器创建一个 `Dataset<Bloggers>`：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, creating Datasets in Scala and Java requires a bit of forethought,
    as you have to know all the individual column names and types for the rows you
    are reading. Unlike with DataFrames, where you can optionally let Spark infer
    the schema, the Dataset API requires that you define your data types ahead of
    time and that your case class or JavaBean class matches your schema.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，在Scala和Java中创建数据集需要一些预先考虑，因为您必须了解读取的每行的所有单独列名和类型。与DataFrame不同，您可以选择让Spark推断模式，但是Dataset
    API要求您提前定义数据类型，并且您的案例类或JavaBean类必须与您的模式匹配。
- en: Note
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The names of the fields in the Scala case class or Java class definition must
    match the order in the data source. The column names for each row in the data
    are automatically mapped to the corresponding names in the class and the types
    are automatically preserved.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala案例类或Java类定义中，字段的名称必须与数据源中的顺序匹配。数据中每行的列名会自动映射到类中相应的名称，并且类型会自动保留。
- en: You may use an existing Scala case class or JavaBean class if the field names
    match with your input data. Working with the Dataset API is as easy, concise,
    and declarative as working with DataFrames. For most of the Dataset’s transformations,
    you can use the same relational operators you’ve learned about in the previous
    chapters.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果字段名称与输入数据匹配，则可以使用现有的Scala案例类或JavaBean类。使用Dataset API与使用DataFrame一样简单、简洁和声明性。对于大多数数据集的转换，您可以使用与之前章节中学习的相同的关系运算符。
- en: Let’s examine some aspects of working with a sample Dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些使用示例数据集的方面。
- en: Working with Datasets
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据集
- en: 'One simple and dynamic way to create a sample Dataset is using a `SparkSession`
    instance. In this scenario, for illustration purposes, we dynamically create a
    Scala object with three fields: `uid` (unique ID for a user), `uname` (randomly
    generated username string), and `usage` (minutes of server or service usage).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 创建示例数据集的一种简单而动态的方式是使用 `SparkSession` 实例。在此场景中，为了说明目的，我们动态创建一个Scala对象，其中包含三个字段：`uid`（用户的唯一ID）、`uname`（随机生成的用户名字符串）和`usage`（服务器或服务使用的分钟数）。
- en: Creating Sample Data
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建示例数据
- en: 'First, let’s generate some sample data:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们生成一些示例数据：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In Java the idea is similar, but we have to use explicit `Encoder`s (in Scala,
    Spark handles this implicitly):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中，思路类似，但我们必须使用显式的 `Encoder`（在Scala中，Spark会隐式处理这一点）：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The generated Dataset between Scala and Java will differ because the random
    seed algorithm may be different. Hence, your Scala’s and Java’s query results
    will differ.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Scala和Java生成的数据集会有所不同，因为随机种子算法可能不同。因此，Scala和Java的查询结果会有所不同。
- en: Now that we have our generated Dataset, `dsUsage`, let’s perform some of the
    common transformations we have done in previous chapters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了生成的数据集 `dsUsage`，让我们执行一些在之前章节中已经做过的常见转换。
- en: Transforming Sample Data
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换示例数据
- en: Recall that Datasets are strongly typed collections of domain-specific objects.
    These objects can be transformed in parallel using functional or relational operations.
    Examples of these transformations include `map()`, `reduce()`, `filter()`, `select()`,
    and `aggregate()`. As examples of [higher-order functions](https://oreil.ly/KHaqt),
    these methods can take lambdas, closures, or functions as arguments and return
    the results. As such, they lend themselves well to [functional programming](https://oreil.ly/jvWtM).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，数据集是强类型的特定领域对象的集合。这些对象可以并行转换，使用功能或关系操作。这些转换的示例包括 `map()`、`reduce()`、`filter()`、`select()`
    和 `aggregate()`。作为[高阶函数](https://oreil.ly/KHaqt)的示例，这些方法可以接受lambda、闭包或函数作为参数并返回结果。因此，它们非常适合[函数式编程](https://oreil.ly/jvWtM)。
- en: Scala is a functional programming language, and more recently lambdas, functional
    arguments, and closures have been added to Java too. Let’s try a couple of higher-order
    functions in Spark and use functional programming constructs with the sample data
    we created earlier.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Scala 是一种函数式编程语言，最近 Java 也添加了 lambda、函数参数和闭包。让我们在 Spark 中尝试一些高阶函数，并使用之前创建的样本数据进行函数式编程构造。
- en: Higher-order functions and functional programming
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高阶函数和函数式编程
- en: 'For a simple example, let’s use `filter()` to return all the users in our `dsUsage`
    Dataset whose usage exceeds 900 minutes. One way to do this is to use a functional
    expression as an argument to the `filter()` method:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 举个简单的例子，让我们使用 `filter()` 返回 `dsUsage` Dataset 中所有使用超过 900 分钟的用户。一种方法是使用函数表达式作为
    `filter()` 方法的参数：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Another way is to define a function and supply that function as an argument
    to `filter()`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方式是定义一个函数，并将该函数作为 `filter()` 的参数提供：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the first case we used a lambda expression, `{d.usage > 900}`, as an argument
    to the `filter()` method, whereas in the second case we defined a Scala function,
    `def filterWithUsage(u: Usage) = u.usage > 900`. In both cases, the `filter()`
    method iterates over each row of the `Usage` object in the distributed Dataset
    and applies the expression or executes the function, returning a new Dataset of
    type `Usage` for rows where the value of the expression or function is `true`.
    (See the [Scala documentation](https://oreil.ly/5yW8d) for method signature details.)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '在第一个案例中，我们使用了一个 lambda 表达式 `{d.usage > 900}` 作为 `filter()` 方法的参数，而在第二个案例中，我们定义了一个
    Scala 函数 `def filterWithUsage(u: Usage) = u.usage > 900`。在两种情况下，`filter()` 方法迭代分布式
    Dataset 中的每一行 `Usage` 对象，并应用表达式或执行函数，返回一个新的 `Usage` 类型的 Dataset，其中表达式或函数的值为 `true`。（详见
    [Scala 文档](https://oreil.ly/5yW8d) 获取方法签名的详细信息。）'
- en: 'In Java, the argument to `filter()` is of type [`FilterFunction<T>`](https://oreil.ly/PBNt4).
    This can be defined either inline anonymously or with a named function. For this
    example, we will define our function by name and assign it to the variable `f`.
    Applying this function in `filter()` will return a new Dataset with all the rows
    for which our filter condition is `true`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Java 中，`filter()` 的参数类型为 [`FilterFunction<T>`](https://oreil.ly/PBNt4)。这可以匿名内联定义，也可以使用命名函数。在本例中，我们将通过命名方式定义我们的函数，并将其赋值给变量
    `f`。将此函数应用于 `filter()` 将返回一个新的 Dataset，其中包含所有满足我们过滤条件的行（条件为 `true`）：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Not all lambdas or functional arguments must evaluate to `Boolean` values; they
    can return computed values too. Consider this example using the higher-order function
    `map()`, where our aim is to find out the usage cost for each user whose `usage`
    value is over a certain threshold so we can offer those users a special price
    per minute.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的 lambda 或函数参数都必须评估为 `Boolean` 值；它们也可以返回计算出的值。考虑使用高阶函数 `map()` 的这个例子，我们的目的是找出每个
    `usage` 值超过某个阈值的用户的使用费用，以便我们可以为这些用户提供每分钟的特价。
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To use `map()` in Java, you have to define a [`MapFunction<T>`](https://oreil.ly/BP0iY).
    This can either be an anonymous class or a defined class that extends `MapFunction<T>`.
    For this example, we use it inline—that is, in the method call itself:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Java 中使用 `map()`，必须定义一个 [`MapFunction<T>`](https://oreil.ly/BP0iY)。这可以是匿名类，也可以是扩展了
    `MapFunction<T>` 的定义类。在本例中，我们在方法调用本身内联使用它：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Though we have computed values for the cost of usage, we don’t know which users
    the computed values are associated with. How do we get this information?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经计算出了使用费用的值，但我们不知道这些计算值与哪些用户相关联。我们如何获取这些信息呢？
- en: 'The steps are simple:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤很简单：
- en: Create a Scala case class or JavaBean class, `UsageCost`, with an additional
    field or column named `cost`.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 Scala case 类或 JavaBean 类 `UsageCost`，带有一个名为 `cost` 的附加字段或列。
- en: Define a function to compute the `cost` and use it in the `map()` method.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来计算 `cost`，并在 `map()` 方法中使用它。
- en: 'Here’s what this looks like in Scala:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这在 Scala 中的实现如下：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now we have a transformed Dataset with a new column, `cost`, computed by the
    function in our `map()` transformation, along with all the other columns.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个经过转换的 Dataset，其中包含通过我们 `map()` 转换中的函数计算的新列 `cost`，以及所有其他列。
- en: 'Likewise, in Java, if we want the cost associated with each user we need to
    define a JavaBean class `UsageCost` and `MapFunction<T>`. For the complete JavaBean
    example, see the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2);
    for brevity, we will only show the inline `MapFunction<T>` here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在Java中，如果我们想要每个用户关联的成本，我们需要定义一个JavaBean类 `UsageCost` 和 `MapFunction<T>`。有关完整的JavaBean示例，请参见本书的[GitHub
    repo](https://github.com/databricks/LearningSparkV2)；为简洁起见，我们仅在此展示内联的 `MapFunction<T>`：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'There are a few things to observe about using higher-order functions and Datasets:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高阶函数和Datasets需要注意的几点：
- en: We are using typed JVM objects as arguments to functions.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在使用作为函数参数的类型化JVM对象。
- en: We are using dot notation (from object-oriented programming) to access individual
    fields within the typed JVM object, making it easier to read.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用点符号（来自面向对象编程）来访问类型化的JVM对象内的各个字段，使其更易于阅读。
- en: Some of our functions and lambda signatures can be type-safe, ensuring compile-time
    error detection and instructing Spark what data types to work on, what operations
    to perform, etc.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的一些函数和lambda签名可以是类型安全的，确保在编译时检测错误，并指导Spark在哪些数据类型上工作，执行什么操作等。
- en: Our code is readable, expressive, and concise, using Java or Scala language
    features in lambda expressions.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的代码可读性强，表达力强，使用Java或Scala语言特性中的lambda表达式。
- en: 'Spark provides the equivalent of `map()` and `filter()` without higher-order
    functional constructs in both Java and Scala, so you are not forced to use functional
    programming with Datasets or DataFrames. Instead, you can simply use conditional
    DSL operators or SQL expressions: for example, `dsUsage.filter("usage > 900")`
    or `dsUsage($"usage" > 900)`. (For more on this, see [“Costs of Using Datasets”](#costs_of_using_datasets).)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark提供了在Java和Scala中无需高阶函数构造的 `map()` 和 `filter()` 的等效方法，因此您并不需要强制使用Datasets或DataFrames中的函数式编程。相反，您可以简单地使用条件DSL操作符或SQL表达式：例如，`dsUsage.filter("usage
    > 900")` 或 `dsUsage($"usage" > 900)`。（有关更多信息，请参见 [“Costs of Using Datasets”](#costs_of_using_datasets)。）
- en: For Datasets we use encoders, a mechanism to efficiently convert data between
    JVM and Spark’s internal binary format for its data types (more on that in [“Dataset
    Encoders”](#dataset_encoders)).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Datasets，我们使用编码器来高效地在JVM和Spark的内部二进制格式之间转换数据类型（关于此的更多信息请见 [“Dataset Encoders”](#dataset_encoders)）。
- en: Note
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Higher-order functions and functional programming are not unique to Spark Datasets;
    you can use them with DataFrames too. Recall that a DataFrame is a `Dataset[Row]`,
    where `Row` is a generic untyped JVM object that can hold different types of fields.
    The method signature takes expressions or functions that operate on `Row`, meaning
    that each `Row`’s data type can be input value to the expression or function.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 高阶函数和函数式编程并不是Spark Datasets独有的；您也可以在DataFrames中使用它们。回想一下，一个DataFrame是一个 `Dataset[Row]`，其中
    `Row` 是一个通用的未类型化JVM对象，可以包含不同类型的字段。该方法签名接受在 `Row` 上操作的表达式或函数，这意味着每个 `Row` 的数据类型可以作为表达式或函数的输入值。
- en: Converting DataFrames to Datasets
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将DataFrames转换为Datasets
- en: 'For strong type checking of queries and constructs, you can convert DataFrames
    to Datasets. To convert an existing DataFrame `df` to a Dataset of type `SomeCaseClass`,
    simply use the `df.as[SomeCaseClass]` notation. We saw an example of this earlier:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对查询和结构进行强类型检查，您可以将DataFrame转换为Datasets。要将现有的DataFrame `df` 转换为类型为 `SomeCaseClass`
    的Dataset，只需使用 `df.as[SomeCaseClass]` 表示法。我们之前已经看到了一个例子：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`spark.read.format("json")` returns a `DataFrame<Row>`, which in Scala is a
    type alias for `Dataset[Row]`. Using `.as[Bloggers]` instructs Spark to use encoders,
    discussed later in this chapter, to serialize/deserialize objects from Spark’s
    internal memory representation to JVM `Bloggers` objects.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.read.format("json")` 返回一个 `DataFrame<Row>`，在Scala中是 `Dataset[Row]` 的类型别名。使用
    `.as[Bloggers]` 指示Spark使用编码器（本章后面讨论）来将对象从Spark的内部内存表示序列化/反序列化为JVM `Bloggers` 对象。'
- en: Memory Management for Datasets and DataFrames
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Datasets和DataFrames的内存管理
- en: 'Spark is an intensive in-memory distributed big data engine, so its efficient
    use of memory is crucial to its execution speed.^([1](ch06.html#ch01fn6)) Throughout
    its release history, Spark’s usage of memory has [significantly evolved](https://oreil.ly/sL56g):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一种高效的内存分布式大数据引擎，因此其对内存的高效利用对其执行速度至关重要。^([1](ch06.html#ch01fn6)) 在其发布历史中，Spark对内存的使用已经[显著发展](https://oreil.ly/sL56g)：
- en: Spark 1.0 used RDD-based Java objects for memory storage, serialization, and
    deserialization, which was expensive in terms of resources and slow. Also, storage
    was allocated *on the Java heap,* so you were at the mercy of the JVM’s garbage
    collection (GC) for large data sets.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 1.0使用基于RDD的Java对象进行内存存储、序列化和反序列化，这在资源和速度方面都很昂贵。此外，存储是在Java堆上分配的，因此在处理大数据集时受到JVM的垃圾回收（GC）的影响。
- en: Spark 1.x introduced [Project Tungsten](https://oreil.ly/wCQZB). One of its
    prominent features was a new internal row-based format to lay out Datasets and
    DataFrames in off-heap memory, using offsets and pointers. Spark uses an efficient
    mechanism called *encoders* to serialize and deserialize between the JVM and its
    internal Tungsten format. Allocating memory off-heap means that Spark is less
    encumbered by GC.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 1.x引入了[Project Tungsten](https://oreil.ly/wCQZB)。其突出特点之一是引入了新的基于行的内部格式，用于在非堆内存中布局Datasets和DataFrames，使用偏移量和指针。Spark使用高效的*encoders*机制在JVM和其内部Tungsten格式之间进行序列化和反序列化。在非堆内存分配内存意味着Spark不会受到GC的过多限制。
- en: Spark 2.x introduced the [second-generation Tungsten engine](https://oreil.ly/hmjz_),
    featuring whole-stage code generation and vectorized column-based memory layout.
    Built on ideas and techniques from modern compilers, this new version also capitalized
    on modern CPU and cache architectures for fast parallel data access with the “single
    instruction, multiple data” (SIMD) approach.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.x引入了[第二代Tungsten引擎](https://oreil.ly/hmjz_)，具备整体阶段代码生成和矢量化基于列的内存布局。建立在现代编译器的思想和技术之上，这个新版本还利用了现代CPU和缓存架构，采用“单指令多数据”（SIMD）方法进行快速并行数据访问。
- en: Dataset Encoders
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dataset Encoders
- en: Encoders convert data in off-heap memory from Spark’s internal Tungsten format
    to JVM Java objects. In other words, they serialize and deserialize Dataset objects
    from Spark’s internal format to JVM objects, including primitive data types. For
    example, an `Encoder[T]` will convert from Spark’s internal Tungsten format to
    `Dataset[T]`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Encoders将数据从Spark的内部Tungsten格式转换为JVM Java对象的非堆内存。换句话说，它们将Spark的Dataset对象从内部格式序列化和反序列化为JVM对象，包括基本数据类型。例如，一个`Encoder[T]`将会把数据从Spark的内部Tungsten格式转换为`Dataset[T]`。
- en: Spark has built-in support for automatically generating encoders for primitive
    types (e.g., string, integer, long), Scala case classes, and JavaBeans. Compared
    to Java and Kryo serialization and deserialization, Spark encoders are [significantly
    faster](https://oreil.ly/zz-x9).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Spark具有内置支持，可自动生成原始类型（例如字符串、整数、长整数）、Scala case类和JavaBeans的encoders。与Java和Kryo序列化和反序列化相比，Spark的encoders速度[显著更快](https://oreil.ly/zz-x9)。
- en: 'In our earlier Java example, we explicitly created an encoder:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的Java示例中，我们明确地创建了一个编码器：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: However, for Scala, Spark automatically generates the bytecode for these efficient
    converters. Let’s take a peek at Spark’s internal Tungsten row-based format.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于Scala，Spark会自动为这些高效的转换器生成字节码。让我们来看看Spark的内部Tungsten基于行的格式。
- en: Spark’s Internal Format Versus Java Object Format
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark的内部格式与Java对象格式对比
- en: Java objects have large overheads—header info, hashcode, Unicode info, etc.
    Even a simple Java string such as “abcd” takes 48 bytes of storage, instead of
    the 4 bytes you might expect. Imagine the overhead to create, for example, a `MyClass(Int,
    String, String)` object.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Java对象具有较大的开销——头部信息、哈希码、Unicode信息等。即使是一个简单的Java字符串如“abcd”，也占用48字节的存储空间，而不是你可能期望的4字节。想象一下创建一个`MyClass(Int,
    String, String)`对象的开销。
- en: Instead of creating JVM-based objects for Datasets or DataFrames, Spark allocates
    *off-heap* Java memory to lay out their data and employs encoders to convert the
    data from in-memory representation to JVM object. For example, [Figure 6-1](#jvm_object_stored_in_contiguous_off_heap)
    shows how the JVM object `MyClass(Int, String, String)` would be stored internally.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 不创建基于JVM的Datasets或DataFrames对象，而是Spark分配*非堆*Java内存来布局它们的数据，并使用encoders将数据从内存表示转换为JVM对象。例如，[Figure 6-1](#jvm_object_stored_in_contiguous_off_heap)展示了JVM对象`MyClass(Int,
    String, String)`在内部的存储方式。
- en: '![JVM object stored in contiguous off-heap Java memory managed by Spark](assets/lesp_0601.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![JVM对象存储在由Spark管理的连续非堆Java内存中](assets/lesp_0601.png)'
- en: Figure 6-1\. JVM object stored in contiguous off-heap Java memory managed by
    Spark
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. JVM对象存储在由Spark管理的连续非堆Java内存中
- en: When data is stored in this contiguous manner and accessible through pointer
    arithmetic and offets, encoders can quickly serialize or deserialize that data.
    What does that mean?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据以这种连续的方式存储并通过指针算术和偏移量访问时，编码器可以快速地序列化或反序列化这些数据。这意味着什么？
- en: Serialization and Deserialization (SerDe)
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列化和反序列化（SerDe）
- en: A concept not new in distributed computing, where data frequently travels over
    the network among computer nodes in a cluster, *serialization and deserialization*
    is the process by which a typed object is *encoded* (serialized) into a binary
    presentation or format by the sender and *decoded* (deserialized) from binary
    format into its respective data-typed object by the receiver.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式计算中并不是一个新概念，数据经常在集群中的计算机节点之间传输，*序列化和反序列化*是发送方将一个类型化对象进行*编码*（序列化）成二进制表示或格式的过程，并由接收方将其从二进制格式进行*解码*（反序列化）成其相应的数据类型对象。
- en: For example, if the JVM object `MyClass` in [Figure 6-1](#jvm_object_stored_in_contiguous_off_heap)
    had to be shared among nodes in a Spark cluster, the sender would serialize it
    into an array of bytes, and the receiver would deserialize it back into a JVM
    object of type `MyClass`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果在Spark集群中的节点之间共享JVM对象`MyClass`（参见图[6-1](#jvm_object_stored_in_contiguous_off_heap)），发送方将其序列化为字节数组，接收方将其反序列化回类型为`MyClass`的JVM对象。
- en: The JVM has its own built-in Java serializer and deserializer, but it’s inefficient
    because (as we saw in the previous section) the Java objects created by the JVM
    in the heap memory are bloated. Hence, the process is slow.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: JVM具有其内置的Java序列化器和反序列化器，但效率低下，因为（正如我们在前一节中看到的那样）JVM在堆内存中创建的Java对象臃肿。因此，该过程较慢。
- en: 'This is where the Dataset encoders come to the rescue, for a few reasons:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是数据集编码器发挥作用的地方，由于几个原因：
- en: Spark’s internal Tungsten binary format (see Figures [6-1](#jvm_object_stored_in_contiguous_off_heap)
    and [6-2](#sparkapostrophes_internal_tungsten_row_b)) stores objects off the Java
    heap memory, and it’s compact so those objects occupy less space.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的内部Tungsten二进制格式（参见图[6-1](#jvm_object_stored_in_contiguous_off_heap)和[6-2](#sparkapostrophes_internal_tungsten_row_b)）将对象存储在Java堆内存之外，并且紧凑，因此这些对象占用的空间较少。
- en: Encoders can quickly serialize by traversing across the memory using simple
    pointer arithmetic with memory addresses and offsets ([Figure 6-2](#sparkapostrophes_internal_tungsten_row_b)).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器可以通过使用简单的指针算术与内存地址和偏移量遍历内存，快速进行序列化（参见图[6-2](#sparkapostrophes_internal_tungsten_row_b)）。
- en: On the receiving end, encoders can quickly deserialize the binary representation
    into Spark’s internal representation. Encoders are not hindered by the JVM’s garbage
    collection pauses.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在接收端，编码器可以快速将二进制表示反序列化为Spark内部的表示形式。编码器不受JVM垃圾收集暂停的影响。
- en: '![Spark’s internal Tungsten row-based format](assets/lesp_0602.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![Spark内部Tungsten基于行的格式](assets/lesp_0602.png)'
- en: Figure 6-2\. Spark’s internal Tungsten row-based format
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. Spark内部Tungsten基于行的格式
- en: However, most good things in life come at a price, as we discuss next.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，生活中的大多数好事都是要付出代价的，接下来我们将讨论这一点。
- en: Costs of Using Datasets
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据集的成本
- en: In [“DataFrames Versus Datasets”](ch03.html#dataframes_versus_datasets) in [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis),
    we outlined some of the benefits of using Datasets—but these benefits come at
    a cost. As noted in the preceding section, when Datasets are passed to higher-order
    functions such as `filter()`, `map()`, or `flatMap()` that take lambdas and functional
    arguments, there is a cost associated with deserializing from Spark’s internal
    Tungsten format into the JVM object.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“数据框架与数据集”](ch03.html#dataframes_versus_datasets)中的[第三章](ch03.html#apache_sparkapostrophes_structured_apis)，我们概述了使用数据集的一些好处，但这些好处也伴随着代价。如前一节所述，当数据集传递给高阶函数（如`filter()`、`map()`或`flatMap()`）时，这些函数接受Lambda和函数参数，从Spark内部Tungsten格式反序列化为JVM对象存在成本。
- en: Compared to other serializers used before encoders were introduced in Spark,
    this cost is minor and tolerable. However, over larger data sets and many queries,
    this cost accrues and can affect performance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与在引入Spark编码器之前使用的其他序列化器相比，这种成本是较小且可容忍的。然而，对于更大的数据集和许多查询，这种成本会累积，并可能影响性能。
- en: Strategies to Mitigate Costs
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓解成本的策略
- en: One strategy to mitigate excessive serialization and deserialization is to use
    DSL expressions in your queries and avoid excessive use of lambdas as anonymous
    functions as arguments to higher-order functions. Because lambdas are anonymous
    and opaque to the Catalyst optimizer until runtime, when you use them it cannot
    efficiently discern what you’re doing (you’re not telling Spark *what to do*)
    and thus cannot optimize your queries (see [“The Catalyst Optimizer”](ch03.html#the_catalyst_optimizer)
    in [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解过度序列化和反序列化的一种策略是在查询中使用DSL表达式，避免将lambda作为匿名函数传递给高阶函数。因为lambda是匿名的且在运行时对Catalyst优化器不透明，所以当您使用它们时，它不能有效地辨别您在做什么（您没有告诉Spark
    *要做什么*），因此不能优化您的查询（参见[“The Catalyst Optimizer”](ch03.html#the_catalyst_optimizer)在[第3章](ch03.html#apache_sparkapostrophes_structured_apis)）。
- en: The second strategy is to chain your queries together in such a way that serialization
    and deserialization is minimized. Chaining queries together is a common practice
    in Spark.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种策略是将您的查询链在一起，以最小化序列化和反序列化。在Spark中，将查询链在一起是一种常见的做法。
- en: 'Let’s illustrate with a simple example. Suppose we have a Dataset of type `Person`,
    where `Person` is defined as a Scala case class:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来说明。假设我们有一个类型为`Person`的数据集，其中`Person`定义为Scala案例类：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We want to issue a set of queries to this Dataset, using functional programming.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望对这个数据集发出一系列查询，使用函数式编程。
- en: 'Let’s examine a case where we compose a query inefficiently, in such a way
    that we unwittingly incur the cost of repeated serialization and deserialization:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个案例，我们以一种低效的方式组合查询，从而无意中产生了重复序列化和反序列化的成本：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can observe in [Figure 6-3](#an_inefficient_way_to_chain_queries_with),
    each time we move from lambda to DSL (`filter($"salary" > 8000)`) we incur the
    cost of serializing and deserializing the `Person` JVM object.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[图6-3](#an_inefficient_way_to_chain_queries_with)中所观察到的那样，每当我们从lambda转到DSL（`filter($"salary"
    > 8000)`）时，我们都会产生序列化和反序列化`Person` JVM对象的成本。
- en: '![An inefficient way to chain queries with lambdas and DSL](assets/lesp_0603.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![使用lambda和DSL串联查询的低效方法](assets/lesp_0603.png)'
- en: Figure 6-3\. An inefficient way to chain queries with lambdas and DSL
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. 使用lambda和DSL串联查询的低效方法
- en: 'By contrast, the following query uses only DSL and no lambdas. As a result,
    it’s much more efficient—no serialization/deserialization is required for the
    entire composed and chained query:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，下面的查询仅使用DSL而不使用lambda。因此，它效率更高——在整个组合和链式查询过程中不需要序列化/反序列化：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: For the curious, you can see the timing difference between the two runs in the
    notebook for this chapter in the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于好奇的人，您可以在本书GitHub存储库的本章笔记本中查看两次运行之间的时间差异。
- en: Summary
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we elaborated on how to work with Datasets in Java and Scala.
    We explored how Spark manages memory to accommodate Dataset constructs as part
    of its unified and high-level API, and we considered some of the costs associated
    with using Datasets and how to mitigate those costs. We also showed you how to
    use Java and Scala’s functional programming constructs in Spark.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细讨论了如何在Java和Scala中处理数据集。我们探讨了Spark如何管理内存以适应数据集构造作为其统一且高级API的一部分，并考虑了使用数据集的一些成本及其如何减少这些成本。我们还展示了如何在Spark中使用Java和Scala的函数式编程构造。
- en: Finally, we took a look under the hood at how encoders serialize and deserialize
    from Spark’s internal Tungsten binary format to JVM objects.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们深入探讨了编码器如何将数据从Spark的内部Tungsten二进制格式序列化和反序列化为JVM对象。
- en: In the next chapter, we’ll look at how to optimize Spark by examining efficient
    I/O strategies, optimizing and tuning Spark configurations, and what attributes
    and signals to look for while debugging Spark applications.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过检查高效的I/O策略、优化和调整Spark配置，以及在调试Spark应用程序时要查找的属性和信号，来优化Spark。
- en: ^([1](ch06.html#ch01fn6-marker)) For more details on how Spark manages memory,
    check out the references provided in the text and the presentations [“Apache Spark
    Memory Management”](https://oreil.ly/BlR_u) and [“Deep Dive into Project Tungsten
    Bringing Spark Closer to Bare Metal”](https://oreil.ly/YuH3a).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.html#ch01fn6-marker)) 想要了解更多关于Spark如何管理内存的详细信息，请参阅文本和演示中提供的参考资料以及[“Apache
    Spark Memory Management”](https://oreil.ly/BlR_u)和[“Deep Dive into Project Tungsten
    Bringing Spark Closer to Bare Metal”](https://oreil.ly/YuH3a)。
