- en: Chapter 8\. Streaming SQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。流SQL
- en: Let’s talk SQL. In this chapter, we’re going to start somewhere in the middle
    with the punchline, jump back in time a bit to establish additional context, and
    finally jump back to the future to wrap everything up with a nice bow. Imagine
    Quentin Tarantino held a degree in computer science and was super pumped to tell
    the world about the finer points of streaming SQL, and so he offered to ghostwrite
    this chapter with me; it’s sorta like that. Minus the violence.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈SQL。在本章中，我们将从中间某个地方开始，然后回到过去一点，以建立额外的背景，最后再回到未来，用一个漂亮的蝴蝶结来总结一切。想象一下，如果昆汀·塔伦蒂诺拥有计算机科学学位，并且非常兴奋地向世界讲述流SQL的精髓，所以他提出要和我一起幽灵写作这一章；有点像那样。当然没有暴力。
- en: What Is Streaming SQL?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是流SQL？
- en: I would argue that the answer to this question has eluded our industry for decades.
    In all fairness, the database community has understood maybe 99% of the answer
    for quite a while now. But I have yet to see a truly cogent and comprehensive
    definition of streaming SQL that encompasses the full breadth of robust streaming
    semantics. That’s what we’ll try to come up with here, although it would be hubris
    to assume we’re 100% of the way there now. Maybe 99.1%? Baby steps.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这个问题在我们行业中已经困扰了几十年。公平地说，数据库界可能已经理解了答案的99%。但我还没有看到一个真正有力和全面的流SQL定义，它包括了强大的流语义的全部广度。这就是我们将在这里尝试提出的，尽管假设我们现在已经走了99.1%的路，这可能是傲慢的。一步一步。
- en: Regardless, I want to point out up front that most of what we’ll discuss in
    this chapter is still purely hypothetical as of the time of writing. This chapter
    and the one that follows (covering streaming joins) both describe an idealistic
    vision for what streaming SQL could be. Some pieces are already implemented in
    systems like Apache Calcite, Apache Flink, and Apache Beam. Many others aren’t
    implemented anywhere. Along the way, I’ll try to call out a few of the things
    that do exist in concrete form, but given what a moving target that is, your best
    bet is to simply consult the documentation for your specific system of interest.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 不管怎样，我想提前指出，我们在本章中讨论的大部分内容在写作时仍然是纯粹假设的。本章和接下来的一章（涵盖流连接）都描述了流SQL可能的理想愿景。一些部分已经在Apache
    Calcite、Apache Flink和Apache Beam等系统中实现。许多其他部分在任何地方都没有实现。在这个过程中，我会尽量指出一些已经以具体形式存在的东西，但考虑到这是一个不断变化的目标，你最好的选择就是简单地查阅你感兴趣的特定系统的文档。
- en: On that note, it’s also worth highlighting that the vision for streaming SQL
    presented here is the result of a collaborative discussion between the Calcite,
    Flink, and Beam communities. Julian Hyde, the lead developer on Calcite, has [long
    pitched](http://bit.ly/2JTzR4V) his vision for what streaming SQL might look like.
    In 2016, members of the Flink community integrated Calcite SQL support into Flink
    itself, and began adding streaming-specific features such as windowing constructs
    to the Calcite SQL dialect. Then, in 2017, all three communities began a [discussion](http://s.apache.org/streaming-sql-spec)
    to try to come to agreement on what language extensions and semantics for robust
    stream processing in Calcite SQL should look like. This chapter attempts to distill
    the ideas from that discussion down into a clear and cohesive narrative about
    integrating streaming concepts into SQL, regardless of whether it’s Calcite or
    some other dialect.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，这里提出的流SQL的愿景是Calcite、Flink和Beam社区之间合作讨论的结果。Calcite的首席开发人员Julian Hyde长期以来一直提出了他对流SQL可能的样子的愿景。2016年，Flink社区的成员将Calcite
    SQL支持集成到Flink本身，并开始向Calcite SQL方言添加流特定功能，如窗口构造。然后，在2017年，所有三个社区开始了一场讨论，试图就Calcite
    SQL中用于强大流处理的语言扩展和语义达成一致。本章试图将该讨论中的想法概括为一个清晰而连贯的叙述，关于将流概念整合到SQL中，无论是Calcite还是其他方言。
- en: Relational Algebra
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关系代数
- en: 'When talking about what streaming means for SQL, it’s important to keep in
    mind the theoretical foundation of SQL: relational algebra. Relational algebra
    is simply a mathematical way of describing relationships between data that consist
    of named, typed tuples. At the heart of relational algebra is the relation itself,
    which is a set of these tuples. In classic database terms, a relation is something
    akin to a table, be it a physical database table, the result of a SQL query, a
    view (materialized or otherwise), and so on; it’s a set of rows containing named
    and typed columns of data.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到SQL的流式处理意味着什么，重要的是要记住SQL的理论基础：关系代数。关系代数简单地描述了由命名、类型化元组组成的数据之间的关系的数学方式。在关系代数的核心是关系本身，它是这些元组的集合。在经典的数据库术语中，关系类似于表，无论是物理数据库表，SQL查询的结果，视图（实体化或其他），等等；它是包含命名和类型化数据列的行的集合。
- en: 'One of the more critical aspects of relational algebra is its closure property:
    applying any operator from the relational algebra to any valid relation¹ always
    yields another relation. In other words, relations are the common currency of
    relational algebra, and all operators consume them as input and produce them as
    output.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 关系代数的一个更为关键的方面是其封闭性质：将关系代数中的任何运算符应用于任何有效的关系¹，总是产生另一个关系。换句话说，关系是关系代数的通用货币，所有运算符都将其作为输入并将其作为输出。
- en: 'Historically, many attempts to support streaming in SQL have fallen short of
    satisfying the closure property. They treat streams separately from classic relations,
    providing new operators to convert between the two, and restricting the operations
    that can be applied to one or the other. This significantly raises the bar of
    adoption for any such streaming SQL system: would-be users must learn the new
    operators and understand the places where they’re applicable, where they aren’t,
    and similarly relearn the rules of applicability in this new world for any old
    operators. What’s worse, most of these systems still fall short of providing the
    full suite of streaming semantics that we would want, such as support for robust
    out-of-order processing and strong temporal join support (the latter of which
    we cover in Chapter 9). As a result, I would argue that it’s basically impossible
    to name any existing streaming SQL implementation that has achieved truly broad
    adoption. The additional cognitive overhead and restricted capabilities of such
    streaming SQL systems have ensured that they remain a niche enterprise.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，许多支持SQL中的流式处理的尝试都未能满足封闭性质。它们将流与经典关系分开处理，提供新的运算符来在两者之间转换，并限制可以应用于其中一个或另一个的操作。这显著提高了任何此类流式SQL系统的采用门槛：潜在用户必须学习新的运算符，并理解它们适用的地方，以及它们不适用的地方，并且同样需要重新学习在这个新世界中任何旧运算符的适用规则。更糟糕的是，这些系统中大多数仍然无法提供我们想要的完整流语义套件，比如对强大的无序处理和强大的时间连接支持（我们将在第9章中介绍后者）的支持。因此，我认为基本上不可能指出任何现有的流式SQL实现已经实现了真正广泛的采用。这些流式SQL系统的额外认知负担和受限能力确保它们仍然是一个小众企业。
- en: To change that, to truly bring streaming SQL to the forefront, what we need
    is a way for streaming to become a first-class citizen within the relational algebra
    itself, such that the entire standard relational algebra can apply naturally in
    both streaming and nonstreaming use cases. That isn’t to say that streams and
    tables should be treated as exactly the same thing; they most definitely are not
    the same, and recognizing that fact lends clarity to understanding and power to
    navigating the stream/table relationship, as we’ll see shortly. But the core algebra
    should apply cleanly and naturally to both worlds, with minimal extensions beyond
    the standard relational algebra only in the cases where absolutely necessary.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了真正将流式SQL引入前沿，我们需要一种方法，使流式处理在关系代数本身内成为一等公民，以便标准的关系代数可以自然地适用于流式和非流式用例。这并不是说流和表应该被视为完全相同的东西；它们绝对不是一样的，认识到这一点可以清晰地理解和掌握流/表关系的力量，我们很快就会看到。但是核心代数应该干净自然地适用于两个世界，只有在绝对必要的情况下才需要在标准关系代数之外进行最小的扩展。
- en: Time-Varying Relations
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时变关系
- en: 'To cut to the chase, the punchline I referred to at the beginning of the chapter
    is this: the key to naturally integrating streaming into SQL is to extend relations,
    the core data objects of relational algebra, to represent a set of data *over
    time* rather than a set of data at a *specific point* in time. More succinctly,
    instead of *point-in-time* relations, we need *time-varying relations*.²'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我在本章开头提到的要点是：将流式处理自然地整合到SQL中的关键是扩展关系代数的核心数据对象，以表示一组数据随着时间的推移而不是在特定时间点的数据集。更简洁地说，我们需要的不是“特定时间点”的关系，而是“时变关系”。
- en: But what are time-varying relations? Let’s first define them in terms of classic
    relational algebra, after which we’ll also consider their relationship to stream
    and table theory.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但是时变关系是什么？让我们首先从经典关系代数的角度来定义它们，之后我们还将考虑它们与流和表理论的关系。
- en: In terms of relational algebra, a time-varying relation is really just the evolution
    of a classic relation over time. To understand what I mean by that, imagine a
    raw dataset consisting of user events. Over time, as users generate new events,
    the dataset continues to grow and evolve. If you observe that set at a specific
    point in time, that’s a classic relation. But if you observe the holistic evolution
    of the set *over time*, that’s a time-varying relation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从关系代数的角度来看，时变关系实际上只是经典关系随时间的演变。要理解我的意思，想象一个由用户事件组成的原始数据集。随着用户生成新事件，数据集会不断增长和演变。如果你在特定时间观察这个集合，那就是一个经典关系。但是如果你观察这个集合随着时间的整体演变，那就是一个时变关系。
- en: Put differently, if classic relations are like two-dimensional tables consisting
    of named, typed columns in the x-axis and rows of records in the y-axis, time-varying
    relations are like three-dimensional tables with x- and y-axes as before, but
    an additional z-axis capturing different versions of the two-dimensional table
    over time. As the relation changes, new snapshots of the relation are added in
    the z dimension.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果经典关系就像是由x轴上具有命名和类型的列和y轴上的记录行组成的二维表，那么时变关系就像是具有x和y轴的三维表，但是还有一个额外的z轴，用来捕捉随时间变化的二维表的不同版本。随着关系的变化，关系的新快照被添加到z维度中。
- en: 'Let’s look at an example. Imagine our raw dataset is users and scores; for
    example, per-user scores from a mobile game as in most of the other examples throughout
    the book. And suppose that our example dataset here ultimately ends up looking
    like this when observed at a specific point in time, in this case 12:07:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。想象一下我们的原始数据集是用户和分数；例如，来自手机游戏的每个用户的分数，就像本书中大部分其他例子一样。假设我们的例子数据集最终在特定时间点观察时看起来像这样，比如12:07：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In other words, it recorded the arrivals of four scores over time: Julie’s
    score of 7 at 12:01, both Frank’s score of 3 and Julie’s second score of 1 at
    12:03, and, finally, Julie’s third score of 4 at 12:07 (note that the `Time` column
    here contains processing-time timestamps representing the *arrival time* of the
    records within the system; we get into event-time timestamps a little later on).
    Assuming these were the only data to ever arrive for this relation, it would look
    like the preceding table any time we observed it after 12:07\. But if instead
    we had observed the relation at 12:01, it would have looked like the following,
    because only Julie’s first score would have arrived by that point:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它记录了随时间到达的四个分数：12:01时朱莉的7分，12:03时弗兰克的3分和朱莉的第二个分数1分，最后12:07时朱莉的第三个分数4分（请注意，这里的“时间”列包含表示系统内记录的*到达时间*的处理时间戳；我们稍后会介绍事件时间戳）。假设这是该关系曾经到达的唯一数据，那么无论我们在12:07之后何时观察它，它看起来都像前面的表格。但如果我们在12:01观察关系，它会看起来像下面这样，因为那时只有朱莉的第一个分数到达了：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If we had then observed it again at 12:03, Frank’s score and Julie’s second
    score would have also arrived, so the relation would have evolved to look like
    this:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在12:03再次观察它，弗兰克的分数和朱莉的第二个分数也会到达，所以关系会发展成这样：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'From this example we can begin to get a sense for what the *time-varying* relation
    for this dataset must look like: it would capture the entire evolution of the
    relation over time. Thus, if we observed the time-varying relation (or TVR) at
    or after 12:07, it would thus look like the following (note the use of a hypothetical
    `TVR` keyword to signal that we want the query to return the full time-varying
    relation, not the standard point-in-time snapshot of a classic relation):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个例子，我们可以开始对这个数据集的*时间变化*关系有所了解：它将捕捉关系随时间的整体演变。因此，如果我们在12:07或之后观察时间变化关系（或TVR），它将看起来像下面这样（请注意使用假设的`TVR`关键字来表示我们希望查询返回完整的时间变化关系，而不是经典关系的标准时点快照）：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Because the printed/digital page remains constrained to two dimensions, I’ve
    taken the liberty of flattening the third dimension into a grid of two-dimensional
    relations. But you can see how the time-varying relation essentially consists
    of a sequence of classic relations (ordered left to right, top to bottom), each
    capturing the full state of the relation for a specific range of time (all of
    which, by definition, are contiguous).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于印刷/数字页面仍然受限于二维，我已经将第三维压缩成了二维关系网格。但你可以看到，时间变化关系本质上是一系列经典关系（从左到右，从上到下排序），每个关系捕捉了特定时间范围内关系的完整状态（根据定义，所有这些时间范围都是连续的）。
- en: 'What’s important about defining time-varying relations this way is that they
    really are, for all intents and purposes, just a sequence of classic relations
    that each exist independently within their own disjointed (but adjacent) time
    ranges, with each range capturing a period of time during which the relation did
    not change. This is important, because it means that the application of a relational
    operator to a time-varying relation is equivalent to individually applying that
    operator to each classic relation in the corresponding sequence. And taken one
    step further, the result of individually applying a relational operator to a sequence
    of relations, each associated with a time interval, will always yield a corresponding
    sequence of relations with the same time intervals. In other words, the result
    is a corresponding time-varying relation. This definition gives us two very important
    properties:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 定义时间变化关系的重要之处在于，它们实际上只是一系列经典关系，每个关系在其自己的不相交（但相邻）时间范围内独立存在，每个时间范围捕捉了关系在其中关系未发生变化的时间段。这一点很重要，因为这意味着将关系运算符应用于时间变化关系等同于分别将该运算符应用于相应序列中的每个经典关系。再进一步，将关系运算符分别应用于与时间间隔相关联的一系列关系的结果，将始终产生具有相同时间间隔的相应关系序列。换句话说，结果是相应的时间变化关系。这个定义给我们带来了两个非常重要的属性：
- en: The *full set of operators* from classic relational algebra *remain valid* when
    applied to time-varying relations, and furthermore continue to behave exactly
    as you’d expect.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从经典关系代数中的*完整运算符集*在应用于时间变化关系时*仍然有效*，而且继续表现得正如你所期望的那样。
- en: The *closure property* of relational algebra *remains intact* when applied to
    time-varying relations.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当应用于时间变化关系时，关系代数的*闭包性*仍然保持完整。
- en: 'Or more succinctly, *all the rules of classic relational algebra continue to
    hold when applied to time-varying relations*. This is huge, because it means that
    our substitution of time-varying relations for classic relations hasn’t altered
    the parameters of the game in any way. Everything continues to work the way it
    did back in classic relational land, just on sequences of classic relations instead
    of singletons. Going back to our examples, consider two more time-varying relations
    over our raw dataset, both observed at some time after 12:07\. First a simple
    filtering relation using a `WHERE` clause:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 或者更简洁地说，*当应用于时间变化关系时，所有经典关系代数的规则仍然保持不变*。这是非常重要的，因为这意味着我们用时间变化关系替代经典关系并没有以任何方式改变游戏的参数。一切继续按照经典关系领域的方式运作，只是在一系列经典关系而不是单个关系上。回到我们的例子，考虑一下我们原始数据集上的另外两个时间变化关系，都是在12:07之后观察到的。首先是使用`WHERE`子句的简单过滤关系：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you would expect, this relation looks a lot like the preceding one, but with
    Frank’s scores filtered out. Even though the time-varying relation captures the
    added dimension of time necessary to record the evolution of this dataset over
    time, the query behaves exactly as you would expect, given your understanding
    of SQL.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所期望的那样，这个关系看起来很像前面的关系，但弗兰克的分数被过滤掉了。尽管时间变化关系捕捉了记录数据集随时间演变所需的额外维度时间，但查询的行为与你对SQL的理解一样。
- en: 'For something a little more complex, let’s consider a grouping relation in
    which we’re summing up all the per-user scores to generate a total overall score
    for each user:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂一些的情况，让我们考虑一个分组关系，我们将所有每个用户的得分相加，以生成每个用户的总体得分：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Again, the time-varying version of this query behaves exactly as you would expect,
    with each classic relation in the sequence simply containing the sum of the scores
    for each user. And indeed, no matter how complicated a query we might choose,
    the results are always identical to applying that query independently to the commensurate
    classic relations composing the input time-varying relation. I cannot stress enough
    how important this is!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这个查询的时间变化版本的行为与你所期望的完全一样，序列中的每个经典关系简单地包含了每个用户得分的总和。而且，无论我们选择多么复杂的查询，结果总是与独立应用该查询到输入时间变化关系组成的相应经典关系相同。我无法强调这一点有多重要！
- en: All right, that’s all well and good, but time-varying relations themselves are
    more of a theoretical construct than a practical, physical manifestation of data;
    it’s pretty easy to see how they could grow to be quite huge and unwieldy for
    large datasets that change frequently. To see how they actually tie into real-world
    stream processing, let’s now explore the relationship between time-varying relations
    and stream and table theory.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这一切都很好，但时间变化关系本身更多的是一个理论构想，而不是数据的实际、物理表现；很容易看出，它们可能会变得非常庞大和难以控制，对于频繁变化的大型数据集来说。为了了解它们如何实际与现实世界的流处理联系起来，让我们现在探讨时间变化关系与流和表理论之间的关系。
- en: Streams and Tables
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流和表
- en: 'For this comparison, let’s consider again our grouped time-varying relation
    that we looked at earlier:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个比较，让我们再次考虑一下我们之前看过的分组时间变化关系：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We understand that this sequence captures the full history of the relation over
    time. Given our understanding of tables and streams from Chapter 6, it’s not too
    difficult to understand how time-varying relations relate to stream and table
    theory.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道这个序列捕捉了关系随时间的完整历史。鉴于我们在第6章对表和流的理解，理解时间变化关系与流和表理论的关系并不太困难。
- en: 'Tables are quite straightforward: because a time-varying relation is essentially
    a sequence of classic relations (each capturing a snapshot of the relation at
    a specific point in time), and classic relations are analogous to tables, observing
    a time-varying relation as a table simply yields the point-in-time relation snapshot
    for the time of observation.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表非常简单：因为时间变化关系本质上是经典关系的序列（每个捕捉了特定时间点上的关系快照），而经典关系类似于表，将时间变化关系观察为表，简单地产生了观察时间点的关系快照。
- en: 'For example, if we were to observe the previous grouped time-varying relation
    as a table at 12:01, we’d get the following (note the use of another hypothetical
    keyword, `TABLE`, to explicitly call out our desire for the query to return a
    table):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们在12:01观察之前的分组时间变化关系作为表，我们将得到以下结果（注意使用另一个假设关键字`TABLE`，明确表示我们希望查询返回一个表）：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And observing at 12:07 would yield the expected:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在12:07观察到的结果将是预期的：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'What’s particularly interesting here is that there’s actually support for the
    idea of time-varying relations within SQL, even as it exists today. The SQL 2011
    standard provides “temporal tables,” which store a versioned history of the table
    over time (in essence, time-varying relations) as well an `AS OF SYSTEM TIME`
    construct that allows you to explicitly query and receive a snapshot of the temporal
    table/time-varying relation at whatever point in time you specified. For example,
    even if we performed our query at 12:07, we could still see what the relation
    looked like back at 12:03:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里特别有趣的是，实际上SQL中已经支持了时间变化关系的想法。SQL 2011标准提供了“时间表”，它存储了表随时间的版本历史（实质上是时间变化关系），以及一个`AS
    OF SYSTEM TIME`结构，允许您明确地查询和接收在您指定的任何时间点的时间表/时间变化关系的快照。例如，即使我们在12:07执行了我们的查询，我们仍然可以看到关系在12:03时是什么样子的：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So there’s some amount of precedent for time-varying relations in SQL already.
    But I digress. The main point here is that tables capture a snapshot of the time-varying
    relation at a specific point in time. Most real-world table implementations simply
    track real time as we observe it; others maintain some additional historical information,
    which in the limit is equivalent to a full-fidelity time-varying relation capturing
    the entire history of a relation over time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SQL中已经有一些关于时间变化关系的先例。但我岔开了。这里的主要观点是，表在特定时间点捕捉时间变化关系的快照。大多数真实世界的表实现简单地跟踪我们观察到的实时时间；其他保留一些额外的历史信息，这在极限情况下等同于捕捉关系在时间上的整个历史的完整保真时间变化关系。
- en: Streams are slightly different beasts. We learned in Chapter 6 that they too
    capture the evolution of a table over time. But they do so somewhat differently
    than the time-varying relations we’ve looked at so far. Instead of holistically
    capturing snapshots of the entire relation each time it changes, they capture
    the *sequence of changes* that result in those snapshots within a time-varying
    relation. The subtle difference here becomes more evident with an example.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 流畅有些不同。我们在第6章学到，它们也捕捉了表随时间的演变。但它们与我们迄今为止所看到的时间变化关系有些不同。它们不是在每次变化时整体捕捉整个关系的快照，而是捕捉导致这些快照的*变化序列*在时间变化关系中。这里微妙的差异在一个例子中变得更加明显。
- en: 'As a refresher, recall again our baseline example `TVR` query:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个提醒，再次回想一下我们的基准例子`TVR`查询：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s now observe our time-varying relation as a stream as it exists at a few
    distinct points in time. At each step of the way, we’ll compare the original table
    rendering of the TVR at that point in time with the evolution of the stream up
    to that point. To see what stream renderings of our time-varying relation look
    like, we’ll need to introduce two new hypothetical keywords:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们观察我们的时变关系作为一个流在几个不同的时间点存在的情况。在每一步，我们将比较TVR在那个时间点的原始表格呈现与流到那个时间点的演变。为了看到我们的时变关系的流呈现是什么样子，我们需要引入两个新的假设关键词：
- en: A `STREAM` keyword, similar to the `TABLE` keyword I’ve already introduced,
    that indicates we want our query to return an event-by-event stream capturing
    the evolution of the time-varying relation over time. You can think of this as
    applying a per-record trigger to the relation over time.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`STREAM`关键词，类似于我已经介绍的`TABLE`关键词，表示我们希望查询返回一个事件流，捕捉时变关系随时间的演变。你可以把这看作是在时间上对关系应用每条记录触发器。
- en: A special `Sys.Undo`³ column that can be referenced from a `STREAM` query, for
    the sake of identifying rows that are retractions. More on this in a moment.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个特殊的`Sys.Undo`³列，可以从`STREAM`查询中引用，用于识别撤销的行。稍后会详细介绍。
- en: 'Thus, starting out from 12:01, we’d have the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从12:01开始，我们将有以下情况：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The table and stream renderings look almost identical at this point. Mod the
    `Undo` column (discussed in more detail in the next example), there’s only one
    difference: whereas the table version is complete as of 12:01 (signified by the
    final line of dashes closing off the bottom end of the relation), the stream version
    remains incomplete, as signified by the final ellipsis-like line of periods marking
    both the open tail of the relation (where additional data might be forthcoming
    in the future) as well as the processing-time range of data observed so far. And
    indeed, if executed on a real implementation, the `STREAM` query would wait indefinitely
    for additional data to arrive. Thus, if we waited until 12:03, three new rows
    would show up for the `STREAM` query. Compare that to a fresh `TABLE` rendering
    of the TVR at 12:03:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表格和流的呈现在这一点上几乎是相同的。除了`Undo`列（在下一个例子中会更详细地讨论），只有一个区别：表格版本在12:01时是完整的（通过下方的虚线表示关系的底部结束），而流版本仍然是不完整的，通过最后的省略号一样的一行点表示，标记了关系的开放尾部（未来可能会有额外的数据）以及迄今为止观察到的处理时间范围。实际上，如果在真实实现中执行，`STREAM`查询将无限期地等待额外的数据到达。因此，如果等到12:03，`STREAM`查询将出现三行新数据。与12:03的新`TABLE`呈现进行比较：
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here’s an interesting point worth addressing: why are there *three* new rows
    in the stream (Frank’s 3 and Julie’s undo-7 and 8) when our original dataset contained
    only *two* rows (Frank’s 3 and Julie’s 1) for that time period? The answer lies
    in the fact that here we are observing the stream of changes to an *aggregation*
    of the original inputs; in particular, for the time period from 12:01 to 12:03,
    the stream needs to capture two important pieces of information regarding the
    change in Julie’s aggregate score due to the arrival of the new 1 value:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个值得讨论的有趣观点：为什么在流中有*三*行新数据（Frank的3和Julie的撤销-7和8），而我们原始数据集中只包含*两*行（Frank的3和Julie的1）？答案在于我们观察到的是原始输入的*聚合*变化流；特别是，在12:01到12:03的时间段内，流需要捕捉关于Julie的聚合分数变化的两个重要信息：
- en: The previously reported total of 7 was incorrect.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先前报告的总数为7是错误的。
- en: The new total is 8.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的总数是8。
- en: 'That’s what the special `Sys.Undo` column allows us to do: distinguish between
    normal rows and rows that are a *retraction* of a previously reported value.⁴'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是特殊的`Sys.Undo`列允许我们做的事情：区分普通行和以前报告的值的*撤销*行。⁴
- en: 'A particularly nice feature of `STREAM` queries is that you can begin to see
    how all of this relates to the world of classic Online Transaction Processing
    (OLTP) tables: the `STREAM` rendering of this query is essentially capturing a
    sequence of `INSERT` and `DELETE` operations that you could use to materialize
    this relation over time in an OLTP world (and really, when you think about it,
    OLTP tables themselves are essentially time-varying relations mutated over time
    via a stream of `INSERT`s, `UPDATE`s, and `DELETE`s).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`STREAM`查询的一个特别好的特性是，你可以开始看到所有这些与经典的在线事务处理（OLTP）表有关：这个查询的`STREAM`呈现基本上捕捉了一系列`INSERT`和`DELETE`操作，你可以用它来在OLTP世界中随时间实现这个关系（实际上，当你考虑一下，OLTP表本身本质上就是通过`INSERT`、`UPDATE`和`DELETE`的流来随时间变化的时变关系）。'
- en: 'Now, if we don’t care about the retractions in the stream, it’s also perfectly
    fine not to ask for them. In that case, our `STREAM` query would look like this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们不关心流中的撤销，也完全可以不要求它们。在这种情况下，我们的`STREAM`查询将如下所示：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'But there’s clearly value in understanding what the full stream looks like,
    so we’ll go back to including the `Sys.Undo` column for our final example. Speaking
    of which, if we waited another four minutes until 12:07, we’d be greeted by two
    additional rows in the `STREAM` query, whereas the `TABLE` query would continue
    to evolve as before:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 但显然了解完整的流是有价值的，所以我们将回到在我们的最后一个例子中包括`Sys.Undo`列。说到这一点，如果我们再等四分钟到12:07，我们将在`STREAM`查询中看到另外两行新数据，而`TABLE`查询将继续像以前一样演变。
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And by this time, it’s quite clear that the `STREAM` version of our time-varying
    relation is a very different beast from the table version: the table captures
    a snapshot of the entire relation *at a specific point in time*, whereas the stream
    captures a view of the individual changes to the relation *over time*.⁵ Interestingly
    though, that means that the `STREAM` rendering has more in common with our original,
    table-based TVR rendering:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，很明显`STREAM`版本的时变关系与表格版本非常不同：表格捕捉了*特定时间点*的整个关系快照，而流捕捉了关系*随时间*的个别变化的视图。有趣的是，这意味着`STREAM`呈现与我们原始的基于表格的TVR呈现有更多的共同点：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Indeed, it’s safe to say that the `STREAM` query simply provides an alternate
    rendering of the entire history of data that exists in the corresponding table-based
    `TVR` query. The value of the `STREAM` rendering is its conciseness: it captures
    only the delta of changes between each of the point-in-time relation snapshots
    in the `TVR`. The value of the sequence-of-tables `TVR` rendering is the clarity
    it provides: it captures the evolution of the relation over time in a format that
    highlights its natural relationship to classic relations, and in doing so provides
    for a simple and clear definition of relational semantics within the context of
    streaming as well as the additional dimension of time that streaming brings.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，可以说`STREAM`查询只是提供了与相应基于表的`TVR`查询中存在的整个数据历史的另一种呈现方式。`STREAM`呈现的价值在于它的简洁性：它仅捕捉了`TVR`中每个时间点关系快照之间的变化增量。序列表`TVR`呈现的价值在于它提供的清晰度：它以突出其与经典关系的自然关系的格式捕捉了关系随时间的演变，并在此过程中提供了对于在流媒体环境中的关系语义的简单和清晰定义，以及流媒体带来的额外时间维度。
- en: 'Another important aspect of the similarities between the `STREAM` and table-based
    `TVR` renderings is the fact that they are essentially equivalent in the overall
    data they encode. This gets to the core of the stream/table duality that its proponents
    have long preached: streams and tables⁶ are really just two different sides of
    the same coin. Or to resurrect the bad physics analogy from Chapter 6, streams
    and tables are to time-varying relations as waves and particles are to light:⁷
    a complete time-varying relation is both a table and a stream at the same time;
    tables and streams are simply different physical manifestations of the same concept,
    depending upon the context.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`STREAM`和基于表的`TVR`呈现之间相似之处的另一个重要方面是它们在总体上编码的数据实际上是等价的。这涉及到流/表对偶的核心，即其支持者长期以来一直宣扬的观点：流和表⁶实际上只是同一概念的两个不同物理表现形式，取决于上下文，就像波和粒子对于光一样⁷：完整的时变关系同时是表和流；表和流只是同一概念的不同物理表现形式。'
- en: Now, it’s important to keep in mind that this stream/table duality is true only
    as long as both versions encode the same information; that is, when you have full-fidelity
    tables or streams. In many cases, however, full fidelity is impractical. As I
    alluded to earlier, encoding the full history of a time-varying relation, no matter
    whether it’s in stream or table form, can be rather expensive for a large data
    source. It’s quite common for stream and table manifestations of a TVR to be lossy
    in some way. Tables typically encode only the most recent version of a TVR; those
    that support temporal or versioned access often compress the encoded history to
    specific point-in-time snapshots, and/or garbage-collect versions that are older
    than some threshold. Similarly, streams typically encode only a limited duration
    of the evolution of a TVR, often a relatively recent portion of that history.
    Persistent streams like Kafka afford the ability to encode the entirety of a TVR,
    but again this is relatively uncommon, with data older than some threshold typically
    thrown away via a garbage-collection process.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，重要的是要记住，只有在两个版本编码相同信息时，即当你有全保真度的表或流时，流/表的这种对偶才是真实的。然而，在许多情况下，全保真度是不切实际的。正如我之前所暗示的，无论是以流的形式还是表的形式编码时变关系的完整历史对于大型数据源来说都可能非常昂贵。流/表的表现形式通常在某种程度上是有损的。表通常只编码TVR的最新版本；支持时间或版本访问的表通常将编码的历史压缩到特定的时间点快照，并/或者清理比某个阈值更老的版本。同样，流通常只编码TVR演变的有限时间段，通常是最近历史的一部分。像Kafka这样的持久流可以编码TVR的全部内容，但这相对不常见，通常会通过垃圾回收过程丢弃一些阈值之前的数据。
- en: The main point here is that streams and tables are absolutely duals of one another,
    each a valid way of encoding a time-varying relation. But in practice, it’s common
    for the physical stream/table manifestations of a TVR to be lossy in some way.
    These partial-fidelity streams and tables trade off a decrease in total encoded
    information for some benefit, usually decreased resource costs. And these types
    of trade-offs are important because they’re often what allow us to build pipelines
    that operate over data sources of truly massive scale. But they also complicate
    matters, and require a deeper understanding to use correctly. We discuss this
    topic in more detail later on when we get to SQL language extensions. But before
    we try to reason about SQL extensions, it will be useful to understand a little
    more concretely the biases present in both the SQL and non-SQL data processing
    approaches common today.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要观点是流和表绝对是彼此的对偶，每种都是编码时变关系的有效方式。但在实践中，流/表的物理表现形式通常在某种程度上是有损的。这些部分保真度的流和表在总编码信息减少的情况下换取了一些好处，通常是减少资源成本。这些类型的权衡是重要的，因为它们通常是我们能够构建能够处理真正大规模数据源的管道的原因。但它们也使事情变得复杂，并需要更深入的理解才能正确使用。我们将在后面更详细地讨论这个话题，当我们涉及SQL语言扩展时。但在我们尝试推理SQL扩展之前，了解当今常见的SQL和非SQL数据处理方法中存在的偏见会很有用。
- en: 'Looking Backward: Stream and Table Biases'
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾：流和表的偏见
- en: In many ways, the act of adding robust streaming support to SQL is largely an
    exercise in attempting to merge the *where*, *when*, and *how* semantics of the
    Beam Model with the *what* semantics of the classic SQL model. But to do so cleanly,
    and in a way that remains true to the look and feel of classic SQL, requires an
    understanding of how the two models relate to each other. Thus, much as we explored
    the relationship of the Beam Model to stream and table theory in Chapter 6, we’ll
    now explore the relationship of the Beam Model to the classic SQL model, using
    stream and table theory as the underlying framework for our comparison. In doing
    so, we’ll uncover the inherent biases present in each model, which will provide
    us some insights in how to best marry the two in a clean, natural way.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，将强大的流支持添加到SQL中实质上是一种尝试将Beam模型的*where*、*when*和*how*语义与经典SQL模型的*what*语义合并的过程。但要做到干净利落地，并且保持对经典SQL的外观和感觉，需要理解这两种模型之间的关系。因此，就像我们在第6章中探讨了Beam模型与流和表理论的关系一样，现在我们将使用流和表理论作为比较的基础框架，探讨Beam模型与经典SQL模型的关系。通过这样做，我们将发现每个模型中存在的固有偏见，这将为我们提供一些洞察，以便以一种干净、自然的方式最好地将这两种模型结合起来。
- en: 'The Beam Model: A Stream-Biased Approach'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Beam模型：一种流偏向的方法
- en: Let’s begin with the Beam Model, building upon the discussion in Chapter 6.
    To begin, I want to discuss the inherent stream bias in the Beam Model as it exists
    today relative to streams and tables.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从Beam模型开始，基于第6章的讨论。首先，我想讨论Beam模型相对于流和表的固有流偏向。
- en: 'If you think back to Figures 6-11 and 6-12, they showed two different views
    of the same score-summation pipeline that we’ve used as an example throughout
    the book: in Figure 6-11 a logical, Beam-Model view, and in Figure 6-12 a physical,
    streams and tables–oriented view. Comparing the two helped highlight the relationship
    of the Beam Model to streams and tables. But by overlaying one on top of the other,
    as I’ve done in Figure 8-1, we can see an additional interesting aspect of the
    relationship: the Beam Model’s inherent stream bias.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果回想一下图6-11和6-12，它们展示了我们在整本书中一直使用的一个示例——分数求和管道的两种不同视图：图6-11是逻辑的Beam模型视图，图6-12是物理的流和表导向视图。比较这两者有助于突出Beam模型与流和表的关系。但是通过将一个叠加在另一个上面，就像我在图8-1中所做的那样，我们可以看到关系的另一个有趣方面：Beam模型固有的流偏向。
- en: '![](img/stsy_0801.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_0801.png)'
- en: Figure 8-1\. Stream bias in the Beam Model approach
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. Beam模型方法中的流偏向
- en: 'In this figure, I’ve drawn dashed red lines connecting the transforms in the
    logical view to their corresponding components in the physical view. The thing
    that stands out when observed this way is that all of the logical transformations
    are connected by *streams*, even the operations that involve grouping (which we
    know from Chapter 6 results in a table being created *somewhere*). In Beam parlance,
    these transformations are `PTransforms`, and they are always applied to `PCollections`
    to yield new `PCollections`. The important takeaway here is that `PCollections`
    in Beam are *always* streams. As a result, the Beam Model is an inherently stream-biased
    approach to data processing: streams are the common currency in a Beam pipeline
    (even batch pipelines), and tables are always treated specially, either abstracted
    behind sources and sinks at the edges of the pipeline or hidden away beneath a
    grouping and triggering operation somewhere in the pipeline.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图中，我画了虚线连接逻辑视图中的变换与物理视图中对应的组件。以这种方式观察时显而易见的是，所有逻辑变换都由*流*连接，即使涉及分组的操作（我们从第6章知道这会导致*某处*创建表）。在Beam术语中，这些变换是`PTransforms`，它们总是应用于`PCollections`以产生新的`PCollections`。这里的重要观点是，在Beam中，`PCollections`始终是*流*。因此，Beam模型是一种固有的流偏向数据处理方法：流是Beam管道中的通用货币（即使是批处理管道），而表始终被特别对待，要么在管道边缘抽象在源和汇处，要么在管道中的某个地方被隐藏在分组和触发操作之下。
- en: 'Because Beam operates in terms of streams, anywhere a table is involved (sources,
    sinks, and any intermediate groupings/ungroupings), some sort of conversion is
    necessary to keep the underlying table hidden. Those conversions in Beam look
    something like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因为Beam以流为单位运行，任何涉及表的地方（源、汇以及任何中间分组/取消分组），都需要进行某种转换以隐藏底层表。Beam中的这些转换看起来像这样：
- en: '*Sources* that *consume* tables typically hardcode the manner in which those
    tables are *triggered*; there is no way for a user to specify custom triggering
    of the table they want to consume. The source may be written to trigger every
    new update to the table as a record, it might batch groups of updates together,
    or it might provide a single, bounded snapshot of the data in the table at some
    point in time. It really just depends on what’s practical for a given source,
    and what use case the author of the source is trying to address.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*消费*表的*源*通常会硬编码表的*触发*方式；用户无法指定他们想要消费的表的自定义触发方式。源可能被编写为触发对表的每次新更新作为记录，它可能批量组合更新，或者在某个时间点上提供表中数据的单个有界快照。这实际上取决于对于给定源来说什么是实际可行的，以及源的作者试图解决的用例是什么。'
- en: '*Sinks* that *write* tables typically hardcode the manner in which they *group*
    their input streams. Sometimes, this is done in a way that gives the user a certain
    amount of control; for example, by simply grouping on a user-assigned key. In
    other cases, the grouping might be implicitly defined; for example, by grouping
    on a random physical partition number when writing input data with no natural
    key to a sharded output source. As with sources, it really just depends on what’s
    practical for the given sink and what use case the author of the sink is trying
    to address.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*写*表的*汇*通常会硬编码它们*组*输入流的方式。有时，这是以一种给用户一定控制的方式来完成的；例如，通过简单地按用户分配的键进行分组。在其他情况下，分组可能是隐式定义的；例如，通过在写入没有自然键的输入数据时在分片输出源上分组到一个随机物理分区号。与源一样，这实际上取决于给定汇的实际可行性，以及汇的作者试图解决的用例是什么。'
- en: For *grouping/ungrouping operations*, in contrast to sources and sinks, Beam
    provides users complete flexibility in how they group data into tables and ungroup
    them back into streams. This is by design. Flexibility in grouping operations
    is necessary because the way data are grouped is a key ingredient of the algorithms
    that define a pipeline. And flexibility in ungrouping is important so that the
    application can shape the generated streams in ways that are appropriate for the
    use case at hand.⁸
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于*分组/取消分组操作*，与源和汇点相反，Beam为用户提供了完全灵活的方式将数据分组到表中，并将其取消分组为流。这是有意设计的。分组操作的灵活性是必要的，因为数据分组的方式是定义管道的算法的关键组成部分。取消分组的灵活性很重要，以便应用程序可以以适合手头用例的方式塑造生成的流。⁸
- en: 'However, there’s a wrinkle here. Remember from Figure 8-1 that the Beam Model
    is inherently biased toward streams. As result, although it’s possible to cleanly
    apply a grouping operation directly to a stream (this is Beam’s `GroupByKey` operation),
    the model never provides first-class table objects to which a trigger can be directly
    applied. As a result, triggers must be applied somewhere else. There are basically
    two options here:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，这里有一个问题。从图8-1中可以看出，Beam模型本质上偏向于流。因此，虽然可以直接将分组操作清晰地应用于流（这是Beam的`GroupByKey`操作），但该模型从不提供可以直接应用触发器的一等表对象。因此，触发器必须在其他地方应用。基本上有两个选择：
- en: Predeclaration of triggers
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 触发器的预声明
- en: This is where triggers are specified at a point in the pipeline *before* the
    table to which they are actually applied. In this case, you’re essentially prespecifying
    behavior you’d like to see later on in the pipeline after a grouping operation
    is encountered. When declared this way, triggers are *forward-propagating*.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在管道中的某个点*之前*指定触发器的位置应用于它们实际应用的表。在这种情况下，您基本上是预先指定了在管道中遇到分组操作后稍后希望看到的行为。以这种方式声明时，触发器是*向前传播*的。
- en: Post-declaration of triggers
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 触发器声明后
- en: This is where triggers are specified at a point in the pipeline *following*
    the table to which they are applied. In this case, you’re specifying the behavior
    you’d like to see at the point where the trigger is declared. When declared this
    way, triggers are *backward-propagating*.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在管道中的某个点指定触发器的位置*之后*，它们被应用的表。在这种情况下，您正在指定在声明触发器的地方希望看到的行为。以这种方式声明时，触发器是*向后传播*的。
- en: Because post-declaration of triggers allows you to specify the behavior you
    want at the actual place you want to observe it, it’s much more intuitive. Unfortunately,
    Beam as it exists today (2.x and earlier) uses predeclaration of triggers (similar
    to how windowing is also predeclared).
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因为触发器的后声明允许您在实际观察它的地方指定所需的行为，所以这更直观。不幸的是，Beam目前（2.x及更早版本）使用的是触发器的预声明（类似于窗口也是预声明的）。
- en: Even though Beam provides a number of ways to cope with the fact that tables
    are hidden, we’re still left with the fact that tables must always be triggered
    before they can be observed, even if the contents of that table are really the
    final data that you want to consume. This is a shortcoming of the Beam Model as
    it exists today, one which could be addressed by moving away from a stream-centric
    model and toward one that treats both streams and tables as first-class entities.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Beam提供了许多应对表格隐藏的方法，但我们仍然面临一个事实，那就是必须在观察表格之前触发它们，即使该表格的内容确实是您想要消耗的最终数据。这是Beam模型目前存在的一个缺点，可以通过摆脱流中心模型，转向将流和表格视为一等实体的模型来解决。
- en: 'Let’s now look at the Beam Model’s conceptual converse: classic SQL.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下Beam模型的概念对应：经典SQL。
- en: 'The SQL Model: A Table-Biased Approach'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SQL模型：以表为中心的方法
- en: 'In contrast to the Beam Model’s stream-biased approach, SQL has historically
    taken a table-biased approach: queries are applied to tables, and always result
    in new tables. This is similar to the batch processing model we looked at in Chapter 6
    with MapReduce,⁹ but it will be useful to consider a concrete example like the
    one we just looked at for the Beam Model.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与Beam模型的流为中心的方法相反，SQL历来采用以表为中心的方法：查询应用于表，并且总是产生新的表。这类似于我们在第6章中看到的MapReduce的批处理模型，但是考虑到Beam模型的一个类似的具体示例会很有用。
- en: 'Consider the following denormalized SQL table:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下非规范化的SQL表：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It contains user scores, each annotated with the IDs of the corresponding user
    and their corresponding team. There is no primary key, so you can assume that
    this is an append-only table, with each row being identified implicitly by its
    unique physical offset. If we want to compute team scores from this table, we
    could use a query that looks something like this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含用户得分，每个得分都带有相应用户和他们所在团队的ID。没有主键，因此可以假设这是一个仅追加的表，每行都隐式地由其唯一的物理偏移标识。如果我们想要从这个表中计算团队得分，我们可以使用类似于以下内容的查询：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'When executed by a query engine, the optimizer will probably break this query
    down into roughly three steps:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当由查询引擎执行时，优化器可能会将此查询大致分解为三个步骤：
- en: Scanning the input table (i.e., triggering a snapshot of it)
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扫描输入表（即触发其快照）
- en: Projecting the fields in that table down to team and score
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该表中的字段投影到团队和得分
- en: Grouping rows by team and summing the scores
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按团队分组并求和得分
- en: If we look at this using a diagram similar to Figure 8-1, it would look like
    Figure 8-2.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用类似于图8-1的图表来查看这一点，它会看起来像图8-2。
- en: The `SCAN` operation takes the input table and triggers it into a bounded stream
    that contains a snapshot of the contents of that table at query execution time.
    That stream is consumed by the `SELECT` operation, which projects the four-column
    input rows down to two-column output rows. Being a nongrouping operation, it yields
    another stream. Finally, that two-column stream of teams and user scores enters
    the `GROUP BY` and is grouped by team into a table, with scores for the same team
    `SUM`’d together, yielding our output table of teams and their corresponding team
    score totals.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`SCAN`操作将输入表触发为一个有界流，其中包含查询执行时表的内容的快照。该流被`SELECT`操作消耗，将四列输入行投影到两列输出行。作为一个非分组操作，它产生另一个流。最后，团队和用户得分的这个两列流进入`GROUP
    BY`，按团队分组成一个表，相同团队的得分被`SUM`在一起，产生了我们的输出表，团队及其对应的团队得分总数。'
- en: '![](img/stsy_0802.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_0802.png)'
- en: Figure 8-2\. Table bias in a simple SQL query
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2。简单SQL查询中的表倾向
- en: 'This is a relatively simple example that naturally ends in a table, so it really
    isn’t sufficient to highlight the table-bias in classic SQL. But we can tease
    out some more evidence by simply splitting the main pieces of this query (projection
    and grouping) into two separate queries:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相对简单的例子，自然会以一个表结束，因此它实际上并不足以突出经典SQL中的表倾向。但是，我们可以通过简单地将这个查询的主要部分（投影和分组）拆分成两个单独的查询来找出更多的证据：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In these queries, we first project the `UserScores` table down to just the two
    columns we care about, storing the results in a temporary `TeamAndScore` table.
    We then group that table by team, summing up the scores as we do so. After breaking
    things out into a pipeline of two queries, our diagram looks like that shown in
    Figure 8-3.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些查询中，我们首先将`UserScores`表投影到我们关心的两列中，将结果存储在一个临时的`TeamAndScore`表中。然后我们按团队对该表进行分组，同时对得分进行求和。在将事物拆分成两个查询的管道后，我们的图表看起来像图8-3所示。
- en: '![](img/stsy_0803.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_0803.png)'
- en: Figure 8-3\. Breaking the query into two to reveal more evidence of table bias
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3。将查询分成两部分以揭示表倾向的更多证据
- en: If classic SQL exposed streams as first-class objects, you would expect the
    result from the first query, `TeamAndScore`, to be a stream because the `SELECT`
    operation consumes a stream and produces a stream. But because SQL’s common currency
    is tables, it must first convert the projected stream into a table. And because
    the user hasn’t specified any explicit key for grouping, it must simply group
    keys by their identity (i.e., append semantics, typically implemented by grouping
    by the physical storage offset for each row).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果经典SQL将流作为一流对象暴露出来，你会期望第一个查询`TeamAndScore`的结果是一个流，因为`SELECT`操作消耗一个流并产生一个流。但是因为SQL的通用货币是表，它必须首先将投影流转换为表。并且因为用户没有指定任何显式的键来分组，它必须简单地按其标识（即附加语义，通常通过按每行的物理存储偏移量进行分组）分组键。
- en: Because `TeamAndScore` is now a table, the second query must then prepend an
    additional `SCAN` operation to scan the table back into a stream to allow the
    `GROUP BY` to then group it back into a table again, this time with rows grouped
    by team and with their individual scores summed together. Thus, we see the two
    implicit conversions (from a stream and back again) that are inserted due to the
    explicit materialization of the intermediate table.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`TeamAndScore`现在是一个表，第二个查询必须在前面添加一个额外的`SCAN`操作，将表扫描回流，以便`GROUP BY`再次将其分组成表，这次是按团队分组，并将它们的个人得分总和在一起。因此，我们看到了两次隐式转换（从流到表，然后再次转回来），这是由于中间表的显式实现而插入的。
- en: That said, tables in SQL are not *always* explicit; implicit tables can exist,
    as well. For example, if we were to add a `HAVING` clause to the end of the query
    with the `GROUP BY` statement, to filter out teams with scores less than a certain
    threshold, the diagram would change to look something like Figure 8-4.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，SQL中的表并不总是显式的；隐式表也是存在的。例如，如果我们在带有`GROUP BY`语句的查询末尾添加一个`HAVING`子句，以过滤出得分低于某个阈值的团队，那么图表将会改变，看起来会像图8-4所示的样子。
- en: '![](img/stsy_0804.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_0804.png)'
- en: Figure 8-4\. Table bias with a final HAVING clause
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4。最终带有HAVING子句的表倾向
- en: With the addition of the `HAVING` clause, what used to be the user-visible `TeamTotals`
    table is now an implicit, intermediate table. To filter the results of the table
    according to the rules in the `HAVING` clause, that table must be triggered into
    a stream that can be filtered and then that stream must be implicitly grouped
    back into a table to yield the new output table, `LargeTeamTotals`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加`HAVING`子句，原来的用户可见的`TeamTotals`表现在是一个隐式的中间表。为了根据`HAVING`子句中的规则过滤表的结果，必须将该表触发为一个流，然后可以对该流进行过滤，然后该流必须隐式地重新分组成一个表，以产生新的输出表`LargeTeamTotals`。
- en: 'The important takeaway here is the clear table bias in classic SQL. Streams
    are always implicit, and thus for any materialized stream a conversion from/to
    a table is required. The rules for such conversions can be categorized roughly
    as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重要观点是经典SQL中明显的表倾向。流总是隐式的，因此对于任何实现的流，都需要从/到表的转换。这种转换的规则可以大致分类如下：
- en: Input tables (i.e., sources, in Beam Model terms)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输入表（即Beam模型术语中的源）
- en: These are always implicitly triggered in their entirety at a specific point
    in time¹⁰ (generally query execution time) to yield a bounded stream containing
    a snapshot of the table at that time. This is identical to what you get with classic
    batch processing, as well; for example, the MapReduce case we looked at in Chapter 6.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这些总是在特定时间点¹⁰（通常是查询执行时间）完全隐式触发，以产生一个包含该时间点表快照的有界流。这与经典的批处理得到的结果是相同的；例如，我们在第6章中看到的MapReduce案例。
- en: Output tables (i.e., sinks, in Beam Model terms)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出表（即Beam模型术语中的接收器）
- en: These tables are either direct manifestations of a table created by a final
    grouping operation in the query, or are the result of an implicit grouping (by
    some unique identifier for the row) applied to a query’s terminal stream, for
    queries that do not end in a grouping operation (e.g., the projection query in
    the previous examples, or a `GROUP BY` followed by a `HAVING` clause). As with
    inputs, this matches the behavior seen in classic batch processing.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表要么是查询中最终分组操作创建的表的直接表现，要么是应用于查询的终端流的隐式分组（按行的某个唯一标识符）的结果，对于不以分组操作结束的查询（例如前面示例中的投影查询，或者`GROUP
    BY`后跟一个`HAVING`子句）。与输入一样，这与经典批处理中的行为相匹配。
- en: Grouping/ungrouping operations
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 分组/非分组操作
- en: 'Unlike Beam, these operations provide complete flexibility in one dimension
    only: grouping. Whereas classic SQL queries provide a full suite of grouping operations
    (`GROUP BY`, `JOIN`, `CUBE`, etc.), they provide only a single type of implicit
    ungrouping operation: trigger an intermediate table in its entirety after all
    of the upstream data contributing to it have been incorporated (again, the exact
    same implicit trigger provided in MapReduce as part of the shuffle operation).
    As a result, SQL offers great flexibility in shaping algorithms via grouping but
    essentially zero flexibility in shaping the implicit streams that exist under
    the covers during query execution.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与Beam不同，这些操作只在一个维度上提供完全的灵活性：分组。而经典SQL查询提供了一整套分组操作（`GROUP BY`，`JOIN`，`CUBE`等），它们只提供了一种隐式的非分组操作：在所有贡献数据都被合并后触发中间表的整体（再次强调，这与MapReduce中作为洗牌操作的隐式触发完全相同）。因此，SQL在通过分组塑造算法方面提供了很大的灵活性，但在塑造查询执行过程中存在的隐式流方面基本上没有灵活性。
- en: Materialized views
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 物化视图
- en: 'Given how analogous classic SQL queries are to classic batch processing, it
    might be tempting to write off SQL’s inherent table bias as nothing more than
    an artifact of SQL not supporting stream processing in any way. But to do so would
    be to ignore the fact that databases have supported a specific type of stream
    processing for quite some time: *materialized views*. A materialized view is a
    view that is physically materialized as a table and kept up to date *over* time
    by the database as the source table(s) evolve. Note how this sounds remarkably
    similar to our definition of a time-varying relation. What’s fascinating about
    materialized views is that they add a very useful form of stream processing to
    SQL *without* significantly altering the way it operates, including its inherent
    table bias.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到经典SQL查询与经典批处理的相似性，可能会诱使人们认为SQL固有的表偏见只是SQL不以任何方式支持流处理的产物。但这样做将忽视一个事实，即数据库已经支持了一种特定类型的流处理很长一段时间：*物化视图*。物化视图是作为表物理材料化并随着时间由数据库保持更新的视图，源表发生变化时也会相应更新。请注意，这听起来与我们对时变关系的定义非常相似。物化视图的迷人之处在于，它为SQL增加了一种非常有用的流处理形式，而不会显著改变它的操作方式，包括其固有的表偏见。
- en: 'For example, let’s consider the queries we looked at in Figure 8-4. We can
    alter those queries to instead be `CREATE MATERIALIZED VIEW`¹¹ statements:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑一下图8-4中的查询。我们可以将这些查询改为`CREATE MATERIALIZED VIEW`¹¹语句：
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In doing so, we transform them into continuous, standing queries that process
    the updates to the `UserScores` table continuously, in a streaming manner. Even
    so, the resulting physical execution diagram for the views *looks almost exactly
    the same* as it did for the one-off queries; nowhere are streams made into explicit
    first-class objects in order to support this idea of streaming materialized views.
    The *only* noteworthy change in the physical execution plan is the substitution
    of a different trigger: `SCAN-AND-STREAM` instead of `SCAN`, as illustrated in
    Figure 8-5.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们将它们转换为连续的、持续的查询，以流式方式持续处理`UserScores`表的更新。即使如此，物化视图的物理执行图与一次性查询的执行图*几乎完全相同*；在查询执行过程中，流并没有被显式地转换为显式的一流对象来支持这种流式物化视图的概念。物理执行计划中*唯一*值得注意的变化是替换了不同的触发器：`SCAN-AND-STREAM`而不是`SCAN`，如图8-5所示。
- en: '![](img/stsy_0805.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_0805.png)'
- en: Figure 8-5\. Table bias in materialized views
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5。物化视图中的表偏差
- en: What is this `SCAN-AND-STREAM` trigger? `SCAN-AND-STREAM` starts out like a
    `SCAN` trigger, emitting the full contents of the table at a point in time into
    a stream. But instead of stopping there and declaring the stream to be done (i.e.,
    bounded), it continues to also trigger all subsequent modifications to the input
    table, yielding an unbounded stream that captures the evolution of the table over
    time. In the general case, these modifications include not only `INSERT`s of new
    values, but also `DELETE`s of previous values and `UPDATE`s to existing values
    (which, practically speaking, are treated as a simultaneous `DELETE`/`INSERT`
    pair, or `undo`/`redo` values as they are called in Flink).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`SCAN-AND-STREAM`触发器是什么？`SCAN-AND-STREAM`开始时像`SCAN`触发器一样，将表的全部内容在某个时间点发射到流中。但它不会在此停止并声明流已完成（即有界），而是继续触发对输入表的所有后续修改，产生一个捕获表随时间演变的无界流。在一般情况下，这些修改不仅包括新值的`INSERT`，还包括先前值的`DELETE`和现有值的`UPDATE`（实际上被视为同时的`DELETE`/`INSERT`对，或者在Flink中称为`undo`/`redo`值）。
- en: 'Furthermore, if we consider the table/stream conversion rules for materialized
    views, the only real difference is the trigger used:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们考虑物化视图的表/流转换规则，唯一的真正区别是使用的触发器：
- en: '*Input tables* are implicitly triggered via a `SCAN-AND-STREAM` trigger instead
    of a `SCAN` trigger. Everything else is the same as classic batch queries.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入表*通过`SCAN-AND-STREAM`触发器隐式触发，而不是`SCAN`触发器。其他一切都与经典批处理查询相同。'
- en: '*Output tables* are treated the same as classic batch queries.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出表*与经典批处理查询处理方式相同。'
- en: '*Grouping/ungrouping operations* function the same as classic batch queries,
    with the only difference being the use of a `SCAN-AND-STREAM` trigger instead
    of a `SNAPSHOT` trigger for implicit ungrouping operations.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分组/取消分组操作*与经典批处理查询相同，唯一的区别是使用`SCAN-AND-STREAM`触发器而不是`SNAPSHOT`触发器进行隐式取消分组操作。'
- en: Given this example, it’s clear to see that SQL’s inherent table bias is not
    just an artifact of SQL being limited to batch processing:¹² materialized views
    lend SQL the ability to perform a specific type of stream processing without any
    significant changes in approach, including the inherent bias toward tables. Classic
    SQL is just a table-biased model, regardless of whether you’re using it for batch
    or stream processing.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个例子，很明显可以看出SQL固有的表偏向不仅仅是SQL被限制在批处理中的产物：¹²物化视图使SQL能够执行一种特定类型的流处理，而不需要进行任何重大的方法变更，包括对表的固有偏向。经典SQL只是一个偏向表的模型，无论你是用它进行批处理还是流处理。
- en: 'Looking Forward: Toward Robust Streaming SQL'
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 展望未来：朝着强大的流SQL
- en: 'We’ve now looked at time-varying relations, the ways in which tables and streams
    provide different renderings of a time-varying relation, and what the inherent
    biases of the Beam and SQL models are with respect to stream and table theory.
    So where does all of this leave us? And perhaps more to the point, what do we
    need to change or add within SQL to support robust stream processing? The surprising
    answer is: not much if we have good defaults.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看过了时变关系，表和流提供不同的时变关系呈现方式，以及Beam和SQL模型在流和表理论方面的固有偏见。那么这一切对我们意味着什么？也许更重要的是，我们需要在SQL中做出哪些改变或添加以支持强大的流处理？令人惊讶的答案是：如果我们有好的默认值，就不需要太多。
- en: 'We know that the key conceptual change is to replace classic, point-in-time
    relations with time-varying relations. We saw earlier that this is a very seamless
    substitution, one which applies across the full breadth of relational operators
    already in existence, thanks to maintaining the critical closure property of relational
    algebra. But we also saw that dealing in time-varying relations directly is often
    impractical; we need the ability to operate in terms of our two more-common physical
    manifestations: tables and streams. This is where some simple extensions with
    good defaults come in.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，关键的概念变化是用时变关系替换经典的瞬时关系。我们之前看到，这是一个非常无缝的替换，适用于已经存在的关系代数的全部范围，这要归功于保持关系代数的关键闭包性质。但我们也看到，直接处理时变关系通常是不切实际的；我们需要能够以我们两种更常见的物理表现形式：表和流进行操作。这就是一些简单的带有良好默认值的扩展发挥作用的地方。
- en: We also need some tools for robustly reasoning about time, specifically event
    time. This is where things like timestamps, windowing, and triggering come into
    play. But again, judicious choice of defaults will be important to minimize how
    often these extensions are necessary in practice.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一些工具来稳健地推理时间，特别是事件时间。这就是时间戳、窗口和触发器等东西发挥作用的地方。但同样，明智的默认选择将是重要的，以最小化这些扩展在实践中的必要性。
- en: 'What’s great is that we don’t really need anything more than that. So let’s
    now finally spend some time looking in detail at these two categories of extensions:
    *stream/table selection* and *temporal operators*.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒的是，我们实际上不需要比这更多。所以现在让我们最终花一些时间详细研究这两类扩展：*流/表选择*和*时间操作符*。
- en: Stream and Table Selection
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流和表选择
- en: 'As we worked through time-varying relation examples, we already encountered
    the two key extensions related to stream and table selection. They were those
    `TABLE` and `STREAM` keywords we placed after the `SELECT` keyword to dictate
    our desired physical view of a given time-varying relation:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过时变关系示例工作时，我们已经遇到了与流和表选择相关的两个关键扩展。它们是我们在`SELECT`关键字之后放置的`TABLE`和`STREAM`关键字，以指示我们对给定时变关系的期望物理视图。
- en: '[PRE20]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'These extensions are relatively straightforward and easy to use when necessary.
    But the really important thing regarding stream and table selection is the choice
    of good defaults for times when they aren’t explicitly provided. Such defaults
    should honor the classic, table-biased behavior of SQL that everyone is accustomed
    to, while also operating intuitively in a world that includes streams. They should
    also be easy to remember. The goal here is to help maintain a natural feel to
    the system, while also greatly decreasing the frequency with which we must use
    explicit extensions. A good choice of defaults that satisfies all of these requirements
    is:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这些扩展相对直接，需要时易于使用。但是，关于流和表选择的真正重要的事情是选择好的默认值，以便在没有明确提供时使用。这样的默认值应该尊重SQL的经典、偏向表的行为，这是每个人都习惯的，同时在包括流的世界中也能直观地操作。它们也应该容易记住。这里的目标是帮助系统保持自然的感觉，同时大大减少我们必须使用显式扩展的频率。满足所有这些要求的默认值的好选择是：
- en: If *all* of the inputs are *tables*, the output is a *`TABLE`*.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*所有*的输入都是*表*，输出是*`TABLE`*。
- en: If *any* of the inputs are *streams*, the output is a *`STREAM`*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*任何*输入都是*流*，输出是*`STREAM`*。
- en: What’s additionally important to call out here is that these physical renderings
    of a time-varying relation are really only necessary when you want to materialize
    the TVR in some way, either to view it directly or write it to some output table
    or stream. Given a SQL system that operates under the covers in terms of full-fidelity
    time-varying relations, intermediate results (e.g., `WITH AS` or `SELECT INTO`
    statements) can remain as full-fidelity TVRs in whatever format the system naturally
    deals in, with no need to render them into some other, more limited concrete manifestation.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这里另外需要指出的是，这些时变关系的物理呈现只有在你想以某种方式使TVR物化时才是真正必要的，无论是直接查看它还是将其写入某个输出表或流。鉴于SQL系统在全保真度时变关系方面的运行，中间结果（例如`WITH
    AS`或`SELECT INTO`语句）可以保持为系统自然处理的全保真度TVR，无需将它们呈现为其他更有限的具体表现形式。
- en: And that’s really it for stream and table selection. Beyond the ability to deal
    in streams and tables directly, we also need some better tools for reasoning about
    time if we want to support robust, out-of-order stream processing within SQL.
    Let’s now look in more detail about what those entail.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是流和表选择的全部内容。除了直接处理流和表的能力之外，如果我们想要在SQL中支持强大的、无序的流处理，我们还需要一些更好的工具来推理时间。现在让我们更详细地看看这些工具包含了什么。
- en: Temporal Operators
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间操作符
- en: 'The foundation of robust, out-of-order processing is the event-time timestamp:
    that small piece of metadata that captures the time at which an event occurred
    rather than the time at which it is observed. In a SQL world, event time is typically
    just another column of data for a given TVR, one which is natively present in
    the source data themselves.¹³ In that sense, this idea of materializing a record’s
    event time within the record itself is something SQL already handles naturally
    by putting a timestamp in a regular column.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的、无序处理的基础是事件时间戳：这个小的元数据片段捕获了事件发生的时间，而不是观察到它的时间。在SQL世界中，事件时间通常只是给定TVR的另一列数据，它在源数据中是本地存在的。在这个意义上，将记录的事件时间实现在记录本身中的想法是SQL自然地处理的，通过将时间戳放在一个常规列中。
- en: Before we go any further, let’s look at an example. To help tie all of this
    SQL stuff together with the concepts we’ve explored previously in the book, we
    resurrect our running example of summing up nine scores from various members of
    a team to arrive at that team’s total score. If you recall, those scores look
    like Figure 8-6 when plotted on X = event-time/Y = processing-time axes.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们看一个例子。为了帮助将所有这些SQL的东西与我们之前在书中探讨过的概念联系起来，我们重新使用我们运行示例，将团队各成员的九个分数相加，得出团队的总分。如果你回忆一下，当这些分数在X=事件时间/Y=处理时间轴上绘制时，看起来像图8-6。
- en: '![](img/stsy_0806.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_0806.png)'
- en: Figure 8-6\. Data points in our running example
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-6\. 我们运行示例中的数据点
- en: 'If we were to imagine these data as a classic SQL table, they might look something
    like this, ordered by event time (left-to-right in Figure 8-6):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们把这些数据想象成一个经典的SQL表，它们可能看起来像这样，按事件时间排序（图8-6中从左到右）：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: If you recall, we saw this table way back in Chapter 2 when I first introduced
    this dataset. This rendering provides a little more detail on the data than we’ve
    typically shown, explicitly highlighting the fact that the nine scores themselves
    belong to seven different users, each a member of the same team. SQL provides
    a nice, concise way to see the data laid out fully before we begin diving into
    examples.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回忆一下，我们在第2章的时候就看到了这张表，那时我第一次介绍了这个数据集。这个渲染提供了比我们通常展示的更多关于数据的细节，明确地突出了这九个分数属于七个不同用户，每个用户都是同一个团队的成员。在我们开始深入示例之前，SQL提供了一个很好的、简洁的方式来看到数据的完整布局。
- en: Another nice thing about this view of the data is that it fully captures the
    event time and processing time for each record. You can imagine the event-time
    column as being just another piece of the original data, and the processing-time
    column as being something supplied by the system (in this case, using a hypothetical
    `Sys.MTime` column that records the processing-time modification timestamp of
    a given row; that is, the time at which that row arrived in the source table),
    capturing the ingress time of the records themselves into the system.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据视图的另一个好处是，它完全捕获了每条记录的事件时间和处理时间。你可以想象事件时间列只是原始数据的另一部分，而处理时间列是系统提供的东西（在这种情况下，使用一个假设的`Sys.MTime`列记录给定行的处理时间修改时间戳；也就是说，记录本身进入系统的时间）。
- en: 'The fun thing about SQL is how easy it is to view your data in different ways.
    For example, if we instead want to see the data in processing-time order (bottom-to-top
    in Figure 8-6), we could simply update the `ORDER BY` clause:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: SQL的有趣之处在于它可以很容易地以不同的方式查看数据。例如，如果我们希望以处理时间顺序查看数据（图8-6中从下到上），我们可以简单地更新`ORDER
    BY`子句：
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As we learned earlier, these table renderings of the data are really a partial-fidelity
    view of the complete underlying TVR. If we were to instead query the full table-oriented
    `TVR` (but only for the three most important columns, for the sake of brevity),
    it would expand to something like this:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前学到的，这些数据的表格渲染实际上是对完整底层TVR的部分保真视图。如果我们改为查询完整的面向表的`TVR`（但为了简洁起见，只查询三个最重要的列），它会扩展到像这样：
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'That’s a lot of data. Alternatively, the `STREAM` version would render much
    more compactly in this instance; thanks to there being no explicit grouping in
    the relation, it looks essentially identical to the point-in-time `TABLE` rendering
    earlier, with the addition of the trailing footer describing the range of processing
    time captured in the stream so far, plus the note that the system is still waiting
    for more data in the stream (assuming we’re treating the stream as unbounded;
    we’ll see a bounded version of the stream shortly):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是很多数据。另外，`STREAM`版本在这种情况下会更紧凑地呈现；由于关系中没有显式的分组，它看起来与之前的点时间`TABLE`呈现基本相同，另外还有一个尾部描述了迄今为止流中捕获的处理时间范围，以及系统仍在等待流中的更多数据（假设我们将流视为无界；我们很快将看到流的有界版本）：
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: But this is all just looking at the raw input records without any sort of transformations.
    Much more interesting is when we start altering the relations. When we’ve explored
    this example in the past, we’ve always started with classic batch processing to
    sum up the scores over the entire dataset, so let’s do the same here. The first
    example pipeline (previously provided as Example 6-1) looked like Example 8-1
    in Beam.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 但这只是查看原始输入记录，没有任何形式的转换。当我们开始改变关系时，更有趣的是。在过去的探索中，我们总是从经典的批处理开始，对整个数据集的分数进行求和，所以让我们在这里也这样做。第一个示例管道（之前作为示例6-1提供）在Beam中看起来像示例8-1。
- en: Example 8-1\. Summation pipeline
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-1\. 求和管道
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: And rendered in the streams and tables view of the world, that pipeline’s execution
    looked like Figure 8-7.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在世界的流和表视图中呈现，该流水线的执行看起来像图8-7。
- en: <assets/stsy_0807.mp4>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0807.mp4>
- en: '![Streams and tables view of classic batch processing](img/stsy_0807.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![经典批处理的流和表视图](img/stsy_0807.png)'
- en: Figure 8-7\. Streams and tables view of classic batch processing
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-7. 经典批处理的流和表视图
- en: 'Given that we already have our data placed into an appropriate schema, we won’t
    be doing any parsing in SQL; instead, we focus on everything in the pipeline after
    the parse transformation. And because we’re going with the classic batch model
    of retrieving a single answer only after all of the input data have been processed,
    the `TABLE` and `STREAM` views of the summation relation would look essentially
    identical (recall that we’re dealing with bounded versions of our dataset for
    these initial, batch-style examples; as a result, this `STREAM` query actually
    terminates with a line of dashes and an `END-OF-STREAM` marker):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们已经将数据放入了适当的模式中，我们不会在SQL中进行任何解析；相反，我们专注于解析转换之后的所有流水线中的一切。因为我们采用的是传统的批处理模型，在处理完所有输入数据之后才会检索单个答案，所以求和关系的`TABLE`和`STREAM`视图看起来基本相同（请记住，对于这些初始的批处理样例，我们处理的是数据集的有界版本；因此，这个`STREAM`查询实际上以一行短横线和一个`END-OF-STREAM`标记终止）：
- en: '[PRE26]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: More interesting is when we start adding windowing into the mix. That will give
    us a chance to begin looking more closely at the temporal operations that need
    to be added to SQL to support robust stream processing.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的是当我们开始将窗口加入到混合中时。这将让我们有机会更仔细地查看需要添加到SQL中以支持鲁棒流处理的时间操作。
- en: '*Where*: windowing'
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '*何处*：窗口'
- en: As we learned in Chapter 6, windowing is a modification of grouping by key,
    in which the window becomes a secondary part of a hierarchical key. As with classic
    programmatic batch processing, you can window data into more simplistic windows
    quite easily within SQL as it exists now by simply including time as part of the
    `GROUP BY` parameter. Or, if the system in question provides it, you can use a
    built-in windowing operation. We look at SQL examples of both in a moment, but
    first, let’s revisit the programmatic version from Chapter 3. Thinking back to
    Example 6-2, the windowed Beam pipeline looked like that shown in Example 8-2.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第6章中学到的，窗口是对按键分组的修改，其中窗口成为分层键的次要部分。与经典的程序化批处理一样，你可以通过简单地将时间作为`GROUP BY`参数的一部分，很容易地在现有的SQL中将数据窗口化。或者，如果所涉及的系统提供了，你可以使用内置的窗口操作。我们马上看一下两者的SQL示例，但首先，让我们重新访问第3章中的程序化版本。回想一下例子6-2，窗口化的Beam流水线看起来就像例子8-2中所示的那样。
- en: Example 8-2\. Summation pipeline
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例子8-2. 求和流水线
- en: '[PRE28]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: And the execution of that pipeline (in streams and tables rendering from Figure 6-5),
    looked like the diagrams presented in Figure 8-8.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 而该流水线的执行（从图6-5中的流和表呈现），看起来像图8-8中呈现的图表。
- en: <assets/stsy_0808.mp4>
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0808.mp4>
- en: '![Streams and tables view of windowed summation on a batch engine](img/stsy_0808.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![批处理引擎上窗口求和的流和表视图](img/stsy_0808.png)'
- en: Figure 8-8\. Streams and tables view of windowed summation on a batch engine
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-8. 批处理引擎上窗口求和的流和表视图
- en: As we saw before, the only material change from Figure 8-7 to 8-8 is that the
    table created by the `SUM` operation is now partitioned into fixed, two-minute
    windows of time, yielding four windowed answers at the end rather than the single
    global sum that we had previously.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，从图8-7到8-8的唯一实质性变化是由`SUM`操作创建的表现现在被分成了固定的两分钟时间窗口，最终产生了四个窗口化的答案，而不是之前的单个全局总和。
- en: 'To do the same thing in SQL, we have two options: implicitly window by including
    some unique feature of the window (e.g., the end timestamp) in the `GROUP BY`
    statement, or use a built-in windowing operation. Let’s look at both.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL中做同样的事情，我们有两个选择：通过在`GROUP BY`语句中包含窗口的某个唯一特征（例如结束时间戳）来隐式地进行窗口操作，或者使用内置的窗口操作。让我们来看看两者。
- en: 'First, ad hoc windowing. In this case, we perform the math of calculating windows
    ourselves in our SQL statement:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是临时窗口。在这种情况下，我们在SQL语句中自己执行计算窗口的数学运算：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can also achieve the same result using an explicit windowing statement such
    as those supported by Apache Calcite:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用显式的窗口语句来实现相同的结果，比如Apache Calcite支持的那些。
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This then begs the question: if we can implicitly window using existing SQL
    constructs, why even bother supporting explicit windowing constructs? There are
    two reasons, only the first of which is apparent in this example (we’ll see the
    other one in action later on in the chapter):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这就引出了一个问题：如果我们可以使用现有的SQL构造隐式地进行窗口操作，为什么还要支持显式的窗口构造呢？有两个原因，这个例子中只有第一个原因是明显的（我们将在本章后面看到另一个原因）：
- en: Windowing takes care of the window-computation math for you. It’s a lot easier
    to consistently get things right when you specify basic parameters like width
    and slide directly rather than computing the window math yourself.¹⁴
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 窗口化为你处理窗口计算数学。当你直接指定基本参数如宽度和滑动时，要保持一致地正确得到结果要容易得多，而不是自己计算窗口数学。¹⁴
- en: Windowing allows the concise expression of more complex, dynamic groupings such
    as sessions. Even though SQL is technically able to express the every-element-within-some-temporal-gap-of-another-element
    relationship that defines session windows, the corresponding incantation is a
    tangled mess of analytic functions, self joins, and array unnesting that no mere
    mortal could be reasonably expected to conjure on their own.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 窗口允许简洁地表达更复杂、动态的分组，比如会话。尽管SQL在技术上能够表达定义会话窗口的另一个元素时间间隔内的每个元素的关系，但相应的表达式是一团乱麻的分析函数、自连接和数组展开，普通人不可能合理地自己构造出来。
- en: Both are compelling arguments for providing first-class windowing constructs
    in SQL, in addition to the ad hoc windowing capabilities that already exist.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个都是支持SQL中提供一流的窗口构造的有力论据，除了已经存在的临时窗口功能。
- en: 'At this point, we’ve seen what windowing looks like from a classic batch/classic
    relational perspective when consuming the data as a table. But if we want to consume
    the data as a stream, we get back to that third question from the Beam Model:
    when in processing time do we materialize outputs?'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，当我们将数据作为表格消耗时，我们已经从经典的批处理/经典关系的角度看到了窗口的样子。但是，如果我们想将数据作为流来消耗，我们就回到了Beam模型中的第三个问题：在处理时间中，我们何时实现输出？
- en: '*When*: triggers'
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '*何时*：触发器'
- en: 'The answer to that question, as before, is triggers and watermarks. However,
    in the context of SQL, there’s a strong argument to be made for having a different
    set of defaults than those we introduced with the Beam Model in Chapter 3: rather
    than defaulting to using a single watermark trigger, a more SQL-ish default would
    be to take a cue from materialized views and trigger on every element. In other
    words, any time a new input arrives, we produce a corresponding new output.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，这个问题的答案是触发器和水印。然而，在SQL的上下文中，有一个强有力的论点支持使用不同的默认值，而不是我们在第3章中引入的Beam模型的默认值：与其默认使用单个水印触发器，不如从物化视图中获取灵感，并在每个元素上触发。换句话说，每当有新的输入到达时，我们就会产生相应的新输出。
- en: 'A SQL-ish default: per-record triggers'
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SQL风格的默认值：每条记录触发器
- en: 'There are two compelling benefits to using trigger-every-record as the default:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使用每条记录触发器作为默认值有两个强有力的好处：
- en: Simplicity
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 简单性
- en: The semantics of per-record updates are easy to understand; materialized views
    have operated this way for years.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 每条记录更新的语义易于理解；物化视图多年来一直以这种方式运作。
- en: Fidelity
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 忠实度
- en: As in change data capture systems, per-record triggering yields a full-fidelity
    stream rendering of a given time-varying relation; no information is lost as part
    of the conversion.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 与变更数据捕获系统一样，每条记录触发产生了给定时变关系的完整保真度流呈现；在转换过程中没有丢失任何信息。
- en: 'The downside is primarily cost: triggers are always applied after a grouping
    operation, and the nature of grouping often presents an opportunity to reduce
    the cardinality of data flowing through the system, thus commensurately reducing
    the cost of further processing those aggregate results downstream. Even so, the
    benefits in clarity and simplicity for use cases where cost is not prohibitive
    arguably outweigh the cognitive complexity of defaulting to a non-full-fidelity
    trigger up front.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点主要是成本：触发器总是应用于分组操作之后，而分组的性质通常提供了减少通过系统流动的数据的基数的机会，从而相应地减少了下游处理这些聚合结果的成本。即便如此，在成本不是禁锢的用例中，清晰和简单的好处可以说超过了默认使用非完整保真度触发器的认知复杂性。
- en: Thus, for our first take at consuming aggregate team scores as a stream, let’s
    see what things would look like using a per-record trigger. Beam itself doesn’t
    have a precise per-record trigger, so, as demonstrated in Example 8-3, we instead
    use a repeated `AfterCount(1)` trigger, which will fire immediately any time a
    new record arrives.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于我们首次尝试将团队得分作为流来消耗的情况，让我们看看使用每条记录触发器会是什么样子。Beam本身没有精确的每条记录触发器，因此，如示例8-3所示，我们使用重复的`AfterCount(1)`触发器，每当有新记录到达时就会立即触发。
- en: Example 8-3\. Per-record trigger
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-3。每条记录触发器
- en: '[PRE31]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: A streams and tables rendering of this pipeline would then look something like
    that depicted in Figure 8-9.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这个管道的流和表格呈现将看起来像图8-9中所示的样子。
- en: <assets/stsy_0809.mp4>
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0809.mp4>
- en: '![Streams and tables view of windowed summation on a streaming engine with
    per-record triggering](img/stsy_0809.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![具有每条记录触发的流引擎上窗口求和的流和表格视图](img/stsy_0809.png)'
- en: Figure 8-9\. Streams and tables view of windowed summation on a streaming engine
    with per-record triggering
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-9。流和表格视图的窗口求和在具有每条记录触发的流引擎上
- en: An interesting side effect of using per-record triggers is how it somewhat masks
    the effect of data being brought to rest because they are then immediately put
    back into motion again by the trigger. Even so, the aggregate artifact from the
    grouping remains at rest in the table, as the ungrouped stream of values flows
    away from it.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用每条记录触发器的一个有趣的副作用是，它在某种程度上掩盖了数据被静止的效果，因为触发器立即将其重新激活。即使如此，来自分组的聚合物件仍然静止在表中，而未分组的值流则从中流走。
- en: 'Moving back to SQL, we can see now what the effect of rendering the corresponding
    time-value relation as a stream would be. It (unsurprisingly) looks a lot like
    the stream of values in the animation in Figure 8-9:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 回到SQL，我们现在可以看到将相应的时间-值关系呈现为流的效果会是什么样子。它（不出所料）看起来很像图8-9中动画中的值流：
- en: '[PRE32]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: But even for this simple use case, it’s pretty chatty. If we’re building a pipeline
    to process data for a large-scale mobile application, we might not want to pay
    the cost of processing downstream updates for each and every upstream user score.
    This is where custom triggers come in.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使对于这个简单的用例来说，它也是相当啰嗦的。如果我们要构建一个处理大规模移动应用程序数据的管道，我们可能不希望为每个上游用户分数的下游更新付出成本。这就是自定义触发器发挥作用的地方。
- en: Watermark triggers
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 水印触发器
- en: If we were to switch the Beam pipeline to use a watermark trigger, for example,
    we could get exactly one output per window in the stream version of the TVR, as
    demonstrated in Example 8-4 and shown in Figure 8-10.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将Beam管道切换为使用水印触发器，例如，我们可以在TVR的流版本中每个窗口获得一个输出，如示例8-4所示，并如图8-10所示。
- en: Example 8-4\. Watermark trigger
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-4。水印触发器
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: <assets/stsy_0810.mp4>
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0810.mp4>
- en: '![Windowed summation with watermark triggering](img/stsy_0810.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![带水印触发的窗口求和](img/stsy_0810.png)'
- en: Figure 8-10\. Windowed summation with watermark triggering
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-10。带水印触发的窗口求和
- en: To get the same effect in SQL, we’d need language support for specifying a custom
    trigger. Something like an `EMIT *<when>*` statement, such as `EMIT WHEN WATERMARK
    PAST *<column>*`. This would signal to the system that the table created by the
    aggregation should be triggered into a stream exactly once per row, when the input
    watermark for the table exceeds the timestamp value in the specified column (which
    in this case happens to be the end of the window).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 要在SQL中获得相同的效果，我们需要语言支持来指定自定义触发器。类似于`EMIT *<when>*`语句，比如`EMIT WHEN WATERMARK
    PAST *<column>*`。这将向系统发出信号，即聚合创建的表应该在输入水印超过指定列中的时间戳值时触发一次流，这在这种情况下恰好是窗口的结束时间。
- en: 'Let’s look at this relation rendered as a stream. From the perspective of understanding
    when trigger firings occur, it’s also handy to stop relying on the `MTime` values
    from the original inputs and instead capture the current timestamp at which rows
    in the stream are emitted:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下这个关系呈现为流。从理解触发器触发发生的时间的角度来看，停止依赖于原始输入的`MTime`值，并且捕获流中的行发出的当前时间戳也是很方便的：
- en: '[PRE34]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The main downside here is the late data problem due to the use of a heuristic
    watermark, as we encountered in previous chapters. In light of late data, a nicer
    option might be to also immediately output an update any time a late record shows
    up, using a variation on the watermark trigger that supported repeated late firings,
    as shown in Example 8-5 and Figure 8-11.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要缺点是由于启发式水印的使用而导致的延迟数据问题，正如我们在前几章中遇到的那样。考虑到延迟数据，一个更好的选择可能是在每次出现延迟记录时立即输出更新，使用支持重复延迟触发的水印触发器的变体，如示例8-5和图8-11所示。
- en: Example 8-5\. Watermark trigger with late firings
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-5。带有延迟触发的水印触发器
- en: '[PRE35]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: <assets/stsy_0811.mp4>
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0811.mp4>
- en: '![Windowed summation with on-time/late triggering](img/stsy_0811.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![带有准时/延迟触发的窗口求和](img/stsy_0811.png)'
- en: Figure 8-11\. Windowed summation with on-time/late triggering
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-11。带有准时/延迟触发的窗口求和
- en: 'We can do the same thing in SQL by allowing the specification of two triggers:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过允许指定两个触发器来在SQL中做同样的事情：
- en: 'A watermark trigger to give us an initial value: `WHEN WATERMARK PAST *<column>*`,
    with the end of the window used as the timestamp `*<column>*`.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个水印触发器给我们一个初始值：`WHEN WATERMARK PAST *<column>*`，窗口的结束时间被用作时间戳`*<column>*`。
- en: 'A repeated delay trigger for late data: `AND THEN AFTER *<duration>*`, with
    a `*<duration>*` of 0 to give us per-record semantics.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于延迟数据的重复延迟触发器：`AND THEN AFTER *<duration>*`，其中`*<duration>*`为0，以给出每条记录的语义。
- en: 'Now that we’re getting multiple rows per window, it can also be useful to have
    another two system columns available: the timing of each row/pane for a given
    window relative to the watermark (`Sys.EmitTiming`), and the index of the pane/row
    for a given window (`Sys.EmitIndex`, to identify the sequence of revisions for
    a given row/window):'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们每个窗口可以获得多行，还可以有另外两个系统列可用：每行/窗格相对于水印的时间（`Sys.EmitTiming`），以及每个窗口的窗格/行的索引（`Sys.EmitIndex`，用于标识给定行/窗口的修订序列）：
- en: '[PRE36]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: For each pane, using this trigger, we’re able to get a single on-time answer
    that is likely to be correct, thanks to our heuristic watermark. And for any data
    that arrives late, we can get an updated version of the row amending our previous
    results.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个触发器，对于每个窗格，我们能够得到一个准时的答案，这很可能是正确的，这要归功于我们的启发式水印。对于任何延迟到达的数据，我们可以得到一行的更新版本，修正我们之前的结果。
- en: Repeated delay triggers
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重复延迟触发器
- en: The other main temporal trigger use case you might want is repeated delayed
    updates; that is, trigger a window one minute (in processing time) after any new
    data for it arrive. Note that this is different than triggering on aligned boundaries,
    as you would get with a microbatch system. As Example 8-6 shows, triggering via
    a delay relative to the most recent new record arriving for the window/row helps
    spread triggering load out more evenly than a bursty, aligned trigger would. It
    also does not require any sort of watermark support. Figure 8-12 presents the
    results.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想要的另一个主要时间触发器用例是重复延迟更新；也就是说，在任何新数据到达后的一分钟（在处理时间上）触发窗口。请注意，这与在微批处理系统中触发对齐边界是不同的。正如示例8-6所示，通过相对于窗口/行的最近新记录到达的延迟触发，有助于更均匀地分散触发负载，而不像突发的对齐触发那样。它也不需要任何水印支持。图8-12呈现了结果。
- en: Example 8-6\. Repeated triggering with one-minute delays
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-6。重复触发，延迟一分钟
- en: '[PRE37]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: <assets/stsy_0812.mp4>
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0812.mp4>
- en: '![Windowed summation with on-time/late triggering](img/stsy_0812.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![带有准时/延迟触发的窗口求和](img/stsy_0812.png)'
- en: Figure 8-12\. Windowed summation with repeated one-minute-delay triggering
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-12。带有重复一分钟延迟触发的窗口求和
- en: The effect of using such a trigger is very similar to the per-record triggering
    we started out with but slightly less chatty thanks to the additional delay introduced
    in triggering, which allows the system to elide some number of the rows being
    produced. Tweaking the delay allows us to tune the volume of data generated, and
    thus balance the tensions of cost and timeliness as appropriate for the use case.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这样的触发器的效果与我们最初开始的每条记录触发非常相似，但由于触发中引入了额外的延迟，稍微减少了一些冗余，这使得系统能够省略产生的某些行。调整延迟可以让我们调节生成的数据量，从而平衡成本和及时性的张力，以适应使用情况。
- en: 'Rendered as a SQL stream, it would look something like this:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 作为SQL流呈现，它可能看起来像这样：
- en: '[PRE38]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Data-driven triggers
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据驱动触发器
- en: Before moving on to the final question in the Beam Model, it’s worth briefly
    discussing the idea of *data-driven triggers*. Because of the dynamic way types
    are handled in SQL, it might seem like data-driven triggers would be a very natural
    addition to the proposed `EMIT *<when>*` clause. For example, what if we want
    to trigger our summation any time the total score exceeds 10? Wouldn’t something
    like `EMIT WHEN Score > 10` work very naturally?
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入梁模型的最后一个问题之前，值得简要讨论“数据驱动触发器”的概念。由于SQL中处理类型的动态方式，似乎数据驱动触发器会是提议的`EMIT *<when>*`子句的一个非常自然的补充。例如，如果我们想在总分超过10时触发我们的总和，类似`EMIT
    WHEN Score > 10`的东西会非常自然地工作吗？
- en: 'Well, yes and no. Yes, such a construct would fit very naturally. But when
    you think about what would actually be happening with such a construct, you essentially
    would be triggering on every record, and then executing the `Score > 10` predicate
    to decide whether the triggered row should be propagated downstream. As you might
    recall, this sounds a lot like what happens with a `HAVING` clause. And, indeed,
    you can get the exact same effect by simply prepending `HAVING Score > 10` to
    the end of the query. At which point, it begs the question: is it worth adding
    explicit data-driven triggers? Probably not. Even so, it’s still encouraging to
    see just how easy it is to get the desired effect of data-driven triggers using
    standard SQL and well-chosen defaults.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，是的和不。是的，这样的构造会非常自然。但是当你考虑这样一个构造实际上会发生什么时，你基本上会在每条记录上触发，然后执行`Score > 10`谓词来决定触发的行是否应该向下游传播。你可能还记得，这听起来很像`HAVING`子句的情况。实际上，你可以通过简单地在查询的末尾添加`HAVING
    Score > 10`来获得完全相同的效果。在这一点上，它引出了一个问题：值得添加显式的数据驱动触发器吗？可能不值得。即便如此，看到使用标准SQL和精心选择的默认值如何轻松地获得所需的数据驱动触发器效果仍然令人鼓舞。
- en: '*How*: accumulation'
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '*如何*：累积'
- en: So far in this section, we’ve been ignoring the `Sys.Undo` column that I introduced
    toward the beginning of this chapter. As a result, we’ve defaulted to using *accumulating
    mode* to answer the question of *how* refinements for a window/row relate to one
    another. In other words, any time we observed multiple revisions of an aggregate
    row, the later revisions built upon the previous revisions, accumulating new inputs
    together with old ones. I opted for this approach because it matches the approach
    used in an earlier chapter, and it’s a relatively straightforward translation
    from how things work in a table world.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本节中，我们一直忽略了我在本章开头介绍的`Sys.Undo`列。因此，我们默认使用*累积模式*来回答窗口/行的细化如何相互关联的问题。换句话说，每当我们观察到聚合行的多个修订时，后续的修订都建立在前面的修订之上，将新的输入与旧的输入累积在一起。我选择这种方法是因为它与早期章节中使用的方法相匹配，并且相对于表世界中的工作方式，这是一个相对简单的转换。
- en: That said, accumulating mode has some major drawbacks. In fact, as we discussed
    in Chapter 2, it’s plain broken for any query/pipeline with a sequence of two
    or more grouping operations due to over counting. The only sane way to allow for
    the consumption of multiple revisions of a row within a system that allows for
    queries containing more than one serial grouping operation is if it operates by
    default in *accumulating and retracting* mode. Otherwise, you run into issues
    where a given input record is included multiple times in a single aggregation
    due to the blind incorporation of multiple revisions for a single row.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，累积模式有一些主要缺点。实际上，正如我们在第2章中讨论的那样，对于具有两个或更多分组操作序列的任何查询/管道来说，它对于过度计数是明显错误的。在允许包含多个序列分组操作的查询的系统中，允许对行的多个修订进行消耗的唯一明智的方法是默认情况下以*累积和撤销*模式运行。否则，由于对单行的多个修订的盲目合并，会出现一个给定输入记录在单个聚合中被多次包含的问题。
- en: So, when we come to the question of incorporating accumulation mode semantics
    into a SQL world, the option that fits best with our goal of providing an intuitive
    and natural experience is if the system uses retractions by default under the
    covers.¹⁵ As noted when I introduced the `Sys.Undo` column earlier, if you don’t
    care about the retractions (as in the examples in this section up until now),
    you don’t need to ask for them. But if you do ask for them, they should be there.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们考虑将累积模式语义纳入SQL世界时，最符合我们提供直观和自然体验目标的选项是系统在底层默认使用撤销。正如我之前介绍`Sys.Undo`列时所指出的，如果你不关心撤销（就像直到现在本节中的示例一样），你不需要要求它们。但是如果你要求它们，它们应该在那里。
- en: Retractions in a SQL world
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在SQL世界中的撤销
- en: 'To see what I mean, let’s look at another example. To motivate the problem
    appropriately, let’s look at a use case that’s relatively impractical without
    retractions: building session windows and writing them incrementally to a key/value
    store like HBase. In this case, we’ll be producing incremental sessions from our
    aggregation as they are built up. But in many cases, a given session will simply
    be an evolution of one or more previous sessions. In that case, you’d really like
    to delete the previous session(s) and replace it/them with the new one. But how
    do you do that? The only way to tell whether a given session replaces another
    one is to compare them to see whether the new one overlaps the old one. But that
    means duplicating some of the session-building logic in a separate part of your
    pipeline. And, more important, it means that you no longer have idempotent output,
    and you’ll thus need to jump through a bunch of extra hoops if you want to maintain
    end-to-end exactly-once semantics. Far better would be for the pipeline to simply
    tell you which sessions were removed and which were added in their place. This
    is what retractions give you.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明我的意思，让我们看另一个例子。为了适当地激发问题，让我们看一个相对不切实际的用例，即构建会话窗口并将它们逐步写入到HBase等键值存储中。在这种情况下，我们将从聚合中产生增量会话，但在许多情况下，给定的会话只是一个或多个先前会话的演变。在这种情况下，您真的希望删除先前的会话，并用新的会话替换它们。但是你该怎么做呢？判断给定的会话是否替换了另一个会话的唯一方法是将它们进行比较，看看新会话是否与旧会话重叠。但这意味着在管道的另一个部分中复制一些会话构建逻辑。更重要的是，这意味着您不再具有幂等输出，因此如果要保持端到端的一次性语义，就需要跳过一系列额外的步骤。更好的方法是，管道直接告诉您哪些会话被删除，哪些会话被替换。这就是撤销给您的东西。
- en: 'To see this in action (and in SQL), let’s modify our example pipeline to compute
    session windows with a gap duration of one minute. For simplicity and clarity,
    we go back to using the default per-record trigger. Note that I’ve also shifted
    a few of the data points within processing time for these session examples to
    make the diagram cleaner; event-time timestamps remain the same. The updated dataset
    looks like this (with shifted processing-time timestamps highlighted in yellow):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到这个示例的效果（以及SQL中的效果），让我们修改我们的示例管道，计算具有一分钟间隔的会话窗口。为了简单和清晰起见，我们回到使用默认的每条记录触发。请注意，我还将处理时间内的一些数据点移动到这些会话示例中，以使图表更清晰；事件时间戳保持不变。更新后的数据集如下（用黄色突出显示了移动的处理时间戳）：
- en: '[PRE39]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: To begin with, let’s look at the pipeline without retractions. After it’s clear
    why that pipeline is problematic for the use case of writing incremental sessions
    to a key/value store, we’ll then look at the version with retractions.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一下没有撤销的管道。在清楚了为什么该管道对于将增量会话写入键/值存储的用例是有问题之后，我们将看一下带有撤销的版本。
- en: The Beam code for the nonretracting pipeline would look something like Example 8-7.
    Figure 8-13 shows the results.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 不撤销管道的Beam代码看起来像示例8-7。图8-13显示了结果。
- en: Example 8-7\. Session windows with per-record triggering and accumulation but
    no retractions
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-7。具有每条记录触发和累积但没有撤销的会话窗口
- en: '[PRE40]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: <assets/stsy_0813.mp4>
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0813.mp4>
- en: '![Session window summation with accumulation but no retractions](img/stsy_0813.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![使用累积但没有撤销的会话窗口总结](img/stsy_0813.png)'
- en: Figure 8-13\. Session window summation with accumulation but no retractions
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-13。使用累积但没有撤销的会话窗口总结
- en: 'And finally, rendered in SQL, the output stream would look like this:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在SQL中呈现的输出流将如下所示：
- en: '[PRE41]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The important thing to notice in here (in the animation as well as the SQL rendering)
    is what the stream of incremental sessions looks like. From our holistic viewpoint,
    it’s pretty easy to visually identify in the animation which later sessions supersede
    those that came before. But imagine receiving elements in this stream one by one
    (as in the SQL listing) and needing to write them to HBase in a way that eventually
    results in the HBase table containing only the two final sessions (with values
    36 and 12). How would you do that? Well, you’d need to do a bunch of read-modify-write
    operations to read all of the existing sessions for a key, compare them with the
    new session, determine which ones overlap, issue deletes for the obsolete sessions,
    and then finally issue a write for the new session—all at significant additional
    cost, and with a loss of idempotence, which would ultimately leave you unable
    to provide end-to-end, exactly-once semantics. It’s just not practical.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里要注意的重要事情（在动画和SQL渲染中）是增量会话流的样子。从我们的整体观点来看，很容易在动画中直观地识别出哪些后续会话取代了之前的会话。但是想象一下，逐个接收这个流中的元素（就像在SQL列表中一样），并需要以一种最终使HBase表只包含两个最终会话（值为36和12）的方式将它们写入HBase。你会怎么做呢？嗯，你需要进行一系列的读取-修改-写入操作，读取一个键的所有现有会话，将它们与新会话进行比较，确定哪些会话重叠，删除过时的会话，最后为新会话发出写入操作——所有这些都需要额外的成本，并且会丧失幂等性，最终导致无法提供端到端的、一次性的语义。这是不切实际的。
- en: Contrast this then with the same pipeline, but with retractions enabled, as
    demonstrated in Example 8-8 and depicted in Figure 8-14.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将这个与启用撤销的相同管道进行对比，就像示例8-8和图8-14中所示的那样。
- en: Example 8-8\. Session windows with per-record triggering, accumulation, and
    retractions
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-8。具有每条记录触发、累积和撤销的会话窗口
- en: '[PRE42]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: <assets/stsy_0814.mp4>
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0814.mp4>
- en: '![Session window summation with accumulation and retractions](img/stsy_0814.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![使用累积和撤销的会话窗口总结](img/stsy_0814.png)'
- en: Figure 8-14\. Session window summation with accumulation and retractions
  id: totrans-273
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-14。使用累积和撤销的会话窗口总结
- en: 'And, lastly, in SQL form. For the SQL version, we’re assuming that the system
    is using retractions under the covers by default, and individual retraction rows
    are then materialized in the stream any time we request the special `Sys.Undo`
    column.¹⁶ As I described originally, the value of that column is that it allows
    us to distinguish retraction rows (labeled `undo` in the `Sys.Undo` column) from
    normal rows (unlabeled in the `Sys.Undo` column here for clearer contrast, though
    they could just as easily be labeled `redo`, instead):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在SQL形式上。对于SQL版本，我们假设系统默认情况下正在使用撤销，并且每当我们请求特殊的`Sys.Undo`列时，单独的撤销行就会在流中实现。正如我最初描述的那样，该列的价值在于它允许我们区分撤销行（在`Sys.Undo`列中标记为“撤销”）和正常行（在这里`Sys.Undo`列中未标记，以便更清晰地对比，尽管它们也可以被标记为“重做”）：
- en: '[PRE43]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'With retractions included, the sessions stream no longer just includes new
    sessions, but also retractions for the old sessions that have been replaced. With
    this stream, it’s trivial¹⁷ to properly build up the set of sessions in HBase
    over time: you simply write new sessions as they arrive (unlabeled `redo` rows)
    and delete old sessions as they’re retracted (`undo` rows). Much better!'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 包括撤销在内，会话流不再仅包括新会话，还包括已被替换的旧会话的撤销。有了这个流，随着时间的推移，逐步构建HBase中的会话集变得微不足道：您只需在新会话到达时写入新会话（未标记为“重做”行），并在它们被撤销时删除旧会话（“撤销”行）。好得多！
- en: Discarding mode, or lack thereof
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 丢弃模式，或者缺乏丢弃模式
- en: With this example, we’ve shown how simply and naturally you can incorporate
    retractions into SQL to provide both *accumulating mode* and *accumulating and
    retracting mode* semantics. But what about *discarding mode*?
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个例子，我们展示了如何简单而自然地将撤销纳入SQL中，以提供*累积模式*和*累积和撤销模式*语义。但是*丢弃模式*呢？
- en: For specific use cases such as very simple pipelines that partially aggregate
    high-volume input data via a single grouping operation and then write them into
    a storage system, which itself supports aggregation (e.g., a database-like system),
    discarding mode can be extremely valuable as a resource-saving option. But outside
    of those relatively narrow use cases, discarding mode is confusing and error-prone.
    As such, it’s probably not worth incorporating directly into SQL. Systems that
    need it can provide it as an option outside of the SQL language itself. Those
    that don’t can simply provide the more natural default of *accumulating and retracting
    mode*, with the option to ignore retractions when they aren’t needed.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定用例，例如通过单个分组操作部分聚合高容量输入数据，然后将其写入支持聚合的存储系统（例如类似数据库的系统），丢弃模式可以作为节省资源的选项非常有价值。但在那些相对狭窄的用例之外，丢弃模式是令人困惑和容易出错的。因此，将其直接纳入SQL可能并不值得。需要它的系统可以在SQL语言本身之外提供它作为一个选项。那些不需要的系统可以简单地提供更自然的默认值*累积和撤销模式*，并在不需要时忽略撤销。
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This has been a long journey but a fascinating one. We’ve covered a ton of information
    in this chapter, so let’s take a moment to reflect on it all.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个漫长而迷人的旅程。我们在本章中涵盖了大量信息，让我们花点时间来反思一下。
- en: First, we reasoned that the key difference between streaming and nonstreaming
    data processing is the *added dimension of time*. We observed that relations (the
    foundational data object from relational algebra, which itself is the basis for
    SQL) themselves evolve over time, and from that derived the notion of a *TVR*,
    which captures the evolution of a relation as a sequence of classic snapshot relations
    over time. From that definition, we were able to see that the *closure property*
    of relational algebra *remains intact* in a world of TVRs, which means that the
    entire suite of relational operators (and thus SQL constructs) continues to function
    as one would expect as we move from a world of point-in-time snapshot relations
    into a streaming-compatible world of TVRs.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们推断出流处理和非流处理数据处理之间的关键区别是*时间的增加维度*。我们观察到关系（关系代数的基础数据对象，它本身是SQL的基础）本身随时间演变，并从中推导出了*TVR*的概念，它将关系的演变捕捉为经典快照关系的序列。从这个定义中，我们能够看到关系代数的*闭包性质*在TVR的世界中*保持完整*，这意味着整套关系运算符（因此也是SQL构造）在我们从瞬时快照关系的世界转移到流兼容的TVR的世界时继续像预期的那样运行。
- en: Second, we explored the biases inherent in both the Beam Model and the classic
    SQL model as they exist today, coming to the conclusion that Beam has a stream-oriented
    approach, whereas SQL takes a table-oriented approach.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们探讨了Beam模型和经典SQL模型中固有的偏见，得出结论Beam具有面向流的方法，而SQL采用面向表的方法。
- en: 'And finally, we looked at the hypothetical language extensions needed to add
    support for robust stream processing to SQL,¹⁸ as well as some carefully chosen
    defaults that can greatly decrease the need for those extensions to be used:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看了一下需要对SQL进行语言扩展以支持健壮流处理的假设性语言扩展，以及一些精心选择的默认值，这些默认值可以大大减少对这些扩展的需求：
- en: Table/stream selection
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 表/流选择
- en: Given that any time-varying relation can be rendered in two different ways (table
    or stream), we need the ability to choose which rendering we want when materializing
    the results of a query. We introduced the `TABLE`, `STREAM`, and `TVR` keywords
    to provide a nice explicit way to choose the desired rendering.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于任何时变关系都可以以两种不同的方式呈现（表或流），我们需要在实现查询结果时选择所需的呈现方式。我们引入了“TABLE”、“STREAM”和“TVR”关键字，以提供一种明确选择所需呈现方式的方式。
- en: Even better is not needing to explicitly specify a choice, and that’s where
    good defaults come in. If all the inputs are tables, a good default is for the
    output to be a table, as well; this gives you the classic relational query behavior
    everyone is accustomed to. Conversely, if any of the inputs are streams, a reasonable
    default is for the output to be a stream, as well.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是不需要明确指定选择，这就是好的默认值的作用。如果所有输入都是表，那么输出为表是一个很好的默认值；这给了您所习惯的经典关系查询行为。相反，如果任何输入是流，则输出为流是一个合理的默认值。
- en: Windowing
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口化
- en: 'Though you can declare some types of simple windows declaratively using existing
    SQL constructs, there is still value in having explicit windowing operators:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以使用现有的SQL构造声明一些类型的简单窗口，但是具有显式窗口化运算符仍然具有价值：
- en: Windowing operators encapsulate the window-computation math.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口化运算符封装了窗口计算数学。
- en: Windowing allows the concise expression of complex, dynamic groupings like sessions.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口化允许简洁地表达复杂的、动态的分组，比如会话。
- en: Thus, the addition of simple windowing constructs for use in grouping can help
    make queries less error prone while also providing capabilities (like sessions)
    that are impractical to express in declarative SQL as it exists today.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，添加用于分组的简单窗口化构造可以帮助使查询更少出错，同时还提供了（例如会话）在现有的声明性SQL中难以表达的功能。
- en: Watermarks
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 水印
- en: This isn’t so much a SQL extension as it is a system-level feature. If the system
    in question integrates watermarks under the covers, they can be used in conjunction
    with triggers to generate streams containing a single, authoritative version of
    a row only after the input for that row is believed to be complete. This is critical
    for use cases in which it’s impractical to poll a materialized view table for
    results, and instead the output of the pipeline must be consumed directly as a
    stream. Examples are notifications and anomaly detection.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅是SQL的扩展，而是一个系统级特性。如果所涉及的系统在内部集成了水印，它们可以与触发器一起使用，以在相信该行的输入已经完成后生成包含单个、权威版本的流。这对于那些不可能为结果轮询物化视图表的用例至关重要，而是必须直接将管道的输出作为流进行消耗。示例包括通知和异常检测。
- en: Triggers
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 触发器
- en: 'Triggers define the shape of a stream as it is created from a TVR. If unspecified,
    the default should be per-record triggering, which provides straightforward and
    natural semantics matching those of materialized views. Beyond the default, there
    are essentially two main types of useful triggers:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 触发器定义了从TVR创建的流的形状。如果未指定，默认应该是每条记录触发，这提供了与物化视图相匹配的直接和自然的语义。除了默认值，基本上有两种主要类型的有用触发器：
- en: '*Watermark triggers*, for yielding a single output per window when the inputs
    to that window are believed to be complete.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*水印触发器*，用于在相信该窗口的输入已经完成时，为每个窗口产生单个输出。'
- en: '*Repeated delay triggers*, for providing periodic updates.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重复延迟触发器*，用于提供周期性更新。'
- en: Combinations of those two can also be useful, especially in the case of heuristic
    watermarks, to provide the early/on-time/late pattern we saw earlier.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者的组合也可能很有用，特别是在启发式水印的情况下，以提供我们之前看到的早期/准时/迟的模式。
- en: Special system columns
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊系统列
- en: 'When consuming a TVR as a stream, there are some interesting metadata that
    can be useful and which are most easily exposed as system-level columns. We looked
    at four:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 当将TVR作为流进行消耗时，有一些有趣的元数据可能会很有用，而且最容易暴露为系统级列。我们看了四个：
- en: '`Sys.MTime`'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sys.MTime`'
- en: The processing time at which a given row was last modified in a TVR.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 给定行在TVR中上次修改的处理时间。
- en: '`Sys.EmitTiming`'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sys.EmitTiming`'
- en: The timing of the row emit relative to the watermark (early, on-time, late).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 行发出相对于水印的时间（早、准时、迟）。
- en: '`Sys.EmitIndex`'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sys.EmitIndex`'
- en: The zero-based index of the emit version for this row.¹⁹
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 该行的发出版本的从零开始的索引。¹⁹
- en: '`Sys.Undo`'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sys.Undo`'
- en: Whether the row is a normal row or a retraction (`undo`). By default, the system
    should operate with retractions under the covers, as is necessary any time a series
    of more than one grouping operation might exist. If the `Sys.Undo` column is not
    projected when rendering a TVR as a stream, only normal rows will be returned,
    providing a simple way to toggle between *accumulating* and *accumulating and
    retracting* modes.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 该行是正常行还是撤销（`undo`）。默认情况下，系统应该在内部使用撤销，这在可能存在一系列多个分组操作的任何时候是必要的。如果在将TVR呈现为流时未投影`Sys.Undo`列，那么只会返回正常行，这提供了在*累积*和*累积和撤销*模式之间切换的简单方法。
- en: Stream processing with SQL doesn’t need to be difficult. In fact, stream processing
    in SQL is quite common already in the form of materialized views. The important
    pieces really boil down to capturing the evolution of datasets/relations over
    time (via time-varying relations), providing the means of choosing between physical
    table or stream representations of those time-varying relations, and providing
    the tools for reasoning about time (windowing, watermarks, and triggers) that
    we’ve been talking about throughout this book. And, critically, you need good
    defaults to minimize how often these extensions need to be used in practice.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SQL进行流处理并不需要很困难。事实上，SQL中的流处理已经相当普遍，以物化视图的形式存在。真正重要的部分实际上归结为捕获数据集/关系随时间的演变（通过时变关系），提供在物理表或流表示之间进行选择的手段，以及提供关于时间的推理工具（窗口化、水印和触发器），这些我们在本书中一直在讨论的。而且，至关重要的是，你需要很好的默认值，以最小化这些扩展在实践中需要被使用的频率。
- en: ¹ What I mean by “valid relation” here is simply a relation for which the application
    of a given operator is well formed. For example, for the SQL query `SELECT x FROM
    y`​, a valid relation y would be any relation containing an attribute/column named
    x. Any relation not containing a such-named attribute would be invalid and, in
    the case of a real database system, would yield a query execution error.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 这里我所说的“有效关系”简单地是指对于给定操作符的应用是良好形式的关系。例如，对于SQL查询`SELECT x FROM y`，一个有效的关系y将是任何包含名为x的属性/列的关系。任何不包含这样命名属性的关系将是无效的，并且在实际数据库系统的情况下，将产生查询执行错误。
- en: ² Much credit to Julian Hyde for this name and succinct rendering of the concept.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ² 对Julian Hyde的这个名称和概念的简洁表达表示非常感谢。
- en: ³ Note that the `Sys.Undo` name used here is riffing off the concise [undo/redo
    nomenclature from Apache Flink](https://flink.apache.org/news/2017/04/04/dynamic-tables.html),
    which I think is a very clean way to capture the ideas of retraction and nonretraction
    rows.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 请注意，这里使用的`Sys.Undo`名称是在[Apache Flink的撤销/重做命名法](https://flink.apache.org/news/2017/04/04/dynamic-tables.html)的基础上进行的，我认为这是捕捉撤销和非撤销行的想法的一种非常简洁的方式。
- en: ⁴ Now, in this example, it’s not too difficult to figure out that the new value
    of 8 should replace the old value of 7, given that the mapping is 1:1\. But we’ll
    see a more complicated example later on when we talk about sessions that is much
    more difficult to handle without having retractions as a guide.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 现在，在这个例子中，很容易发现新值8应该替换旧值7，因为映射是1:1。但当我们谈论会话时，我们将在稍后看到一个更复杂的例子，没有撤销作为指导，处理起来会更加困难。
- en: ⁵ And indeed, this is a key point to remember. There are some systems that advocate
    treating streams and tables as identical, claiming that we can simply treat streams
    like never-ending tables. That statement is accurate inasmuch as the true underlying
    primitive is the time-varying relation, and all relational operations may be applied
    equally to any time-varying relation, regardless of whether the actual physical
    manifestation is a stream or a table. But that sort of approach conflates the
    two very different types of views that tables and streams provide for a given
    time-varying relation. Pretending that two very different things are the same
    might seem simple on the surface, but it’s not a road toward understanding, clarity,
    and correctness.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵ 而且，这是一个需要记住的关键点。有一些系统主张将流和表视为相同，声称我们可以简单地将流视为永不结束的表。这种说法在某种程度上是准确的，因为真正的基础原语是时变关系，所有关系操作都可以等同地应用于任何时变关系，无论实际的物理表现形式是流还是表。但这种方法混淆了表和流为给定的时变关系提供的两种非常不同的视图类型。假装两个非常不同的东西是相同的，表面上看起来很简单，但这不是通向理解、清晰和正确的道路。
- en: ⁶ Here referring to tables in the sense of tables that can vary over time; that
    is, the table-based TVRs we’ve been looking at.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ⁶ 这里指的是随时间变化的表；也就是我们一直在看的基于表的TVR。
- en: ⁷ This one courtesy Julian Hyde.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ⁷ 这是朱利安·海德的礼貌。
- en: ⁸ Though there are a number of efforts in flight across various projects that
    are trying to simplify the specification of triggering/ungrouping semantics. The
    most compelling proposal, made independently within both the Flink and Beam communities,
    is that triggers should simply be specified at the outputs of a pipeline and automatically
    propagated up throughout the pipeline. In this way, one would describe only the
    desired shape of the streams that actually create materialized output; the shape
    of all other streams in the pipeline would be implicitly derived from there.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ⁸ 尽管各个项目中有许多正在进行的工作，试图简化触发/取消分组语义的规范。在Flink和Beam社区内部独立提出的最具说服力的建议是，触发器应该简单地在管道的输出处指定，并自动在整个管道中传播。通过这种方式，只需描述实际创建物化输出的流的期望形状；所有其他流的形状将从那里隐式地派生出来。
- en: ⁹ Though, of course, a single SQL query has vastly more expressive power than
    a single MapReduce, given the far less-confining set of operations and composition
    options available.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ⁹ 当然，单个SQL查询的表达能力远远超过单个MapReduce，因为它具有更少限制的操作和组合选项。
- en: ¹⁰ Note that we’re speaking conceptually here; there are of course a multitude
    of optimizations that can be applied in actual execution; for example, looking
    up specific rows via an index rather than scanning the entire table.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ¹⁰ 请注意，我们在这里是在概念上讨论；当然，在实际执行中可以应用许多优化；例如，通过索引查找特定行而不是扫描整个表。
- en: '¹¹ It’s been brought to my attention multiple times that the “`MATERIALIZED`”
    aspect of these queries is just an optimization: semantically speaking, these
    queries could just as easily be replaced with generic `CREATE VIEW` statements,
    in which case the database might instead simply rematerialize the entire view
    each time it is referenced. This is true. The reason I use the `MATERIALIZED`
    variant here is that the semantics of a materialized view are to incrementally
    update the view table in response to a stream of changes, which is indicative
    of the streaming nature behind them. That said, the fact that you can instead
    provide a similar experience by re-executing a bounded query each time a view
    is accessed provides a nice link between streams and tables as well as a link
    between streaming systems and the way batch systems have been historically used
    for processing data that evolves over time. You can either incrementally process
    changes as they occur or you can reprocess the entire input dataset from time
    to time. Both are valid ways of processing an evolving table of data.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹ 有多次提到这些查询的“`MATERIALIZED`”方面只是一种优化：从语义上讲，这些查询可以很容易地用通用的`CREATE VIEW`语句替换，这种情况下，数据库可能会在每次引用时重新生成整个视图。这是真的。我在这里使用`MATERIALIZED`变体的原因是，物化视图的语义是根据变化流增量更新视图表，这表明了它们背后的流式特性。也就是说，你可以根据发生的变化增量处理变化，也可以不时地重新处理整个输入数据集。这两种方式都是处理不断变化的数据表的有效方式。
- en: ¹² Though it’s probably fair to say that SQL’s table bias is likely an artifact
    of SQL’s *roots* in batch processing.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ¹² 虽然可以说SQL的表偏向可能是SQL在批处理中的根源。
- en: ¹³ For some use cases, capturing and using the current processing time for a
    given record as its event time going forward can be useful (for example, when
    logging events directly into a TVR, where the time of ingress is the natural event
    time for that record).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ¹³ 对于某些用例，捕获和使用给定记录的当前处理时间作为其未来事件时间可能是有用的（例如，当直接将事件记录到TVR中时，入口时间就是该记录的自然事件时间）。
- en: ¹⁴ Maths are easy to get wrong.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ¹⁴ 数学很容易出错。
- en: ¹⁵ It’s sufficient for retractions to be used by default and not simply always
    because the system only needs the *option* to use retractions. There are specific
    use cases; for example, queries with a single grouping operation whose results
    are being written into an external storage system that supports per-key updates,
    where the system can detect retractions are not needed and disable them as an
    optimization.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 15 默认情况下，使用撤回就足够了，而不仅仅是因为系统只需要*选择*使用撤回。有特定的用例；例如，具有单个分组操作的查询，其结果正在写入支持按键更新的外部存储系统，系统可以检测到不需要撤回并将其禁用作为优化。
- en: ¹⁶ Note that it’s a little odd for the simple addition of a new column in the
    `SELECT` statement to result in a new rows appearing in a query. A fine alternative
    approach would be to require `Sys.Undo` rows to be filtered out via a `WHERE`
    clause when not needed.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 16 请注意，仅仅在`SELECT`语句中简单添加新列就导致查询中出现新行有点奇怪。一个很好的替代方法是在不需要时通过`WHERE`子句过滤掉`Sys.Undo`行。
- en: ¹⁷ Not that this triviality applies only in cases for which eventual consistency
    is sufficient. If you need to always have a globally coherent view of all sessions
    at any given time, you must 1) be sure to write/delete (via tombstones) each session
    at its emit time, and 2) only ever read from the HBase table at a timestamp that
    is less than the output watermark from your pipeline (to synchronize reads against
    the multiple, independent writes/deletes that happen when sessions merge). Or
    better yet, cut out the middle person and serve the sessions from your state tables
    directly.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 17 请注意，这种琐事只适用于最终一致性足够的情况。如果您需要始终在任何给定时间具有全局一致的视图，则必须 1）确保在其发出时间写入/删除（通过墓碑）每个会话，并且
    2）仅从HBase表中的时间戳读取，该时间戳小于管道的输出水印（以使读取与会话合并时发生的多个独立写入/删除同步）。或者更好的是，直接从状态表中提供会话，而不是中间人。
- en: ¹⁸ To be clear, they’re not all hypothetical. Calcite has support for the windowing
    constructs described in this chapter.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 18 明确地说，它们并非都是假设的。Calcite支持本章描述的窗口构造。
- en: ¹⁹ Note that the definition of “index” becomes complicated in the case of merging
    windows like sessions. A reasonable approach is to take the maximum of all of
    the previous sessions being merged together and increment by one.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 19 请注意，在像会话这样的合并窗口的情况下，“索引”的定义变得复杂。一个合理的方法是取所有先前合并在一起的会话的最大值，并递增一。
