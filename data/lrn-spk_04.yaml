- en: Chapter 3\. Apache Spark’s Structured APIs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. Apache Spark的结构化API
- en: In this chapter, we will explore the principal motivations behind adding structure
    to Apache Spark, how those motivations led to the creation of high-level APIs
    (DataFrames and Datasets), and their unification in Spark 2.x across its components.
    We’ll also look at the Spark SQL engine that underpins these structured high-level
    APIs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索向Apache Spark添加结构的主要动机，这些动机如何导致创建高级API（DataFrames和Datasets），以及它们在Spark
    2.x中的组件之间的统一。我们还将看看支撑这些结构化高级API的Spark SQL引擎。
- en: When [Spark SQL](https://oreil.ly/cfd1r) was first introduced in the early Spark
    1.x releases, followed by [DataFrames](https://oreil.ly/kErKh) as a successor
    to [SchemaRDDs](https://oreil.ly/-o1-k) in Spark 1.3, we got our first glimpse
    of structure in Spark. Spark SQL introduced high-level expressive operational
    functions, mimicking SQL-like syntax, and DataFrames, which laid the foundation
    for more structure in subsequent releases, paved the path to performant operations
    in Spark’s computational queries.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当[Spark SQL](https://oreil.ly/cfd1r)在早期Spark 1.x版本中首次引入时，紧随其后的是[DataFrames](https://oreil.ly/kErKh)作为[SchemaRDDs](https://oreil.ly/-o1-k)的后继者。我们第一次在Spark中看到了结构。Spark
    SQL引入了高级表达操作函数，模仿类SQL语法，以及DataFrames，为后续版本中更多结构提供了基础，为Spark计算查询中的高性能操作铺平了道路。
- en: But before we talk about the newer Structured APIs, let’s get a brief glimpse
    of what it’s like to not have structure in Spark by taking a peek at the simple
    RDD programming API model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们讨论新的结构化API之前，让我们简要了解一下在Spark中没有结构的情况，通过简单的RDD编程API模型来看一下。
- en: 'Spark: What’s Underneath an RDD?'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark：RDD底层是什么？
- en: 'The [RDD](https://oreil.ly/KON5Y) is the most basic abstraction in Spark. There
    are three vital characteristics associated with an RDD:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[RDD](https://oreil.ly/KON5Y) 是Spark中最基本的抽象。与RDD关联的有三个关键特征：'
- en: Dependencies
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖项
- en: Partitions (with some locality information)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区（带有一些本地化信息）
- en: 'Compute function: Partition => `Iterator[T]`'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算函数：Partition => `Iterator[T]`
- en: All three are integral to the simple RDD programming API model upon which all
    higher-level functionality is constructed. First, a list of *dependencies* that
    instructs Spark how an RDD is constructed with its inputs is required. When necessary
    to reproduce results, Spark can recreate an RDD from these dependencies and replicate
    operations on it. This characteristic gives RDDs resiliency.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个特性对于构建所有高级功能的简单RDD编程API模型至关重要。首先，需要一个*依赖项*列表，指导Spark如何使用其输入构建RDD。在需要重现结果时，Spark可以从这些依赖项重新创建RDD，并在其上复制操作。这一特性赋予了RDD的弹性。
- en: Second, *partitions* provide Spark the ability to split the work to parallelize
    computation on partitions across executors. In some cases—for example, reading
    from HDFS—Spark will use locality information to send work to executors close
    to the data. That way less data is transmitted over the network.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，*分区*使得Spark能够将工作分割并行化处理分区上的计算。在某些情况下，例如从HDFS读取数据，Spark将使用本地化信息将工作发送到靠近数据的执行者。这样可以减少通过网络传输的数据量。
- en: And finally, an RDD has a *compute function* that produces an `Iterator[T]`
    for the data that will be stored in the RDD.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，RDD具有一个*计算函数*，用于生成将存储在RDD中的数据的`Iterator[T]`。
- en: Simple and elegant! Yet there are a couple of problems with this original model.
    For one, the compute function (or computation) is opaque to Spark. That is, Spark
    does not know what you are doing in the compute function. Whether you are performing
    a join, filter, select, or aggregation, Spark only sees it as a lambda expression.
    Another problem is that the `Iterator[T]` data type is also opaque for Python
    RDDs; Spark only knows that it’s a generic object in Python.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 简单而优雅！然而，这个原始模型存在一些问题。首先，计算函数（或计算过程）对Spark来说是不透明的。也就是说，Spark不知道你在计算函数中做了什么。无论是执行连接、过滤、选择还是聚合操作，Spark只把它视为一个lambda表达式。另一个问题是，对于Python
    RDDs来说，`Iterator[T]`数据类型也是不透明的；Spark只知道它是Python中的一个通用对象。
- en: Furthermore, because it’s unable to inspect the computation or expression in
    the function, Spark has no way to optimize the expression—it has no comprehension
    of its intention. And finally, Spark has no knowledge of the specific data type
    in `T`. To Spark it’s an opaque object; it has no idea if you are accessing a
    column of a certain type within an object. Therefore, all Spark can do is serialize
    the opaque object as a series of bytes, without using any data compression techniques.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于无法检查函数中的计算或表达式，Spark 无法优化表达式 —— 它无法理解其意图。最后，Spark 对于 `T` 的具体数据类型一无所知。对于
    Spark 来说，它是一个不透明的对象；它无法知道您是否在访问对象内的某个特定类型的列。因此，Spark 只能将不透明对象序列化为一系列字节，而无法使用任何数据压缩技术。
- en: This opacity clearly hampers Spark’s ability to rearrange your computation into
    an efficient query plan. So what’s the solution?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不透明性显然阻碍了 Spark 重新排列您的计算以生成高效的查询计划的能力。那么解决方案是什么呢？
- en: Structuring Spark
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化 Spark
- en: Spark 2.x introduced a few key schemes for structuring Spark. One is to express
    computations by using common patterns found in data analysis. These patterns are
    expressed as high-level operations such as filtering, selecting, counting, aggregating,
    averaging, and grouping. This provides added clarity and simplicity.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.x 引入了几种关键方案来结构化 Spark。其中一种方法是使用在数据分析中常见的通用模式来表达计算。这些模式被表达为诸如过滤、选择、计数、聚合、求平均和分组等高级操作。这样做提供了额外的清晰度和简单性。
- en: This specificity is further narrowed through the use of a set of common operators
    in a DSL. Through a set of operations in DSL, available as APIs in Spark’s supported
    languages (Java, Python, Spark, R, and SQL), these operators let you tell Spark
    what you wish to compute with your data, and as a result, it can construct an
    efficient query plan for execution.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 DSL 中一组常见操作符的使用，此特定性进一步通过 DSL 中一组常见操作符的使用进行缩小。通过这些操作符，作为 Spark 支持的语言（Java、Python、Spark、R
    和 SQL）中的 API 提供，这些操作符让您告诉 Spark 您希望如何计算您的数据，因此，它可以构建一个高效的查询计划来执行。
- en: And the final scheme of order and structure is to allow you to arrange your
    data in a tabular format, like a SQL table or spreadsheet, with supported structured
    data types (which we will cover shortly).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的顺序和结构方案是允许您将数据以表格格式排列，如 SQL 表或电子表格，使用支持的结构化数据类型（我们将很快介绍）。
- en: But what’s all this structure good for?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但所有这些结构有什么好处呢？
- en: Key Merits and Benefits
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键优势和益处
- en: 'Structure yields a number of benefits, including better performance and space
    efficiency across Spark components. We will explore these benefits further when
    we talk about the use of the DataFrame and Dataset APIs shortly, but for now we’ll
    concentrate on the other advantages: expressivity, simplicity, composability,
    and uniformity.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化带来了许多好处，包括在 Spark 组件之间提升性能和空间效率。我们稍后将进一步探讨使用 DataFrame 和 Dataset API 时的这些好处，但现在我们将集中讨论其他优势：表达性、简单性、可组合性和统一性。
- en: 'Let’s demonstrate expressivity and composability first, with a simple code
    snippet. In the following example, we want to aggregate all the ages for each
    name, group by name, and then average the ages—a common pattern in data analysis
    and discovery. If we were to use the low-level RDD API for this, the code would
    look as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们通过一个简单的代码片段来展示表达性和可组合性。在下面的示例中，我们想要按名称分组，然后计算每个名称的所有年龄的平均值 —— 这是数据分析和探索中常见的模式。如果我们要使用低级别的
    RDD API，代码将如下所示：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: No one would dispute that this code, which tells Spark *how to* aggregate keys
    and compute averages with a string of lambda functions, is cryptic and hard to
    read. In other words, the code is instructing Spark how to compute the query.
    It’s completely opaque to Spark, because it doesn’t communicate the intention.
    Furthermore, the equivalent RDD code in Scala would look very different from the
    Python code shown here.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人会反驳这段代码的晦涩和难以阅读，它告诉 Spark *如何* 对键进行聚合并计算平均值，使用了一系列的 Lambda 函数。换句话说，该代码在指导
    Spark 如何计算查询。对于 Spark 来说完全不透明，因为它无法传达意图。此外，在 Scala 中等效的 RDD 代码与此处展示的 Python 代码也有很大不同。
- en: 'By contrast, what if we were to express the same query with high-level DSL
    operators and the DataFrame API, thereby instructing Spark *what to do*? Have
    a look:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果我们使用高级 DSL 操作符和 DataFrame API 表达相同的查询，从而指导 Spark *做什么*？看一下：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This version of the code is far more expressive as well as simpler than the
    earlier version, because we are using high-level DSL operators and APIs to tell
    Spark what to do*.* In effect, we have employed these operators to compose our
    query. And because Spark can inspect or parse this query and understand our intention,
    it can optimize or arrange the operations for efficient execution. Spark knows
    exactly *what* we wish to do: group people by their names, aggregate their ages,
    and then compute the average age of all people with the same name. We’ve composed
    an entire computation using high-level operators as a single simple query—how
    expressive is that?'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本的代码不仅更富表现力，而且更简单，因为我们使用高级DSL操作符和API告诉Spark要做什么。实际上，我们已经使用这些操作符来组合我们的查询。由于Spark可以检查或解析这个查询并理解我们的意图，它可以优化或安排操作以实现高效执行。Spark清楚地知道我们希望做什么：按姓名分组人们，聚合他们的年龄，然后计算具有相同姓名的所有人的平均年龄。我们使用高级操作符组合了整个计算作为一个简单的查询——这是多么富有表现力啊？
- en: Some would contend that by using only high-level, expressive DSL operators mapped
    to common or recurring data analysis patterns to introduce order and structure,
    we are limiting the scope of the developers’ ability to instruct the compiler
    or control how their queries should be computed. Rest assured that you are not
    confined to these structured patterns; you can switch back at any time to the
    unstructured low-level RDD API, although we hardly ever find a need to do so.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有人或许会认为，通过仅使用高级别的、富有表现力的DSL操作符来映射常见或重复的数据分析模式，以引入秩序和结构，我们限制了开发者指导编译器或控制查询计算方式的能力。请放心，您并不局限于这些结构化模式；您随时可以切换回不结构化的低级RDD
    API，尽管我们几乎从不需要这样做。
- en: 'As well as being simpler to read, the structure of Spark’s high-level APIs
    also introduces uniformity across its components and languages. For example, the
    Scala code shown here does the same thing as the previous Python code—and the
    API looks nearly identical:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了更易于阅读之外，Spark高级API的结构还在其组件和语言之间引入了统一性。例如，这里展示的Scala代码与之前的Python代码执行相同的操作——而且API看起来几乎相同：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Some of these DSL operators perform relational-like operations that you’ll be
    familiar with if you know SQL, such as selecting, filtering, grouping, and aggregation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些DSL操作符中的一些执行类似关系型操作，如果您了解SQL，您会很熟悉，比如选择、过滤、分组和聚合。
- en: All of this simplicity and expressivity that we developers cherish is possible
    because of the Spark SQL engine upon which the high-level Structured APIs are
    built. It is because of this engine, which underpins all the Spark components,
    that we get uniform APIs. Whether you express a query against a DataFrame in Structured
    Streaming or MLlib, you are always transforming and operating on DataFrames as
    structured data. We’ll take a closer look at the Spark SQL engine later in this
    chapter, but for now let’s explore those APIs and DSLs for common operations and
    how to use them for data analytics.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些我们开发者所珍视的简洁性和表达性，都是因为Spark SQL引擎的存在，它支持构建高级结构化API。正是由于这个引擎，作为所有Spark组件的基础，我们才能获得统一的API。无论您是在结构化流处理还是MLlib中对DataFrame表达查询，您始终是在操作和转换结构化数据。本章稍后我们将更详细地了解Spark
    SQL引擎，但现在让我们来探索这些API和DSL，用于常见操作以及如何用于数据分析。
- en: The DataFrame API
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame API
- en: 'Inspired by [pandas DataFrames](https://oreil.ly/z93hD) in structure, format,
    and a few specific operations, Spark DataFrames are like distributed in-memory
    tables with named columns and schemas, where each column has a specific data type:
    integer, string, array, map, real, date, timestamp, etc. To a human’s eye, a Spark
    DataFrame is like a table. An example is shown in [Table 3-1](#the_table_like_format_of_a_dataframe).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 受到[pandas DataFrames](https://oreil.ly/z93hD)在结构、格式以及一些特定操作的启发，Spark DataFrames就像是带有命名列和模式的分布式内存中的表格，其中每列具有特定的数据类型：整数、字符串、数组、映射、实数、日期、时间戳等。对于人眼来说，Spark
    DataFrame就像是一张表。示例见[Table 3-1](#the_table_like_format_of_a_dataframe)。
- en: Table 3-1\. The table-like format of a DataFrame
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-1\. DataFrame的表格化格式
- en: '| `Id (Int)` | `First (String)` | `Last (String)` | `Url (String)` | `Published
    (Date)` | `Hits (Int)` | `Campaigns (List[Strings])` |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| `Id (Int)` | `First (String)` | `Last (String)` | `Url (String)` | `Published
    (Date)` | `Hits (Int)` | `Campaigns (List[Strings])` |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| `1` | `Jules` | `Damji` | `https://tinyurl.1` | `1/4/2016` | `4535` | `[twitter,
    LinkedIn]` |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `Jules` | `Damji` | `https://tinyurl.1` | `1/4/2016` | `4535` | `[twitter,
    LinkedIn]` |'
- en: '| `2` | `Brooke` | `Wenig` | `https://tinyurl.2` | `5/5/2018` | `8908` | `[twitter,
    LinkedIn]` |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| `2` | `Brooke` | `Wenig` | `https://tinyurl.2` | `5/5/2018` | `8908` | `[twitter,
    LinkedIn]` |'
- en: '| `3` | `Denny` | `Lee` | `https://tinyurl.3` | `6/7/2019` | `7659` | `[web,
    twitter, FB, LinkedIn]` |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| `3` | `Denny` | `Lee` | `https://tinyurl.3` | `6/7/2019` | `7659` | `[web,
    twitter, FB, LinkedIn]` |'
- en: '| `4` | `Tathagata` | `Das` | `https://tinyurl.4` | `5/12/2018` | `10568` |
    `[twitter, FB]` |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| `4` | `Tathagata` | `Das` | `https://tinyurl.4` | `5/12/2018` | `10568` |
    `[twitter, FB]` |'
- en: '| `5` | `Matei` | `Zaharia` | `https://tinyurl.5` | `5/14/2014` | `40578` |
    `[web, twitter, FB, LinkedIn]` |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| `5` | `Matei` | `Zaharia` | `https://tinyurl.5` | `5/14/2014` | `40578` |
    `[web, twitter, FB, LinkedIn]` |'
- en: '| `6` | `Reynold` | `Xin` | `https://tinyurl.6` | `3/2/2015` | `25568` | `[twitter,
    LinkedIn]` |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| `6` | `Reynold` | `Xin` | `https://tinyurl.6` | `3/2/2015` | `25568` | `[twitter,
    LinkedIn]` |'
- en: When data is visualized as a structured table, it’s not only easy to digest
    but also easy to work with when it comes to common operations you might want to
    execute on rows and columns. Also recall that, as you learned in [Chapter 2](ch02.html#downloading_apache_spark_and_getting_sta),
    DataFrames are immutable and Spark keeps a lineage of all transformations. You
    can add or change the names and data types of the columns, creating new DataFrames
    while the previous versions are preserved. A named column in a DataFrame and its
    associated Spark data type can be declared in the schema.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据以结构化表格的形式展示时，不仅易于理解，而且在进行常见的行列操作时也易于处理。此外，请回忆一下，正如您在 [第 2 章](ch02.html#downloading_apache_spark_and_getting_sta)
    中学到的那样，DataFrame 是不可变的，Spark 会保留所有转换的历史记录。您可以添加或更改列的名称和数据类型，创建新的 DataFrame，同时保留之前的版本。可以在模式中声明
    DataFrame 中命名的列及其关联的 Spark 数据类型。
- en: Let’s examine the generic and structured data types available in Spark before
    we use them to define a schema. Then we’ll illustrate how to create a DataFrame
    with a schema, capturing the data in [Table 3-1](#the_table_like_format_of_a_dataframe).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在使用它们定义模式之前，先检查 Spark 中可用的通用和结构化数据类型。然后，我们将说明如何使用它们创建具有模式的 DataFrame，捕获 [表 3-1](#the_table_like_format_of_a_dataframe)
    中的数据。
- en: Spark’s Basic Data Types
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 的基本数据类型
- en: 'Matching its supported programming languages, Spark supports basic internal
    data types. These data types can be declared in your Spark application or defined
    in your schema. For example, in Scala, you can define or declare a particular
    column name to be of type `String`, `Byte`, `Long`, or Map, etc. Here, we define
    variable names tied to a Spark data type:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与其支持的编程语言相匹配，Spark 支持基本的内部数据类型。这些数据类型可以在您的 Spark 应用程序中定义或声明。例如，在 Scala 中，您可以定义或声明特定列名为
    `String`、`Byte`、`Long` 或 `Map` 等类型。在这里，我们定义了与 Spark 数据类型相关联的变量名：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Table 3-2](#basic_scala_data_types_in_spark) lists the basic Scala data types
    supported in Spark. They all are subtypes of the class [`DataTypes`](https://oreil.ly/_GifO),
    except for `DecimalType`.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3-2](#basic_scala_data_types_in_spark) 列出了 Spark 支持的基本 Scala 数据类型。它们都是 [`DataTypes`](https://oreil.ly/_GifO)
    类的子类型，除了 `DecimalType`。'
- en: Table 3-2\. Basic Scala data types in Spark
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-2\. Spark 中的基本 Scala 数据类型
- en: '| Data type | Value assigned in Scala | API to instantiate |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | Scala 中分配的值 | 实例化的 API |'
- en: '| --- | --- | --- |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `ByteType` | `Byte` | `DataTypes.ByteType` |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| `ByteType` | `Byte` | `DataTypes.ByteType` |'
- en: '| `ShortType` | `Short` | `DataTypes.ShortType` |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| `ShortType` | `Short` | `DataTypes.ShortType` |'
- en: '| `IntegerType` | `Int` | `DataTypes.IntegerType` |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `IntegerType` | `Int` | `DataTypes.IntegerType` |'
- en: '| `LongType` | `Long` | `DataTypes.LongType` |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `LongType` | `Long` | `DataTypes.LongType` |'
- en: '| `FloatType` | `Float` | `DataTypes.FloatType` |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `FloatType` | `Float` | `DataTypes.FloatType` |'
- en: '| `DoubleType` | `Double` | `DataTypes.DoubleType` |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `DoubleType` | `Double` | `DataTypes.DoubleType` |'
- en: '| `StringType` | `String` | `DataTypes.StringType` |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `StringType` | `String` | `DataTypes.StringType` |'
- en: '| `BooleanType` | `Boolean` | `DataTypes.BooleanType` |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `BooleanType` | `Boolean` | `DataTypes.BooleanType` |'
- en: '| `DecimalType` | `java.math.BigDecimal` | `DecimalType` |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `DecimalType` | `java.math.BigDecimal` | `DecimalType` |'
- en: Spark supports similar basic [Python data types](https://oreil.ly/HuREJ), as
    enumerated in [Table 3-3](#basic_python_data_types_in_spark).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 支持类似的基本 [Python 数据类型](https://oreil.ly/HuREJ)，如 [表 3-3](#basic_python_data_types_in_spark)
    中所列。
- en: Table 3-3\. Basic Python data types in Spark
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-3\. Spark 中的基本 Python 数据类型
- en: '| Data type | Value assigned in Python | API to instantiate |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: 数据类型 | Python 中分配的值 | 实例化的 API |
- en: '| --- | --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `ByteType` | `int` | `DataTypes.ByteType` |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `ByteType` | `int` | `DataTypes.ByteType` |'
- en: '| `ShortType` | `int` | `DataTypes.ShortType` |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `ShortType` | `int` | `DataTypes.ShortType` |'
- en: '| `IntegerType` | `int` | `DataTypes.IntegerType` |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `IntegerType` | `int` | `DataTypes.IntegerType` |'
- en: '| `LongType` | `int` | `DataTypes.LongType` |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `LongType` | `int` | `DataTypes.LongType` |'
- en: '| `FloatType` | `float` | `DataTypes.FloatType` |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `FloatType` | `float` | `DataTypes.FloatType` |'
- en: '| `DoubleType` | `float` | `DataTypes.DoubleType` |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| `DoubleType` | `float` | `DataTypes.DoubleType` |'
- en: '| `StringType` | `str` | `DataTypes.StringType` |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| `StringType` | `str` | `DataTypes.StringType` |'
- en: '| `BooleanType` | `bool` | `DataTypes.BooleanType` |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| `BooleanType` | `bool` | `DataTypes.BooleanType` |'
- en: '| `DecimalType` | `decimal.Decimal` | `DecimalType` |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `DecimalType` | `decimal.Decimal` | `DecimalType` |'
- en: Spark’s Structured and Complex Data Types
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 的结构化和复杂数据类型
- en: 'For complex data analytics, you won’t deal only with simple or basic data types.
    Your data will be complex, often structured or nested, and you’ll need Spark to
    handle these complex data types. They come in many forms: maps, arrays, structs,
    dates, timestamps, fields, etc. [Table 3-4](#scala_structured_data_types_in_spark)
    lists the Scala structured data types that Spark supports.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂数据分析，您不仅仅处理简单或基本数据类型。您的数据可能是复杂的，通常是结构化或嵌套的，您需要 Spark 来处理这些复杂的数据类型。它们有多种形式：映射（maps）、数组（arrays）、结构（structs）、日期（dates）、时间戳（timestamps）、字段等。[表 3-4](#scala_structured_data_types_in_spark)
    列出了 Spark 支持的 Scala 结构化数据类型。
- en: Table 3-4\. Scala structured data types in Spark
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-4\. Scala 中的结构化数据类型在 Spark 中
- en: '| Data type | Value assigned in Scala | API to instantiate |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: 数据类型 | 在 Scala 中分配的值 | 实例化的 API |
- en: '| --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `BinaryType` | `Array[Byte]` | `DataTypes.BinaryType` |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `BinaryType` | `Array[Byte]` | `DataTypes.BinaryType` |'
- en: '| `TimestampType` | `java.sql.Timestamp` | `DataTypes.TimestampType` |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `TimestampType` | `java.sql.Timestamp` | `DataTypes.TimestampType` |'
- en: '| `DateType` | `java.sql.Date` | `DataTypes.DateType` |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `DateType` | `java.sql.Date` | `DataTypes.DateType` |'
- en: '| `ArrayType` | `scala.collection.Seq` | `DataTypes.createArrayType(ElementType)`
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `ArrayType` | `scala.collection.Seq` | `DataTypes.createArrayType(ElementType)`
    |'
- en: '| `MapType` | `scala.collection.Map` | `DataTypes.createMapType(keyType, valueType)`
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `MapType` | `scala.collection.Map` | `DataTypes.createMapType(keyType, valueType)`
    |'
- en: '| `StructType` | `org.apache.spark.sql.Row` | `StructType(ArrayType[fieldTypes])`
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| `StructType` | `org.apache.spark.sql.Row` | `StructType(ArrayType[fieldTypes])`
    |'
- en: '| `StructField` | A value type corresponding to the type of this field | `StructField(name,
    dataType, [nullable])` |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| `StructField` | 与此字段类型对应的值类型 | `StructField(name, dataType, [nullable])`
    |'
- en: The equivalent structured data types in Python that Spark supports are enumerated
    in [Table 3-5](#python_structured_data_types_in_spark).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 支持的 Python 中等效的结构化数据类型在 [表 3-5](#python_structured_data_types_in_spark)
    中列举。
- en: Table 3-5\. Python structured data types in Spark
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-5\. Python 中的结构化数据类型在 Spark 中
- en: '| Data type | Value assigned in Python | API to instantiate |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: 数据类型 | 在 Python 中分配的值 | 实例化的 API |
- en: '| --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `BinaryType` | `bytearray` | `BinaryType()` |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `BinaryType` | `bytearray` | `BinaryType()` |'
- en: '| `TimestampType` | `datetime.datetime` | `TimestampType()` |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `TimestampType` | `datetime.datetime` | `TimestampType()` |'
- en: '| `DateType` | `datetime.date` | `DateType()` |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `DateType` | `datetime.date` | `DateType()` |'
- en: '| `ArrayType` | List, tuple, or array | `ArrayType(dataType, [nullable])` |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `ArrayType` | 列表、元组或数组 | `ArrayType(dataType, [nullable])` |'
- en: '| `MapType` | `dict` | `MapType(keyType, valueType, [nullable])` |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `MapType` | `dict` | `MapType(keyType, valueType, [nullable])` |'
- en: '| `StructType` | List or tuple | `StructType([fields])` |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `StructType` | 列表或元组 | `StructType([fields])` |'
- en: '| `StructField` | A value type corresponding to the type of this field | `StructField(name,
    dataType, [nullable])` |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `StructField` | 与此字段类型对应的值类型 | `StructField(name, dataType, [nullable])`
    |'
- en: While these tables showcase the myriad types supported, it’s far more important
    to see how these types come together when you define a schema for your data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些表展示了支持的多种类型，但更重要的是看到当您为数据定义 schema 时这些类型如何结合在一起。
- en: Schemas and Creating DataFrames
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Schema 和创建 DataFrame
- en: 'A *schema* in Spark defines the column names and associated data types for
    a DataFrame. Most often, schemas come into play when you are reading structured
    data from an external data source (more on this in the next chapter). Defining
    a schema up front as opposed to taking a schema-on-read approach offers three
    benefits:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，*schema* 定义了 DataFrame 的列名和相关的数据类型。通常情况下，当您从外部数据源读取结构化数据时（更多内容将在下一章讨论），schema
    就会发挥作用。与在读取时定义 schema 相比，提前定义 schema 具有三个优点：
- en: You relieve Spark from the onus of inferring data types.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解除 Spark 推断数据类型的负担。
- en: You prevent Spark from creating a separate job just to read a large portion
    of your file to ascertain the schema, which for a large data file can be expensive
    and time-consuming.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止 Spark 创建一个单独的作业仅仅是为了读取文件的大部分内容以确定 schema，对于大数据文件来说，这可能既昂贵又耗时。
- en: You can detect errors early if data doesn’t match the schema.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据与 schema 不匹配，您可以及早发现错误。
- en: So, we encourage you to always define your schema up front whenever you want
    to read a large file from a data source. For a short illustration, let’s define
    a schema for the data in [Table 3-1](#the_table_like_format_of_a_dataframe) and
    use that schema to create a DataFrame.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们鼓励您无论何时希望从数据源中读取大型文件时都提前定义模式。为了简短示例，让我们为 [表 3-1](#the_table_like_format_of_a_dataframe)
    中的数据定义一个模式，并使用该模式创建一个 DataFrame。
- en: Two ways to define a schema
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 两种定义模式的方式
- en: Spark allows you to define a schema in two ways. One is to define it programmatically,
    and the other is to employ a Data Definition Language (DDL) string, which is much
    simpler and easier to read.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 允许您以两种方式定义模式。一种是以编程方式定义，另一种是使用数据定义语言（DDL）字符串，后者更简单且更易读。
- en: 'To define a schema programmatically for a DataFrame with three named columns,
    `author`, `title`, and `pages`, you can use the Spark DataFrame API. For example:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要为具有三个命名列 `author`、`title` 和 `pages` 的 DataFrame 编程定义模式，您可以使用 Spark DataFrame
    API。例如：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Defining the same schema using DDL is much simpler:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DDL 定义相同的模式要简单得多：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can choose whichever way you like to define a schema. For many examples,
    we will use both:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择任何一种方式来定义模式。在许多示例中，我们将同时使用两种方法：
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Running this program from the console will produce the following output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台运行此程序将产生以下输出：
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you want to use this schema elsewhere in your code, simply execute `blogs_df.schema`
    and it will return the schema definition:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在代码的其他位置使用此模式，只需执行 `blogs_df.schema`，它将返回模式定义：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can observe, the DataFrame layout matches that of [Table 3-1](#the_table_like_format_of_a_dataframe)
    along with the respective data types and schema output.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，DataFrame 的布局与 [表 3-1](#the_table_like_format_of_a_dataframe) 的布局以及相应的数据类型和模式输出匹配。
- en: 'If you were to read the data from a JSON file instead of creating static data,
    the schema definition would be identical. Let’s illustrate the same code with
    a Scala example, this time reading from a JSON file:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您要从 JSON 文件中读取数据而不是创建静态数据，则模式定义将相同。让我们用 Scala 示例说明相同的代码，这次是从 JSON 文件中读取：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Not surprisingly, the output from the Scala program is no different than that
    from the Python program:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，Scala 程序的输出与 Python 程序的输出相同：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that you have an idea of how to use structured data and schemas in DataFrames,
    let’s focus on DataFrame columns and rows and what it means to operate on them
    with the DataFrame API.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解如何在 DataFrame 中使用结构化数据和模式，让我们关注 DataFrame API 中的列和行及其操作的含义。
- en: Columns and Expressions
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列和表达式
- en: 'As mentioned previously, named columns in DataFrames are conceptually similar
    to named columns in pandas or R DataFrames or in an RDBMS table: they describe
    a type of field. You can list all the columns by their names, and you can perform
    operations on their values using relational or computational expressions. In Spark’s
    supported languages, columns are objects with public methods (represented by the
    `Column` type).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，DataFrame 中的命名列在概念上类似于 pandas 或 R 中的命名列，或者类似于 RDBMS 表中的命名列：它们描述了字段的类型。您可以按其名称列出所有列，并可以使用关系或计算表达式对其值进行操作。在
    Spark 支持的语言中，列是具有公共方法的对象（由 `Column` 类型表示）。
- en: You can also use logical or mathematical expressions on columns. For example,
    you could create a simple expression using `expr("columnName * 5")` or `(expr("columnName
    - 5") > col(anothercolumnName))`, where `columnName` is a Spark type (integer,
    string, etc.). `expr()` is part of the `pyspark.sql.functions` (Python) and `org.apache.spark.sql.functions`
    (Scala) packages. Like any other function in those packages, `expr()` takes arguments
    that Spark will parse as an expression, computing the result.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在列上使用逻辑或数学表达式。例如，您可以使用 `expr("columnName * 5")` 或 `(expr("columnName - 5")
    > col(anothercolumnName))` 创建一个简单的表达式，其中 `columnName` 是 Spark 类型（整数、字符串等）。`expr()`
    是 `pyspark.sql.functions`（Python）和 `org.apache.spark.sql.functions`（Scala）包中的一部分。像这些包中的任何其他函数一样，`expr()`
    接受 Spark 将解析为表达式并计算结果的参数。
- en: Note
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Scala, Java, and Python all have [public methods associated with columns](https://oreil.ly/xVBIX).
    You’ll note that the Spark documentation refers to both `col` and `Column`. `Column`
    is the name of the object, while `col()` is a standard built-in function that
    returns a `Column`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Scala、Java 和 Python 都具有与列关联的 [公共方法](https://oreil.ly/xVBIX)。您会注意到 Spark 文档同时提到了
    `col` 和 `Column`。`Column` 是对象的名称，而 `col()` 是一个标准内置函数，返回一个 `Column`。
- en: 'Let’s take a look at some examples of what we can do with columns in Spark.
    Each example is followed by its output:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些在 Spark 中可以对列执行的操作示例。每个示例后面都跟着它的输出：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this last example, the expressions `blogs_df.sort(col("Id").desc)` and `blogsDF.sort($"Id".desc)`
    are identical. They both sort the DataFrame column named `Id` in descending order:
    one uses an explicit function, `col("Id")`, to return a `Column` object, while
    the other uses `$` before the name of the column, which is a function in Spark
    that converts column named `Id` to a `Column`.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个最后的例子中，表达式 `blogs_df.sort(col("Id").desc)` 和 `blogsDF.sort($"Id".desc)` 是相同的。它们都按照降序排序名为
    `Id` 的 DataFrame 列：一个使用显式函数 `col("Id")` 返回一个 `Column` 对象，而另一个在列名前使用 `$`，这是 Spark
    中将列名 `Id` 转换为 `Column` 的函数。
- en: Note
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We have only scratched the surface here, and employed just a couple of methods
    on `Column` objects. For a complete list of all public methods for `Column` objects,
    we refer you to the Spark [documentation](https://oreil.ly/TZd3c).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里只是浅尝辄止，并且仅仅使用了 `Column` 对象上的几种方法。有关 `Column` 对象的所有公共方法的完整列表，请参阅 Spark [文档](https://oreil.ly/TZd3c)。
- en: '`Column` objects in a DataFrame can’t exist in isolation; each column is part
    of a row in a record and all the rows together constitute a DataFrame, which as
    we will see later in the chapter is really a `Dataset[Row]` in Scala.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DataFrame 中，`Column` 对象不能单独存在；每列都是记录中的一部分，所有行一起构成一个 DataFrame，在本章后面我们将看到它实际上是
    Scala 中的 `Dataset[Row]`。
- en: Rows
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行
- en: 'A row in Spark is a generic [`Row` object](https://oreil.ly/YLMnw), containing
    one or more columns. Each column may be of the same data type (e.g., integer or
    string), or they can have different types (integer, string, map, array, etc.).
    Because `Row` is an object in Spark and an ordered collection of fields, you can
    instantiate a `Row` in each of Spark’s supported languages and access its fields
    by an index starting at 0:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，一行是一个通用的 [`Row` 对象](https://oreil.ly/YLMnw)，包含一个或多个列。每列可以是相同的数据类型（例如整数或字符串），也可以是不同的类型（整数、字符串、映射、数组等）。因为
    `Row` 是 Spark 中的对象，是一个有序的字段集合，你可以在 Spark 支持的每种语言中实例化一个 `Row`，并通过从 0 开始的索引访问其字段：
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`Row` objects can be used to create DataFrames if you need them for quick interactivity
    and exploration:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要快速互动和探索，可以使用 `Row` 对象来创建 DataFrame：
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In practice, though, you will usually want to read DataFrames from a file as
    illustrated earlier. In most cases, because your files are going to be huge, defining
    a schema and using it is a quicker and more efficient way to create DataFrames.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，通常会从文件中读取 DataFrame，就像前面所示。在大多数情况下，由于文件可能会很大，定义架构并使用它是创建 DataFrame 的更快更高效的方法。
- en: After you have created a large distributed DataFrame, you are going to want
    to perform some common data operations on it. Let’s examine some of the Spark
    operations you can perform with high-level relational operators in the Structured
    APIs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了一个大型分布式 DataFrame 后，您可能希望对其执行一些常见的数据操作。让我们来看一些您可以在结构化 API 中使用高级关系操作符执行的
    Spark 操作。
- en: Common DataFrame Operations
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见的 DataFrame 操作
- en: To perform common data operations on DataFrames, you’ll first need to load a
    DataFrame from a data source that holds your structured data. Spark provides an
    interface, [`DataFrameReader`](https://oreil.ly/v3WLZ), that enables you to read
    data into a DataFrame from myriad data sources in formats such as JSON, CSV, Parquet,
    Text, Avro, ORC, etc. Likewise, to write a DataFrame back to a data source in
    a particular format, Spark uses [`DataFrameWriter`](https://oreil.ly/vzjau).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 DataFrame 上执行常见的数据操作，首先需要从包含结构化数据的数据源中加载 DataFrame。Spark 提供了一个接口 [`DataFrameReader`](https://oreil.ly/v3WLZ)，它允许你从多种数据源（如
    JSON、CSV、Parquet、Text、Avro、ORC 等格式）中读取数据到 DataFrame 中。同样，要将 DataFrame 写回特定格式的数据源，Spark
    使用 [`DataFrameWriter`](https://oreil.ly/vzjau)。
- en: Using DataFrameReader and DataFrameWriter
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `DataFrameReader` 和 `DataFrameWriter`
- en: Reading and writing are simple in Spark because of these high-level abstractions
    and contributions from the community to connect to a wide variety of data sources,
    including common NoSQL stores, RDBMSs, streaming engines such as Apache Kafka
    and Kinesis, and more.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些高级抽象和社区的贡献，Spark 中的读写操作变得简单，可以连接到各种数据源，包括常见的 NoSQL 存储、RDBMS、流处理引擎（如 Apache
    Kafka 和 Kinesis）等。
- en: To get started, let’s read a large CSV file containing data on San Francisco
    Fire Department calls.^([1](ch03.html#ch03fn1)) As noted previously, we will define
    a schema for this file and use the `DataFrameReader` class and its methods to
    tell Spark what to do. Because this file contains 28 columns and over 4,380,660
    records,^([2](ch03.html#ch03fn2)) it’s more efficient to define a schema than
    have Spark infer it.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，让我们读取一个包含旧金山消防部门通话数据的大型CSV文件。^([1](ch03.html#ch03fn1))如前所述，我们将为此文件定义一个模式，并使用`DataFrameReader`类及其方法告诉Spark该做什么。因为这个文件包含28列和超过4,380,660条记录，^([2](ch03.html#ch03fn2))定义模式比让Spark推断模式更有效率。
- en: Note
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you don’t want to specify the schema, Spark can infer schema from a sample
    at a lesser cost. For example, you can use the `samplingRatio` option:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想指定模式，Spark可以从较低成本的样本中推断模式。例如，您可以使用`samplingRatio`选项：
- en: '[PRE18]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s take a look at how to do this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何做到这一点：
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `spark.read.csv()` function reads in the CSV file and returns a DataFrame
    of rows and named columns with the types dictated in the schema.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.read.csv()`函数读取CSV文件，并返回一个带有命名列和模式规定类型的DataFrame行。'
- en: To write the DataFrame into an external data source in your format of choice,
    you can use the `DataFrameWriter` interface. Like `DataFrameReader`, it supports
    [multiple data sources](https://oreil.ly/4rYNZ). Parquet, a popular columnar format,
    is the default format; it uses snappy compression to compress the data. If the
    DataFrame is written as Parquet, the schema is preserved as part of the Parquet
    metadata. In this case, subsequent reads back into a DataFrame do not require
    you to manually supply a schema.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 要将DataFrame写入您选择的外部数据源中，您可以使用`DataFrameWriter`接口。像`DataFrameReader`一样，它支持[多个数据源](https://oreil.ly/4rYNZ)。Parquet是一种流行的列格式，默认使用snappy压缩来压缩数据。如果DataFrame被写入Parquet，则模式将作为Parquet元数据的一部分保留。在这种情况下，后续将DataFrame读回时不需要手动提供模式。
- en: Saving a DataFrame as a Parquet file or SQL table
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将DataFrame保存为Parquet文件或SQL表
- en: 'A common data operation is to explore and transform your data, and then persist
    the DataFrame in Parquet format or save it as a SQL table. Persisting a transformed
    DataFrame is as easy as reading it. For example, to persist the DataFrame we were
    just working with as a file after reading it you would do the following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的数据操作是探索和转换数据，然后将DataFrame持久化为Parquet格式或将其保存为SQL表。持久化转换后的DataFrame与读取它一样简单。例如，要将我们刚刚使用的DataFrame持久化为文件，您可以执行以下操作：
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Alternatively, you can save it as a table, which registers metadata with the
    Hive metastore (we will cover SQL managed and unmanaged tables, metastores, and
    DataFrames in the next chapter):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以将其保存为表格，其中注册Hive元存储的元数据（我们将在下一章中介绍SQL管理和非管理表格、元存储和DataFrame）：
- en: '[PRE23]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Let’s walk through some common operations to perform on DataFrames after you
    have read the data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步介绍一些在读取数据后对DataFrame执行的常见操作。
- en: Transformations and actions
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换和操作
- en: Now that you have a distributed DataFrame composed of San Francisco Fire Department
    calls in memory, the first thing you as a developer will want to do is examine
    your data to see what the columns look like. Are they of the correct types? Do
    any of them need to be converted to different types? Do they have `null` values?
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经将旧金山消防部门通话的分布式DataFrame存储在内存中，作为开发人员，您首先要做的是检查数据以查看列的外观。它们是否是正确的类型？是否有需要转换为不同类型的列？它们是否有`null`值？
- en: In [“Transformations, Actions, and Lazy Evaluation”](ch02.html#transformationscomma_actionscomma_and_l)
    in [Chapter 2](ch02.html#downloading_apache_spark_and_getting_sta)*,* you got
    a glimpse of how transformations and actions are used to operate on DataFrames,
    and saw some common examples of each. What can we find out from our San Francisco
    Fire Department calls using these?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“转换、操作和惰性评估”](ch02.html#transformationscomma_actionscomma_and_l)中的[第2章](ch02.html#downloading_apache_spark_and_getting_sta)*,*，您可以看到如何使用转换和操作来操作DataFrame，并看到每个的一些常见示例。我们能从旧金山消防部门的通话中了解到什么？
- en: Projections and filters
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 投影和过滤器
- en: 'A *projection* in relational parlance is a way to return only the rows matching
    a certain relational condition by using filters. In Spark, projections are done
    with the `select()` method, while filters can be expressed using the `filter()`
    or `where()` method. We can use this technique to examine specific aspects of
    our SF Fire Department data set:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在关系术语中，*投影*是通过使用过滤器返回仅匹配某个关系条件的行的一种方式。在Spark中，使用`select()`方法进行投影，而过滤器可以使用`filter()`或`where()`方法来表达。我们可以使用这种技术来检查我们的SF消防局数据集的特定方面。
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'What if we want to know how many distinct `CallType`s were recorded as the
    causes of the fire calls? These simple and expressive queries do the job:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想知道作为火灾呼叫原因记录的不同`CallType`的数量，这些简单而表达力强的查询可以完成工作：
- en: '[PRE27]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can list the distinct call types in the data set using these queries:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下查询列出数据集中的不同呼叫类型：
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Renaming, adding, and dropping columns
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重命名、添加和删除列
- en: Sometimes you want to rename particular columns for reasons of style or convention,
    and at other times for readability or brevity. The original column names in the
    SF Fire Department data set had spaces in them. For example, the column name `IncidentNumber`
    was `Incident Number`. Spaces in column names can be problematic, especially when
    you want to write or save a DataFrame as a Parquet file (which prohibits this).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 有时您希望出于样式或约定原因重命名特定列，其他时候则出于可读性或简洁性。SF消防局数据集中的原始列名带有空格。例如，列名`IncidentNumber`是`Incident
    Number`。列名中的空格可能会有问题，特别是当您想要将DataFrame写入或保存为Parquet文件时（这是不允许的）。
- en: By specifying the desired column names in the schema with `StructField`, as
    we did, we effectively changed all names in the resulting DataFrame.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在架构中使用`StructField`指定所需的列名，我们有效地更改了结果DataFrame中的所有名称。
- en: 'Alternatively, you could selectively rename columns with the `withColumnRenamed()`
    method. For instance, let’s change the name of our `Delay` column to `ResponseDelayed``inMins`
    and take a look at the response times that were longer than five minutes:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用`withColumnRenamed()`方法有选择地重命名列。例如，让我们将`Delay`列的名称更改为`ResponseDelayed`并查看响应时间超过五分钟的情况：
- en: '[PRE31]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This gives us a new renamed column:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个新的重命名列：
- en: '[PRE33]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note
  id: totrans-188
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Because DataFrame transformations are immutable, when we rename a column using
    `withColumnRenamed()` we get a new DataFrame while retaining the original with
    the old column name.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 因为DataFrame转换是不可变的，当我们使用`withColumnRenamed()`重命名列时，我们会得到一个新的DataFrame，同时保留具有旧列名的原始DataFrame。
- en: Modifying the contents of a column or its type are common operations during
    data exploration. In some cases the data is raw or dirty, or its types are not
    amenable to being supplied as arguments to relational operators. For example,
    in our SF Fire Department data set, the columns `CallDate`, `WatchDate`, and `AlarmDtTm`
    are strings rather than either Unix timestamps or SQL dates, both of which Spark
    supports and can easily manipulate during transformations or actions (e.g., during
    a date- or time-based analysis of the data).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据探索期间，修改列的内容或其类型是常见操作。在某些情况下，数据是原始或脏乱的，或者其类型不适合作为关系操作符的参数。例如，在我们的SF消防局数据集中，列`CallDate`，`WatchDate`和`AlarmDtTm`是字符串，而不是Unix时间戳或SQL日期，Spark都支持并且可以在数据的转换或操作（例如，在数据的日期或时间分析期间）中轻松操作。
- en: 'So how do we convert them into a more usable format? It’s quite simple, thanks
    to some high-level API methods. `spark.sql.functions` has a set of to/from date/timestamp
    functions such as `to_timestamp()` and `to_date()` that we can use for just this
    purpose:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何将它们转换为更可用的格式呢？由于一些高级API方法的帮助，这非常简单。`spark.sql.functions`有一组日期/时间戳函数，如`to_timestamp()`和`to_date()`，我们可以专门为此目的使用它们：
- en: '[PRE34]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Those queries pack quite a punch—a number of things are happening. Let’s unpack
    what they do:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这些查询非常有效——发生了很多事情。让我们分解它们的功能：
- en: Convert the existing column’s data type from string to a Spark-supported timestamp.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将现有列的数据类型从字符串转换为Spark支持的时间戳。
- en: Use the new format specified in the format string `"MM/dd/yyyy"` or `"MM/dd/yyyy
    hh:mm:ss a"` where appropriate.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据适当的格式字符串`"MM/dd/yyyy"`或`"MM/dd/yyyy hh:mm:ss a"`使用新格式。
- en: After converting to the new data type, `drop()` the old column and append the
    new one specified in the first argument to the `withColumn()` method.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在转换为新数据类型后，使用`drop()`旧列并将新列指定为`withColumn()`方法的第一个参数。
- en: Assign the new modified DataFrame to `fire_ts_df`.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新修改的DataFrame分配给`fire_ts_df`。
- en: 'The queries result in three new columns:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这些查询结果包含三个新列：
- en: '[PRE36]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now that we have modified the dates, we can query using functions from `spark.sql.functions`
    like `month()`, `year()`, and `day()` to explore our data further. We could find
    out how many calls were logged in the last seven days, or we could see how many
    years’ worth of Fire Department calls are included in the data set with this query:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经修改了日期，我们可以使用 `spark.sql.functions` 中的函数如 `month()`、`year()` 和 `day()`
    进行查询，以进一步探索我们的数据。我们可以找出最近七天内记录了多少呼叫，或者我们可以查看数据集中包含了多少年的消防部门呼叫数据：
- en: '[PRE37]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'So far in this section, we have explored a number of common data operations:
    reading and writing DataFrames; defining a schema and using it when reading in
    a DataFrame; saving a DataFrame as a Parquet file or table; projecting and filtering
    selected columns from an existing DataFrame; and modifying, renaming, and dropping
    columns.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本节中，我们已经探讨了许多常见的数据操作：读写 DataFrames；定义模式并在读取 DataFrame 时使用它；将 DataFrame
    保存为 Parquet 文件或表；从现有的 DataFrame 投影和过滤选定的列；以及修改、重命名和删除列。
- en: One final common operation is grouping data by values in a column and aggregating
    the data in some way, like simply counting it. This pattern of grouping and counting
    is as common as projecting and filtering. Let’s have a go at it.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个常见操作是按列中的值对数据进行分组，并以某种方式聚合数据，比如简单地计数。这种按组和计数的模式和投影、过滤一样常见。让我们试一试。
- en: Aggregations
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 聚合
- en: What if we want to know what the most common types of fire calls were, or what
    zip codes accounted for the most calls? These kinds of questions are common in
    data analysis and exploration.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想知道最常见的火警呼叫类型是什么，或者哪些邮政编码占了大多数呼叫？这类问题在数据分析和探索中很常见。
- en: A handful of transformations and actions on DataFrames, such as `groupBy()`,
    `orderBy()`, and `count()`, offer the ability to aggregate by column names and
    then aggregate counts across them.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一些在 DataFrames 上的转换和操作，如 `groupBy()`、`orderBy()` 和 `count()`，提供了按列名聚合，然后在它们之间聚合计数的能力。
- en: Note
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For larger DataFrames on which you plan to conduct frequent or repeated queries,
    you could benefit from caching. We will cover DataFrame caching strategies and
    their benefits in later chapters.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计划对其进行频繁或重复查询的更大的 DataFrame，您可以从缓存中受益。我们将在后续章节中介绍 DataFrame 缓存策略及其好处。
- en: 'Let’s take our first question: what were the most common types of fire calls?'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来回答第一个问题：什么是最常见的火警呼叫类型？
- en: '[PRE39]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: From this output we can conclude that the most common call type is Medical Incident.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个输出，我们可以得出最常见的呼叫类型是医疗事件。
- en: Note
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The DataFrame API also offers the `collect()` method, but for extremely large
    DataFrames this is resource-heavy (expensive) and dangerous, as it can cause out-of-memory
    (OOM) exceptions. Unlike `count()`, which returns a single number to the driver,
    `collect()` returns a collection of all the `Row` objects in the entire DataFrame
    or Dataset. If you want to take a peek at some `Row` records you’re better off
    with `take(*n*)`, which will return only the first `*n* Row` objects of the DataFrame.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API 还提供了 `collect()` 方法，但对于非常大的 DataFrame 而言，这是资源密集型（昂贵的）且危险的，因为它可能引起内存溢出（OOM）异常。不像
    `count()` 返回给驱动程序一个单一的数字，`collect()` 返回整个 DataFrame 或 Dataset 中所有 `Row` 对象的集合。如果你想看看一些
    `Row` 记录，最好使用 `take(*n*)`，它只会返回 DataFrame 的前 `*n*` 个 `Row` 对象。
- en: Other common DataFrame operations
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他常见的 DataFrame 操作
- en: Along with all the others we’ve seen, the DataFrame API provides descriptive
    statistical methods like `min()`, `max()`, `sum()`, and `avg()`. Let’s take a
    look at some examples showing how to compute them with our SF Fire Department
    data set.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们已经看到的所有操作，DataFrame API 还提供了描述性统计方法，如 `min()`、`max()`、`sum()` 和 `avg()`。让我们看一些示例，展示如何使用我们的
    SF Fire Department 数据集计算它们。
- en: 'Here we compute the sum of alarms, the average response time, and the minimum
    and maximum response times to all fire calls in our data set, importing the PySpark
    functions in a Pythonic way so as not to conflict with the built-in Python functions:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们计算了数据集中所有火警呼叫的警报总数、平均响应时间以及最小和最大响应时间，以 Pythonic 的方式导入 PySpark 函数，以避免与内置
    Python 函数冲突：
- en: '[PRE41]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: For more advanced statistical needs common with data science workloads, read
    the API documentation for methods like `stat()`, `describe()`, `correlation()`,
    `covariance()`, `sampleBy()`, `approxQuantile()`, `frequentItems()`, and so on.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据科学工作负载中常见的更高级的统计需求，请阅读关于诸如 `stat()`、`describe()`、`correlation()`、`covariance()`、`sampleBy()`、`approxQuantile()`、`frequentItems()`
    等方法的 API 文档。
- en: As you can see, it’s easy to compose and chain expressive queries with DataFrames’
    high-level API and DSL operators. We can’t imagine the opacity and comparative
    unreadability of the code if we were to try to do the same with RDDs!
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，使用 DataFrame 的高级 API 和 DSL 运算符可以轻松组合和链式表达查询。如果我们尝试使用 RDD 进行相同操作，代码的可读性和比较的不透明性将难以想象！
- en: End-to-End DataFrame Example
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 端到端 DataFrame 示例
- en: There are many possibilities for exploratory data analysis, ETL, and common
    data operations on the San Francisco Fire Department public data set, above and
    beyond what we’ve shown here.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 旧金山消防局公共数据集上的探索性数据分析、ETL 和常见数据操作有许多可能性，超出我们在此展示的内容。
- en: 'For brevity we won’t include all the example code here, but the book’s [GitHub
    repo](https://github.com/databricks/LearningSparkV2) provides Python and Scala
    notebooks for you to try to complete an end-to-end DataFrame example using this
    data set. The notebooks explore and answer the following common questions that
    you might ask, using the DataFrame API and DSL relational operators:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们不会在这里包含所有示例代码，但书籍的 [GitHub 仓库](https://github.com/databricks/LearningSparkV2)
    提供了 Python 和 Scala 笔记本，供您尝试使用此数据集完成端到端的 DataFrame 示例。笔记本探索并回答以下您可能会问到的常见问题，使用
    DataFrame API 和 DSL 关系运算符：
- en: What were all the different types of fire calls in 2018?
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2018 年所有不同类型的火警呼叫是什么？
- en: What months within the year 2018 saw the highest number of fire calls?
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2018 年哪些月份的火警呼叫次数最高？
- en: Which neighborhood in San Francisco generated the most fire calls in 2018?
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2018 年旧金山哪个社区产生了最多的火警呼叫？
- en: Which neighborhoods had the worst response times to fire calls in 2018?
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2018 年哪些社区对火警响应时间最差？
- en: Which week in the year in 2018 had the most fire calls?
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2018 年的哪个周有最多的火警呼叫？
- en: Is there a correlation between neighborhood, zip code, and number of fire calls?
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社区、邮政编码和火警呼叫次数之间是否存在相关性？
- en: How can we use Parquet files or SQL tables to store this data and read it back?
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何使用 Parquet 文件或 SQL 表存储这些数据并读取它？
- en: So far we have extensively discussed the DataFrame API, one of the Structured
    APIs that span Spark’s MLlib and Structured Streaming components, which we cover
    later in the book.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已广泛讨论了 DataFrame API，这是涵盖 Spark MLlib 和 Structured Streaming 组件的结构化
    API 之一，我们将在本书后面进行讨论。
- en: Next, we’ll shift our focus to the Dataset API and explore how the two APIs
    provide a unified, structured interface to developers for programming Spark. We’ll
    then examine the relationship between the RDD, DataFrame, and Dataset APIs, and
    help you determine when to use which API and why.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把焦点转向 Dataset API，并探索这两个 API 如何为开发者提供统一的结构化接口来编程 Spark。然后，我们将检查 RDD、DataFrame
    和 Dataset API 之间的关系，并帮助您确定何时以及为什么使用哪个 API。
- en: The Dataset API
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dataset API
- en: 'As stated earlier in this chapter, Spark 2.0 [unified](https://oreil.ly/t3RGF)
    the DataFrame and Dataset APIs as Structured APIs with similar interfaces so that
    developers would only have to learn a single set of APIs. Datasets take on two
    characteristics: [*typed* and *untyped* APIs](https://oreil.ly/_3quT), as shown
    in [Figure 3-1](#structured_apis_in_apache_spark).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章前面所述，Spark 2.0 [统一了](https://oreil.ly/t3RGF) DataFrame 和 Dataset API 作为具有类似接口的结构化
    API，以便开发者只需学习一组 API。数据集具有两种特性：[*类型* 和 *无类型* 的 API](https://oreil.ly/_3quT)，如 [图 3-1](#structured_apis_in_apache_spark)
    所示。
- en: '![Structured APIs in Apache Spark](assets/lesp_0301.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark 中的结构化 API](assets/lesp_0301.png)'
- en: Figure 3-1\. Structured APIs in Apache Spark
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. Apache Spark 中的结构化 API
- en: 'Conceptually, you can think of a DataFrame in Scala as an alias for a collection
    of generic objects, `Dataset[Row]`, where a `Row` is a generic untyped JVM object
    that may hold different types of fields. A Dataset, by contrast, is a collection
    of strongly typed JVM objects in Scala or a class in Java. Or, as the [Dataset
    documentation](https://oreil.ly/wSkcJ) puts it, a Dataset is:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，在 Scala 中，您可以将 DataFrame 看作是一组通用对象的别名，即 `Dataset[Row]`，其中 `Row` 是一个通用的无类型
    JVM 对象，可能包含不同类型的字段。相比之下，Dataset 是 Scala 中的强类型 JVM 对象集合或 Java 中的类。或者，正如 [Dataset
    文档](https://oreil.ly/wSkcJ) 所述，Dataset 是：
- en: a strongly typed collection of domain-specific objects that can be transformed
    in parallel using functional or relational operations. Each Dataset [in Scala]
    also has an untyped view called a DataFrame, which is a Dataset of `Row`.
  id: totrans-241
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一种强类型的领域特定对象集合，可以使用函数式或关系操作并行转换。每个数据集 [在 Scala 中] 还有一个称为 DataFrame 的无类型视图，它是一个`Row`的数据集。
- en: Typed Objects, Untyped Objects, and Generic Rows
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类型化对象、无类型对象和通用行
- en: 'In Spark’s supported languages, Datasets make sense only in Java and Scala,
    whereas in Python and R only DataFrames make sense. This is because Python and
    R are not compile-time type-safe; types are dynamically inferred or assigned during
    execution, not during compile time. The reverse is true in Scala and Java: types
    are bound to variables and objects at compile time. In Scala, however, a DataFrame
    is just an alias for untyped `Dataset[Row]`. [Table 3-6](#typed_and_untyped_objects_in_spark)
    distills it in a nutshell.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 支持的语言中，数据集仅在 Java 和 Scala 中有意义，而在 Python 和 R 中只有数据框架有意义。这是因为 Python
    和 R 不是编译时类型安全的；类型在执行期间动态推断或分配，而不是在编译时。在 Scala 和 Java 中情况恰恰相反：类型在编译时绑定到变量和对象。然而，在
    Scala 中，DataFrame 只是非类型化的 `Dataset[Row]` 的别名。[表 3-6](#typed_and_untyped_objects_in_spark)
    简要总结了这一点。
- en: Table 3-6\. Typed and untyped objects in Spark
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-6\. Spark 中的类型化和非类型化对象
- en: '| Language | Typed and untyped main abstraction | Typed or untyped |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 语言 | 类型化和非类型化的主要抽象 | 类型化或非类型化 |'
- en: '| --- | --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Scala | `Dataset[T]` and DataFrame (alias for `Dataset[Row]`) | Both typed
    and untyped |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Scala | `Dataset[T]` 和 DataFrame（`Dataset[Row]` 的别名）| 既有类型又有非类型 |'
- en: '| Java | `Dataset<T>` | Typed |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Java | `Dataset<T>` | Typed |'
- en: '| Python | DataFrame | Generic `Row` untyped |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Python | DataFrame | 通用的 `Row` 非类型化 |'
- en: '| R | DataFrame | Generic `Row` untyped |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| R | DataFrame | 通用的 `Row` 非类型化 |'
- en: '`Row` is a generic object type in Spark, holding a collection of mixed types
    that can be accessed using an index. Internally, Spark manipulates `Row` objects,
    converting them to the equivalent types covered in [Table 3-2](#basic_scala_data_types_in_spark)
    and [Table 3-3](#basic_python_data_types_in_spark). For example, an `Int` as one
    of your fields in a `Row` will be mapped or converted to `IntegerType` or `IntegerType()`
    respectively for Scala or Java and Python:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`Row` 是 Spark 中的一个通用对象类型，保存一个可以使用索引访问的混合类型集合。在内部，Spark 操纵 `Row` 对象，将它们转换为 [表
    3-2](#basic_scala_data_types_in_spark) 和 [表 3-3](#basic_python_data_types_in_spark)
    中涵盖的等效类型。例如，在 `Row` 的字段中，一个 `Int` 将分别映射或转换为 Scala 或 Java 中的 `IntegerType` 或 `IntegerType()`，以及
    Python 中的相应类型：'
- en: '[PRE43]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Using an index into the `Row` object, you can access individual fields with
    its public *getter* methods:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Row` 对象的索引，您可以使用其公共 *getter* 方法访问各个字段：
- en: '[PRE45]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: By contrast, typed objects are actual Java or Scala class objects in the JVM.
    Each element in a Dataset maps to a JVM object.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，类型化对象是 JVM 中实际的 Java 或 Scala 类对象。数据集中的每个元素映射到一个 JVM 对象。
- en: Creating Datasets
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据集
- en: As with creating DataFrames from data sources, when creating a Dataset you have
    to know the schema. In other words, you need to know the data types. Although
    with JSON and CSV data it’s possible to infer the schema, for large data sets
    this is resource-intensive (expensive). When creating a Dataset in Scala, the
    easiest way to specify the schema for the resulting Dataset is to use a case class.
    In Java, JavaBean classes are used (we further discuss JavaBean and Scala case
    class in [Chapter 6](ch06.html#spark_sql_and_datasets)).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 与从数据源创建数据框架一样，创建数据集时必须了解模式。换句话说，您需要知道数据类型。虽然可以通过 JSON 和 CSV 数据推断模式，但对于大型数据集来说，这是资源密集型的（昂贵的）。在
    Scala 中创建数据集的最简单方法是使用案例类来指定结果数据集的模式。在 Java 中，使用 JavaBean 类（我们在 [第 6 章](ch06.html#spark_sql_and_datasets)
    进一步讨论 JavaBean 和 Scala 案例类）。
- en: 'Scala: Case classes'
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Scala：案例类
- en: When you wish to instantiate your own domain-specific object as a Dataset, you
    can do so by defining a case class in Scala. As an example, let’s look at a collection
    of readings from Internet of Things (IoT) devices in a JSON file (we use this
    file in the end-to-end example later in this section).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 当您希望实例化自己的领域特定对象作为数据集时，您可以在 Scala 中定义一个案例类。例如，让我们看一个来自 JSON 文件的物联网设备（IoT）读数集合的示例（我们在本节的最后端对端示例中使用此文件）。
- en: 'Our file has rows of JSON strings that look as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的文件包含一系列 JSON 字符串行，如下所示：
- en: '[PRE47]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To express each JSON entry as `DeviceIoTData`, a domain-specific object, we
    can define a Scala case class:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将每个 JSON 条目表示为 `DeviceIoTData`，一个特定于域的对象，我们可以定义一个 Scala 案例类：
- en: '[PRE48]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Once defined, we can use it to read our file and convert the returned `Dataset[Row]`
    into `Dataset[DeviceIoTData]` (output truncated to fit on the page):'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义，我们可以用它来读取文件并将返回的 `Dataset[Row]` 转换为 `Dataset[DeviceIoTData]`（输出被截断以适应页面）：
- en: '[PRE49]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Dataset Operations
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集操作
- en: 'Just as you can perform transformations and actions on DataFrames, so you can
    with Datasets. Depending on the kind of operation, the results will vary:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 就像您可以对数据框架执行转换和操作一样，您也可以对数据集执行操作。根据操作的类型，结果会有所不同：
- en: '[PRE50]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'In this query, we used a function as an argument to the Dataset method `filter()`.
    This is an overloaded method with many signatures. The version we used, `filter(func:
    (T) > Boolean): Dataset[T]`, takes a lambda function, `func: (T) > Boolean`, as
    its argument.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个查询中，我们将函数用作数据集方法 `filter()` 的参数。这是一个具有多个签名的重载方法。我们使用的版本 `filter(func: (T)
    > Boolean): Dataset[T]` 接受一个 lambda 函数 `func: (T) > Boolean` 作为其参数。'
- en: The argument to the lambda function is a JVM object of type `DeviceIoTData`.
    As such, we can access its individual data fields using the dot (`.`) notation,
    like you would in a Scala class or JavaBean.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 函数的参数是 `DeviceIoTData` 类型的 JVM 对象。因此，我们可以使用点（`.`）符号访问其各个数据字段，就像在 Scala
    类或 JavaBean 中一样。
- en: Another thing to note is that with DataFrames, you express your `filter()` conditions
    as SQL-like DSL operations, which are language-agnostic (as we saw earlier in
    the fire calls examples). With Datasets, we use language-native expressions as
    Scala or Java code.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的是，使用数据帧，您可以将 `filter()` 条件表达为类 SQL 的 DSL 操作，这些操作是与语言无关的（正如我们在消防调用示例中看到的）。而在使用数据集时，我们使用语言本地的表达式，如
    Scala 或 Java 代码。
- en: 'Here’s another example that results in another, smaller Dataset:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一个示例，生成了另一个较小的数据集：
- en: '[PRE51]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Or you can inspect only the first row of your Dataset:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以仅检查数据集的第一行：
- en: '[PRE52]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Alternatively, you could express the same query using column names and then
    cast to a `Dataset[DeviceTempByCountry]`:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用列名表达相同的查询，然后转换为 `Dataset[DeviceTempByCountry]`：
- en: '[PRE53]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Note
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Semantically, `select()` is like `map()` in the previous query, in that both
    of these queries select fields and generate equivalent results.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 从语义上讲，`select()` 类似于前一个查询中的 `map()`，因为这两个查询都选择字段并生成等效的结果。
- en: To recap, the operations we can perform on Datasets—`filter()`, `map()`, `groupBy()`,
    `select()`, `take()`, etc.—are similar to the ones on DataFrames. In a way, Datasets
    are similar to RDDs in that they provide a similar interface to its aforementioned
    methods and compile-time safety but with a much easier to read and an object-oriented
    programming interface.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们可以在数据集上执行的操作——`filter()`、`map()`、`groupBy()`、`select()`、`take()` 等——与数据帧上的操作类似。从某种意义上讲，数据集类似于RDD，因为它们提供了与上述方法相似的接口和编译时安全性，但具有更易于阅读和面向对象的编程接口。
- en: When we use Datasets, the underlying Spark SQL engine handles the creation,
    conversion, serialization, and deserialization of the JVM objects. It also takes
    care of off-Java heap memory management with the help of Dataset encoders. (We
    will talk more about Datasets and memory management in [Chapter 6](ch06.html#spark_sql_and_datasets).)
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用数据集时，底层的 Spark SQL 引擎负责处理 JVM 对象的创建、转换、序列化和反序列化。它还通过数据集编码器处理非 Java 堆内存管理。（我们将在[第6章](ch06.html#spark_sql_and_datasets)详细讨论数据集和内存管理。）
- en: End-to-End Dataset Example
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 端到端数据集示例
- en: In this end-to-end Dataset example you’ll conduct similar exploratory data analysis,
    ETL (extract, transform, and load), and data operations as in the DataFrame example,
    using the IoT data set. This data set is small and fake, but our main goal here
    is to illustrate the clarity with which you can express queries with Datasets
    and the readability of those queries, just as we did with DataFrames.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个端到端的数据集示例中，您将进行与数据帧示例相似的探索性数据分析、ETL（提取、转换和加载）和数据操作，使用 IoT 数据集。该数据集虽然小而虚构，但我们的主要目标是展示您如何使用数据集表达查询的清晰度以及这些查询的可读性，就像我们使用数据帧时一样。
- en: 'Again, for brevity, we won’t include all the example code here; however, we
    have furnished the notebook in the [GitHub repo](https://github.com/databricks/LearningSparkV2/).
    The notebook explores common operations you might conduct with this data set.
    Using the Dataset API, we attempt to do the following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 出于简洁起见，我们不会在此处列出所有示例代码；但是，我们已经在[GitHub 仓库](https://github.com/databricks/LearningSparkV2/)中提供了笔记本。笔记本探讨了您可能对此数据集进行的常见操作。使用数据集
    API，我们尝试执行以下操作：
- en: Detect failing devices with battery levels below a threshold.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检测电池电量低于阈值的故障设备。
- en: Identify offending countries with high levels of CO2 emissions.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 辨别二氧化碳排放高的有问题的国家。
- en: Compute the min and max values for temperature, battery level, CO2, and humidity.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算温度、电池电量、CO2 和湿度的最小值和最大值。
- en: Sort and group by average temperature, CO2, humidity, and country.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按平均温度、CO2、湿度和国家进行排序和分组。
- en: DataFrames Versus Datasets
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据帧与数据集
- en: 'By now you may be wondering why and when you should use DataFrames or Datasets.
    In many cases either will do, depending on the languages you are working in, but
    there are some situations where one is preferable to the other. Here are a few
    examples:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可能想知道何时应该使用 DataFrames 或 Datasets。在许多情况下，两者都可以使用，取决于您使用的编程语言，但在某些情况下，一个可能比另一个更可取。以下是一些示例：
- en: If you want to tell Spark *what to do*, not *how to do it*, use DataFrames or
    Datasets.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您希望告诉 Spark *要做什么*，而不是 *如何做*，请使用 DataFrames 或 Datasets。
- en: If you want rich semantics, high-level abstractions, and DSL operators, use
    DataFrames or Datasets.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您希望使用丰富的语义、高级抽象和 DSL 操作符，请使用 DataFrames 或 Datasets。
- en: If you want strict compile-time type safety and don’t mind creating multiple
    case classes for a specific `Dataset[T]`, use Datasets.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您希望严格的编译时类型安全，并且不介意为特定的 `Dataset[T]` 创建多个 case 类，请使用 Datasets。
- en: If your processing demands high-level expressions, filters, maps, aggregations,
    computing averages or sums, SQL queries, columnar access, or use of relational
    operators on semi-structured data, use DataFrames or Datasets.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的处理需求高级表达式、过滤器、映射、聚合、计算平均值或总和、SQL 查询、列式访问或在半结构化数据上使用关系运算符，请使用 DataFrames
    或 Datasets。
- en: If your processing dictates relational transformations similar to SQL-like queries,
    use DataFrames.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的处理要求类似于 SQL 查询的关系转换，请使用 DataFrames。
- en: If you want to take advantage of and benefit from Tungsten’s efficient serialization
    with Encoders, , [use Datasets](https://oreil.ly/13XHQ).
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您希望利用和受益于 Tungsten 的高效序列化与编码器，请使用 [Datasets](https://oreil.ly/13XHQ)。
- en: If you want unification, code optimization, and simplification of APIs across
    Spark components, use DataFrames.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您希望统一、优化代码并简化 Spark 组件间的 API 使用，请使用 DataFrames。
- en: If you are an R user, use DataFrames.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您是 R 用户，请使用 DataFrames。
- en: If you are a Python user, use DataFrames and drop down to RDDs if you need more
    control.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您是 Python 用户，请使用 DataFrames，并在需要更多控制时切换到 RDDs。
- en: If you want space and speed efficiency, use DataFrames.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您希望获得空间和速度效率，请使用 DataFrames。
- en: If you want errors caught during compilation rather than at runtime, choose
    the appropriate API as depicted in [Figure 3-2](#when_errors_are_detected_using_the_struc)**.**
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您希望在编译时捕获错误而不是在运行时，请选择适当的 API，如 [图 3-2](#when_errors_are_detected_using_the_struc)**。**
- en: '![When errors are detected using the Structured APIs](assets/lesp_0302.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![使用结构化 API 检测到错误时](assets/lesp_0302.png)'
- en: Figure 3-2\. When errors are detected using the Structured APIs
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. 使用结构化 API 检测到错误时。
- en: When to Use RDDs
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用 RDDs
- en: 'You may ask: Are RDDs being relegated to second-class citizens? Are they being
    deprecated? The answer is a resounding *no*! The RDD API will continue to be supported,
    although all future development work in Spark 2.x and Spark 3.0 will continue
    to have a DataFrame interface and semantics rather than using RDDs.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会问：RDDs 是否被降为二等公民？它们是否已经被弃用？答案是坚定的 *不*！RDD API 将继续得到支持，尽管在 Spark 2.x 和 Spark
    3.0 中所有未来的开发工作将继续采用 DataFrame 接口和语义，而不是使用 RDDs。
- en: 'There are some scenarios where you’ll want to consider using RDDs, such as
    when you:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些情况下，您需要考虑使用 RDDs，例如：
- en: Are using a third-party package that’s written using RDDs
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用第三方包编写的 RDDs
- en: Can forgo the code optimization, efficient space utilization, and performance
    benefits available with DataFrames and Datasets
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以放弃使用 DataFrames 和 Datasets 提供的代码优化、有效的空间利用和性能优势。
- en: Want to precisely instruct Spark *how to do* a query
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想要精确指导 Spark *如何进行* 查询。
- en: What’s more, you can seamlessly move between DataFrames or Datasets and RDDs
    at will using a simple API method call, `df.rdd`. (Note, however, that this does
    have a cost and should be avoided unless necessary.) After all, DataFrames and
    Datasets are built on top of RDDs, and they get decomposed to compact RDD code
    during whole-stage code generation, which we discuss in the next section.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以通过简单的 API 方法调用 `df.rdd` 在 DataFrames 或 Datasets 之间无缝移动（但请注意，这样做会有成本，并且应避免除非必要）。毕竟，DataFrames
    和 Datasets 是建立在 RDDs 之上的，并且它们在整个阶段代码生成期间被分解为紧凑的 RDD 代码，我们将在下一节讨论此问题。
- en: Finally, the preceding sections provided some intuition on how Structured APIs
    in Spark enable developers to use easy and friendly APIs to compose expressive
    queries on structured data. In other words, you tell Spark *what to do*, not *how
    to do it*, using high-level operations, and it ascertains the most efficient way
    to build a query and generates compact code for you.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，前面的章节介绍了 Spark 中的结构化 API 如何使开发人员能够使用简单友好的 API 在结构化数据上编写表达性查询。换句话说，您告诉 Spark
    *要做什么*，而不是 *如何做*，使用高级操作，它会确保构建查询的最有效方式并为您生成紧凑的代码。
- en: This process of building efficient queries and generating compact code is the
    job of the Spark SQL engine. It’s the substrate upon which the Structured APIs
    we’ve been looking at are built. Let’s peek under the hood at that engine now.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 构建高效查询并生成紧凑代码的过程是 Spark SQL 引擎的工作。这是我们所关注的结构化 API 构建的基础。现在让我们来看看引擎的内部工作原理。
- en: Spark SQL and the Underlying Engine
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL 和底层引擎
- en: 'At a programmatic level, Spark SQL allows developers to issue ANSI SQL:2003–compatible
    queries on structured data with a schema. Since its introduction in Spark 1.3,
    Spark SQL has evolved into a substantial engine upon which many high-level structured
    functionalities have been built. Apart from allowing you to issue SQL-like queries
    on your data, the Spark SQL engine:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在编程层面上，Spark SQL 允许开发人员对具有模式的结构化数据发出符合 ANSI SQL:2003 兼容的查询。自 Spark 1.3 引入以来，Spark
    SQL 已经发展成为一个重要的引擎，许多高级结构化功能都建立在其之上。除了允许您在数据上发出类似 SQL 的查询之外，Spark SQL 引擎还：
- en: Unifies Spark components and permits abstraction to DataFrames/Datasets in Java,
    Scala, Python, and R, which simplifies working with structured data sets.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Java、Scala、Python 和 R 中统一 Spark 组件，并允许对 DataFrame/Dataset 进行抽象化，从而简化了处理结构化数据集。
- en: Connects to the Apache Hive metastore and tables.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到 Apache Hive 元存储和表。
- en: Reads and writes structured data with a specific schema from structured file
    formats (JSON, CSV, Text, Avro, Parquet, ORC, etc.) and converts data into temporary
    tables.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从结构化文件格式（JSON、CSV、文本、Avro、Parquet、ORC等）中读取和写入具有特定模式的结构化数据，并将数据转换为临时表。
- en: Offers an interactive Spark SQL shell for quick data exploration.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供交互式的 Spark SQL Shell 用于快速数据探索。
- en: Provides a bridge to (and from) external tools via standard database JDBC/ODBC
    connectors.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过标准数据库 JDBC/ODBC 连接器提供与外部工具的桥接。
- en: Generates optimized query plans and compact code for the JVM, for final execution.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 JVM 生成优化的查询计划和紧凑代码，用于最终执行。
- en: '[Figure 3-3](#spark_sql_and_its_stack) shows the components that Spark SQL
    interacts with to achieve all of this.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-3](#spark_sql_and_its_stack)显示了 Spark SQL 与之交互以实现所有这些功能的组件。'
- en: '![Spark SQL and its stack](assets/lesp_0303.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![Spark SQL 及其堆栈](assets/lesp_0303.png)'
- en: Figure 3-3\. Spark SQL and its stack
  id: totrans-325
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. Spark SQL 及其堆栈
- en: At the core of the Spark SQL engine are the Catalyst optimizer and Project Tungsten.
    Together, these support the high-level DataFrame and Dataset APIs and SQL queries.
    We’ll talk more about Tungsten in [Chapter 6](ch06.html#spark_sql_and_datasets);
    for now, let’s take a closer look at the optimizer.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 引擎的核心是 Catalyst 优化器和 Project Tungsten。这两者共同支持高级的 DataFrame 和 Dataset
    API 以及 SQL 查询。我们将在[第 6 章](ch06.html#spark_sql_and_datasets)详细讨论 Tungsten；现在让我们更仔细地看看优化器。
- en: The Catalyst Optimizer
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Catalyst 优化器
- en: 'The Catalyst optimizer takes a computational query and converts it into an
    execution plan. It goes through [four transformational phases](https://oreil.ly/jMDOi),
    as shown in [Figure 3-4](#a_spark_computationapostrophes_four_phas):'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst 优化器接受计算查询并将其转换为执行计划。它经历四个转换阶段，如[图 3-4](https://oreil.ly/jMDOi)所示。
- en: Analysis
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析
- en: Logical optimization
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逻辑优化
- en: Physical planning
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 物理计划
- en: Code generation
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码生成
- en: '![A Spark computation’s four-phase journey](assets/lesp_0304.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 计算的四阶段旅程](assets/lesp_0304.png)'
- en: Figure 3-4\. A Spark computation’s four-phase journey
  id: totrans-334
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-4\. Spark 计算的四阶段旅程
- en: 'For example, consider one of the queries from our M&Ms example in [Chapter 2](ch02.html#downloading_apache_spark_and_getting_sta).
    Both of the following sample code blocks will go through the same process, eventually
    ending up with a similar query plan and identical bytecode for execution. That
    is, regardless of the language you use, your computation undergoes the same journey
    and the resulting bytecode is likely the same:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑我们在[第 2 章](ch02.html#downloading_apache_spark_and_getting_sta)的 M&Ms 示例中的一个查询。以下两个示例代码块将经历相同的过程，最终得到类似的查询计划和相同的执行字节码。换句话说，无论您使用哪种语言，您的计算都经历相同的过程，并且生成的字节码可能是相同的：
- en: '[PRE54]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'To see the different stages the Python code goes through*,* you can use the
    `count_mnm_df.explain(True)` method on the DataFrame. Or, to get a look at the
    different logical and physical plans, in Scala you can call `df.queryExecution.logical`
    or `df.queryExecution.optimizedPlan`. (In [Chapter 7](ch07.html#optimizing_and_tuning_spark_applications),
    we will discuss more about tuning and debugging Spark and how to read query plans.)
    This gives us the following output:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看Python代码经历的不同阶段*，* 您可以在DataFrame上使用`count_mnm_df.explain(True)`方法。或者，要查看不同的逻辑和物理计划，在Scala中可以调用`df.queryExecution.logical`或`df.queryExecution.optimizedPlan`。（在[第7章](ch07.html#optimizing_and_tuning_spark_applications)中，我们将讨论更多关于调整和调试Spark以及如何阅读查询计划的内容。）这给出了以下输出：
- en: '[PRE56]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let’s consider another DataFrame computation example. The following Scala code
    undergoes a similar journey as the underlying engine optimizes its logical and
    physical plans:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑另一个DataFrame计算的例子。以下Scala代码经历了类似的旅程，底层引擎优化其逻辑和物理计划：
- en: '[PRE57]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: After going through an initial analysis phase, the query plan is transformed
    and rearranged by the Catalyst optimizer as shown in [Figure 3-5](#an_example_of_a_specific_query_transform).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 经过初步分析阶段后，查询计划由Catalyst优化器转换和重排，如[图3-5](#an_example_of_a_specific_query_transform)所示。
- en: '![An example of a specific query transformation](assets/lesp_0305.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![一个特定查询转换的示例](assets/lesp_0305.png)'
- en: Figure 3-5\. An example of a specific query transformation
  id: totrans-344
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5\. 一个特定查询转换的示例
- en: Let’s go through each of the four query optimization phases..
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个讨论四个查询优化阶段..
- en: 'Phase 1: Analysis'
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第1阶段：分析
- en: The Spark SQL engine begins by generating an [abstract syntax tree (AST)](https://oreil.ly/mOIv4)
    for the SQL or DataFrame query. In this initial phase, any columns or table names
    will be resolved by consulting an internal `Catalog`, a programmatic interface
    to Spark SQL that holds a list of names of columns, data types, functions, tables,
    databases, etc. Once they’ve all been successfully resolved, the query proceeds
    to the next phase.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL引擎首先为SQL或DataFrame查询生成一个[抽象语法树（AST）](https://oreil.ly/mOIv4)。在此初始阶段，通过查询内部的`Catalog`解析任何列或表名，这是Spark
    SQL的程序接口，保存列名、数据类型、函数、表、数据库等名称的列表。一旦它们都被成功解析，查询将进入下一个阶段。
- en: 'Phase 2: Logical optimization'
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第2阶段：逻辑优化
- en: As [Figure 3-4](#a_spark_computationapostrophes_four_phas) shows, this phase
    comprises two internal stages. Applying a standard-rule based optimization approach,
    the Catalyst optimizer will first construct a set of multiple plans and then,
    using its [cost-based optimizer (CBO)](https://oreil.ly/xVVpP), assign costs to
    each plan. These plans are laid out as operator trees (like in [Figure 3-5](#an_example_of_a_specific_query_transform));
    they may include, for example, the process of constant folding, predicate pushdown,
    projection pruning, Boolean expression simplification, etc. This logical plan
    is the input into the physical plan.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[图3-4](#a_spark_computationapostrophes_four_phas)所示，此阶段包括两个内部阶段。应用标准基于规则的优化方法，Catalyst优化器首先构建一组多个计划，然后使用其[基于成本的优化器
    (CBO)](https://oreil.ly/xVVpP)为每个计划分配成本。这些计划被布置为操作树（如[图3-5](#an_example_of_a_specific_query_transform)中所示）；它们可能包括常量折叠、谓词下推、投影修剪、布尔表达式简化等过程。这个逻辑计划是物理计划的输入。
- en: 'Phase 3: Physical planning'
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第3阶段：物理规划
- en: In this phase, Spark SQL generates an optimal physical plan for the selected
    logical plan, using physical operators that match those available in the Spark
    execution engine.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，Spark SQL为选定的逻辑计划生成了一个最优的物理计划，使用与Spark执行引擎中可用的物理操作符相匹配的操作符。
- en: 'Phase 4: Code generation'
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第4阶段：代码生成
- en: The final phase of query optimization involves generating efficient Java bytecode
    to run on each machine. Because Spark SQL can operate on data sets loaded in memory,
    Spark can use state-of-the-art compiler technology for code generation to speed
    up execution. In other words, it [acts as a compiler](https://oreil.ly/_PYnW).
    Project Tungsten, which facilitates whole-stage code generation, plays a role
    here.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 查询优化的最后阶段涉及生成高效的Java字节码，以在每台机器上运行。由于Spark SQL可以操作内存中加载的数据集，Spark可以利用最先进的编译器技术来进行代码生成以加快执行速度。换句话说，它[充当编译器](https://oreil.ly/_PYnW)。项目Tungsten在此处发挥了作用，促进了整个阶段的代码生成。
- en: Just what is whole-stage code generation? It’s a physical query optimization
    phase that collapses the whole query into a single function, getting rid of virtual
    function calls and employing CPU registers for intermediate data. The second-generation
    Tungsten engine, introduced in Spark 2.0, uses this approach to generate compact
    RDD code for final execution. This streamlined strategy significantly improves
    CPU efficiency and [performance](https://oreil.ly/B3A7y).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是整阶段代码生成？它是一个物理查询优化阶段，将整个查询折叠成一个单一函数，消除了虚拟函数调用，并利用CPU寄存器处理中间数据。第二代Tungsten引擎，即Spark
    2.0引入的，使用这种方法为最终执行生成紧凑的RDD代码。这种精简的策略显著提高了CPU效率和[性能](https://oreil.ly/B3A7y)。
- en: Note
  id: totrans-355
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'We have talked at a conceptual level about the workings of the Spark SQL engine,
    with its two principal components: the Catalyst optimizer and Project Tungsten.
    The internal technical workings are beyond the scope of this book; however, for
    the curious, we encourage you to check out the references in the text for in-depth
    technical discussions.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在概念层面上讨论了Spark SQL引擎的工作原理，其主要组成部分为：Catalyst优化器和Project Tungsten。本书不涵盖内部技术工作细节；但是，对于好奇的读者，我们鼓励你查阅文中的参考资料，进行深入的技术讨论。
- en: Summary
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we took a deep dive into Spark’s Structured APIs, beginning
    with a look at the history and merits of structure in Spark.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了Spark结构化API，首先回顾了Spark中结构的历史和优点。
- en: Through illustrative common data operations and code examples, we demonstrated
    that the high-level DataFrame and Dataset APIs are far more expressive and intuitive
    than the low-level RDD API. Designed to make processing of large data sets easier,
    the Structured APIs provide domain-specific operators for common data operations,
    increasing the clarity and expressiveness of your code.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 通过说明常见的数据操作和代码示例，我们展示了高级DataFrame和Dataset API比低级RDD API更具表达力和直观性。结构化API专为简化大数据集的处理而设计，提供了领域特定的操作符，增强了代码的清晰度和表现力。
- en: We explored when to use RDDs, DataFrames, and Datasets, depending on your use
    case scenarios.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了根据使用情景选择RDD、DataFrame和Dataset的时机。
- en: And finally, we took a look under the hood to see how the Spark SQL engine’s
    main components—the Catalyst optimizer and Project Tungsten—support structured
    high-level APIs and DSL operators. As you saw, no matter which of the Spark-supported
    languages you use, a Spark query undergoes the same optimization journey, from
    logical and physical plan construction to final compact code generation.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们深入了解了Spark SQL引擎的主要组件——Catalyst优化器和Project Tungsten——是如何支持结构化的高级API和DSL操作符的。正如你所见，无论你使用Spark支持的哪种语言，Spark查询都经历相同的优化过程，从逻辑和物理计划的构建到最终紧凑的代码生成。
- en: The concepts and code examples in this chapter have laid the groundwork for
    the next two chapters, in which we will further illustrate the seamless interoperability
    between DataFrames, Datasets, and Spark SQL.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的概念和代码示例为接下来的两章奠定了基础，我们将进一步阐述DataFrame、Dataset和Spark SQL之间无缝互操作的能力。
- en: ^([1](ch03.html#ch03fn1-marker)) This public data is available at [https://oreil.ly/iDzQK](https://oreil.ly/iDzQK).
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.html#ch03fn1-marker)) 这些公共数据可以在[https://oreil.ly/iDzQK](https://oreil.ly/iDzQK)获取。
- en: ^([2](ch03.html#ch03fn2-marker)) The original data set has over 60 columns.
    We dropped a few unnecessary columns, removed records with null or invalid values,
    and added an extra Delay column.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch03.html#ch03fn2-marker)) 原始数据集有超过60列。我们删除了一些不必要的列，移除了包含空值或无效值的记录，并增加了额外的延迟列。
