- en: Chapter 29\. Other Distributed Real-Time Stream Processing Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 29 章。其他分布式实时流处理系统
- en: As we have demonstrated throughout this book, stream processing is a crucial
    technology for every data-oriented enterprise. There are many stream-processing
    stacks out there that can help us in the task of processing streaming data, both
    proprietary and in the open source domain. They differ in capabilities, APIs,
    and offer different trade-offs in the balance between latency and throughput.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书所示，流处理对于每个数据导向型企业都是至关重要的技术。在处理流数据的任务中，有许多流处理堆栈可供选择，既有专有的也有开源的。它们在功能、API
    和延迟与吞吐量之间的平衡中提供不同的权衡。
- en: Following the principle of *the right tool for the job*, they should be compared
    and contrasted against the requirements of every new project to make the right
    choice.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循“适合工作的正确工具”的原则，应当对每个新项目的要求进行比较和对比，以作出正确的选择。
- en: Furthermore, the evolving importance of the cloud beyond being an infrastructure
    provider has created a new class of offerings, where the functionality of the
    system is offered as a managed service (Software as a Service [SAAS]).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，云的演进重要性不仅仅是作为基础设施提供者，还创建了一种新的服务类别，将系统功能作为托管服务（软件即服务 [SAAS]）提供。
- en: In this chapter, we are going to briefly survey the most relevant open source
    stream processors currently maintained, such as Apache Storm, Apache Flink, Apache
    Beam, and Kafka Streams and offer an overview of the offering of the dominant
    cloud providers in the streaming arena.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将简要概述当前维护的最相关的开源流处理器，如 Apache Storm、Apache Flink、Apache Beam 和 Kafka
    Streams，并概述主要云提供商在流处理领域的提供情况。
- en: Apache Storm
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Storm
- en: '[Apache Storm](http://storm.apache.org/) is an open source project created
    originally by Nathan Marz at BackType. It was then used at Twitter and open sourced
    in 2011, and consists of a mix of Java and Closure code. It’s an open source,
    distributed, real-time computation system. It was the first “big data” streaming
    engine to be fast, scalable, and partially fault-tolerant, which, back then, meant
    it was considered the “Hadoop of Streaming.” With this background, it has also
    inspired a lot of streaming systems and engines, including Apache Spark Streaming.
    Storm is programming-language agnostic and guarantees that data will be processed
    at least once, despite edge-case limitations in fault tolerance.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[Apache Storm](http://storm.apache.org/) 是由 Nathan Marz 在 BackType 创建的开源项目。后来在
    Twitter 上使用并于 2011 年开源，由 Java 和 Closure 代码组成。它是一个开源的、分布式的、实时计算系统。它是第一个快速、可扩展且部分容错的“大数据”流引擎，当时被视为“流式处理中的
    Hadoop”。凭借这一背景，它还激发了许多流处理系统和引擎，包括 Apache Spark Streaming。Storm 是与编程语言无关的，并保证数据至少会被处理一次，尽管在容错性方面存在边缘案例的限制。'
- en: Storm is made for real-time analytics and stream processing at scale as well
    as online machine learning and continuous computation due to its very versatile
    programming model. It was the first real-time distributed streaming system that
    garnered a lot of adoption. Storm has a particular vocabulary of which the basics
    should be introduced to get an intuition of the level at which the programming
    API is placed. In particular, programming a Storm job embodies the concept of
    deploying a topology. But before we get to that topology, we need to have an idea
    of what is sent through a stream in Storm.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Storm 是专为大规模实时分析和流处理、在线机器学习和持续计算而设计的，其非常灵活的编程模型使其能够胜任各种任务。它是第一个广受欢迎的实时分布式流处理系统。Storm
    有其特定的术语，需要先介绍其基础知识，以便能够直观地理解其编程 API 的层次。特别是，编写一个 Storm 任务体现了部署拓扑的概念。但在我们深入讨论拓扑之前，需要了解
    Storm 中流的传输内容。
- en: Processing Model
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理模型
- en: 'A stream in Storm is a stream of tuples, which is its main data structure.
    A Storm tuple is a named list of values, where each value can be any type. Tuples
    are dynamically typed: the types of the fields do not need to be declared. That
    unbounded sequence of tuples represents the stream that we are interested in.
    That stream’s data flow is represented by the edges of a topology, where we are
    going to define what the vertexes consist of exactly. For those tuples to be meaningful,
    the content of what is in a stream is defined with a schema. That schema is tracked
    over a graph, which is the Storm topology and represents the computation. In that
    topology, the nodes of the graph can be of, roughly, two different types:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在Storm中，流是元组的流，这是其主要数据结构。Storm元组是一个具有命名值列表的列表，其中每个值可以是任何类型。元组是动态类型的：字段的类型无需声明。这些无界序列的元组代表了我们感兴趣的流。该流的数据流由拓扑的边表示，我们将定义顶点的确切内容。为了使这些元组具有意义，流中的内容通过模式（schema）进行定义。该模式跟踪在一个图中，这个图就是Storm拓扑，它代表计算。在该拓扑中，图的节点大致可以分为两种类型：
- en: '*Spouts*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*喷口*'
- en: Spouts are a source of a stream and are the origins of the data. This is where
    our directed graph presenting the topology always starts. An example of a spout
    could be a log file being replayed, a messaging system, a Kafka server, and so
    on. That spout creates tuples that are sent to other nodes. One of these other
    nodes could be, for example, a bolt.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 喷口是流的来源，也是数据的起源。这是我们表示拓扑的有向图始终开始的地方。喷口的一个例子可以是重播的日志文件，一个消息系统，一个Kafka服务器等。该喷口创建元组，这些元组被发送到其他节点。其中一个其他节点可以是一个螺栓。
- en: '*Bolts*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*螺栓*'
- en: A bolt processes input streams and might produce new streams as an output. It
    can do anything, meaning that it can filter, do streaming joins, or accumulate
    an intermediate result. It can also receive several streams, do aggregations,
    read from and write to a database, among others. In particular, it can be an endpoint
    to the streaming system’s computation, meaning that no other node consumes the
    output of that bolt. In which case, that final node is called a sink. Therefore,
    we can say that all sinks in the topology are bolts, but not all bolts are sinks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个螺栓处理输入流，可能产生新的输出流。它可以执行任何操作，包括过滤，流连接或累积中间结果。它还可以接收多个流，进行聚合，从数据库读取和写入等操作。特别地，它可以是流系统计算的终点，这意味着没有其他节点消耗该螺栓的输出。在这种情况下，该最终节点被称为汇点（sink）。因此，我们可以说拓扑中的所有汇点都是螺栓，但并非所有螺栓都是汇点。
- en: The Storm Topology
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Storm拓扑
- en: The topology in Storm is a network of spouts and bolts, where the last layer
    of bolts are the sinks. They are a container for the application’s logic and equivalent
    to a job in Spark, but run forever. A Storm topology, therefore could be a couple
    of log file generators, such as web servers, which could each be sent to a splitter
    bolt that preprocesses and messages the data using some basic extract, transform,
    and load (ETL) rules, selecting interesting elements, for example. Of those two
    bolts, we could get a join that counts irregular elements and joins them by a
    time stamp to get an idea of the chronology of the events of those values web
    servers. That could be sent to a final bolt that sends its side effects to a particular
    dashboard, indicating errors and events, such as alerts that could have occurred
    in a distributed web service.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在Storm中，拓扑结构是由喷口（spouts）和螺栓（bolts）组成的网络，其中最后一层的螺栓是汇点（sinks）。它们是应用逻辑的容器，类似于Spark中的作业，但会持续运行。因此，一个Storm拓扑可能包括几个日志文件生成器，如Web服务器，每个都可以发送到一个分割螺栓，该螺栓通过一些基本的ETL规则进行预处理和数据消息化，选择感兴趣的元素。在这两个螺栓中，我们可以进行连接，统计不规则元素，并通过时间戳将它们连接起来，以了解这些值Web服务器事件的时间顺序。这些可以发送到一个最终螺栓，将其副作用发送到特定的仪表板，指示可能在分布式Web服务中发生的错误和事件，如警报。
- en: The Storm Cluster
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Storm集群
- en: 'In this context, a Storm Cluster is managed by three elements: the definition
    of the topology, which is the definition of the job itself is passed to a scheduler
    called Nimbus, which deals with Supervisors that deploy the topology. A Nimbus
    daemon is the scheduler of the cluster and manages the topologies that exist on
    the cluster. It’s comparable to the idea of JobTracker in a YARN API. Besides
    this, a Supervisor daemon is a component that spawns workers. It’s comparable
    to the idea of a Hadoop TaskTracker and the workers that are spawned by Supervisors
    can receive an instance of one of the particular elements of the Storm topology.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，一个Storm集群由三个元素管理：拓扑的定义，也就是作业的定义，被传递给一个名为Nimbus的调度器，它处理部署拓扑的Supervisor。Nimbus守护进程是集群的调度器，负责管理集群上存在的拓扑。这与YARN
    API中的JobTracker概念类似。此外，Supervisor守护进程是一个生成工作者的组件。它类似于Hadoop的TaskTracker概念，并且由Supervisor生成的工作者可以接收Storm拓扑中特定元素的实例。
- en: Compared to Spark
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与Spark比较
- en: When comparing Apache Spark to Apache Storm, we can see that Spark Streaming
    has been influenced by Apache Storm and its organization, notably in the deference
    to a resource manager for the purpose of spawning a job across a pool of Spark
    streaming executors.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较Apache Spark和Apache Storm时，我们可以看到Spark Streaming受到Apache Storm及其组织的影响，尤其是在将资源管理器推迟到跨池的Spark流执行器之中的目的。
- en: Storm has the advantage that it deals with tuples *one by one* rather than in
    time-indexed *microbatches*. As they are being received, they are pushed immediately
    to bolts and new workers down the direct computation graph of a topology. On the
    other hand, to manage a topology, we need to describe the parallelism that we
    anticipate in the replication of bolts we expect to handle part of the stream.
    In this way, we have a lot more effort to perform in directly specifying the distribution
    that we want at each stage of the graph.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Storm有一个优势，它处理元组是*逐个处理*，而不是按时间索引的*微批处理*。当它们被接收时，它们立即被推送到螺栓和拓扑的直接计算图中的新工作者。另一方面，要管理一个拓扑，我们需要描述我们预期的螺栓复制中的并行性。通过这种方式，我们在直接指定我们希望在图的每个阶段中实现的分布时，会有更多的工作。
- en: The simpler and straightforward *distribution* paradigm for a Spark Streaming
    job is its advantage, where Spark Streaming (especially in its dynamic allocation
    modes) will do its best to deploy successive stages of our program in a way that
    makes sense for very high throughput. This difference in paradigm will come up
    often as the main highlight of the comparison of Apache Spark’s streaming approach
    with other systems, with the caveat that Spark is working on continuous processing.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Spark Streaming作业的更简单和直接的*分布*范例是其优势，其中Spark Streaming（特别是在其动态分配模式下）将尽力以一种对非常高吞吐量有意义的方式部署我们程序的连续阶段。这种范例上的差异经常作为与其他系统比较Apache
    Spark流处理方法的主要亮点，但需要注意的是Spark正在进行连续处理。
- en: It is also why several benchmarks ([[Chintapallu2015]](app01.xhtml#Chintapalli2015))
    have shown that although Storm usually delivers lower latencies than Spark Streaming,
    in general, the throughput of the equivalent Spark Streaming application is higher.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是为什么几个基准测试（[[Chintapallu2015]](app01.xhtml#Chintapalli2015)）表明，尽管Storm通常提供比Spark
    Streaming更低的延迟，但总体而言，等效的Spark Streaming应用的吞吐量更高。
- en: Apache Flink
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Flink
- en: '[Apache Flink](https://flink.apache.org/), originally named [StratoSphere](http://stratosphere.eu/)
    ([[Alexandrov2014]](app05.xhtml#Alexandrov2014)), is a streaming framework born
    from Technischen Universität Berlin and neighboring universities.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[Apache Flink](https://flink.apache.org/)，最初称为[StratoSphere](http://stratosphere.eu/)（[[Alexandrov2014]](app05.xhtml#Alexandrov2014)），是一种来自柏林工业大学及其附属大学的流处理框架。'
- en: 'Flink ([[Carbone2017]](app05.xhtml#Carbone2017)) is the first open source engine
    that supported out-of-order processing, given that the implementations of MillWheel
    and Cloud Dataflow are private Google offerings. It offers Java and Scala APIs
    that make it look similar to Apache Spark’s RDD/DStream functional API:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Flink（[[Carbone2017]](app05.xhtml#Carbone2017)）是第一个支持无序处理的开源引擎，考虑到MillWheel和Cloud
    Dataflow的实现是Google的私有产品。它提供了Java和Scala API，使其看起来与Apache Spark的RDD/DStream功能API类似：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Like Google Cloud Dataflow (which we mention late in the chapter), it uses the
    *dataflow programming model*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 类似Google Cloud Dataflow（在本章后面提到），它使用*数据流编程模型*。
- en: Dataflow Programming
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据流编程
- en: '[Dataflow programming](http://bit.ly/2Wrnt3q) is the conceptual name for a
    type of programming that models computation as a graph of data flowing between
    operations. It is closely related to functional programming in that it emphasizes
    the movement of data and models programs as a series of connections. Explicitly
    defined inputs and outputs are connected by operations, which are considered as
    black boxes from one another.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据流编程](http://bit.ly/2Wrnt3q) 是一种将计算建模为数据流图的编程类型的概念名称。它与函数式编程密切相关，强调数据的移动，并将程序建模为一系列连接的操作。明确定义的输入和输出通过操作连接在一起，操作之间视为彼此的黑盒子。'
- en: It was pioneered by Jack Dennis and his graduate students at MIT in the 1960s.
    Google then reused this name for its cloud programming API.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由MIT在1960年代的Jack Dennis及其研究生首创的。Google随后将此名称重用于其云编程API。
- en: A Streaming-First Framework
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流优先框架
- en: Flink is a one-at-a-time streaming framework, and it also offers snapshots to
    armor computation against failures, despite its lack of a synchronous batch boundary.
    Nowadays, this very complete framework offers a lower-level API than Structured
    Streaming, but it is a compelling alternative to Spark Streaming, if its low-latency
    focus is of interest. Flink has APIs in Scala and Java.^([1](ch29.xhtml#idm46385808198632))
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[Flink](http://bit.ly/2Wrnt3q) 是一种逐条流处理框架，它还提供快照来保护计算免受故障影响，尽管它缺乏同步批处理边界。如今，这个非常完整的框架比结构化流处理提供了更低级别的API，但如果低延迟是关注点的话，它是对Spark
    Streaming的一个引人注目的替代选择。Flink 在 Scala 和 Java 中提供API。^([1](ch29.xhtml#idm46385808198632))'
- en: Compared to Spark
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与Spark比较
- en: Apache Spark, when compared to these alternative frameworks, retains its main
    advantage of a tightly integrated, high-level API for data processing with minimal
    changes between batch and streaming.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与这些替代框架相比，Apache Spark 保持其主要优势，即紧密集成的高级数据处理API，批处理和流处理之间的变化最小。
- en: With the development of Structured Streaming, Apache Spark has caught up with
    the rich algebra of time queries (event-time, triggers, etc.) that Dataflow had
    and Spark Streaming used to lack. Yet Structured Streaming keeps a high degree
    of compatibility with the batch DataSet and Dataframe APIs well-established in
    Apache Spark.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随着结构化流处理的发展，Apache Spark 已经赶上了时间查询代数的丰富功能（事件时间，触发器等），这些功能Dataflow拥有而Spark Streaming曾经缺乏。然而，结构化流处理保持了与Apache
    Spark中已经确立的批DataSet和Dataframe API的高度兼容性。
- en: 'This seamless extension of Spark’s DataSet and Dataframe API with streaming
    functionality is the main value of Structured Streaming’s approach: it is really
    possible to compute on streaming data with a minimum of purpose-specific training,
    and with little cognitive overload.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理通过无缝扩展Spark的DataSet和Dataframe API，添加了流处理功能，这是其主要价值所在：可以在流数据上进行计算，几乎不需要专门的培训，并且认知负荷很小。
- en: One of the most interesting aspects of this integration is the running of streaming
    dataset queries in Structured Streaming through the query planner of Catalyst,
    leading to consistent optimizations of the user’s queries and making streaming
    computations less error prone than if they had to be written using a dataflow-like
    system. Note also that Flink has a system close to Apache Spark’s Tungsten that
    allows it to manage its own memory segments off-heap, taking advantage of powerful
    low-level JIT optimizations ([[Hueske2015]](app05.xhtml#Hueske2015), [[Ewen2015]](app05.xhtml#Ewen2015)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这种集成的一个最有趣的方面是，通过Catalyst的查询规划器在结构化流处理中运行流数据集查询，从而优化用户查询，并使流计算比使用类似数据流系统编写的计算
    less error prone。同时请注意，Flink 有一个类似于Apache Spark Tungsten的系统，允许它管理其自己的堆外内存段，利用强大的低级JIT优化。([[Hueske2015]](app05.xhtml#Hueske2015),
    [[Ewen2015]](app05.xhtml#Ewen2015))
- en: Finally, note that Apache Spark is also the subject of research on scheduling
    that hints at better low latency to come for a system such as Spark, reusing scheduling
    decisions across microbatches ([[Venkataraman2016]](app01.xhtml#Venkataraman2016)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，Apache Spark 也是关于调度研究的对象，这表明对于像Spark这样的系统，未来将会有更好的低延迟。它可以重复使用跨微批次的调度决策。([[Venkataraman2016]](app01.xhtml#Venkataraman2016))
- en: In sum, Apache Spark exhibits, as an ecosystem, very strong arguments for its
    continued streaming performance, particularly in a context for which exchanging
    code with batch analytics is relevant, and Apache Beam, as an interface with other
    ways of developing streaming computations, seems an interesting platform for making
    this kind of development “write once, run on any cluster.”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，作为生态系统，Apache Spark在继续流处理性能方面表现出非常强的论点，特别是在与批处理分析交换代码相关的背景下，而Apache Beam作为与其他流计算开发方式接口的平台，似乎是一个有趣的平台，用于开发“一次编写，任意集群运行”的这种开发方式。
- en: Kafka Streams
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Streams
- en: To continue this tour in other processing systems, we must mention the young
    Kafka Streams library.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要在其他处理系统中继续此导览，我们必须提到年轻的Kafka Streams库。
- en: '[Kafka Streams](https://kafka.apache.org/documentation/streams/) ([[Kreps2016]](app05.xhtml#Kreps2016)),
    introduced in 2016, is an integrated stream-processing engine within the Apache
    Kafka project.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kafka Streams](https://kafka.apache.org/documentation/streams/)（[Kreps2016](app05.xhtml#Kreps2016)），于2016年推出，是Apache
    Kafka项目内集成的流处理引擎。'
- en: Kafka Streams provides a Java and a Scala API that we use to write client applications
    enriched with stream-processing capabilities. Whereas Spark, Storm, and Flink
    are *frameworks* that take a job definition and manage its execution on a cluster,
    Kafka Streams is a library. It’s included in applications as a dependency and
    delivers an API that developers use to add streaming features to their application.
    Kafka Streams also provides backend support for a streaming SQL-like query language
    called KSQL.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams提供Java和Scala API，我们可以使用这些API编写具有流处理功能的客户端应用程序。而Spark、Storm和Flink是*框架*，它们接受作业定义并在集群上管理其执行，而Kafka
    Streams则是一个库。它作为依赖项包含在应用程序中，并提供API供开发人员使用，以增加流处理功能。Kafka Streams还为名为KSQL的流式SQL查询语言提供后端支持。
- en: Kafka Streams Programming Model
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka Streams编程模型
- en: Kafka Streams exploits the stream–table duality by offering *views* of the streams
    that are backed by stateful data stores. It puts forward the observation that
    tables are aggregated streams. This insight is rooted in the same fundamental
    concepts present in the Structured Streaming model we saw in [Part II](part02.xhtml#str-str).
    With Kafka Streams, you can benefit from one-event-at-a-time processing. Its processing
    support includes distributed joins, aggregations, and stateful processing. Windowing
    and event-time processing are also available as well as the rich distributed processing
    and fault tolerance guarantees that Kafka offers using offset-replay (reprocessing).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams通过提供由有状态数据存储支持的流视图，利用了流-表二元性。它提出了表是聚合流的观点。这一观点根植于我们在[第二部分](part02.xhtml#str-str)中看到的Structured
    Streaming模型中的同一基本概念。使用Kafka Streams，您可以从一次处理一个事件中受益。其处理支持包括分布式连接、聚合和有状态处理。窗口和事件时间处理也可用，以及Kafka提供的丰富的分布式处理和容错保证，使用偏移回放（重新处理）。
- en: Compared to Spark
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与Spark比较
- en: The key difference between Spark’s model and Kafka Streams is that Kafka Streams
    is used as a client library within the scope of client applications whereas Spark
    is a distributed framework that takes the responsibility of coordinating the work
    among workers in a cluster. In terms of modern application architectures, Kafka
    Streams apps can be considered microservices that use Kafka as a data backend.
    Their scalability model is based on replicas—running several copies of the application—and
    it’s bound to the number of partitions of the topic being consumed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Spark模型和Kafka Streams之间的关键区别在于，Kafka Streams被用作客户端应用程序范围内的客户端库，而Spark是一个分布式框架，负责在集群中的工作协调。在现代应用架构方面，Kafka
    Streams应用可以被视为使用Kafka作为数据后端的微服务。它们的可伸缩性模型基于副本——运行应用程序的多个副本——并且它绑定于正在消费的主题的分区数。
- en: The main use of Kafka Streams is to “enrich” client applications with stream
    processing features or create simple stream processing applications that use Kafka
    as their source and sink.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams的主要用途是为客户端应用程序“增加”流处理功能，或创建简单的流处理应用程序，这些应用程序使用Kafka作为它们的数据源和接收端。
- en: However, Kafka Streams has the disadvantage of not having the rich ecosystem
    focused on streaming that is developed around Apache Spark, both around Spark
    Streaming and Structured Streaming, such as machine learning capabilities with
    MLlib or external interactions with the broad data source support that is available
    in Spark. Moreover, Kafka Streams does not have the rich interplay with batch
    libraries and batch processing offered by Spark. As such, it becomes difficult
    to envision tomorrow’s complex machine learning pipelines, taking advantage of
    a vast amount of scientific computation libraries, as deployed on Kafka Streams
    alone.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Kafka Streams 的劣势在于没有围绕 Apache Spark 开发的围绕流处理的丰富生态系统，如具有 MLlib 的机器学习能力或与广泛数据源支持的外部交互。此外，Kafka
    Streams 也不具备与 Spark 提供的批处理库和批处理处理的丰富互动。因此，单纯依赖 Kafka Streams 会难以构想未来复杂的机器学习管道，无法充分利用大量科学计算库的优势。
- en: In the Cloud
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云上
- en: 'Spark’s expressive programming models and advanced analytics capabilities can
    be used on the cloud, including the offerings of the major players: Amazon, Microsoft,
    and Google. In this section, we provide a brief tour of the ways in which the
    streaming capabilities of Spark can be used on the cloud infrastructure and with
    native cloud functions, and, if relevant, how they compare with the cloud providers’
    own proprietary stream-processing system.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的表达性编程模型和先进的分析能力可以在云上使用，包括主要厂商的提供：亚马逊、微软和谷歌。在本节中，我们简要介绍了 Spark 流处理能力在云基础设施上及其与本地云功能的结合方式，以及在相关情况下与云提供商自有的专有流处理系统的比较。
- en: Amazon Kinesis on AWS
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊 Kinesis 在 AWS 上
- en: '[Amazon Kinesis](https://aws.amazon.com/kinesis) is the streaming delivery
    platform of Amazon Web Services (AWS). It comes with rich semantics for defining
    producers and consumers of streaming data, along with connectors for the pipelines
    created with those stream endpoints. We touched on Kinesis in [Chapter 19](ch19.xhtml#sps-sources),
    in which we described the connector between Kinesis and Spark Streaming.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[亚马逊 Kinesis](https://aws.amazon.com/kinesis) 是亚马逊网络服务（AWS）的流处理传输平台。它具有丰富的语义来定义流数据的生产者和消费者，以及用于与这些流端点创建的流水线连接器。我们在
    [第19章](ch19.xhtml#sps-sources) 中提到了 Kinesis，在那里我们描述了 Kinesis 与 Spark Streaming
    之间的连接器。'
- en: 'There is a connector between Kinesis and Structured Streaming, as well, which
    is available in two flavors:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis 和 Structured Streaming 之间有连接器，提供两种方式：
- en: One offered natively to users of the Databricks edition of Spark, itself available
    on the AWS and Microsoft Azure clouds
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks 版本的 Spark 原生提供给用户，可在 AWS 和微软 Azure 云上使用。
- en: The open source connector under JIRA [Spark-18165](https://issues.apache.org/jira/browse/SPARK-18165),
    which offers a way to stream data out of Kinesis easily
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 JIRA 下的开源连接器 [Spark-18165](https://issues.apache.org/jira/browse/SPARK-18165)，提供了一种轻松从
    Kinesis 流式传输数据的方式。
- en: Those connectors are necessary because Kinesis, by design, does not come with
    a comprehensive stream-processing paradigm besides a language of continuous queries
    on [AWS analytics](https://amzn.to/2QMPPyY), which cover simpler SQL-based queries.
    Therefore, the value of Kinesis is to let clients implement their own processing
    from robust sources and sinks produced with the battle-tested clients of the Kinesis
    SDK. With Kinesis, it is possible to use the monitoring and throttling tools of
    the AWS platform, getting a production-ready stream delivery “out of the box”
    on the AWS cloud. You can find more details in [[Amazon2017]](app05.xhtml#Amazon2017).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些连接器是必要的，因为按设计，Kinesis 除了在 [AWS 分析](https://amzn.to/2QMPPyY) 上覆盖较简单的基于 SQL
    的查询外，并未提供全面的流处理范式。因此，Kinesis 的价值在于让客户从经过实战验证的 Kinesis SDK 客户端生成的强大源和汇聚中实现自己的处理。使用
    Kinesis，可以利用 AWS 平台的监控和限流工具，获得在 AWS 云上即可用的生产就绪流传输。更多详细信息可以在 [[Amazon2017]](app05.xhtml#Amazon2017)
    中找到。
- en: The open source connector between Amazon Kinesis and Structured Streaming is
    a contribution from Qubole engineers that you can find at [[Georgiadis2018]](app05.xhtml#Georgiadis2018).
    This library was developed and tested against Spark 2.2 and allows Kinesis to
    be a full citizen of the Spark ecosystem, letting Spark users define analytic
    processing of arbitrary complexity.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 Kinesis 与 Structured Streaming 之间的开源连接器是 Qubole 工程师的贡献，您可以在 [[Georgiadis2018]](app05.xhtml#Georgiadis2018)
    找到。该库已在 Spark 2.2 上进行开发和测试，允许 Kinesis 成为 Spark 生态系统的完整成员，让 Spark 用户定义任意复杂的分析处理。
- en: Finally, note that although the Kinesis connector for Spark Streaming was based
    on the older receiver model, which comes with some performance issues. This Structured
    Streaming client is much more modern in its implementation, but it has not yet
    migrated to the version 2 of the data source APIs, introduced in Spark 2.3.0\.
    Kinesis is a region of the Spark ecosystem which would welcome easy contributions
    updating the quality of its implementations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，尽管Kinesis连接器用于Spark Streaming是基于旧的接收器模型，这带来了一些性能问题。而这个Structured Streaming客户端在其实现上更加现代化，但尚未迁移到Spark
    2.3.0引入的数据源API的第2版本。Kinesis是Spark生态系统中一个欢迎易于贡献更新其实现质量的区域。
- en: As a summary, Kinesis in AWS is a stream-delivery mechanism that introduces
    producing and consuming streams, and connects them to particular endpoints. However,
    it comes with limited built-in analytics capabilities, which makes it complementary
    to a streaming analytics engine, such as Spark’s streaming modules.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，AWS中的Kinesis是一种流传输机制，引入了生产和消费流，并将它们连接到特定的端点。但是，它的内置分析能力有限，这使得它与流分析引擎（如Spark的流模块）互补。
- en: Microsoft Azure Stream Analytics
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Microsoft Azure Stream Analytics
- en: Azure Streaming Analytics ([[Chiu2014]](app05.xhtml#Chiu2014)) is a cloud platform
    available on Microsoft Azure that is inspired by DryadLINQ, a Microsoft research
    project on compiling language queries using logical processing plans, adapted
    to stream processing. It exposes a high-level SQL-like language to describe a
    user query, with experimental Javascript functions accessible as well for the
    user to define some custom processing. Its goal is to express advanced time-focused
    streaming queries using that SQL-like language.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Streaming Analytics（[[Chiu2014]](app05.xhtml#Chiu2014)）是Microsoft Azure上的云平台，灵感来源于DryadLINQ，这是一个Microsoft研究项目，用于使用逻辑处理计划编译语言查询，适用于流处理。它提供了一个高级SQL-like语言来描述用户查询，同时也可通过实验性JavaScript函数让用户定义一些自定义处理。其目标是使用这种类SQL语言表达高级的面向时间的流查询。
- en: In that respect, it is similar to Structured Streaming, supporting many temporal
    processing patterns. Besides the usual analytics functions—which include aggregates,
    last and first elements, conversions, date and time functions—Azure Stream Analytics
    supports window aggregates and temporal joins.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 就这一点而言，它与Structured Streaming类似，支持多种时间处理模式。除了通常的分析函数——包括聚合、最后和第一个元素、转换、日期和时间函数——Azure
    Stream Analytics还支持窗口聚合和时间连接。
- en: Temporal joins are SQL joins that include a temporal constraint in the matched
    events. This predicate on joining can allow a user to express, for example, that
    two joined events must have timestamps that follow each other but by a limited
    time delay. This rich query language is well supported by Microsoft, which attempted
    to reimplement it on Spark Streaming 1.4 around 2016 ([[Chen2016]](app05.xhtml#Chen2016)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 时间连接是SQL连接，其中包含对匹配事件的时间约束。在连接时使用的谓词可以允许用户表达，例如，两个连接的事件必须具有按时间延迟有限的时间戳。这种丰富的查询语言得到了Microsoft的大力支持，他们尝试在2016年左右（[[Chen2016]](app05.xhtml#Chen2016)）在Spark
    Streaming 1.4上重新实现它。
- en: This work is not complete, and as such, it is not yet well integrated within
    Azure Streaming Analytics in production today. Structured Streaming has since
    caught up with those features and now offers temporal joins as a native feature
    as part of its inner join facility.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作尚未完成，因此在今天的生产环境中，它在Azure Streaming Analytics中的整合还不够完善。Structured Streaming已经追赶上了这些特性，现在作为其内部连接设施的一部分，也提供了时间连接作为本地特性。
- en: As a consequence, Azure Stream Analytics once had the upper hand in the ease
    of implementing complex time-based queries, but it is now offering fewer native
    facilities than Structured Streaming, which besides SQL-like queries offers rich
    processing capabilities in its Streaming Dataset API.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Azure Stream Analytics曾经在实现复杂的基于时间的查询方面具有优势，但现在提供的本地设施比Structured Streaming少，后者除了类SQL查询外，在其Streaming
    Dataset API中还提供了丰富的处理能力。
- en: Hence, for an advanced stream-processing project on the Microsoft Cloud, it
    seems that deploying Spark on Azure is the more robust approach. The options include
    HDInsight managed Spark, Databricks on Azure, or using Azure’s native resource
    provisioning capabilities, offering a managed Kubernetes (AKS) as well as virtual
    machines.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于在Microsoft Cloud上进行高级流处理项目而言，部署Spark在Azure上似乎是更为健壮的方法。选项包括HDInsight管理的Spark、Azure上的Databricks，或使用Azure的本地资源配置能力，提供托管的Kubernetes（AKS）以及虚拟机。
- en: Apache Beam/Google Cloud Dataflow
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Beam/Google Cloud Dataflow
- en: Modern stream-processing systems are many and include among others, Apache Flink,
    Apache Spark, Apache Apex, Apache Gearpump, Hadoop MapReduce, JStorm, IBM Streams,
    and Apache Samza. Apache Beam, an open source project led by Google, is a way
    to manage the existence of this cottage industry of streaming systems while offering
    good integration with the Google Cloud Dataflow computing engine. Let’s explain
    how this works.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现代流处理系统有许多种，包括但不限于Apache Flink，Apache Spark，Apache Apex，Apache Gearpump，Hadoop
    MapReduce，JStorm，IBM Streams和Apache Samza。Apache Beam是由谷歌领导的开源项目，旨在管理这个流处理系统行业的存在，同时提供与谷歌云数据流计算引擎的良好集成。让我们解释一下这是如何实现的。
- en: In 2013, Google had another internal cloud stream-processing system called MillWheel
    [[Akidau2013]](app05.xhtml#Akidau2013). When the time had come to give it a new
    coat of paint and link it to a mature cloud offering that would open it to the
    public, MillWheel became the Google Cloud Dataflow [[Akidau2014]](app05.xhtml#Akidau2014),
    adding several key streaming concepts in the area of fault tolerance and event
    triggers. There is more to it, which you can find in [[Akidau2017]](app05.xhtml#Akidau2017).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在2013年，谷歌拥有另一个名为MillWheel的内部云流处理系统[[Akidau2013]](app05.xhtml#Akidau2013)。当时机成熟，要给它一次翻新，并将其与成熟的云服务进行连接，以便向公众开放，MillWheel就变成了谷歌云数据流（Google
    Cloud Dataflow）[[Akidau2014]](app05.xhtml#Akidau2014)，在容错性和事件触发领域增加了几个关键的流处理概念。关于此还有更多内容，你可以在[[Akidau2017]](app05.xhtml#Akidau2017)中找到。
- en: But why offer yet another alternative to all of the others that we have listed,
    when it would possible to implement one system under one API, that runs stream
    processing under all of those computation engines?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当我们已经列出了所有这些其他替代方案之后，为什么要提供另一种选择呢？是否可能在一个API下实现一个系统，它在所有这些计算引擎下都可以运行流处理？
- en: That API turned into an open source project, Apache Beam, which aims at offering
    a single programming API for stream processing that can be plugged to any stream
    computation engine among the ones that we cited earlier, as well as Apex, Gearpump,
    MapReduce, JStorm, IBM Streams, and Samza. All of these computational engines
    are exposed as backend plug-ins (or “runners”) of Apache Beam, which aims to be
    the *lingua franca* of stream processing.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 那个API变成了一个开源项目，Apache Beam，旨在提供一个供流处理使用的单一编程API，可以连接到我们之前提到的所有流计算引擎中的任何一个，以及Apex，Gearpump，MapReduce，JStorm，IBM
    Streams和Samza。所有这些计算引擎都作为Apache Beam的后端插件（或"runners"）公开，旨在成为流处理的*通用语言*。
- en: 'For example, for computing integer indexed sums of 30-minute windows, we could
    use the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要计算30分钟窗口的整数索引总和，我们可以使用以下方式：
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we are summing integers by key, with a fixed 30-minute window, and triggering
    the `sum` output when the watermark passes the end of a window, which reflects
    “when we estimate the window is complete.”
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过关键字对整数进行求和，使用一个固定的30分钟窗口，并在水印通过窗口末尾时触发`sum`输出，这反映了"我们估计窗口已完成"的时刻。
- en: Note
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: One element of note is that contrary to Structured Streaming, triggers and outputs
    are not independent of the query itself. In Dataflow (and Beam), a window must
    also select an output mode and trigger, which conflates the semantics of the identifier
    with its runtime characteristics. In Structured Streaming, one can also have these
    for queries that don’t logically use windows, making the separation of concepts
    simpler.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，与结构化流式处理不同，触发器和输出与查询本身并不独立。在Dataflow（以及Beam中），窗口还必须选择输出模式和触发器，这使标识符的语义与其运行时特性混淆在一起。在结构化流式处理中，即使对于逻辑上不使用窗口的查询，也可以拥有这些，从而使概念的分离更加简单。
- en: Apache Beam offers its API in a Java SDK, along with Python SDK and a smaller
    but still notable Go SDK and a few SQL primitive queries. It allows one single
    language supporting a series of concepts that are universally adaptable to stream
    processing and hopefully can run on almost every stream computing engine. Besides
    the classic aggregates that we have seen in previous chapters, like the window-based
    slicing, and event-time processing, the beam API allows triggers in processing
    time including count triggers, allowed lateness, and event-time triggers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Beam提供了Java SDK中的API，以及Python SDK和一个更小但仍然显著的Go SDK，以及一些SQL原始查询。它允许一种单一的语言支持一系列在流处理中通用可适应的概念，并希望可以运行在几乎每个流计算引擎上。除了我们在之前章节中看到的经典聚合操作，例如基于窗口的分片和事件时间处理，beam
    API还允许在处理时间中包含触发器，包括计数触发器，允许延迟，以及事件时间触发器。
- en: But where Beam shines is offering portability between different streaming systems,
    centralizing one single API (sadly with no Scala SDK) for streaming programs that
    can run almost anywhere. This is exciting, but note that when executed under a
    particular computation engine, it has only the capacities that this computation
    engine and its “runner” plug-in have in implementing the totality of the API of
    Apache Beam.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 但 Beam 的亮点在于提供不同流处理系统之间的可移植性，集中一个单一的 API（遗憾的是没有 Scala SDK）用于几乎可以在任何地方运行的流处理程序。这是令人兴奋的，但请注意，当在特定计算引擎下执行时，它只具备此计算引擎及其“运行器”插件在实现
    Apache Beam API 全部功能方面的能力。
- en: In particular, the computational engine of Apache Spark is exposing the capabilities
    of Spark Streaming, not those of Structured Streaming. As of this writing, it
    does not implement stateful streams fully, or any event-time processing, because
    those capabilities are either limited in Spark Streaming alone or because the
    “runner” plug-in has not caught up to the changes in Spark Streaming itself. As
    a consequence, expressing your program with Apache Beam is often a game of being
    slightly behind the expressivity of Structured Streaming while the developers
    of the Spark runner for Apache Beam catch up.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，Apache Spark 的计算引擎暴露了 Spark Streaming 的能力，而不是 Structured Streaming 的能力。截至本文撰写时，它尚未完全实现有状态流或任何事件时间处理，因为这些功能在仅限于
    Spark Streaming 中或者“运行器”插件尚未跟上 Spark Streaming 自身变化的情况下受到限制。因此，用 Apache Beam 表达您的程序通常是一个稍微落后于
    Structured Streaming 表达能力的游戏，同时 Apache Beam 的 Spark 运行器正在追赶。
- en: Naturally, the Spark ecosystem is made greater only through collaboration with
    other related streaming projects, so we would, of course, encourage you to do
    so, to help contribute to the Spark runner for [Apache Beam](https://github.com/apache/beam),
    so that projects using the Beam API can benefit from the gains in efficiency of
    Spark’s streaming engines.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Spark 生态系统仅通过与其他相关流处理项目的协作才变得更强大，因此我们当然鼓励您这样做，以帮助为 [Apache Beam](https://github.com/apache/beam)
    的 Spark 运行器贡献力量，以便使用 Beam API 的项目能够从 Spark 流处理引擎的效率提升中受益。
- en: In summary, Apache Beam is an open source project that aims at offering a single
    SDK and API for a very expressive stream-processing model. It is an API that is
    efficiently implemented in Google’s Cloud Dataflow which allows you to run such
    a program on Google Cloud as well as a flurry of related streaming systems, under
    the caveat that they do not all have the same capabilities. You would be well
    advised to refer to the [Apache Beam capability matrix](https://beam.apache.org/documentation/runners/capability-matrix/),
    which presents an overview of the differences.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，Apache Beam 是一个旨在提供非常表现力强大的流处理模型的开源项目。它是一个在 Google Cloud Dataflow 中高效实现的
    API，允许您在 Google Cloud 上运行此类程序以及众多相关流处理系统，但需要注意它们并非都具备相同的能力。建议参考 [Apache Beam 能力矩阵](https://beam.apache.org/documentation/runners/capability-matrix/)，了解差异概览。
- en: However, note that the Google Cloud also allows running native Apache Spark,
    either on nodes or on Kubernetes, and, in practice, it might therefore not be
    necessary to switch to the Beam API if you know that you are going to run your
    program on systems that support deploying Apache Spark easily. If you need to
    support both the Google Cloud and other streaming systems as deployment systems,
    Apache Beam might make a lot of sense.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 但请注意，Google Cloud 也允许在节点或 Kubernetes 上运行本机 Apache Spark，在实践中，如果您知道将在支持轻松部署 Apache
    Spark 的系统上运行程序，则可能不需要切换到 Beam API。如果需要支持 Google Cloud 和其他流处理系统作为部署系统，Apache Beam
    可能是个不错的选择。
- en: ^([1](ch29.xhtml#idm46385808198632-marker)) To keep a fair amount of consistency
    between the Scala and Java APIs, some of the features that allow a high-level
    of expressiveness in Scala have been left out from the standard APIs for both
    batch and streaming. If you want to enjoy the full Scala experience you can choose
    to opt-in to extensions that enhance the Scala API via implicit conversions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch29.xhtml#idm46385808198632-marker)) 为了保持 Scala 和 Java API 之间的一定一致性，一些允许在
    Scala 中表达高级表现力的特性已从批处理和流处理的标准 API 中剔除。如果您希望享受完整的 Scala 经验，可以选择通过隐式转换选择扩展功能。
