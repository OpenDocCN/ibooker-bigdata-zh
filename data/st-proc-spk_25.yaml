- en: Chapter 19\. Spark Streaming Sources
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第19章\. Spark Streaming 数据源
- en: As you learned earlier in [Chapter 2](ch02.xhtml#stream-processing-model), a
    streaming source is a data provider that continuously delivers data. In Spark
    Streaming, *sources* are adaptors running within the context of the Spark Streaming
    job that implement the interaction with the external streaming source and provide
    the data to Spark Streaming using the DStream abstraction. From the programming
    perspective, consuming a streaming data source means creating a DStream using
    the appropriate implementation for the corresponding source.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在 [第2章](ch02.xhtml#stream-processing-model) 中学到的，流数据源是持续提供数据的数据提供者。在Spark
    Streaming中，*数据源*是在Spark Streaming作业上下文中运行的适配器，它们实现与外部流数据源的交互，并使用DStream抽象将数据提供给Spark
    Streaming。从编程的角度来看，消费流数据源意味着使用相应源的实现创建一个DStream。
- en: In the [“The DStream Abstraction”](ch16.xhtml#dstream_intro), we saw an example
    of how to consume data from a network socket. Let’s revisit that example in [Example 19-1](#socket-source).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [“DStream 抽象”](ch16.xhtml#dstream_intro) 中，我们看到了如何从网络套接字消费数据的示例。让我们在 [示例 19-1](#socket-source)
    中重新访问该示例。
- en: Example 19-1\. Creating a text stream from a socket connection
  id: totrans-3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 19-1\. 从套接字连接创建文本流
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In [Example 19-1](#socket-source), we can see that the creation of a streaming
    source is provided by a dedicated implementation. In this case, it is provided
    by the `ssc` instance, the *streaming context*, and results in a `DStream[String]`
    that contains the text data delivered by the socket typed with the content of
    the DStream. Although the implementation for each source is different, this pattern
    is the same for all of them: creating a source requires a `streamingContext` and
    results in a DStream that represents the contents of the stream. The streaming
    application further operates on the resulting DStream to implement the logic of
    the job at hand.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 19-1](#socket-source) 中，我们可以看到流数据源的创建由专门的实现提供。在这种情况下，由`ssc`实例，即*流上下文*提供，结果是一个包含通过套接字传递的文本数据的`DStream[String]`。尽管每种数据源的实现方式不同，但这种模式对所有数据源都是相同的：创建一个数据源需要一个`streamingContext`，并且产生一个表示流内容的DStream。流应用程序进一步在结果DStream上操作，以实现所需作业的逻辑。
- en: Types of Sources
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据源类型
- en: As a generic stream-processing framework, Spark Streaming can integrate with
    a wide variety of streaming data sources.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作为通用的流处理框架，Spark Streaming 可以与各种流数据源集成。
- en: 'When classified by their mode of operation, there are three different types
    of sources:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 根据操作模式分类，数据源有三种不同类型：
- en: Basic
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础
- en: Receiver based
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于接收器的
- en: Direct source
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接数据源
- en: Basic Sources
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础数据源
- en: Basic sources are natively provided by the `streamingContext`. They are provided
    primarily as examples or test sources and do not offer failure-recovery semantics.
    Therefore, they are not recommended to be used in production systems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基本数据源由`streamingContext`本地提供。它们主要作为示例或测试数据源提供，并且不提供故障恢复语义。因此，在生产系统中不推荐使用它们。
- en: 'Following are the basic sources:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是基本的数据源：
- en: The File source
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 文件数据源
- en: Used to monitor directories of a filesystem and read new files. Files are a
    widespread mechanism of communicating data between systems, especially in systems
    evolving from a batch-based integration model, such as data warehouses and many
    data lake implementations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 用于监视文件系统目录并读取新文件。文件是在系统之间传递数据的广泛机制，特别是在从基于批处理的集成模型（如数据仓库和许多数据湖实现）发展而来的系统中。
- en: The Queue source
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 队列数据源
- en: A producer-consumer queue local to the `streamingContext` that can be used to
    inject data into Spark Streaming. Typically used for testing.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本地于`streamingContext`的生产者-消费者队列，可用于将数据注入到Spark Streaming中。通常用于测试。
- en: We are going to talk about the `ConstantInputDStream`, which is not officially
    a source, but it performs a function similar to the *Queue* source, and it’s much
    easier to use.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论`ConstantInputDStream`，它虽然不是官方的数据源，但它执行与*Queue*数据源类似的功能，并且使用起来更加简单。
- en: Receiver-Based Sources
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于接收器的数据源
- en: As we discussed in [Chapter 18](ch18.xhtml#dstream-fund), receivers are special
    processes in Spark Streaming that are in charge of accepting data from a streaming
    source and delivering it in a reliable way to Spark in the form of RDDs. Receivers
    are responsible for implementing data delivery reliability, even if the backing
    source cannot provide such guarantees. For that purpose, they receive and replicate
    the data within the cluster before making it available to Spark for processing.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第18章](ch18.xhtml#dstream-fund)中讨论的，接收器是Spark Streaming中的特殊进程，负责从流源接收数据，并以RDD的形式可靠地将其传递给Spark。接收器负责实现数据传输的可靠性，即使支持的数据源不能提供此类保证。为此，它们在集群内接收并复制数据，然后才能使其可用于Spark进行处理。
- en: Within this class of sources, we also have the reliable receivers, which improve
    the data reception guarantees through the use of a write-ahead log (WAL).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一类源中，我们还有可靠的接收器，通过使用预写日志（WAL）改进数据接收保证。
- en: From the perspective of the programming model, each receiver is linked to a
    single DStream that represents the data delivered by its receiver to the Spark
    Streaming application.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从编程模型的角度来看，每个接收器都与一个单一的DStream相关联，该DStream表示其接收器传递给Spark Streaming应用程序的数据。
- en: To scale-up the number of receivers in a distributed context, we create several
    instances of a DStream that will consume its data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式环境中扩展接收器的数量，我们创建多个DStream实例来消费其数据。
- en: 'Given that the receiver model was the original interaction model implemented
    in Spark Streaming, all sources that were supported since the initial versions
    are available as receiver-based sources, although many of them are deprecated
    in favor of the direct sources. The most commonly used receivers are: Socket,
    Kafka, Kinesis, Flume, and Twitter.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于接收模型是最初在Spark Streaming中实现的原始交互模型，所有从最初版本起支持的源都作为基于接收器的源可用，尽管其中许多已被推荐使用直接源取代。最常用的接收器包括：Socket、Kafka、Kinesis、Flume和Twitter。
- en: The receiver API also allows for the creation of custom sources. This enabled
    the proliferation of third-party sources for Spark Streaming and also let us create
    our own custom source, to connect to legacy enterprise systems, for example.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接收器API还允许创建自定义源。这促进了Spark Streaming的第三方源的增长，也让我们可以创建自己的定制源，例如连接传统企业系统。
- en: Direct Sources
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直接源
- en: As we discussed previously in [Chapter 18](ch18.xhtml#dstream-fund), the realization
    that some sources, such as Kafka, could natively provide strong data delivery
    guarantees, rendered the data reliability of the receiver model irrelevant for
    those sources.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前在[第18章](ch18.xhtml#dstream-fund)中讨论的，有些源（如Kafka）本地提供强大的数据传输保证，这使得接收器模型对于这些源的数据可靠性变得无关紧要。
- en: The direct model, also known as the *receiverless model*, is a lightweight controller
    that keeps track of the metadata related to data consumption from the corresponding
    source and computes microbatches from it, leaving the actual data transfer and
    processing to the core Spark engine. This simplified process relies directly on
    the data delivery semantics of the streaming source backend and the reliable computing
    model of Spark.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 直接模型，也称为*无接收器模型*，是一个轻量级的控制器，负责跟踪与从相应源消费数据相关的元数据，并从中计算微批次，将实际的数据传输和处理留给核心Spark引擎。这一简化的流程直接依赖于流源后端的数据传输语义和Spark的可靠计算模型。
- en: The most popular sources implemented using the direct approach are Kafka and
    Kinesis.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用直接方法实现的最流行的数据源是Kafka和Kinesis。
- en: Commonly Used Sources
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常用的数据源
- en: Given the widespread adoption of Spark Streaming, there are many open source
    and proprietary sources available. In the rest of this chapter, we cover the most
    popular sources delivered by the Spark project.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于Spark Streaming的广泛采用，有许多开源和专有的可用源。在本章的其余部分，我们将介绍Spark项目提供的最受欢迎的源。
- en: We begin with the basic sources, the *File* and the *Queue* sources, because
    they are quite easy to use and can provide a low-threshold entrance to start developing
    some examples in Spark Streaming.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从基本源开始，*File*和*Queue*源，因为它们非常易于使用，可以提供低门槛的入门体验，用于开始在Spark Streaming中开发一些示例。
- en: 'After reviewing the built-in basic sources, we move to one example of a receiver-based
    source: the *socket* source, a source that implements a TCP client socket that
    can connect to and receive data from a TCP server on a network port.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了内置的基本源之后，我们转向一个基于接收器的源的例子：*socket*源，这是一个实现TCP客户端套接字的源，可以连接到网络端口上的TCP服务器并接收数据。
- en: Next, we discuss the Kafka source, given that Apache Kafka is probably the most
    popular open source event broker currently used to build streaming systems. Given
    the widespread use of Kafka, it has a detailed, up-to-date online coverage of
    its integration with Spark Streaming. In this discussion, we highlight the usage
    patterns as the starting point of adoption.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论Kafka源，鉴于Apache Kafka可能是当前用于构建流系统的最流行的开源事件代理。鉴于Kafka的广泛使用，它具有详细和最新的在线覆盖其与Spark
    Streaming集成的内容。在本讨论中，我们强调使用模式作为采用的起点。
- en: We close this chapter with a note about Apache Bahir, where you can find many
    more sources for Spark Streaming.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一章中提到了Apache Bahir，您可以在那里找到更多关于Spark Streaming的资源。
- en: The File Source
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件源
- en: The File source monitors a given directory in a filesystem and processes new
    files as they are discovered in the directory. The target filesystem must be Hadoop
    compatible and addressable from the distributed environment where Spark Streaming
    is running. A common storage choice is Hadoop Distributed File System (HDFS).
    Cloud block-storage systems, like Simple Storage Service (Amazon S3), are also
    supported, although they require additional testing regarding their behavior with
    respect to when new files are reported. This source is useful to bridge legacy
    systems, which usually deliver their results in batches of files.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 文件源监视文件系统中的给定目录，并在发现目录中的新文件时处理它们。目标文件系统必须与运行Spark Streaming的分布式环境兼容和可寻址。常见的存储选择是Hadoop分布式文件系统（HDFS）。云块存储系统，如Simple
    Storage Service（Amazon S3），也得到支持，尽管需要进一步测试其对报告新文件时的行为的影响。此源非常适用于桥接传统系统，这些系统通常以文件批次的形式交付其结果。
- en: The File source comes in the form of dedicated methods in the `StreamingContext`.
    The `StreamingContext` provides several versions with increasing levels of configuration
    options.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 文件源以`StreamingContext`中的专用方法形式出现。`StreamingContext`提供了几个版本，具有不断增加的配置选项。
- en: 'The most simple method is used to load a stream of text files from a filesystem
    directory path:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法用于从文件系统目录路径加载文本文件流：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And a similar method is used to load a stream of binary files, containing fixed-length
    records:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的方法用于加载包含固定长度记录的二进制文件流：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For custom data formats, the general form of the File source takes types `K`
    for the `KeyClass`, `V` for the `ValueClass`, and `F` as the `InputFormatClass`.
    All these types are defined using the Hadoop API, which provides many available
    implementations for commonly used types. The result is a DStream of key–value
    pairs that correspond to the provided type definition:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自定义数据格式，文件源的一般形式采用`K`作为`KeyClass`，`V`作为`ValueClass`，`F`作为`InputFormatClass`的类型。所有这些类型都使用Hadoop
    API定义，该API提供了许多常用类型的可用实现。结果是一个与提供的类型定义对应的key-value对的DStream：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Where the parameters are the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 参数如下所示：
- en: '`directory: String`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`directory: String`'
- en: The directory to monitor for new files.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要监视的新文件目录。
- en: '`filter: Path => Boolean`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter: Path => Boolean`'
- en: 'A predicate used to evaluate the files to be processed. An example of a filter
    predicate to select only `.log` files would be:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 用于评估要处理的文件的谓词。选择仅`.log`文件的过滤谓词示例如下：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`newFilesOnly: Boolean`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`newFilesOnly: Boolean`'
- en: A flag used to indicate whether existing files in the monitored directory should
    be considered at the beginning of the streaming process. When `newFilesOnly` is
    `true`, files present in the directory are considered using the time rules specified
    in the example that follows. When `false`, all files present in the monitored
    folder at the start of a job will be selected for processing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 用于指示在流处理开始时是否应考虑监视目录中的现有文件的标志。当`newFilesOnly`为`true`时，将根据示例中指定的时间规则考虑目录中存在的文件。当为`false`时，将选择作业开始时在监视文件夹中存在的所有文件进行处理。
- en: '`conf : Configuration`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`conf: Configuration`'
- en: 'This is a Hadoop Configuration instance. We can use it to set up particular
    behavior, like end-of-line characters or credentials for a specific storage provider.
    For example, we could manually specify an implementation provider and credentials
    to access a given Amazon S3 bucket, like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个Hadoop配置实例。我们可以使用它来设置特定行为，如行尾字符或特定存储提供程序的凭据。例如，我们可以手动指定一个实现提供程序和凭据，以访问给定的Amazon
    S3存储桶，如下所示：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How It Works
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理
- en: At each batch interval, the File source checks the listing of the monitored
    directory. All new files found in the directory are selected for processing, read
    as an RDD, and given to Spark for processing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 每个批次间隔时，文件源会检查监视目录的列表。所有在目录中找到的新文件都会被选中进行处理，作为 RDD 读取，并交给 Spark 进行处理。
- en: 'How the File source defines new files deserves particular attention:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 文件源如何定义新文件值得特别关注：
- en: At each batch interval, the directory listing is evaluated.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个批次间隔时，会评估目录列表。
- en: The age of the file is determined by its last-modified timestamp.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件的年龄由其最后修改时间戳确定。
- en: Files with a modified timestamp within the processing window interval are considered
    for processing and added to a list of processed files.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理窗口间隔内具有修改时间戳的文件被视为待处理的文件，并添加到已处理文件列表中。
- en: Processed files are remembered for the length of the processing window interval
    so that files already processed are not selected again.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已处理的文件会在处理窗口间隔的长度内被记忆，这样已经处理过的文件就不会再次被选择。
- en: Files older than the processing window interval are ignored. If the file was
    previously remembered, it is removed from the *remembered* list, becoming *forgotten*.
    We illustrate that process in [Figure 19-1](#sps-file-remember-window).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超过处理窗口间隔的文件会被忽略。如果文件之前已经被记忆，则会从*记忆*列表中移除，并变为*遗忘*。我们在[图 19-1](#sps-file-remember-window)中展示了这个过程。
- en: '![spas 1901](Images/spas_1901.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![spas 1901](Images/spas_1901.png)'
- en: Figure 19-1\. Spark Streaming File source remember window at t0
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 19-1\. Spark Streaming 文件源在 t0 时的记忆窗口
- en: 'Let’s examine more closely what’s happening in [Figure 19-1](#sps-file-remember-window):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地研究[图 19-1](#sps-file-remember-window)中发生的情况：
- en: The current batch time is indicated as *t0*.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前批次时间被标记为 *t0*。
- en: The remember window consists of *n* microbatches.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记忆窗口由 *n* 个微批次组成。
- en: Files F1 and F2 are in the *Ignored* area. They might have been processed in
    the past, but Spark Streaming has no knowledge of that.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件 F1 和 F2 处于*忽略*区域。它们可能在过去已经处理过，但是 Spark Streaming 并不知情。
- en: Files F3 and F4 have been processed already and are currently remembered.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件 F3 和 F4 已经被处理，并且目前还在记忆中。
- en: Files F5 and F6 are new. They become selected for processing and are included
    in the remembered list.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件 F5 和 F6 是新文件。它们被选中进行处理，并包含在记忆列表中。
- en: When time advances to the next batch interval, as shown in [Figure 19-2](#sps-file-remember-window-t1),
    we can observe how F3 has aged to become part of the ignored list. The new file
    F7 is selected for processing and included in the remembered list.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当时间推进到下一个批次间隔时，如[图 19-2](#sps-file-remember-window-t1)所示，我们可以观察到 F3 已经变老并成为忽略列表的一部分。新文件
    F7 被选中进行处理，并包含在记忆列表中。
- en: '![spas 1902](Images/spas_1902.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![spas 1902](Images/spas_1902.png)'
- en: Figure 19-2\. Spark Streaming File source remember window at t1
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 19-2\. Spark Streaming 文件源在 t1 时的记忆窗口
- en: This process goes on for as long as the Spark Streaming process is operational.
    We can configure the length of the remember window by using the setting `spark.streaming.minRememberDuration`,
    which defaults to 60 seconds. Note that this process assumes that the clock of
    the filesystem is synchronized with the clock of the executor running the Spark
    Streaming job.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 只要 Spark Streaming 进程运行，此过程就会持续下去。我们可以通过设置 `spark.streaming.minRememberDuration`
    来配置记忆窗口的长度，默认为 60 秒。需要注意的是，此过程假定文件系统时钟与运行 Spark Streaming 作业的执行器时钟是同步的。
- en: The remember window is calculated in terms of microbatches. Even though the
    configuration parameter `spark.streaming.minRememberDuration` is provided as a
    time duration, the actual window will be computed as the `ceiling(remember_duration/batch_interval)`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆窗口以微批次的形式计算。尽管配置参数 `spark.streaming.minRememberDuration` 提供了时间间隔，实际窗口将计算为
    `ceiling(remember_duration/batch_interval)`。
- en: For example, using the default remember duration of 60 seconds and a batch interval
    of 45 seconds, the number of remember batches will be `ceil(60/45) = 2`. This
    means that the actual duration of the remember period is 90 seconds.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用默认的记忆持续时间 60 秒和批次间隔 45 秒，记忆批次的数量将为 `ceil(60/45) = 2`。这意味着记忆期的实际持续时间为 90
    秒。
- en: Warning
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The File source does not offer any data reliability guarantees. Upon restart
    of a streaming process that uses the File source, the recovery semantics will
    be based on the clock time and the dynamics of the remember window depicted in
    [Figure 19-2](#sps-file-remember-window-t1). This means that a quick recovery
    might incur in duplicated records as files already processed become eligible again,
    while if the recovery time is long, unprocessed files might age past the point
    of the remember window and become ineligible, resulting in lost data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: File source 不提供任何数据可靠性保证。在使用 File source 的流处理进程重新启动时，恢复语义将基于时钟时间和 [Figure 19-2](#sps-file-remember-window-t1)
    中记忆窗口的动态。这意味着快速恢复可能会导致重复记录，因为已处理的文件再次符合条件；而如果恢复时间较长，未处理的文件可能会超过记忆窗口的点而变得无效，导致数据丢失。
- en: For a robust file-based stream integration we recommend the use of Structured
    Streaming and its File source.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于稳健的基于文件的流集成，我们建议使用结构化流和其 File source。
- en: The Queue Source
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Queue Source
- en: The Queue source is a programmatic source. It does not receive data from an
    external system. Instead, it provides a producer-consumer queue that allows the
    creation of a DStream as the consumer, which can be fed with data from within
    the process itself acting as the producer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Queue source 是一个程序化的数据源。它不接收来自外部系统的数据。相反，它提供一个生产者-消费者队列，允许创建作为消费者的 DStream，并可以从进程内部作为生产者提供数据。
- en: 'As a basic source, the Queue source is provided by the `streamingContext` instance:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基本数据源，Queue source 由 `streamingContext` 实例提供：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here are the parameters:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是参数：
- en: '`queue: Queue[RDD[T]]`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`queue: Queue[RDD[T]]`'
- en: A `scala.collection.mutable.Queue` of the type `RDD[T]`. This queue must be
    created beforehand. It might already be populated with data or data can be pushed
    at a later stage.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 类型为 `RDD[T]` 的 `scala.collection.mutable.Queue`。这个队列必须事先创建。它可能已经填充了数据，或者数据可以稍后推送。
- en: '`oneAtATime: Boolean`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`oneAtATime: Boolean`'
- en: A flag to indicate how the data from the queue is to be dosed. When `oneAtATime
    = true` a single RDD element from the queue will be taken at each batch interval
    for processing. When `oneAtATime = false`, all RDD elements available in the queue
    at each batch interval will be consumed at once.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一个标志，指示如何处理来自队列的数据。当 `oneAtATime = true` 时，每个批次间隔将从队列中取出单个 RDD 元素进行处理。当 `oneAtATime
    = false` 时，每个批次间隔将一次性消费队列中所有可用的 RDD 元素。
- en: '`defaultRDD: RDD[T]`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`defaultRDD: RDD[T]`'
- en: An RDD instance to be offered for processing in the case that the queue is empty.
    This option may be used to ensure that there’s always data at the consumer side
    independently of the producer. There’s an available overload with this parameter
    omitted, in which case, there is no data when the queue is empty.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 RDD 实例，用于在队列为空时提供处理。这个选项可以确保在生产者独立于消费者的情况下始终有数据。当队列为空时，有一个可用的重载，省略了这个参数，此时没有数据可用。
- en: How It Works
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理
- en: The Queue source implements a producer-consumer pattern using the `queue` as
    an intermediary. A programmatic producer adds RDDs of data to the queue. The consumer
    side implements the DStream interface and presents the data to the streaming system.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Queue source 实现了使用 `queue` 作为中介的生产者-消费者模式。程序化生产者向队列添加 RDD 数据。消费者端实现了 DStream
    接口，并将数据呈现给流处理系统。
- en: The primary use case of the Queue source is the creation of unit tests for Spark
    Streaming programs. Test data is prepared and added to the queue. The test execution
    uses the DStream attached to the Queue source and the result is asserted against
    expectations.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Queue source 的主要用例是为 Spark Streaming 程序创建单元测试。测试数据被准备并添加到队列中。测试执行使用与 Queue source
    关联的 DStream，并根据预期进行断言。
- en: Using a Queue Source for Unit Testing
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Queue Source 进行单元测试
- en: For example, let’s assume that we want to test `streamWordCount`, a streaming
    implementation of the now famous word count program that counts instances of words
    in a dataset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想测试 `streamWordCount`，这是一个流实现的著名单词计数程序，用于统计数据集中单词的实例。
- en: 'A streaming version of word count could look like this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 单词计数的流版本可能如下所示：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is a functional representation of the word count computation. Note that
    instead of starting with a given DStream implementation, we expect it as a parameter
    to the function. In this way, we separate the DStream instance from the process,
    allowing us to use the `queueDStream` as input.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是单词计数计算的功能性表示。请注意，我们期望以函数的参数形式而不是给定的 DStream 实现为起点。通过这种方式，我们将 DStream 实例与处理过程分离，允许我们将
    `queueDStream` 作为输入。
- en: 'To create the `queueDStream`, we require a `queue` and some data, where the
    data must already be in RDDs:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建`queueDStream`，我们需要一个`queue`和一些数据，其中数据必须已经是RDD：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With the data and queue in place, we can create the `queueDStream`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有了数据和队列，我们可以创建`queueDStream`：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The next step involves a method to extract the results from the streaming output.
    Because we are in the mood for `queues`, we also use one to capture the results:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤涉及一种从流输出中提取结果的方法。因为我们正在考虑使用`queues`，所以我们还使用它来捕获结果：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'At this point, we are ready to define the execution of our test:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备定义我们测试的执行：
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we can assert that we received the results that we expect:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以断言我们收到了期望的结果：
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'A Simpler Alternative to the Queue Source: The ConstantInputDStream'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 队列源的简单替代方法：`ConstantInputDStream`
- en: 'The `ConstantInputDStream` allows us to provide a single RDD value to the stream
    on each batch interval. Although not officially a *source*, the `ConstantInputDStream`
    provides functionality similar to the Queue source and it is much easier to set
    up. While the Queue source allows us to programmatically provide microbatches
    of custom data to a streaming job, the `ConstantInputDStream` lets us provide
    a single RDD that will constantly be replayed for each batch interval:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConstantInputDStream`允许我们在每个批次间隔内向流提供单个RDD值。虽然它并非官方的*源*，但`ConstantInputDStream`提供了类似于队列源的功能，并且设置起来要容易得多。虽然队列源允许我们以编程方式向流作业提供自定义数据的微批次，但`ConstantInputDStream`允许我们提供一个将在每个批次间隔内不断重放的单个RDD：'
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here are the parameters:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是参数：
- en: '`ssc: StreamingContext`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`ssc: StreamingContext`'
- en: This is the active `StreamingContext` instance.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是当前活动的`StreamingContext`实例。
- en: '`rdd: RDD[T]`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd: RDD[T]`'
- en: This is the RDD to replay at each batch interval.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是每个批次间隔内要重播的RDD。
- en: How it works
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它的工作原理
- en: The `RDD` instance provided to the `ConstantInputDStream` at creation time will
    be replayed at each batch interval. This creates a constant source of data that
    can be used for testing purposes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建时提供给`ConstantInputDStream`的`RDD`实例将在每个批次间隔内重新播放。这创建了一个可以用于测试目的的恒定数据源。
- en: ConstantInputDStream as a random data generator
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`ConstantInputDStream`作为随机数据生成器'
- en: Oftentimes, we need to generate a random dataset for tests or simulation purposes.
    As we just learned, the `ConstantInputDStream` repeats the same RDD over and over
    again. The clue of this technique is that functions are values. Instead of creating
    a data RDD, we create an RDD of random data generator functions. These functions
    are evaluated at each batch interval, creating in this way a continuous flow of
    random data into our streaming application.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 常常我们需要为测试或仿真目的生成一个随机数据集。正如我们刚学到的，`ConstantInputDStream`一次又一次地重复相同的RDD。这种技术的关键在于函数就是值。我们不是创建数据RDD，而是创建函数的RDD，这些函数在每个批次间隔内评估，从而在我们的流应用程序中连续生成随机数据流。
- en: 'For this technique, we need to first create a random data generator, which
    is a function from `Unit` into our desired type: `() => T`. In this example, we
    are going to generate a stream of sensor records. Each record is a comma-separated
    `String` that contains an `id`, a `timestamp`, and the `value`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种技术，我们需要首先创建一个随机数据生成器，它是从`Unit`到我们期望类型的函数：`() => T`。在本例中，我们将生成一个传感器记录流。每个记录都是一个逗号分隔的`String`，其中包含一个`id`，一个`timestamp`和一个`value`：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'With this `recordGeneratorFunction`, we can create an `RDD` of functions:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个`recordGeneratorFunction`，我们可以创建一个函数的`RDD`：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The rationale behind this method is that, in Scala, functions are values. RDDs
    are collections of values, and hence we can create collections of functions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的理念在于，在Scala中，函数本身就是值。RDD是值的集合，因此我们可以创建函数的集合。
- en: 'We can now create our `ConstantInputDStream` using the `sensorDataGeneratorRDD`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`sensorDataGeneratorRDD`创建我们的`ConstantInputDStream`：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Note the type signature of the `DStream[() => String]`. To make the values
    concrete, we need to evaluate that function. Given that this transformation is
    part of the DStream lineage, it will happen at each batch interval, effectively
    generating new values each time:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`DStream[() => String]`的类型签名。要使值具体化，我们需要评估该函数。考虑到这种转换是DStream血统的一部分，它将在每个批次间隔内发生，有效地每次生成新的值：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If we are using the Spark Shell, we can observe these values by using the `print`
    output operation and starting the `streamingContext`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在使用Spark Shell，我们可以通过使用`print`输出操作并启动`streamingContext`来观察这些值：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This technique is very useful to have test data at early stages of the development
    phase, without requiring the time and effort to set up an external streaming system.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术非常有用，可以在开发阶段的早期阶段获得测试数据，而无需花费时间和精力设置外部流系统。
- en: The Socket Source
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Socket 源
- en: The Socket source acts as a TCP client and is implemented as a receiver-based
    source, where the receiver process instantiates and manages the TCP client connection.
    It connects to a TCP server running on a network location, identified by its `host:port`
    combination.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Socket 源作为一个 TCP 客户端，并实现为一个基于接收器的源，接收器进程实例化和管理 TCP 客户端连接。它连接到运行在网络位置上的 TCP 服务器，由其
    `host:port` 组合标识。
- en: 'The Socket source is available as a method of the `sparkContext`. Its general
    form is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Socket 源作为 `sparkContext` 的一种方法可用。其一般形式如下：
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It has the following parameters:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有以下参数：
- en: '`hostname: String`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`hostname: String`'
- en: This is the network host of the server to which to connect.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是要连接的服务器的网络主机。
- en: '`port: Int`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`port: Int`'
- en: The network port to which to connect.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接的网络端口。
- en: '`converter: (InputStream) => Iterator[Type]`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`converter: (InputStream) => Iterator[Type]`'
- en: A function able to decode the input stream into the target type specified.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 能够将输入流解码为指定目标类型的函数。
- en: '`storageLevel: StorageLevel`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`storageLevel: StorageLevel`'
- en: The `StorageLevel` to use for the data received by this source. A recommended
    starting point is `StorageLevel.MEMORY_AND_DISK_SER_2`, which is the common default
    for other sources.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 用于接收此源接收的数据的 `StorageLevel`。一个推荐的起点是 `StorageLevel.MEMORY_AND_DISK_SER_2`，这是其他源的通用默认值。
- en: 'There is also a simplified version for a text stream encoding using the UTF-8
    charset. This alternative is the most commonly used, given its simplicity:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一个简化版本，用于使用 UTF-8 字符集进行文本流编码。鉴于其简易性，这种替代方案是最常用的：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: How It Works
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理
- en: The Socket source is implemented as a receiver-based process that takes care
    of the socket connection and related logic to receive the stream of data.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Socket 源实现为一个基于接收器的进程，负责处理套接字连接和相关逻辑以接收数据流。
- en: 'The Socket source is commonly used as a test source. Given the simplicity of
    creating a network server using command-line utilities like `netcat`, the Socket
    source has been the go-to source for many of the available basic examples of Spark
    Streaming. In this scenario, it’s common to run the client and the server in the
    same machine, leading to the use of `localhost` as the *host* specification, like
    in the following snippet:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Socket 源通常用作测试源。由于可以简单地使用诸如 `netcat` 等命令行实用程序创建网络服务器的简易性，因此 Socket 源已成为 Spark
    Streaming 可用的许多基本示例的首选源。在这种情况下，通常在同一台机器上运行客户端和服务器，因此使用 `localhost` 作为 *host* 指定的常见做法，如以下代码片段所示：
- en: '[PRE21]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: It’s worth noting that the use of `localhost` works only when Spark is running
    in local mode. If Spark runs in a cluster, the Socket source receiver process
    will be hosted in an arbitrary executor. Therefore, in a cluster setup, proper
    use of IP addresses or DNS names is required for the Socket source to connect
    to the server.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，仅当 Spark 运行在本地模式时，使用 `localhost` 才有效。如果 Spark 运行在集群中，则 Socket 源接收器进程将托管在任意执行程序上。因此，在集群设置中，必须正确使用
    IP 地址或 DNS 名称以便 Socket 源连接到服务器。
- en: Tip
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The Socket source implementation in the Spark project at [SocketInputDStream.scala](http://bit.ly/2vqPRCv)
    is a good example of how to develop a custom receiver.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 项目中 Socket 源的实现位于 [SocketInputDStream.scala](http://bit.ly/2vqPRCv)，是开发自定义接收器的良好示例。
- en: The Kafka Source
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka 源
- en: When it comes to streaming platforms, Apache Kafka is one of the most popular
    choices for a scalable messaging broker. Apache Kafka is a highly scalable distributed
    streaming platform based on the abstraction of a distributed commit log.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在流平台方面，Apache Kafka 是可伸缩消息代理的最受欢迎选择之一。Apache Kafka 是基于分布式提交日志抽象的高度可伸缩的分布式流平台。
- en: 'Kafka implements a publish/subscribe pattern: clients, called *producers* in
    Kafka terminology, publish data to the brokers. The consumers use a pull-based
    subscription, making it possible that different subscribers consume the available
    data at different rates. One subscriber might consume the data as it becomes available
    for real-time use cases, whereas another might choose to pick larger chunks of
    data over time; for example, when we want to generate a report over the last hour
    of data. This particular behavior makes Kafka an excellent match for Spark Streaming
    because it is complementary to the microbatch approach: longer microbatches naturally
    hold more data and can increase the throughput of the application, whereas a shorter
    batch interval improves the latency of the application at the expense of lower
    throughput.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka实现了发布/订阅模式：客户端（在Kafka术语中称为*生产者*）将数据发布到代理中。消费者使用基于拉取的订阅，这使得不同的订阅者可以以不同的速度消费可用的数据。一个订阅者可能会在数据实时可用时消费数据，而另一个可能会选择随时间取得较大的数据块；例如，当我们想要生成过去一小时数据的报告时。这种特定的行为使得Kafka与Spark
    Streaming非常匹配，因为它与微批处理方法互补：更长的微批处理自然会包含更多数据，并且可以增加应用程序的吞吐量，而更短的批处理间隔则会改善应用程序的延迟，但会降低吞吐量。
- en: The Kafka source is available as a separate library that needs to be imported
    in the dependencies of the project to use it.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka源作为一个单独的库可用，需要在项目的依赖项中导入它才能使用。
- en: 'For Spark 2.4, this is the dependency to use:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Spark 2.4，这是要使用的依赖关系：
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Warning
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Note that Kafka integration for Structured Streaming is a different library.
    Make sure you use the correct dependency for the API in use.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，结构化流处理的Kafka集成是一个不同的库。确保您使用正确的依赖项来使用API。
- en: 'To create a Kafka direct stream, we call `createDirectStream` in `KafkaUtils`,
    the implementation provider for this source:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建Kafka直接流，我们在`KafkaUtils`中调用`createDirectStream`，这是此源的实现提供程序：
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Following are the type parameters:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是类型参数：
- en: '`K`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`K`'
- en: The type of the message key.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 消息键的类型。
- en: '`V`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`V`'
- en: The type of the message value.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 消息值的类型。
- en: 'And here are the expected parameters:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 并且这些是预期的参数：
- en: '`ssc: StreamingContext`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`ssc: StreamingContext`'
- en: The active streaming context.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 活动的流处理上下文。
- en: '`locationStrategy: LocationStrategy`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`locationStrategy: LocationStrategy`'
- en: 'The strategy to use to schedule consumers for a given `(topic, partition)`
    on an executor. The choices are as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在执行器上为给定的`(topic, partition)`调度消费者的策略。选择如下：
- en: '`PreferBrokers`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`PreferBrokers`'
- en: Attempts to schedule the consumer on the same executor as the Kafka brokers.
    This works only in the unlikely scenario that Spark and Kafka run on the same
    physical nodes.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试将消费者调度到与Kafka brokers相同的执行器上。这仅在Spark和Kafka运行在同一物理节点上的不太可能的情况下有效。
- en: '`PreferConsistent`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`PreferConsistent`'
- en: Attempts to preserve the consumer-executor mapping for a given `(topic, partition)`.
    This is important for performance reasons because the consumers implement message
    prefetching.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试保留给定的`(topic, partition)`的消费者-执行器映射。出于性能原因，这一点很重要，因为消费者实现了消息预取。
- en: '`PreferFixed(map: java.util.Map[TopicPartition, String])`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`PreferFixed(map: java.util.Map[TopicPartition, String])`'
- en: Place a particular `(topic, partition)` combination on the given executor. The
    preferred strategy is `LocationStrategies.PreferConsistent`. The other two options
    are to be used in very specific cases. Note that the location preference is a
    hint. The actual consumer for a partition can be placed somewhere else, depending
    on the availability of resources.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 将特定的`(topic, partition)`组合放置在指定的执行器上。首选策略是`LocationStrategies.PreferConsistent`。其他两个选项仅在非常特定的情况下使用。注意，位置偏好是一个提示。实际上，分区的消费者可以根据资源的可用性放置在其他位置。
- en: '`consumerStrategy: ConsumerStrategy[K, V]`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`consumerStrategy: ConsumerStrategy[K, V]`'
- en: 'The consumer strategy determines how `(topics, partitions)` are selected for
    consumption. There are three different strategies available:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者策略确定如何选择`(topics, partitions)`用于消费。有三种不同的策略可用：
- en: '`Subscribe`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`Subscribe`'
- en: Subscribes to a collection of named topics.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 订阅一系列命名主题。
- en: '`SubscribePattern`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`SubscribePattern`'
- en: Subscribes to a collection of topics matching the provided `regex` pattern.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 订阅与提供的`regex`模式匹配的主题集合。
- en: '`Assign`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`Assign`'
- en: Provides a fixed list of `(topics, partitions)` to consume. Note that when using
    this method, arbitrary partitions of a given topic might be skipped. Use only
    when the requirements call for such strict strategy. In such a case, prefer to
    compute the `(topic, partition)` assignment instead of relying on static configuration.
    The most common `consumerStrategty` is `Subscribe`. We illustrate the use of the
    Kafka source with this strategy in the next example.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 提供要消耗的`(topics, partitions)`的固定列表。请注意，使用此方法时，可能会跳过给定主题的任意分区。只有在要求调用这种严格策略时才使用。在这种情况下，最好计算`(topic,
    partition)`分配，而不是依赖静态配置。最常见的`consumerStrategy`是`Subscribe`。我们将在下一个示例中说明使用此策略的Kafka源的用法。
- en: Using the Kafka Source
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Kafka源
- en: 'Setting up a Kafka source requires us to define a configuration and use that
    configuration in the creation of the source. This configuration is provided as
    a `map` of `configuration-name, value`. Here are the mandatory elements in the
    configuration:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 设置Kafka源需要我们定义一个配置，并在源的创建中使用该配置。此配置提供为`configuration-name, value`的`map`。以下是配置中的必填元素：
- en: '`bootstrap.servers`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`bootstrap.servers`'
- en: Provide the location of the Kafka broker as a comma-separated list of `host:port`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 将Kafka broker的位置提供为逗号分隔的`host:port`列表。
- en: '`key.deserializer`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`key.deserializer`'
- en: The class used to deserialize the binary stream into the expected key type.
    Kafka comes with the most common types already implemented.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 用于将二进制流反序列化为预期键类型的类。Kafka已经实现了最常见的类型。
- en: '`value.deserializer`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`value.deserializer`'
- en: Analogous to the `key.deserializer` but for the values.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`key.deserializer`，但用于值。
- en: '`group.id`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`group.id`'
- en: The Kafka consumer group name to use.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的Kafka消费者组名称。
- en: '`auto.offset.reset`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`auto.offset.reset`'
- en: The starting point in a partition when a new consumer group subscribes. `earliest`
    starts consuming all of the data available in the topic, whereas `latest` ignores
    all existing data and starts consuming records from the last offset at the moment
    the group joins.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当新的消费者组订阅时，分区中的起始点。`earliest`开始消费主题中所有可用的数据，而`latest`忽略所有现有数据，并从组加入时的最后偏移开始消费记录。
- en: 'There are many knobs that can be tuned on the underlying Kafka consumer. For
    a list of all configuration parameters, consult [the online documentation](http://bit.ly/2vrUH2n):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可以调整的底层Kafka消费者参数。要查看所有配置参数的列表，请参阅[在线文档](http://bit.ly/2vrUH2n)：
- en: '[PRE24]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'With the configuration in place, we can proceed to create the direct Kafka
    source:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 配置就绪后，我们可以继续创建直接的Kafka源：
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In this example, we specify initial offsets for our `topic` using the `offsets`
    parameters for the purpose of illustrating the usage of this option.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们使用`offsets`参数为我们的`topic`指定了初始偏移量，以说明此选项的用法。
- en: How It Works
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作
- en: The Kafka direct stream functions based on offsets, which are indexes of positions
    of elements in the stream.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 基于偏移量的Kafka直接流功能，这些偏移量是流中元素位置的索引。
- en: The gist of data delivery is that the Spark driver queries offsets and decides
    offset ranges for every batch interval from Apache Kafka. After it receives those
    offsets, the driver dispatches them by launching a task for each partition, resulting
    in a 1:1 parallelism between the Kafka partitions and the Spark partitions at
    work. Each task retrieves data using its specific offset ranges. The driver does
    not send data to the executors; instead, it simply sends a few offsets they use
    to directly consume data. As a consequence, the parallelism of data ingestion
    from Apache Kafka is much better than the legacy receiver model, where each stream
    was consumed by a single machine.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 数据传递的要点在于，Spark驱动程序从Apache Kafka查询偏移量，并为每个批处理间隔决定偏移范围。收到这些偏移量后，驱动程序通过为每个分区启动任务来分发它们，从而实现Kafka分区和工作中的Spark分区之间的1:1并行性。每个任务使用其特定的偏移范围检索数据。驱动程序不会将数据发送给执行者；相反，它只发送他们用于直接消费数据的几个偏移量。因此，从Apache
    Kafka摄取数据的并行性要比传统的接收器模型好得多，后者每个流都由单台机器消耗。
- en: This is also more efficient for fault tolerance because the executors in that
    `DirectStream` acknowledge the reception of data for their particular offsets
    by committing the offsets. In case of a failure, the new executor picks up the
    partition data from the latest known committed offsets. This behavior guarantees
    an at-least-once data delivery semantic because output operators can still be
    offered a replay of data that they have already seen. To achieve effectively exactly-once
    semantics, we require the output operations to be idempotent. That is, executing
    the operation more than once has the same result as executing the operation once.
    For example, writing a record on a database using a unique primary key that ensures
    that if the record is inserted, we will find only one instance of it.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于容错性也更有效，因为那些 `DirectStream` 中的执行器通过提交偏移量来确认其特定偏移量的数据接收情况。在发生故障时，新的执行器从已知的最新提交偏移量中获取分区数据。这种行为保证了至少一次的数据传递语义，因为输出操作符仍然可以提供已经看到的数据的重播。为了有效地实现恰好一次的语义，我们要求输出操作是幂等的。也就是说，多次执行操作与执行一次操作的结果相同。例如，使用确保记录唯一主键的数据库写入记录，这样如果记录被插入，我们将只找到一个实例。
- en: Where to Find More Sources
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何找到更多资源
- en: A few sources that started their life as part of the Spark codebase, as well
    as some additional contributions, moved under [Apache Bahir](https://bahir.apache.org),
    a project that serves as an umbrella repository for a number of Apache Spark and
    Apache Flink extensions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 一些源自 Spark 代码库的起源，以及一些额外的贡献，都转移到了 [Apache Bahir](https://bahir.apache.org)，这是一个为多个
    Apache Spark 和 Apache Flink 扩展提供支持的项目的总库。
- en: 'Among these extensions, we find a series of Spark Streaming connectors for
    the following:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些扩展中，我们找到一系列 Spark Streaming 连接器，包括以下内容：
- en: Apache CouchDB/Cloudant
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Apache CouchDB/Cloudant
- en: A NoSQL database
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 NoSQL 数据库
- en: Akka
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Akka
- en: 'An actor system Google Cloud Pub/Sub: A cloud-based pub/sub system proprietary
    to Google'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Pub/Sub 一个基于云的专有 pub/sub 系统
- en: MQTT
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: MQTT
- en: A lightweight machine-to-machine/Internet of Things (IoT) pub/sub protocol
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一个轻量级的机器对机器/物联网（IoT）发布/订阅协议
- en: Twitter
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter
- en: A source to subscribe to tweets from this popular social network
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一个订阅来自这个流行社交网络推文的来源
- en: ZeroMQ
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ZeroMQ
- en: An asynchronous messaging library
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一个异步消息传递库
- en: To use a connector from the Apache Bahir library, add the corresponding dependency
    to the project build definition and use the dedicated method provided by the library
    to create a stream.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Apache Bahir 库中的连接器，将相应的依赖项添加到项目构建定义中，并使用库提供的专用方法创建流。
