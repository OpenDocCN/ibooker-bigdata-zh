- en: Chapter 20\. Spark Streaming Sinks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 20 章. Spark Streaming 下沉
- en: After acquiring data through a source represented as a DStream and applying
    a series of transformations using the DStream API to implement our business logic,
    we would want to inspect, save, or produce that result to an external system.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以 DStream 表示的源获取数据，并使用 DStream API 应用一系列转换来实现业务逻辑后，我们希望检查、保存或将结果生成到外部系统。
- en: As we recall from [Chapter 2](ch02.xhtml#stream-processing-model), in our general
    streaming model, we call the component in charge of externalizing the data from
    the streaming process a sink. In Spark Streaming, sinks are implemented using
    the so-called *output operations*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [第 2 章](ch02.xhtml#stream-processing-model) 中回顾的那样，在我们的通用流处理模型中，负责将数据从流处理过程外部化的组件被称为
    sink。在 Spark Streaming 中，使用所谓的 *输出操作* 实现 sink。
- en: In this chapter, we are going to explore the capabilities and modalities of
    Spark Streaming to produce data to external systems through these output operations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨 Spark Streaming 通过这些输出操作将数据生成到外部系统的能力和方式。
- en: Output Operations
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出操作
- en: Output operations play a crucial role in every Spark Streaming application.
    They are required to trigger the computations over the DStream and, at the same
    time, they provide access to the resulting data through a programmable interface.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 输出操作在每个 Spark Streaming 应用程序中扮演了关键角色。它们需要触发对 DStream 的计算，并通过可编程接口提供对结果数据的访问。
- en: In [Figure 20-1](#sps_example_job) we illustrate a generic Spark Streaming job
    that takes two streams as input, transforms one of them, and then joins them together
    before writing the result to a database. At execution time, the chain of DStream
    transformations that end on that *output operation* becomes a Spark job.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 20-1](#sps_example_job) 中，我们展示了一个通用的 Spark Streaming 作业，它将两个流作为输入，转换其中一个，然后在将结果写入数据库之前将它们连接在一起。在执行时，以
    *输出操作* 结束的 DStream 转换链成为一个 Spark 作业。
- en: '![spas 2001](Images/spas_2001.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2001](Images/spas_2001.png)'
- en: Figure 20-1\. A Spark Streaming job
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 20-1. 一个 Spark Streaming 作业
- en: This job is attached to the Spark Streaming scheduler. In turn, the scheduler
    triggers the execution of the defined job at each batch interval, shown in [Figure 20-2](#sps_scheduler_job).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 该作业连接到 Spark Streaming 调度程序。反过来，调度程序在每个批处理间隔触发定义的作业的执行，如 [图 20-2](#sps_scheduler_job)
    所示。
- en: '![spas 2002](Images/spas_2002.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2002](Images/spas_2002.png)'
- en: Figure 20-2\. Spark Streaming scheduler
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 20-2. Spark Streaming 调度程序
- en: 'Output operations provide the link between the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 输出操作为以下内容提供了链接：
- en: The sequence of lazy transformations on DStreams
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DStreams 上惰性转换的序列
- en: The Spark Streaming scheduler
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming 调度程序
- en: The external systems we produce data to
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将数据生成到的外部系统
- en: 'From the execution model perspective, every output operation declared in the
    streaming program is attached to the Spark Streaming scheduler in the same order
    in which they were declared in the program. This ordering guarantees a sequencing
    semantic in which later output operations will be triggered after the previous
    operation has finished execution. For example, in the following code snippet,
    we are storing media assets in a database before they are added to the searchable
    index. The guaranteed order of execution of output operations in Spark Streaming
    will ensure that the new media is added to the main database before it becomes
    searchable through the search index:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从执行模型的角度来看，流程序中声明的每个输出操作都按照程序中声明的顺序连接到 Spark Streaming 调度程序。这种顺序保证了后续的输出操作会在前一个操作执行完成后触发。例如，在以下代码片段中，我们在将媒体资产存储到数据库之前将其添加到可搜索索引中。Spark
    Streaming 中输出操作的执行顺序保证了新媒体在通过搜索索引变得可搜索之前被添加到主数据库中：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Every Spark Streaming job must have, at least, one output operation. This is
    a logical requirement; otherwise, there’s no way to materialize transformations
    or obtain results. This requirement is enforced at runtime, at the moment that
    the `sparkStreamingContext.start()` operation is called. A streaming job that
    does not provide at least one output operation will fail to start, and throw the
    following error:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Spark Streaming 作业至少必须有一个输出操作。这是一个逻辑要求；否则，就无法实现转换或获取结果。此要求在运行时通过调用 `sparkStreamingContext.start()`
    操作来执行。如果流作业未提供至少一个输出操作，将无法启动，并抛出以下错误：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Output operations, as the name suggests, provide access to the data that results
    from the computations declared on the DStream. We use them to observe the resulting
    data, save it to disk, or feed it to other systems, like storing it in a database
    for later querying or directly sending it to an online monitoring system for observation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 输出操作，顾名思义，提供对声明在 DStream 上的计算结果数据的访问。我们使用它们来观察结果数据，将其保存到磁盘上，或将其提供给其他系统，例如将其存储在数据库中以供后续查询，或直接发送到在线监控系统进行观察。
- en: In general terms, the function of output operations is to schedule the provided
    action at the intervals dictated by the batch interval. For output operations
    that take a closure, the code in the closure executes once at each batch interval
    on the Spark driver, not distributed in the cluster. All output operations return
    `Unit`; that is, they execute only side-effecting actions and are not composable.
    A streaming job can have as many output operations as necessary. They are the
    endpoints for the transformation DAG of DStreams.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，输出操作的功能是按照批处理间隔指定的时间间隔调度提供的操作。对于采用闭包的输出操作，闭包中的代码在每个批处理间隔都会在 Spark 驱动程序上执行，而不是在集群中分布执行。所有的输出操作都返回`Unit`；即，它们只执行具有副作用的操作，并且不可组合。流处理作业可以具有尽可能多的输出操作。它们是
    DStreams 转换 DAG 的端点。
- en: Built-In Output Operations
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内置输出操作
- en: There are a few output operations provided by the Spark Streaming core library.
    We take a look at them in the following subsections.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 核心库提供了一些输出操作。我们将在以下小节中看到它们。
- en: print
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: print
- en: '`print()` outputs the first elements of a DStream to the standard output. When
    used without arguments, it will print the first 10 elements of the DStream at
    every streaming interval, including the timestamp when the operation runs. It’s
    also possible to call `print(num: Int)` with an arbitrary number, to obtain that
    given maximum of elements at each streaming interval.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`print()`将 DStream 的前几个元素输出到标准输出。当没有参数使用时，它将在每个流间隔中打印 DStream 的前 10 个元素，包括操作运行时的时间戳。还可以调用带有任意数量的`print(num:
    Int)`，以获得每个流间隔中给定的最大元素数。'
- en: Given that the results are written only to the standard output, the practical
    use of `print` is limited to exploration and debugging of a streaming computation,
    where we can see on the console a continuous log of the first elements of a DStream.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于结果仅写入标准输出，`print`的实际用途仅限于探索和调试流计算，其中我们可以在控制台上看到 DStream 的前几个元素的连续日志。
- en: 'For example, calling `print()` in the network stream of names that we used
    in [Chapter 2](ch02.xhtml#stream-processing-model), we see the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们在[第二章](ch02.xhtml#stream-processing-model)中使用的名字网络流中调用`print()`，我们看到以下内容：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: saveAsxyz
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: saveAsxyz
- en: 'The `saveAsxyz` family of output operations provides a file-based sink for
    the stream output. Here are the available options:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`saveAsxyz`系列的输出操作为流输出提供了基于文件的接收器。以下是可用的选项：'
- en: '`saveAsTextFiles(prefix, suffix)`'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`saveAsTextFiles(prefix, suffix)`'
- en: Stores the content of the DStream as files in the filesystem. The prefix and
    optional suffix are used to locate and name the file in the target filesystem.
    One file is generated at each streaming interval. The name of each generated file
    will be `prefix-<timestamp_in_milliseconds>[.suffix]`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 将 DStream 的内容存储为文件系统中的文件。前缀和可选的后缀用于定位和命名目标文件系统中的文件。每个流间隔生成一个文件。每个生成的文件的名称将是`prefix-<timestamp_in_milliseconds>[.suffix]`。
- en: '`saveAsObjectFiles(prefix, suffix)`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`saveAsObjectFiles(prefix, suffix)`'
- en: Saves a DStream of serializable objects to files using standard Java serialization.
    The file generation dynamic is the same as for `saveAsTextFiles`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准 Java 序列化将可序列化对象的 DStream 保存到文件中。文件生成的动态与`saveAsTextFiles`相同。
- en: '`saveAsHadoopFiles(prefix, suffix)`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`saveAsHadoopFiles(prefix, suffix)`'
- en: Saves the DStream as Hadoop files. The file generation dynamic is the same as
    for `saveAsTextFiles`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 将 DStream 保存为 Hadoop 文件。文件生成的动态与`saveAsTextFiles`相同。
- en: foreachRDD
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`foreachRDD`'
- en: '`foreachRDD(func)` is a general-purpose output operation that provides access
    to the underlying RDD within the DStream at each streaming interval.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`foreachRDD(func)`是一个通用的输出操作，它提供对每个流间隔内 DStream 中底层 RDD 的访问。'
- en: All other output operations mentioned earlier use `foreachRDD` to back their
    implementations. We could say that `foreachRDD` is the native output operator
    for Spark Streaming, and all other output operations are derived from it.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所有前面提到的其他输出操作都使用`foreachRDD`来支持它们的实现。我们可以说`foreachRDD`是 Spark Streaming 的原生输出操作符，而所有其他输出操作都是由它派生出来的。
- en: It is our workhorse to materialize Spark Streaming results and arguably the
    most useful native output operation. As such, it deserves its own section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们实现Spark Streaming结果的工具函数，可以说是最有用的本地输出操作。因此，它值得拥有自己的部分。
- en: Using foreachRDD as a Programmable Sink
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将foreachRDD用作可编程的Sink
- en: '`foreachRDD` is the primary method to interact with the data that has been
    processed through the transformations declared on the DStream. `foreachRDD` has
    two method overloads:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`foreachRDD`是与通过在DStream上声明的转换上处理的数据交互的主要方法。`foreachRDD`有两种方法重载：'
- en: '`foreachRDD(foreachFunc: RDD[T] => Unit)`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`foreachRDD(foreachFunc: RDD[T] => Unit)`'
- en: The function passed as a parameter takes an `RDD` and applies side-effecting
    operations on it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参数传递的函数接收一个`RDD`并对其应用具有副作用的操作。
- en: '`foreachRDD(foreachFunc: (RDD[T], Time) => Unit)`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`foreachRDD(foreachFunc: (RDD[T], Time) => Unit)`'
- en: This is an alternative in which we also have access to the time when the operation
    takes place, which can be used to differentiate the data from the time-of-arrival
    perspective.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个替代方案，我们还可以访问操作发生的时间，这可以用于区分数据的到达时间的视角。
- en: Note
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It’s important to note that the time provided by the `foreachRDD` method refers
    to processing time. As we discussed in [“The Effect of Time”](ch02.xhtml#event-time-intro),
    this is the time when the streaming engine handles the event. The event of type
    `T`, contained in the `RDD[T]`, might or might not include additional information
    about the time when the event was produced, the time domain we understand as event
    time.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，`foreachRDD`方法提供的时间是指处理时间。正如我们在[“时间的影响”](ch02.xhtml#event-time-intro)中讨论的那样，这是流处理引擎处理事件的时间。类型为`T`的事件包含在`RDD[T]`中，可能包含有关事件生成时间的其他信息，我们理解为事件时间的时间域。
- en: 'Within the closure of the `foreachRDD`, we have access to the two abstraction
    layers of Spark Streaming:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在`foreachRDD`闭包的内部，我们可以访问Spark Streaming的两个抽象层：
- en: The Spark Streaming scheduler
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming调度程序
- en: Functions applied within the `foreachRDD` closure that does not operate on the
    RDD execute locally in the driver program within the scope of the Spark Streaming
    scheduler. This level is useful for bookkeeping of iterations, accessing external
    web services to enrich the streaming data, local (mutable) variables, or the local
    filesystem. It’s important to remark that at this level we have access to the
    `SparkContext` and the `SparkSession`, making it possible to interact with other
    subsystems of Spark, such as Spark SQL, DataFrames, and Datasets.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在`foreachRDD`闭包中应用的函数（不操作RDD）会在Spark Streaming调度程序的驱动程序中本地执行。这个层次对于迭代的记账、访问外部Web服务以丰富流数据、本地（可变）变量或本地文件系统非常有用。需要注意的是，在这个层次上，我们可以访问`SparkContext`和`SparkSession`，从而可以与Spark的其他子系统（如Spark
    SQL、DataFrames和Datasets）进行交互。
- en: RDD operations
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: RDD操作
- en: Operations applied to the RDD provided to the closure function will be executed
    distributedly in the Spark cluster. All usual RDD-based operations are allowed
    in this scope. These operations will follow the typical Spark-core process of
    serialization and distributed execution in the cluster.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于闭包函数提供的RDD的操作将在Spark集群中分布执行。在这个范围内允许所有常规的基于RDD的操作。这些操作将遵循典型的Spark核心流程，即序列化和在集群中分布执行。
- en: 'A common pattern is to observe two closures in `foreachRDD`: an outer scope,
    containing the local operations, and an inner closure applied to the RDD that
    executes in the cluster. This duality is often a source of confusion and is best
    learned by way of a code example.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在`foreachRDD`中观察到的常见模式是两个闭包：外部作用域包含本地操作，内部闭包应用于在集群中执行的RDD。这种二元性经常是混淆的根源，最好通过代码示例学习。
- en: Let’s consider the code snippet in [Example 20-1](#foreach_rdd_ops), in which
    we want to sort the incoming data into a set of *alternatives* (any concrete classification).
    These alternatives change dynamically and are managed by an external service that
    we access through an external web service call. At each batch interval, the external
    service is consulted for the set of alternatives to consider. For each alternative,
    we create a `formatter`. We use this particular `formatter` to transform the corresponding
    records by applying a distributed `map` transformation. Lastly, we use a `foreachPartition`
    operation on the filtered RDD to get a connection to a database and store the
    records. The `DB` connection is not serializable, and therefore we need this particular
    construct on the RDD to get a local instance on each executor.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑 [Example 20-1](#foreach_rdd_ops) 中的代码片段，在其中我们希望将传入的数据按照一组 *alternatives*（任何具体的分类）进行排序。这些备选项会动态变化，并由我们通过外部服务访问的外部
    web 服务进行管理。在每个批次间隔中，会向外部服务查询要考虑的备选项集合。对于每个备选项，我们创建一个 `formatter`。我们使用这个特定的 `formatter`
    来通过分布式 `map` 转换来转换相应的记录。最后，我们在过滤后的 RDD 上使用 `foreachPartition` 操作获取一个连接到数据库并存储记录的连接。由于
    `DB` 连接不可序列化，因此我们需要在 RDD 上使用这个特定的构造来在每个执行器上获取本地实例。
- en: Note
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This is a simplified version of an actual Spark Streaming production job that
    took care of sorting Internet of Things (IoT) data for devices of many different
    customers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简化版的实际 Spark Streaming 生产作业，负责对许多不同客户的设备的物联网（IoT）数据进行排序。
- en: Example 20-1\. RDD operations within foreachRDD
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 20-1\. foreachRDD 内的 RDD 操作
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'At first glance, a lot is going on in this DStream output operation. Let’s
    break it down into more digestible pieces:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这个 DStream 输出操作中发生了很多事情。让我们把它分解成更易消化的部分：
- en: '[![1](Images/1.png)](#co_spark_streaming_sinks_CO1-1)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_spark_streaming_sinks_CO1-1)'
- en: We declare a `foreachRDD` operation on a DStream and use the closure notation
    `f{ x => y }` to provide an inline implementation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 DStream 上声明了一个 `foreachRDD` 操作，并使用闭包表示法 `f{ x => y }` 提供内联实现。
- en: '[![2](Images/2.png)](#co_spark_streaming_sinks_CO1-2)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_spark_streaming_sinks_CO1-2)'
- en: We begin our implementation by caching the provided `RDD` because we are going
    to iterate multiple times over its content. (We discuss caching in detail in [“Caching”](ch26.xhtml#dstream-caching)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从缓存提供的 `RDD` 开始我们的实现，因为我们将多次迭代其内容。（我们在 [“缓存”](ch26.xhtml#dstream-caching)
    中详细讨论了缓存。）
- en: '[![3](Images/3.png)](#co_spark_streaming_sinks_CO1-3)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_spark_streaming_sinks_CO1-3)'
- en: We access the web service. This happens once at each batch interval and the
    execution takes place on the driver host. This is also important from the networking
    perspective, because executors might be “enclosed” in a private network, whereas
    the driver might have firewall access to other services in the infrastructure.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们访问 web 服务。这每个批次间隔发生一次，并且执行发生在驱动程序主机上。这在网络层面也很重要，因为执行器可能被“封闭”在私有网络中，而驱动程序可能具有防火墙访问基础设施中的其他服务。
- en: '[![4](Images/4.png)](#co_spark_streaming_sinks_CO1-4)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_spark_streaming_sinks_CO1-4)'
- en: We start a loop over the values contained in the received set, letting us iteratively
    filter over the values contained by the RDD.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始一个循环，迭代接收集合中的值，使我们能够迭代地过滤 `RDD` 中包含的值。
- en: '[![5](Images/5.png)](#co_spark_streaming_sinks_CO1-5)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_spark_streaming_sinks_CO1-5)'
- en: This declares a `filter` transformation over the RDD. This operation is lazy
    and will take place in a distributed manner in the cluster when some action would
    call for materialization.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这声明了对 RDD 的 `filter` 转换。这个操作是惰性的，并且将在集群中以分布式方式进行，当某些动作需要实现时。
- en: '[![6](Images/6.png)](#co_spark_streaming_sinks_CO1-6)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_spark_streaming_sinks_CO1-6)'
- en: We obtain a local reference to a serializable Formatter instance that we use
    in line 7 to format the records filtered in the previous step.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得一个对可序列化 Formatter 实例的本地引用，我们在第 7 行中用它来格式化上一步骤中过滤的记录。
- en: '[![7](Images/7.png)](#co_spark_streaming_sinks_CO1-7)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_spark_streaming_sinks_CO1-7)'
- en: We use a `foreachPartition` action on our filtered `RDD`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在我们过滤后的 `RDD` 上使用 `foreachPartition` 操作。
- en: '[![8](Images/8.png)](#co_spark_streaming_sinks_CO1-8)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](Images/8.png)](#co_spark_streaming_sinks_CO1-8)'
- en: We need a local instance of the database driver at the executor.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在执行器上有一个本地的数据库驱动程序实例。
- en: '[![9](Images/9.png)](#co_spark_streaming_sinks_CO1-9)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[![9](Images/9.png)](#co_spark_streaming_sinks_CO1-9)'
- en: We issue parallel insertion of the records into the database.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并行插入记录到数据库中。
- en: '[![10](Images/10.png)](#co_spark_streaming_sinks_CO1-10)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[![10](Images/10.png)](#co_spark_streaming_sinks_CO1-10)'
- en: Finally, we `unpersist` the `RDD` to free up the memory for the next iteration.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们`unpersist` `RDD`以释放内存供下一次迭代使用。
- en: In [Figure 20-3](#foreach-scope), we highlight the different execution scopes
    of the code within the closure of a `foreachRDD` call. In a nutshell, all code
    that is not backed by an RDD operation executes locally in the context of the
    driver. Functions and closures called on RDDs are executed in a distributed manner
    in the cluster. We must pay extra attention to avoid passing nonserializable code
    from the local context to the RDD execution context. This is an often-named source
    of issues in Spark Streaming applications.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 20-3](#foreach-scope) 中，我们强调了`foreachRDD`调用闭包内代码的不同执行范围。简而言之，所有不由RDD操作支持的代码在驱动程序上下文中以本地方式执行。对RDD调用的函数和闭包在集群中以分布方式执行。我们必须格外注意避免将非可序列化的代码从本地上下文传递到RDD执行上下文。这在Spark
    Streaming应用中经常是问题的根源。
- en: '![spas 2003](Images/spas_2003.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2003](Images/spas_2003.png)'
- en: Figure 20-3\. Scope of foreachRDD closure
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 20-3\. `foreachRDD`闭包的范围
- en: As we can see, `foreachRDD` is a versatile output operation with which we can
    mix and match local and distributed operations to obtain results of our computations
    and push data to other systems after it has been processed by the streaming logic.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，`foreachRDD`是一个多才多艺的输出操作，我们可以混合和匹配本地和分布式操作，以获得我们计算的结果并在流处理逻辑处理后将数据推送到其他系统。
- en: Third-Party Output Operations
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三方输出操作
- en: Several third-party libraries add Spark Streaming support for specific target
    systems by using the library enrichment pattern in Scala to add output operations
    to DStreams.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 几个第三方库通过在Scala中使用库增强模式为特定目标系统添加了对Spark Streaming的支持，以添加输出操作到DStreams。
- en: For example, the [Spark-Cassandra Connector](http://bit.ly/2IPiUIL) by Datastax
    enables a `saveToCassandra` operation on DStreams to directly save the streaming
    data to a target Apache Cassandra keyspace and table. The [spark-kafka-writer](http://bit.ly/2ZNLrnw)
    by Ben Fradet enables a similar construct, `dStream.writeToKafka`, to write a
    DStream to a Kafka topic.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Datastax的 [Spark-Cassandra Connector](http://bit.ly/2IPiUIL) 允许在DStreams上执行`saveToCassandra`操作，直接将流数据保存到目标Apache
    Cassandra keyspace和table中。由Ben Fradet开发的 [spark-kafka-writer](http://bit.ly/2ZNLrnw)
    提供了类似的构造，`dStream.writeToKafka`，用于将DStream写入Kafka主题。
- en: Elasticsearch provides support for Spark Streaming using the same pattern. The
    Spark support for the Elasticsearch library enriches the `DStream` API with a
    `saveToEs` call. You can find more information in Elastic’s [Spark Integration
    Guide](http://bit.ly/2V3DyLL).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch通过相同的模式为Spark Streaming提供支持。Elasticsearch库对`DStream` API进行了增强，添加了`saveToEs`调用。更多信息可以在Elastic的
    [Spark集成指南](http://bit.ly/2V3DyLL) 中找到。
- en: Those library implementations make use of Scala implicits and `foreachRDD` behind
    the scenes to offer user-friendly, high-level APIs toward specific third-party
    systems, saving the users from the sometimes intricate details that deal with
    the multilevel abstractions within the `foreachRDD` explained previously.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些库的实现利用了Scala的隐式转换和`foreachRDD`在幕后提供了用户友好的高级API，以针对特定的第三方系统，使用户免受与前面解释的`foreachRDD`中多级抽象相关的复杂细节的困扰。
- en: For more connectors, see [SparkPackages](https://spark-packages.org).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 更多连接器，请参见 [SparkPackages](https://spark-packages.org)。
