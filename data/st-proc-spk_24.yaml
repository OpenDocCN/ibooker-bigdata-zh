- en: Chapter 18\. The Spark Streaming Execution Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第18章。Spark Streaming执行模型
- en: When we began our Spark Streaming journey in [Chapter 16](ch16.xhtml#sps-intro),
    we discussed how the DStream abstraction embodies the programming and the operational
    models offered by this streaming API. After learning about the programming model
    in [Chapter 17](ch17.xhtml#sps-programming-model), we are ready to understand
    the execution model behind the Spark Streaming runtime.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[第16章](ch16.xhtml#sps-intro)开始我们的Spark Streaming之旅时，我们讨论了DStream抽象体现了这个流API提供的编程和操作模型。在了解了[第17章](ch17.xhtml#sps-programming-model)中的编程模型之后，我们准备理解Spark
    Streaming运行时背后的执行模型。
- en: In this chapter, you learn about the bulk synchronous architecture and how it
    provides us with a framework to reason about the microbatch streaming model. Then,
    we explore how Spark Streaming consumes data using the receiver model and the
    guarantees that this model provides in terms of data-processing reliability. Finally,
    we examine the direct API as an alternative to receivers for streaming data providers
    that are able to offer reliable data delivery.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解批处理同步架构以及它如何为我们提供一个微批处理流模型的推理框架。然后，我们探讨了Spark Streaming如何使用接收器模型消费数据以及这种模型在数据处理可靠性方面提供的保证。最后，我们探讨了直接API作为传送流数据提供者的替代方法，该方法能够提供可靠的数据传输。
- en: The Bulk-Synchronous Architecture
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批处理同步架构
- en: In [Chapter 5](ch05.xhtml#distributed-processing) we discussed the *bulk-synchronous
    parallelism* or *BSP* model as a theoretical framework that allows us to reason
    how distributed stream processing could be done on microbatches of data from a
    stream.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.xhtml#distributed-processing)中，我们讨论了*批处理同步并行*或*BSP*模型作为一个理论框架，允许我们推理如何在流的微批处理数据上进行分布式流处理。
- en: 'Spark Streaming follows a processing model similar to bulk-synchronous parallelism:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming遵循类似于批处理同步并行的处理模型：
- en: All of the Spark executors on the cluster are assumed to have a synchronous
    clock; for example, synchronized through a network time protocol (NTP) server.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假定集群上的所有Spark执行器都有同步时钟；例如，通过网络时间协议（NTP）服务器同步。
- en: 'In the case of a receiver-based source, one or several of the executors runs
    a special Spark job, a *receiver*. This receiver is tasked with consuming new
    elements of the Stream. It receives two clock ticks:'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于基于接收器的数据源，一个或多个执行器运行一个特殊的Spark作业，一个*接收器*。这个接收器负责消费流的新元素。它接收两个时钟周期：
- en: The most frequent clock tick is called the *block interval*. It signals when
    elements received from the stream should be allocated to a block; that is, the
    portion of the stream that should be processed by a single executor, for this
    current interval. Each such block becomes a partition of the Resilient Distributed
    Dataset (RDD) produced at each batch interval.
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最频繁的时钟周期被称为*块间隔*。它标志着从流接收的元素应该分配到一个块中；也就是说，应该由单个执行器处理的流的部分，对于当前间隔。每个这样的块成为每个批处理间隔产生的Resilient
    Distributed Dataset（RDD）的一个分区。
- en: The second and less frequent is the *batch interval*. It marks when the receiver
    should assemble the data from the stream collected since the last clock tick and
    produces an RDD for distributed processing on the cluster.
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个更少频繁的是*批处理间隔*。它标记着接收器应该在上一个时钟周期以来收集的流数据装配到一起，并为集群上的分布式处理生成一个RDD。
- en: When using the direct approach, only the batch interval tick is relevant.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用直接方法时，只有批处理间隔的时钟周期是相关的。
- en: During all processing, as is the case with a regular (batch) Spark job, blocks
    are signaled to the block manager, a component that ensures any block of data
    put into Spark is replicated according to the configured persistence level, for
    the purpose of fault tolerance.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有处理过程中，与常规（批处理）Spark作业一样，块被传递给块管理器，该组件确保放入Spark的任何数据块根据配置的持久性级别进行复制，以实现容错性。
- en: On each batch interval, the RDD from which data was received during the previous
    batch interval becomes available, and is thus scheduled for processing during
    this batch.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个批处理间隔，上一个批处理间隔接收数据的RDD变得可用，并因此计划在本批处理期间进行处理。
- en: '[Figure 18-1](#dstream-structure-img) illustrates how these elements come together
    to conceptually form a DStream.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 18-1](#dstream-structure-img)展示了这些元素如何在概念上形成一个DStream。'
- en: '![spas 1801](Images/spas_1801.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![spas 1801](Images/spas_1801.png)'
- en: 'Figure 18-1\. The DStream structure: blocks and batches'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-1。DStream结构：块和批次
- en: 'To achieve the concurrent execution with a strict bulk-synchronous model, the
    barrier here would be the arrival of a new *RDD* at the batch interval. Except
    that in Spark Streaming, this is not really a barrier because the data delivery
    happens independently of the state of the cluster at the moment of the arrival
    of the new batch: Spark’s receivers do not wait for the cluster to be finished
    with receiving data to start on the new batch.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现与严格的批量同步模型并发执行，这里的障碍是在批处理间隔到达新的*RDD*。但是在Spark Streaming中，这实际上并不是障碍，因为数据传递独立于集群在新批处理到达时的状态：Spark的接收器不会等待集群完成接收数据才开始新批处理。
- en: 'This is not a design fault; rather, it’s a consequence of Spark Streaming trying
    to do real stream processing at its most honest: despite having a microbatching
    model, Spark Streaming acknowledges that a stream has no predefined end, and that
    the system is required to receive data continuously.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是设计上的错误；相反，这是Spark Streaming试图以最诚实的方式进行实时流处理的结果：尽管具有微批处理模型，但Spark Streaming承认流没有预定义的结束，并且系统需要持续接收数据。
- en: The consequence of this relatively simple model is that the Spark Streaming
    job tasked with receiving data—the receiver—needs to be a job scheduled in the
    cluster to continuously work. If it ever were to crash, it would be restarted
    on another executor, continuing the ingestion of data without further interruption.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种相对简单模型的结果是，负责接收数据的Spark Streaming作业——接收器——需要作为集群中定期工作的作业进行调度。如果它崩溃，将在另一个执行程序上重新启动，继续数据的摄取而无需进一步中断。
- en: The Receiver Model
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接收器模型
- en: As we previously hinted, in Spark Streaming, the receiver is a process able
    to continuously collect data from the input stream regardless of the state of
    processing of the Spark cluster.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前暗示的，在Spark Streaming中，接收器是能够连续从输入流中收集数据的进程，而不考虑Spark集群处理状态的过程。
- en: This component is an adaptor between the data delivered by the stream source
    and the data-processing engine in Spark Streaming. As an adaptor, it implements
    the specific API and semantics of the external stream and delivers that data using
    an internal contract. [Figure 18-2](#receiver-model) shows the role of the receiver
    in the DStream data flow.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 该组件是流源提供的数据与Spark Streaming数据处理引擎之间的适配器。作为适配器，它实现了外部流的特定API和语义，并使用内部契约传递该数据。[图18-2](#receiver-model)展示了DStream数据流中接收器的角色。
- en: '![spas 1802](Images/spas_1802.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![spas 1802](Images/spas_1802.png)'
- en: Figure 18-2\. Receiver model
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-2\. 接收器模型
- en: The Receiver API
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接收器API
- en: 'The most fundamental receiver consists of three methods:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的接收器包括三种方法：
- en: '`def onStart()`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`def onStart()`'
- en: Starts the reception of data from the external sources. In practice, `onStart`
    should asynchronously initiate the inbound data collection process and return
    immediately.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 启动来自外部源的数据接收。在实践中，`onStart`应异步启动入站数据收集过程并立即返回。
- en: '`def store(...)`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`def store(...)`'
- en: Delivers one or several data elements to Spark Streaming. `store` must be called
    from the asynchronous process initiated by `onStart` whenever there is new data
    available.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个或多个数据元素传递给Spark Streaming。每当有新数据可用时，必须从`onStart`启动的异步过程中调用`store`。
- en: '`def stop(...)`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`def stop(...)`'
- en: Stops the receiving process. `stop` must take care of properly cleaning up any
    resources used by the receiving process started by `onStart`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 停止接收过程。`stop`必须负责正确清理`onStart`启动的接收过程使用的任何资源。
- en: This model provides a generic interface that can be implemented to integrate
    a wide range of streaming data providers. Note how the genericity abstracts over
    the data delivery method of the streaming system. We can implement an always-connected
    push-based receiver, like a TCP client socket, as well as a request-based pull
    connector, like a REST/HTTP connector to some system.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型提供了一个通用接口，可以实现集成各种流数据提供者。注意通用性如何抽象出流系统的数据传递方法。我们可以实现一个始终连接的基于推送的接收器，如TCP客户端套接字，以及基于请求的拉取连接器，如与某些系统的REST/HTTP连接器。
- en: How Receivers Work
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接收器工作原理
- en: 'The task of the receiver is to collect data from the stream source and deliver
    it to Spark Streaming. Intuitively, it’s very easy to understand: as data comes
    in, it’s collected and packaged in blocks during the time of the batch interval.
    As soon as each batch interval period of time is completed, the collected data
    blocks are given to Spark for processing.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接收器的任务是从流数据源收集数据并将其传递给Spark Streaming。直观来说，这很容易理解：随着数据的到来，它们在批处理间隔的时间内被收集并打包成数据块。一旦完成每个批处理间隔时间段，收集的数据块就会交给Spark进行处理。
- en: '[Figure 18-3](#receiver-working) depicts how the timing of this sequence of
    events takes place. At the start of the streaming process, the receiver begins
    collecting data. At the end of the *t0* interval, the first collected block `#0`
    is given to Spark for processing. At time *t2*, Spark is processing the data block
    collected at *t1*, while the receiver is collecting the data corresponding to
    block `#2`.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-3](#receiver-working) 描述了这个事件序列的时间安排。在流处理过程开始时，接收器开始收集数据。在*t0*间隔结束时，第一个收集的块`#0`被传递给Spark进行处理。在时间*t2*时，Spark正在处理在*t1*收集的数据块，而接收器正在收集对应于块`#2`的数据。'
- en: '![spas 1803](Images/spas_1803.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![spas 1803](Images/spas_1803.png)'
- en: Figure 18-3\. Receiver in action
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-3\. 接收器的操作
- en: We can generalize that, at any point in time, Spark is processing the previous
    batch of data, while the receivers are collecting data for the current interval.
    After a batch has been processed by Spark (like `#0` in [Figure 18-3](#receiver-working)),
    it can be cleaned up. The time when that RDD will be cleaned up is determined
    by the `spark.cleaner.ttl` setting.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以总结说，在任何时间点，Spark正在处理上一个数据批次，而接收器正在收集当前间隔的数据。在Spark处理完一个批次（如图18-3中的`#0`）后，它可以被清理。RDD将被清理的时间由
    `spark.cleaner.ttl` 设置决定。
- en: The Receiver’s Data Flow
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接收器的数据流
- en: '[Figure 18-4](#receiver-dataflow) illustrates the data flow of a Spark application
    in this case.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-4](#receiver-dataflow) 描述了这种情况下 Spark 应用程序的数据流。'
- en: '![spas 1804](Images/spas_1804.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![spas 1804](Images/spas_1804.png)'
- en: Figure 18-4\. The data flow of Spark’s receiver
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-4\. Spark接收器的数据流
- en: In this figure, we can see that data ingestion occurs as a job that is translated
    into a single task on one executor. This task deals with connecting to a data
    source and initiates the data transfer. It is managed from the Spark Context,
    a bookkeeping object that resides within the driver machine.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，我们可以看到数据摄入作为一个作业发生，它被转化为一个执行器上的单个任务。此任务处理与数据源的连接并启动数据传输。它由Spark Context管理，这是驱动器机器内的一个簿记对象。
- en: On each block interval tick (as measured on the executor that is running the
    receiver), this machine groups the data received for the previous block interval
    into a block. The block is then registered with the Block Manager, also present
    in the bookkeeping of the Spark Context, on the driver. This process initiates
    the replication of the data that this block represents, to ensure that the source
    data in Spark Streaming is replicated the number of times indicated by the storage
    level.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个块间隔周期（在运行接收器的执行器上测量），这台机器将收到的上一个块间隔的数据分组成一个块。然后将这个块注册给Block Manager，也存在于Spark
    Context的簿记中，驱动器上。此过程启动了数据复制，以确保Spark Streaming中的源数据按照存储级别指示的次数进行复制。
- en: On each batch interval tick (as measured on the driver), the driver groups the
    data received for the previous batch interval, which has been replicated the correct
    number of times, into an RDD. Spark registers this RDD with the JobScheduler,
    initiating the scheduling of a job on that particular RDD—in fact, the whole point
    of Spark Streaming’s microbatching model consists of repeatedly scheduling the
    user-defined program on successive batched RDDs of data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个批处理间隔周期（在驱动程序上测量），驱动程序会将接收到的上一个批处理间隔的数据进行分组，已正确复制的数据封装为一个RDD。Spark会将这个RDD注册给作业调度程序，从而启动对该特定RDD的作业调度——事实上，Spark
    Streaming微批处理模型的整个核心在于重复调度用户定义的程序来处理连续的批处理RDD数据。
- en: The Internal Data Resilience
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内部数据的弹性
- en: The fact that receivers are independent jobs has consequences, in particular
    for the resource usage and data delivery semantics. To execute its data collection
    job, the receiver consumes one core on an executor regardless of the amount of
    work it needs to do. Therefore, using a single streaming receiver will result
    in data ingestion done in sequence by a single core in an executor, which becomes
    the limiting factor to the amount of data that Spark Streaming can ingest.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接收器是独立的作业这一事实具有后果，特别是对于资源使用和数据传递语义。为执行其数据收集作业，接收器在执行器上消耗一个核心，无论需要做多少工作。因此，使用单个流接收器将导致数据摄取由执行器中的单个核心按顺序完成，这成为限制
    Spark Streaming 可以摄取数据量的因素。
- en: 'The base unit of replication for Spark is a block: any block can be on one
    or several machines (up to the persistence level indicated in the configuration,
    so at most two by default), and it is only when a block has reached that persistence
    level that it can be processed. Therefore, it is only when an RDD has every block
    replicated that it can be taken into account for job scheduling.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的复制基本单元是块：任何块可以位于一个或多个机器上（最多达到配置中指定的持久性级别，因此默认情况下最多为两个），只有当块达到该持久性级别时，它才能被处理。因此，只有当RDD的每个块都复制了，它才能被考虑用于作业调度。
- en: At the Spark engine side, each block becomes a partition of the RDD. The combination
    of a partition of data and the work that needs to be applied to it becomes a task.
    Each task can be processed in parallel, usually on the executor that contains
    the data locally. Therefore, the level of parallelism we can expect for an RDD
    obtained from a single receiver is exactly the ratio of the batch interval to
    the block interval, as defined in [Equation 18-1](#single-receiver-partitioning).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark引擎端，每个块都成为RDD的一个分区。数据分区和应用于数据的工作的组合成为一个任务。通常每个任务可以并行处理，通常在包含数据的执行器上本地处理。因此，我们可以期望从单个接收器获取的RDD的并行性水平正好是批处理间隔与块间隔的比率，如
    [Equation 18-1](#single-receiver-partitioning) 中定义的那样。
- en: Equation 18-1\. Single receiver partitioning
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Equation 18-1\. 单接收器分区
- en: <math alttext="numberpartitions equals StartFraction batchinterval Over blockinterval
    EndFraction" display="block"><mrow><mtext>numberpartitions</mtext> <mo>=</mo>
    <mfrac><mtext>batchinterval</mtext> <mtext>blockinterval</mtext></mfrac></mrow></math>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="numberpartitions equals StartFraction batchinterval Over blockinterval
    EndFraction" display="block"><mrow><mtext>numberpartitions</mtext> <mo>=</mo>
    <mfrac><mtext>batchinterval</mtext> <mtext>blockinterval</mtext></mfrac></mrow></math>
- en: In Spark, the usual rule of thumb for task parallelism is to have two to three
    times ratio of the number of tasks to the number of available executor cores.
    Taking a factor of three for our discussion, we should set the block interval
    to that shown in [Equation 18-2](#block-interval-tuning).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，任务并行性的通常经验法则是将任务数与可用执行器核心数的比率设置为两到三倍。对于我们的讨论，我们应该将块间隔设置为 [Equation 18-2](#block-interval-tuning)
    中所示的值的三倍。
- en: Equation 18-2\. Block interval tuning
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Equation 18-2\. 块间隔调整
- en: <math alttext="blockinterval equals StartFraction batchinterval Over 3 asterisk
    Sparkcores EndFraction" display="block"><mrow><mtext>blockinterval</mtext> <mo>=</mo>
    <mfrac><mtext>batchinterval</mtext> <mrow><mn>3</mn><mo>*</mo><mtext>Sparkcores</mtext></mrow></mfrac></mrow></math>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="blockinterval equals StartFraction batchinterval Over 3 asterisk
    Sparkcores EndFraction" display="block"><mrow><mtext>blockinterval</mtext> <mo>=</mo>
    <mfrac><mtext>batchinterval</mtext> <mrow><mn>3</mn><mo>*</mo><mtext>Sparkcores</mtext></mrow></mfrac></mrow></math>
- en: Receiver Parallelism
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接收器并行性
- en: We mentioned that a single receiver would be a limiting factor to the amount
    of data that we can process with Spark Streaming.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到单个接收器将成为 Spark Streaming 可以处理的数据量的限制因素。
- en: A simple way to increase incoming data throughput is to declare more DStreams
    at the code level. Each DStream will be attached to its own consumer—and therefore
    each will have its own core consumption—on the cluster. The DStream operation
    `union` allows us to merge those streams, ensuring that we produce a single pipeline
    of data from our various input streams.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 增加数据吞吐量的一个简单方法是在代码级别声明更多的DStreams。每个DStream将附加到其自己的消费者上——因此每个消费者将在集群上消耗自己的核心。DStream操作
    `union` 允许我们合并这些流，确保我们从各个输入流产生单一的数据流水线。
- en: 'Let’s assume that we create DStreams in parallel an put them in a sequence:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们并行创建DStreams并将它们放在一个序列中：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Warning
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The `union` of the created DStreams is important because this reduces the number
    of transformation pipelines on the input DStream to one. Not doing this will multiply
    the number of stages by the number of consumers, resulting in unnecessary overhead.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的DStreams的`union`非常重要，因为这样可以将输入DStream的转换流水线数量减少到一个。如果不这样做，会导致阶段数量乘以消费者数量，从而造成不必要的开销。
- en: In this way, we can exploit the receiver parallelism, here represented by the
    `#parallelism` factor of concurrently created DStreams.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以利用接收器并行性，这里用并行创建的DStreams的 `#parallelism` 因子来表示。
- en: 'Balancing Resources: Receivers Versus Processing Cores'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平衡资源：接收器与处理核心
- en: Given that each created receiver consumes its own core in a cluster, increasing
    consumer parallelism has consequences in the number of cores available for processing
    in our cluster.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每个创建的接收器在集群中消耗自己的核心，增加消费者并行性对于可用于集群处理的核心数有影响。
- en: Let’s imagine that we have a 12-core cluster that we want to dedicate to our
    streaming analytics application. When using a single receiver, we use one core
    for receiving and nine cores for processing data. The cluster might be underutilized
    because a single receiver might not receive enough data to give work to all of
    the available processing cores. [Figure 18-5](#receiver-allocation-1) illustrates
    that situation, in which the green nodes are being used for processing, and the
    gray nodes remain idle.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个12核心的集群，我们希望将其专用于我们的流分析应用程序。当使用单个接收器时，我们使用一个核心用于接收和九个核心用于处理数据。集群可能会未充分利用，因为单个接收器可能无法接收足够的数据以利用所有可用的处理核心。[图18-5](#receiver-allocation-1)说明了这种情况，其中绿色节点用于处理，灰色节点保持空闲。
- en: '![spas 1805](Images/spas_1805.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![spas 1805](Images/spas_1805.png)'
- en: Figure 18-5\. Single-receiver allocation
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-5\. 单接收器分配
- en: To improve cluster utilization, we increase the number of receivers, as we just
    discussed. In our hypothetical scenario, using four receivers provides us with
    a much better resource allocation, as shown in [Figure 18-6](#receiver-allocation-2).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高集群利用率，我们增加接收器的数量，正如我们刚才讨论的那样。在我们的假设场景中，使用四个接收器为我们提供了更好的资源分配，如[图18-6](#receiver-allocation-2)所示。
- en: '![spas 1806](Images/spas_1806.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![spas 1806](Images/spas_1806.png)'
- en: Figure 18-6\. Multiple-receiver allocation
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-6\. 多接收器分配
- en: 'The batch interval is fixed by the needs of the analysis and remains the same.
    What should the block interval be? Well, four DStreams ingesting in parallel will
    necessarily create four times as many blocks per block interval as one single
    DStream would. Therefore, with the same block interval, the number of partitions
    of the unionized DStream will be four times what it was in the original case.
    Hence, we can’t use the same block interval. Instead, we should use the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理间隔由分析需求确定并保持不变。那么，块间隔应该是多少呢？嗯，同时并行摄取的四个DStreams必然会在每个块间隔内创建四倍于单个DStream的块数。因此，对于相同的块间隔，联合DStream的分区数将是原始情况的四倍。因此，我们不能使用相同的块间隔。相反，我们应该使用以下方案：
- en: <math alttext="blockinterval equals StartFraction 4 asterisk batchinterval Over
    3 asterisk Sparkcores EndFraction" display="block"><mrow><mtext>blockinterval</mtext>
    <mo>=</mo> <mfrac><mrow><mn>4</mn><mo>*</mo><mtext>batchinterval</mtext></mrow>
    <mrow><mn>3</mn><mo>*</mo><mtext>Sparkcores</mtext></mrow></mfrac></mrow></math>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="blockinterval equals StartFraction 4 asterisk batchinterval Over
    3 asterisk Sparkcores EndFraction" display="block"><mrow><mtext>blockinterval</mtext>
    <mo>=</mo> <mfrac><mrow><mn>4</mn><mo>*</mo><mtext>batchinterval</mtext></mrow>
    <mrow><mn>3</mn><mo>*</mo><mtext>Sparkcores</mtext></mrow></mfrac></mrow></math>
- en: Because we want *at least* three partitions, we round this number *down* to
    the nearest millisecond.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们希望*至少*有三个分区，所以我们将这个数字*向下*舍入到最近的毫秒数。
- en: 'Generalizing, with an arbitrary set of characteristics, we should use this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，根据任意一组特征，我们应该采用以下方法：
- en: <math alttext="blockinterval equals StartFraction receivers asterisk batchinterval
    Over partitionspercore asterisk Sparkcores EndFraction" display="block"><mrow><mtext>blockinterval</mtext>
    <mo>=</mo> <mfrac><mrow><mtext>receivers</mtext><mo>*</mo><mtext>batchinterval</mtext></mrow>
    <mrow><mtext>partitionspercore</mtext><mo>*</mo><mtext>Sparkcores</mtext></mrow></mfrac></mrow></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="blockinterval equals StartFraction receivers asterisk batchinterval
    Over partitionspercore asterisk Sparkcores EndFraction" display="block"><mrow><mtext>blockinterval</mtext>
    <mo>=</mo> <mfrac><mrow><mtext>receivers</mtext><mo>*</mo><mtext>batchinterval</mtext></mrow>
    <mrow><mtext>partitionspercore</mtext><mo>*</mo><mtext>Sparkcores</mtext></mrow></mfrac></mrow></math>
- en: 'Where the total number of cores used in the system is as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 系统中使用的总核心数如下所示：
- en: <math><mtext>total system cores</mtext><mo>=</mo><mtext># of receivers</mtext><mo>+</mo><mtext>#
    of spark cores</mtext></math>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mtext>总系统核心数</mtext><mo>=</mo><mtext>接收器数</mtext><mo>+</mo><mtext>Spark核心数</mtext></math>
- en: Achieving Zero Data Loss with the Write-Ahead Log
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过预先写日志（WAL）实现零数据丢失
- en: The original receiver design, previous to Spark v1.2, had a major design flaw.
    While the receiver is collecting data for the current block, that data is only
    found in a memory buffer in the receiver process. Only when the block is completed
    and delivered does it become replicated in the cluster. In case of failure of
    the receiver, the data in that buffer gets lost and cannot be recovered, causing
    data loss.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark v1.2之前的原始接收器设计存在重大设计缺陷。当接收器为当前块收集数据时，该数据仅存储在接收器进程的内存缓冲区中。只有在块完成并交付后，数据才会在集群中复制。如果接收器失败，缓冲区中的数据将丢失且无法恢复，导致数据丢失。
- en: To prevent data loss, data collected by the receiver is additionally appended
    to a log file on a reliable filesystem. This is known as the write-ahead log (WAL),
    a component commonly used in database design to guarantee reliable and durable
    data reception.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止数据丢失，接收器收集的数据会额外追加到可靠文件系统上的日志文件中。这被称为预先写日志（WAL），这是数据库设计中常用的组件，用于保证可靠和持久的数据接收。
- en: The WAL is an append-only structure in which data is written before it is delivered
    for processing. When data is known to be correctly processed, its entry in the
    log is marked as processed. In the database world, the equivalent process is the
    commit of the transaction in which the data is involved, making this log also
    known as the *commit log*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: WAL是一个仅追加的结构，在数据传送进行处理之前将其写入。当已知数据正确处理时，其日志条目标记为已处理。在数据库世界中，等效的过程是事务提交，其中涉及的数据使得该日志也称为*提交日志*。
- en: In case of failure, data from the WAL is replayed from the record following
    the last registered commit, compensating in that way for the potential loss of
    data of the receiver. The combination of the WAL and the receiver is known as
    the reliable receiver. Streaming sources based on the reliable receiver model
    are known as *reliable sources*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在失败情况下，从 WAL 中回放数据，从最后一个注册提交之后的记录开始，以补偿接收器可能的数据丢失。WAL 和接收器的组合被称为可靠接收器。基于可靠接收器模型的流式数据源被称为*可靠数据源*。
- en: Enabling the WAL
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用 WAL
- en: 'To enable the WAL-backed data delivery to ensure zero data loss, we need to
    apply the following settings:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用基于 WAL 的数据传递以确保零数据丢失，我们需要应用以下设置：
- en: '`streamingContext.checkpoint(dir)`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`streamingContext.checkpoint(dir)`'
- en: This directory is used for both checkpoints and the write-ahead logs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此目录用于检查点和预写日志。
- en: '`spark.streaming.receiver.writeAheadLog.enable` (default: `false`)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.streaming.receiver.writeAheadLog.enable`（默认值：`false`）'
- en: Set to `true` to enable the write-ahead process.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 设置为 `true` 以启用预写日志过程。
- en: Note that due to the increased work of writing to the log, the overall throughput
    of the streaming job might lower and the overall resource usage might increase.
    As the WAL writes to a reliable filesystem, the infrastructure of that filesystem
    needs to have enough resources to accept the continuous write stream to the log,
    in terms of storage and processing capacity.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于写入日志的工作量增加，流作业的总吞吐量可能会降低，而总资源使用量可能会增加。由于 WAL 写入可靠的文件系统，该文件系统的基础设施需要足够的资源来接受对日志的连续写入流，包括存储和处理能力。
- en: The Receiverless or Direct Model
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无接收器或直接模型
- en: Spark Streaming aims to be a generic stream-processing framework. Within that
    premise, the receiver model provides a general, source-agnostic contract that
    enables the integration of any streaming source. But some sources allow for a
    direct consumption model in which the role of the receiver as an intermediary
    in the data delivery process becomes unnecessary.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 致力于成为一个通用的流处理框架。在这个前提下，接收器模型提供了一个通用的、与数据源无关的合约，使得可以集成任何流式数据源。但是一些数据源允许直接消费模型，其中接收器作为数据传递过程中的中介变得不再必要。
- en: The increasing popularity of Kafka as a streaming backend for Spark Streaming
    jobs made it a focus of additional consideration. In the previous section, we
    learned about the WAL as a solution to achieve zero data loss for the receiver
    model in face of failure.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 作为 Spark Streaming 作业的流后端越来越受欢迎，因此引起了额外的关注。在前一节中，我们了解到 WAL 作为解决接收器模型在面对故障时实现零数据丢失的解决方案。
- en: Kafka is, at its core, an implementation of a distributed commit log. When the
    Kafka reliable receiver was implemented, it became evident that the use of the
    WAL was duplicating the same functionality already present in Kafka. Moreover,
    consuming data from Kafka into a receiver was not even necessary. Let us recall
    that the receiver takes care of data redundancy through duplication of the blocks
    in the Spark memory. Kafka already replicates data reliability and provides equivalent
    data durability guarantees. To consume data from Kafka, all that was required
    was to track the *offset* of the data already processed and compute the offset
    of the new data received in a batch interval. Using these two offsets for each
    partition consumed would be sufficient to launch a Spark job that would directly
    consume the data segment determined by these two offsets and operate on it. When
    the processing of the microbatch succeeds, the consumed offset is committed.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 的核心是分布式提交日志的实现。当实现了 Kafka 的可靠接收器后，发现使用 WAL（写前日志）只是在重复 Kafka 已有的功能。此外，从
    Kafka 消费数据到接收器甚至是不必要的。让我们回顾一下，接收器通过在 Spark 内存中复制块来处理数据冗余。Kafka 已经通过数据复制来保证数据可靠性，并提供了等效的数据耐久性保证。要从
    Kafka 消费数据，只需跟踪已处理数据的*偏移量*，并计算批次间接收到的新数据的偏移量即可。对每个分区消费使用这两个偏移量足以启动一个 Spark 作业，直接消费由这两个偏移量确定的数据段并对其进行操作。当微批处理成功时，消费的偏移量被提交。
- en: The direct connector model is more of a manager than a data broker. Its role
    is to compute data segments to be processed by Spark and maintain the bookkeeping
    of data to be processed versus data already handled. Given the high-performance
    and low-latency data delivery characteristics of Kafka, this method turned out
    to be much faster and requires fewer resources than the receiver-based implementation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 直连器模型更像是一个管理者，而不是一个数据经纪人。其角色是计算由Spark处理的数据段，并维护待处理数据与已处理数据的簿记。鉴于Kafka高性能和低延迟的数据传递特性，这种方法比基于接收器的实现更快，且需要的资源更少。
- en: Note
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For the specific usage of the *direct* connector for Kafka, refer to [Chapter 19](ch19.xhtml#sps-sources).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Kafka的*直连*连接器的具体用法，请参阅[第19章](ch19.xhtml#sps-sources)。
- en: Summary
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'So far, we have seen a primer on the Spark Streaming execution model and the
    fundamentals of how it treats stream processing:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了Spark Streaming执行模型的基础以及它处理流处理的基本原理：
- en: Streams are aggregated data seen over time on a data source. On every block
    interval, a new partition of data is produced and replicated. In every batch interval
    (a multiple of the block interval), the resulting data is assembled into an RDD
    and a job can be scheduled on it.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流是在数据源上看到的随时间聚合的数据。在每个块间隔上，会产生一个新的数据分区并复制。在每个批次间隔（块间隔的倍数）上，生成的数据会组装成RDD，并可以安排作业在其上运行。
- en: Scheduling is done by user-defined functions in a script, but can also be the
    byproduct of some built-in functionality (e.g., checkpointing). The scheduling
    itself has a fixed core.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度由脚本中的用户定义函数完成，但也可以是某些内置功能的副产品（例如检查点）。调度本身具有固定的核心。
- en: The most generic way of creating a DStream is the receiver model, which creates
    a job connecting to the input source on an executor, consuming one core. Parallelism
    can be increased in some cases by creating several DStreams.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建DStream的最通用方式是接收器模型，它在执行器上连接输入源创建作业，消耗一个核心。在某些情况下，通过创建多个DStream可以增加并行性。
- en: Factors such as resource allocation and configuration parameters affect the
    overall performance of a streaming job, and there are options to tune such behavior.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源分配和配置参数等因素影响流作业的整体性能，并有调整此类行为的选项。
- en: Enabling the WAL prevents potential data loss at the expense of additional resource
    usage.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用WAL可以防止潜在的数据丢失，但会增加额外的资源使用。
- en: For particular systems, such as Kafka, that provide high-performance and durable
    data delivery guarantees, it’s possible to reduce the responsibilities of the
    receiver to minimal bookkeeping that computes microbatch intervals in terms native
    to the streaming system. This model, known as the direct model, is both more resource
    efficient and performant than having to copy and replicate the data into the memory
    of the Spark cluster.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于提供高性能和持久数据传递保证的特定系统，如Kafka，可以将接收器的责任降低到最小的簿记，以按照流系统的本机术语计算微批次间隔。这种模型称为直连模型，比将数据复制和复制到Spark集群内存中更节约资源且性能更高。
