- en: Chapter 6\. Streams and Tables
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。流与表
- en: 'You have reached the part of the book where we talk about streams and tables.
    If you recall, back in Chapter 1, we briefly discussed two important but orthogonal
    dimensions of data: *cardinality* and *constitution*. Until now, we’ve focused
    strictly on the cardinality aspects (bounded versus unbounded) and otherwise ignored
    the constitution aspects (stream versus table). This has allowed us to learn about
    the challenges brought to the table by the introduction of unbounded datasets,
    without worrying too much about the lower-level details that really drive the
    way things work. We’re now going to expand our horizons and see what the added
    dimension of constitution brings to the mix.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经到达了书中讨论流和表的部分。如果你还记得，在第一章中，我们简要讨论了数据的两个重要但正交的维度：*基数*和*构成*。到目前为止，我们严格关注基数方面（有界与无界），并且忽略了构成方面（流与表）。这使我们能够了解无界数据集引入的挑战，而不用太担心真正驱动事物运作方式的底层细节。我们现在将扩展我们的视野，看看构成的增加维度给混合带来了什么。
- en: Though it’s a bit of a stretch, one way to think about this shift in approach
    is to compare the relationship of classical mechanics to quantum mechanics. You
    know how in physics class they teach you a bunch of classical mechanics stuff
    like Newtonian theory and so on, and then after you think you’ve more or less
    mastered that, they come along and tell you it was all bunk, and classical physics
    gives you only part of the picture, and there’s actually this other thing called
    quantum mechanics that really explains how things work at a lower level, but it
    didn’t make sense to complicate matters up front by trying to teach you both at
    once, and...oh wait...we also haven’t fully reconciled everything between the
    two yet, so just squint at it and trust us that it all makes sense somehow? Well
    this is a lot like that, except your brain will hurt less because physics is way
    harder than data processing, and you won’t have to squint at anything and pretend
    it makes sense because it actually does come together beautifully in the end,
    which is really cool.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有点牵强，但一种思考这种方法转变的方式是将经典力学与量子力学的关系进行比较。你知道在物理课上他们教你一堆经典力学的东西，比如牛顿理论之类的，然后在你觉得你更或多少掌握了之后，他们告诉你那都是废话，经典物理只给你部分图景，实际上还有这个叫做量子力学的东西，它真正解释了更低层次的事物运作方式，但一开始让事情变得复杂并不合理，所以...哦等等...我们之间还没有完全协调好一切，所以只是眯着眼睛相信我们，总有一天一切都会有意义？嗯，这很像那个，只是你的大脑会疼得少一些，因为物理学比数据处理难得多，你也不必眯着眼睛假装一切都有意义，因为实际上最后一切都会美好地结合在一起，这真的很酷。
- en: 'So, with the stage appropriately set, the point of this chapter is twofold:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，舞台已经适当地设置好了，这一章的重点有两个：
- en: To try to describe the relationship between the Beam Model (as we’ve described
    it in the book up to this point) and the theory of “streams and tables” (as popularized
    by [Martin Kleppmann](http://bit.ly/2LO0cik) and [Jay Kreps](http://bit.ly/2sX0bl8),
    among others, but essentially originating out of the database world). It turns
    out that stream and table theory does an illuminating job of describing the low-level
    concepts that underlie the Beam Model. Additionally, a clear understanding of
    how they relate is particularly informative when considering how robust stream
    processing concepts might be cleanly integrated into SQL (something we consider
    in Chapter 8).
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 试图描述Beam模型（就像我们在书中描述的那样）与“流和表”理论（由[Martin Kleppmann](http://bit.ly/2LO0cik)和[Jay
    Kreps](http://bit.ly/2sX0bl8)等人普及，但实质上起源于数据库世界）之间的关系。事实证明，流和表理论很好地描述了Beam模型的底层概念。此外，当考虑如何将健壮的流处理概念清晰地集成到SQL中时，对它们之间的关系有一个清晰的理解尤为重要（这是我们在第8章中考虑的内容）。
- en: To bombard you with bad physics analogies for the sheer fun of it. Writing a
    book is a lot of work; you have to find little joys here and there to keep you
    going.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了纯粹的乐趣而向你轰炸糟糕的物理学类比。写一本书是一项艰苦的工作；你必须在这里找到一点点小乐趣来继续前行。
- en: 'Stream-and-Table Basics Or: a Special Theory of Stream and Table Relativity'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流与表的基础知识或者说：流与表的相对论特殊理论
- en: 'The basic idea of streams and tables derives from the database world. Anyone
    familiar with SQL is likely familiar with tables and their core properties, roughly
    summarized as: tables contain rows and columns of data, and each row is uniquely
    identified by some sort of key, either explicit or implicit.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 流和表的基本概念源自数据库世界。熟悉SQL的人可能熟悉表及其核心属性，大致总结为：表包含数据的行和列，每行都由某种键唯一标识，可以是显式的也可以是隐式的。
- en: If you think back to your database systems class in college,¹ you’ll probably
    recall the data structure underlying most databases is an *append-only log*. As
    transactions are applied to a table in the database, those transactions are recorded
    in a log, the contents of which are then serially applied to the table to materialize
    those updates. In streams and tables nomenclature, that log is effectively the
    stream.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回想一下大学数据库系统课程，你可能会记得大多数数据库的数据结构是*追加日志*。当事务被应用到数据库中的表时，这些事务被记录在日志中，日志的内容然后被串行应用到表中以实现这些更新。在流和表的命名法中，该日志实际上就是流。
- en: 'From that perspective, we now understand how to create a table from a stream:
    the table is just the result of applying the transaction log of updates found
    in the stream. But how to do we create a stream from a table? It’s essentially
    the inverse: a stream is a changelog for a table. The motivating example typically
    used for table-to-stream conversion is *materialized views*. Materialized views
    in SQL let you specify a query on a table, which itself is then manifested by
    the database system as another first-class table. This materialized view is essentially
    a cached version of that query, which the database system ensures is always up
    to date as the contents of the source table evolve over time. Perhaps unsurprisingly,
    materialized views are implemented via the changelog for the original table; any
    time the source table changes, that change is logged. The database then evaluates
    that change within the context of the materialized view’s query and applies any
    resulting change to the destination materialized view table.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，我们现在明白了如何从流创建表：表只是应用于流中找到的更新事务日志的结果。但是我们如何从表创建流呢？本质上是相反的：流是表的更改日志。通常用于表到流转换的激励示例是*物化视图*。SQL中的物化视图允许您在表上指定查询，然后数据库系统将其本身作为另一个一流表来实现。这个物化视图本质上是该查询的缓存版本，数据库系统确保它始终保持最新，因为源表的内容随时间演变。也许并不奇怪，物化视图是通过原始表的更改日志实现的；每当源表更改时，该更改都会被记录。然后数据库在物化视图查询的上下文中评估该更改，并将任何结果更改应用于目标物化视图表。
- en: 'Combining these two points together and employing yet another questionable
    physics analogy, we arrive at what one might call the Special Theory of Stream
    and Table Relativity:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 将这两点结合起来，并运用另一个值得怀疑的物理学类比，我们就得到了可以称之为流和表相对论的特殊理论：
- en: Streams → tables
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 流→表
- en: The aggregation of a stream of updates over time yields a table.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随时间对更新流的聚合产生一个表。
- en: Tables → streams
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 表→流
- en: The observation of changes to a table over time yields a stream.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随时间观察表的变化产生一个流。
- en: This is a very powerful pair of concepts, and their careful application to the
    world of stream processing is a big reason for the massive success of Apache Kafka,
    the ecosystem that is built around these underlying principles. However, those
    statements themselves are not quite general enough to allow us to tie streams
    and tables to all of the concepts in the Beam Model. For that, we must go a little
    bit deeper.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一对非常强大的概念，它们对流处理世界的精心应用是Apache Kafka巨大成功的一个重要原因，这个生态系统是围绕这些基本原则构建的。然而，这些陈述本身并不够一般化，无法将流和表与Beam模型中的所有概念联系起来。为此，我们必须深入一点。
- en: Toward a General Theory of Stream and Table Relativity
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朝着流和表相对论的一般理论
- en: 'If we want to reconcile stream/table theory with everything we know of the
    Beam Model, we’ll need to tie up some loose ends, specifically:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要将流/表理论与我们对Beam模型的所有了解联系起来，我们需要解决一些问题，具体来说：
- en: How does batch processing fit into all of this?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理如何融入其中？
- en: What is the relationship of streams to bounded and unbounded datasets?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流与有界和无界数据集的关系是什么？
- en: How do the four *what*, *where*, *when*, *how* questions map onto a streams/tables
    world?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四个“什么”、“哪里”、“何时”、“如何”问题如何映射到流/表世界？
- en: 'As we attempt to do so, it will be helpful to have the right mindset about
    streams and tables. In addition to understanding them in relation to each other,
    as captured by the previous definition, it can be illuminating to define them
    independent of each other. Here’s a simple way of looking at it that will underscore
    some of our future analyses:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们试图这样做时，对流和表有正确的心态将会有所帮助。除了理解它们之间的关系，如前面的定义所捕捉的那样，独立于彼此定义它们也是有启发性的。以下是一个简单的看待它的方式，将强调我们未来分析的一些内容：
- en: Tables are data *at rest*.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表是*静态*数据。
- en: This isn’t to say tables are static in any way; nearly all useful tables are
    continuously changing over time in some way. But at any given time, a snapshot
    of the table provides some sort of picture of the dataset contained together as
    a whole.² In that way, tables act as a conceptual resting place for data to accumulate
    and be observed over time. Hence, data at rest.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这并不是说表在任何方面都是静态的；几乎所有有用的表在某种程度上都在不断变化。但在任何给定时间，表的快照提供了数据集的某种整体图片。在这方面，表充当数据随时间累积和观察的概念休息地。因此，静态数据。
- en: Streams are data *in motion*.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流是*运动中*的数据。
- en: Whereas tables capture a view of the dataset as a whole at a *specific point
    in time*, streams capture the evolution of that data *over time*. Julian Hyde
    is fond of saying streams are like the derivatives of tables, and tables the integrals
    of streams, which is a nice way of thinking about it for you math-minded individuals
    out there. Regardless, the important feature of streams is that they capture the
    inherent movement of data within a table as it changes. Hence, data in motion.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表捕捉了在*特定时间点*数据集的整体视图，而流捕捉了数据*随时间*的演变。Julian Hyde喜欢说流就像表的导数，而表就像流的积分，这对于数学思维的人来说是一个很好的思考方式。不管怎样，流的重要特征是它们捕捉了表内数据的固有运动，因为它改变了。因此，数据在运动。
- en: Though tables and streams are intimately related, it’s important to keep in
    mind that they are very much *not* the same thing, even if there are many cases
    in which one might be fully derived from the other. The differences are subtle
    but important, as we’ll see.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管表和流密切相关，但重要的是要记住它们并不完全相同，即使有许多情况下，一个可能完全源自另一个。差异微妙但重要，我们将会看到。
- en: Batch Processing Versus Streams and Tables
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批处理与流和表
- en: With our proverbial knuckles now cracked, let’s start to tie up some loose ends.
    To begin, we tackle the first one, regarding batch processing. At the end, we’ll
    discover that the resolution to the second issue, regarding the relationship of
    streams to bounded and unbounded data, will fall out naturally from the answer
    for the first. Score one for serendipity.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经做好了准备，让我们开始解决一些问题。首先，我们解决第一个问题，关于批处理。最后，我们会发现，关于流与有界和无界数据的关系的解决方案将自然而然地从第一个问题的答案中得出。这是巧合的一次得分。
- en: A Streams and Tables Analysis of MapReduce
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MapReduce的流和表分析
- en: 'To keep our analysis relatively simple, but solidly concrete, as it were, let’s
    look at how a traditional [MapReduce](http://bit.ly/2uvKRe6) job fits into the
    streams/tables world. As alluded to by its name, a MapReduce job superficially
    consists of two phases: Map and Reduce. For our purposes, though, it’s useful
    to look a little deeper and treat it more like six:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持我们的分析相对简单，但又坚实具体，让我们看看传统[MapReduce](http://bit.ly/2uvKRe6)作业如何适应流/表世界。正如其名称所暗示的，MapReduce作业表面上由两个阶段组成：Map和Reduce。然而，为了我们的目的，更深入地看待它并将其视为六个阶段是有用的：
- en: MapRead
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MapRead
- en: This consumes the input data and preprocesses them a bit into a standard key/value
    form for mapping.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段消耗输入数据并对其进行一些预处理，使其成为映射的标准键值形式。
- en: Map
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Map
- en: This repeatedly (and/or in parallel) consumes a single key/value pair³ from
    the preprocessed input and outputs zero or more key/value pairs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段重复（和/或并行）从预处理输入中消耗一个键值对³，并输出零个或多个键值对。
- en: MapWrite
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: MapWrite
- en: This clusters together sets of Map-phase output values having identical keys
    and writes those key/value-list groups to (temporary) persistent storage. In this
    way, the MapWrite phase is essentially a group-by-key-and-checkpoint operation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段将具有相同键的Map阶段输出值组合在一起，并将这些键值对列表组写入（临时）持久存储。这样，MapWrite阶段本质上是一个按键分组和检查点操作。
- en: ReduceRead
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ReduceRead
- en: This consumes the saved shuffle data and converts them into a standard key/value-list
    form for reduction.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段消耗保存的洗牌数据，并将它们转换成标准的键值对列表形式以便进行减少。
- en: Reduce
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Reduce
- en: This repeatedly (and/or in parallel) consumes a single key and its associated
    value-list of records and outputs zero or more records, all of which may optionally
    remain associated with that same key.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段重复（和/或并行）消耗一个键及其关联的值记录列表，并输出零个或多个记录，所有这些记录都可以选择保持与相同键相关联。
- en: ReduceWrite
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ReduceWrite
- en: This writes the outputs from the Reduce phase to the output datastore.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段将Reduce阶段的输出写入输出数据存储。
- en: Note that the MapWrite and ReduceRead phases sometimes are referred to in aggregate
    as the Shuffle phase, but for our purposes, it’s better to consider them independently.
    It’s perhaps also worth noting that the functions served by the MapRead and ReduceWrite
    phases are more commonly referred to these days as sources and sinks. Digressions
    aside, however, let’s now see how this all relates to streams and tables.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，MapWrite和ReduceRead阶段有时被称为Shuffle阶段的一部分，但对于我们的目的，最好将它们视为独立的阶段。也值得注意的是，MapRead和ReduceWrite阶段的功能如今更常被称为源和汇。然而，撇开这些，现在让我们看看这与流和表的关系。 '
- en: Map as streams/tables
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Map作为流/表
- en: Because we start and end with static⁴ datasets, it should be clear that we begin
    with a table and end with a table. But what do we have in between? Naively, one
    might assume that it’s tables all the way down; after all, batch processing is
    (conceptually) known to consume and produce tables. And if you think of a batch
    processing job as a rough analog of executing a classic SQL query, that feels
    relatively natural. But let’s look a little more closely at what’s really happening,
    step by step.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们从静态⁴数据集开始并结束，所以很明显我们从一个表开始并以一个表结束。但在中间我们有什么？天真地，人们可能会认为中间都是表；毕竟，批处理（概念上）被认为是消耗和产生表。如果你把批处理作业看作是执行经典SQL查询的粗略类比，那感觉相对自然。但让我们更仔细地看看一步步发生了什么。
- en: 'First up, MapRead consumes a table and produces *something*. That something
    is consumed next by the Map phase, so if we want to understand its nature, a good
    place to start would be with the Map phase API, which looks something like this
    in Java:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，MapRead消耗一个表并产生*某物*。接下来，Map阶段消耗了这个东西，所以如果我们想要了解它的性质，一个好的起点就是Map阶段的API，它在Java中看起来像这样：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The map call will be repeatedly invoked for each key/value pair in the input
    table. If you think this sounds suspiciously like the input table is being consumed
    as a stream of records, you’d be right. We look more closely at how the table
    is being converted into a stream later, but for now, suffice it to say that the
    MapRead phase is iterating over the data at rest in the input table and putting
    them into motion in the form of a stream that is then consumed by the Map phase.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入表中的每个键值对，map调用将被重复调用。如果你觉得这听起来像输入表被消耗为记录流，那么你是对的。我们稍后会更仔细地看一下表是如何转换为流的，但现在，可以说MapRead阶段正在迭代输入表中的静态数据，并将它们以流的形式放入运动中，然后被Map阶段消耗。
- en: Next up, the Map phase consumes that stream, and then does what? Because the
    map operation is an element-wise transformation, it’s not doing anything that
    will halt the moving elements and put them to rest. It might change the effective
    cardinality of the stream by either filtering some elements out or exploding some
    elements into multiple elements, but those elements all remain independent from
    one another after the Map phase concludes. So, it seems safe to say that the Map
    phase both consumes a stream as well as produces a stream.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，Map阶段消耗了这个流，然后做了什么？因为映射操作是逐元素转换，它并没有做任何会停止移动元素并使其休息的事情。它可能通过过滤一些元素或将一些元素分解成多个元素来改变流的有效基数，但在Map阶段结束后，这些元素仍然相互独立。因此，可以说Map阶段既消耗流，又产生流。
- en: 'After the Map phase is done, we enter the MapWrite phase. As I noted earlier,
    the MapWrite groups records by key and then writes them in that format to persistent
    storage. The *persistent* part of the write actually isn’t strictly necessary
    at this point as long as there’s persistence *somewhere* (i.e., if the upstream
    inputs are saved and one can recompute the intermediate results from them in cases
    of failure, similar to the approach Spark takes with Resilient Distributed Datasets
    [RDDs]). What *is* important is that the records are grouped together into some
    kind of datastore, be it in memory, on disk, or what have you. This is important
    because, as a result of this grouping operation, records that were previously
    flying past one-by-one in the stream are now brought to rest in a location dictated
    by their key, thus allowing per-key groups to accumulate as their like-keyed brethren
    and sistren arrive. Note how similar this is to the definition of stream-to-table
    conversion provided earlier: *the aggregation of a stream of updates over time
    yields a table*. The MapWrite phase, by virtue of grouping the stream of records
    by their keys, has put those data to rest and thus converted the stream back into
    a table.⁵ Cool!'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在Map阶段完成后，我们进入MapWrite阶段。正如我之前所指出的，MapWrite通过键分组记录，然后以该格式将它们写入持久存储。实际上，在这一点上，写入的*持久*部分实际上并不是严格必要的，只要*某个地方*有持久性（即，如果上游输入被保存，并且在失败的情况下可以从中重新计算中间结果，类似于Spark对Resilient
    Distributed Datasets（RDDs）采取的方法）。重要的是记录被组合到某种数据存储中，无论是在内存中，磁盘上，还是其他位置。这很重要，因为由于这个分组操作，以前在流中一个接一个地飞过的记录现在被带到由它们的键所指示的位置，从而允许每个键组积累，就像它们的同类兄弟姐妹到达一样。请注意，这与之前提供的流到表转换的定义有多么相似：*随着时间的推移，对更新流的聚合产生了一个表*。通过根据它们的键对记录进行分组，MapWrite阶段使这些数据得到休息，从而将流转换回表。⁵酷！
- en: We’re now halfway through the MapReduce, so, using Figure 6-1, let’s recap what
    we’ve seen so far.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了MapReduce的一半，所以，使用图6-1，让我们回顾一下到目前为止我们所看到的内容。
- en: We’ve gone from table to stream and back again across three operations. MapRead
    converted the table into a stream, which was then transformed into a new stream
    by Map (via the user’s code), which was then converted back into a table by MapWrite.
    We’re going to find that the next three operations in the MapReduce look very
    similar, so I’ll go through them more quickly, but I still want to point out one
    important detail along the way.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经通过三个操作从表转换为流，然后再转换回来。MapRead将表转换为流，然后Map（通过用户的代码）将其转换为新流，然后MapWrite将其转换回表。我们将发现MapReduce中的接下来的三个操作看起来非常相似，所以我会更快地通过它们，但我仍然想在途中指出一个重要的细节。
- en: '![](img/stsy_0601.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_0601.png)'
- en: Figure 6-1\. Map phases in a MapReduce. Data in a table are converted to a stream
    and back again.
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1。 MapReduce中的映射阶段。表中的数据被转换为流，然后再转换回去。
- en: Reduce as streams/tables
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将流/表减少
- en: Picking up where we left off after the MapWrite phase, ReduceRead itself is
    relatively uninteresting. It’s basically identical to MapRead, except that the
    values being read are singleton lists of values instead of singleton values, because
    the data stored by MapWrite were key/value-list pairs. But it’s still just iterating
    over a snapshot of a table to convert it into a stream. Nothing new here.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapWrite阶段之后，ReduceRead本身相对不那么有趣。它基本上与MapRead相同，只是读取的值是值的单例列表，而不是单个值，因为MapWrite存储的数据是键/值列表对。但它仍然只是在表的快照上进行迭代，将其转换为流。这里没有什么新东西。
- en: And even though it *sounds* like it might be interesting, Reduce in this context
    is really just a glorified Map phase that happens to receive a list of values
    for each key instead of a single value. So it’s still just mapping single (composite)
    records into zero or more new records. Nothing particularly new here, either.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在这种情况下，Reduce听起来可能很有趣，但实际上它只是一个有点特别的Map阶段，它恰好接收每个键的值列表，而不是单个值。因此，它仍然只是将单个（复合）记录映射为零个或多个新记录。这里也没有什么特别新的东西。
- en: ReduceWrite is the one that’s a bit noteworthy. We know already that this phase
    must convert a stream to a table, given that Reduce produces a stream and the
    final output is a table. But how does that happen? If I told you it was a direct
    result of key-grouping the outputs from the previous phase into persistent storage,
    just like we saw with MapWrite, you might believe me, until you remembered that
    I noted earlier that key-association was an *optional* feature of the Reduce phase.
    With that feature enabled, ReduceWrite *is* essentially identical to MapWrite.⁶
    But if that feature is disabled and the outputs from Reduce have no associated
    keys, what exactly is happening to bring those data to rest?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ReduceWrite是一个有点值得注意的阶段。我们已经知道这个阶段必须将流转换为表，因为Reduce产生了一个流，最终输出是一个表。但是这是如何发生的呢？如果我告诉你，这是由于将前一阶段的输出键分组到持久存储中的直接结果，就像我们在MapWrite中看到的那样，你可能会相信我，直到你记得我之前指出的Reduce阶段的键关联是一个*可选*特性。启用了该特性，ReduceWrite
    *基本上* 与MapWrite相同。⁶但是如果禁用了该特性，并且Reduce的输出没有关联的键，那么到底发生了什么来使这些数据得到休息呢？
- en: To understand what’s going on, it’s useful to think again of the semantics of
    a SQL table. Though often recommended, it’s not strictly required for a SQL table
    to have a primary key uniquely identifying each row. In the case of keyless tables,
    each row that is inserted is considered to be a new, independent row (even if
    the data therein are identical to one or more extant rows in the table), much
    as though there were an implicit AUTO_INCREMENT field being used as the key (which
    incidentally, is what’s effectively happening under the covers in most implementations,
    even though the “key” in this case might just be some physical block location
    that is never exposed or expected to be used as a logical identifier). This implicit
    unique key assignment is precisely what’s happening in ReduceWrite with unkeyed
    data. Conceptually, there’s still a group-by-key operation happening; that’s what
    brings the data to rest. But lacking a user-supplied key, the ReduceWrite is treating
    each record as though it has a new, never-before-seen key, and effectively grouping
    each record with itself, resulting again in data at rest.⁷
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解正在发生的事情，重新思考SQL表的语义是有用的。虽然经常建议，但并不严格要求SQL表具有唯一标识每行的主键。在无键表的情况下，插入的每一行都被视为新的、独立的行（即使其中的数据与表中的一个或多个现有行的数据相同），就像有一个隐式的AUTO_INCREMENT字段被用作键一样（顺便说一句，在大多数实现中，实际上就是这样的，即使在这种情况下，“键”可能只是一些从未公开或预期用作逻辑标识符的物理块位置）。这种隐式的唯一键分配正是在没有键数据的ReduceWrite中发生的。从概念上讲，仍然发生着按键分组操作；这就是将数据置于静止状态的原因。但是由于缺少用户提供的键，ReduceWrite将每个记录都视为具有新的、以前从未见过的键，并有效地将每个记录与自身分组，再次导致数据处于静止状态。
- en: Take a look at Figure 6-2, which shows the entire pipeline from the perspective
    of stream/tables. You can see that it’s a sequence of TABLE → STREAM → STREAM
    → TABLE → STREAM → STREAM → TABLE. Even though we’re processing bounded data and
    even though we’re doing what we traditionally think of as batch processing, it’s
    really just streams and tables under the covers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下图6-2，它显示了从流/表的角度看整个管道。你可以看到这是一个TABLE → STREAM → STREAM → TABLE → STREAM →
    STREAM → TABLE的序列。即使我们处理的是有界数据，即使我们正在进行传统意义上的批处理，实际上它只是在表面下进行流和表处理。
- en: '![](img/stsy_0602.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_0602.png)'
- en: Figure 6-2\. Map and Reduce phases in a MapReduce, viewed from the perspective
    of streams and tables
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2。从流和表的角度看MapReduce中的Map和Reduce阶段
- en: Reconciling with Batch Processing
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与批处理的调和
- en: So where does this leave us with respect to our first two questions?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这对我们的前两个问题有什么影响呢？
- en: '**Q:** How does batch processing fit into stream/table theory?'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Q:** 批处理如何适应流/表理论？'
- en: '**A:** Quite nicely. The basic pattern is as follows:'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**A:** 非常好。基本模式如下：'
- en: Tables are read in their entirety to become streams.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 表被完整地读取成为流。
- en: Streams are processed into new streams until a grouping operation is hit.
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流被处理成新的流，直到遇到分组操作。
- en: Grouping turns the stream into a table.
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分组将流转换为表。
- en: Steps a through c repeat until you run out of stages in the pipeline.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤a到c重复，直到管道中没有阶段为止。
- en: '**Q:** How do streams relate to bounded/unbounded data?'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Q:** 流如何与有界/无界数据相关联？'
- en: '**A:** As we can see from the MapReduce example, streams are simply the in-motion
    form of data, regardless of whether they’re bounded or unbounded.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**A:** 从MapReduce示例中可以看出，流只是数据的运动形式，无论它们是有界的还是无界的。'
- en: 'Taken from this perspective, it’s easy to see that stream/table theory isn’t
    remotely at odds with batch processing of bounded data. In fact, it only further
    supports the idea I’ve been harping on that batch and streaming really aren’t
    that different: at the end of the of day, it’s streams and tables all the way
    down.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，很容易看出流/表理论与有界数据的批处理并不矛盾。事实上，它进一步支持了我一直在强调的观点，即批处理和流处理并没有那么不同：归根结底，它一直都是流和表。
- en: With that, we’re well on our way toward a general theory of streams and tables.
    But to wrap things up cleanly, we last need to revisit the four *what*/*where*/*when*/*how*
    questions within the streams/tables context, to see how they all relate.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们已经在通向流和表的一般理论的道路上了。但是为了清晰地总结，我们最后需要重新讨论流/表上下文中的四个*什么*/*哪里*/*何时*/*如何*问题，看看它们如何相关。
- en: '*What*, *Where*, *When*, and *How* in a Streams and Tables World'
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*什么*、*哪里*、*何时*和*如何*在流和表的世界中'
- en: 'In this section, we look at each of the four questions and see how they relate
    to streams and tables. We’ll also answer any questions that may be lingering from
    the previous section, one big one being: if grouping is the thing that brings
    data to rest, what precisely is the “ungrouping” inverse that puts them in motion?
    More on that later. But for now, on to transformations.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看看这四个问题中的每一个，看看它们如何与流和表相关。我们还将回答可能从上一节中挥之不去的任何问题，其中一个重要的问题是：如果分组是将数据置于静止状态的原因，那么“取消分组”的逆过程究竟是什么？稍后再说。但现在，让我们来看看转换。
- en: '*What*: Transformations'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*什么*：转换'
- en: 'In Chapter 3, we learned that transformations tell us *what* the pipeline is
    computing; that is, whether it’s building models, counting sums, filtering spam,
    and so on. We saw in the earlier MapReduce example that four of the six stages
    answered *what* questions:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们了解到转换告诉我们管道正在计算的*是什么*；也就是说，它是在构建模型、计算总和、过滤垃圾邮件等。我们在前面的MapReduce示例中看到，六个阶段中的四个回答了*什么*问题：
- en: Map and Reduce both applied the pipeline author’s element-wise transformation
    on each key/value or key/value-list pair in the input stream, respectively, yielding
    a new, transformed stream.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Map和Reduce都对输入流中的每个键/值或键/值列表对应用了管道作者的逐元素转换，分别产生了一个新的、转换后的流。
- en: MapWrite and ReduceWrite both grouped the outputs from the previous stage according
    to the key assigned by that stage (possibly implicitly, in the optional Reduce
    case), and in doing so transformed the input stream into an output table.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapWrite和ReduceWrite都根据上一阶段分配的键对输出进行分组（在可选的Reduce情况下可能是隐式的），这样做可以将输入流转换为输出表。
- en: 'Viewed in that light, you can see that there are essentially two types of *what*
    transforms from the perspective of stream/table theory:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，你可以看到从流/表理论的角度来看，基本上有两种*what*转换类型：
- en: Nongrouping
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 非分组
- en: These operations (as we saw in Map and Reduce) simply accept a stream of records
    and produce a new, transformed stream of records on the other side. Examples of
    nongrouping transformations are filters (e.g., removing spam messages), exploders
    (i.e., splitting apart a larger composite record into its constituent parts),
    and mutators (e.g., divide by 100), and so on.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作（正如我们在Map和Reduce中看到的）只是接受一系列记录，并在另一侧生成一系列新的转换记录。非分组转换的示例包括过滤器（例如，删除垃圾邮件消息）、扩展器（即，将较大的复合记录拆分为其组成部分）和变换器（例如，除以100），等等。
- en: Grouping
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 分组
- en: These operations (as we saw in MapWrite and ReduceWrite) accept a stream of
    records and group them together in some way, thereby transforming the stream into
    a table. Examples of grouping transformations are joins, aggregations, list/set
    accumulation, changelog application, histogram creation, machine learning model
    training, and so forth.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作（正如我们在MapWrite和ReduceWrite中看到的）接受一系列记录，并以某种方式将它们组合在一起，从而将流转换为表。分组转换的示例包括连接、聚合、列表/集合累积、变更日志应用、直方图创建、机器学习模型训练等。
- en: To get a better sense for how all of this ties together, let’s look at an updated
    version of Figure 2-2, where we first began to look at transformations. To save
    you jumping back there to see what we were talking about, Example 6-1 contains
    the code snippet we were using.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解所有这些是如何联系在一起的，让我们看一下图2-2的更新版本，我们首次开始研究转换。为了避免你跳回去看我们在谈论什么，示例6-1包含了我们正在使用的代码片段。
- en: Example 6-1\. Summation pipeline
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-1。求和管道
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This pipeline is simply reading in input data, parsing individual team member
    scores, and then summing those scores per team. The event-time/processing-time
    visualization of it looks like the diagram presented in Figure 6-3.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道只是简单地读取输入数据，解析单个团队成员的分数，然后对每个团队的分数进行求和。它的事件时间/处理时间可视化看起来像图6-3中呈现的图表。
- en: <assets/stsy_0603.mp4>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0603.mp4>
- en: '![Event-time/processing-time view of classic batch processing](img/stsy_0603.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![经典批处理的事件时间/处理时间视图](img/stsy_0603.png)'
- en: Figure 6-3\. Event-time/processing-time view of classic batch processing
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3。经典批处理的事件时间/处理时间视图
- en: Figure 6-4 depicts a more topological view of this pipeline over time, rendered
    from a streams-and-tables perspective.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-4描述了随着时间推移，从流和表的角度呈现的管道的更顶层视图。
- en: <assets/stsy_0604.mp4>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0604.mp4>
- en: '![Streams and tables view of classic batch processing](img/stsy_0604.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![经典批处理的流和表视图](img/stsy_0604.png)'
- en: Figure 6-4\. Streams and tables view of classic batch processing
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4。经典批处理的流和表视图
- en: In the streams and tables version of this visualization, the passage of time
    is manifested by scrolling the graph area downward in the processing-time dimension
    (y-axis) as time advances. The nice thing about rendering things this way is that
    it very clearly calls out the difference between nongrouping and grouping operations.
    Unlike our previous diagrams, in which I elided all initial transformations in
    the pipeline other than the `Sum.integersByKey`, I’ve included the initial parsing
    operation here, as well, because the nongrouping aspect of the parsing operation
    provides a nice contrast to the grouping aspect of the summation. Viewed in this
    light, it’s very easy to see the difference between the two. The nongrouping operation
    does nothing to halt the motion of the elements in the stream, and as a result
    yields another stream on the other side. In contrast, the grouping operation brings
    all the elements in the stream to rest as it adds them together into the final
    sum. Because this example was running on a batch processing engine over bounded
    data, the final results are emitted only after the end of the input is reached.
    As we noted in Chapter 2 this example is sufficient for bounded data, but is too
    limiting in the context of unbounded data because the input will theoretically
    never end. But is it really insufficient?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种可视化的流和表版本中，时间的流逝通过在处理时间维度（y轴）向下滚动图形区域来体现。以这种方式呈现事物的好处在于，它非常清楚地指出了非分组和分组操作之间的差异。与我们以前的图表不同，在那些图表中，我省略了管道中除了`Sum.integersByKey`之外的所有初始转换操作，但在这里，我也包括了初始的解析操作，因为解析操作的非分组方面与求和的分组方面形成了鲜明对比。从这个角度来看，很容易看出两者之间的区别。非分组操作对流中的元素运动没有任何影响，因此在另一侧产生另一个流。相反，分组操作将流中的所有元素汇聚在一起，将它们相加得到最终的总和。因为这个示例是在有界数据上运行的批处理引擎上运行的，最终结果只有在输入结束后才会被发出。正如我们在第2章中指出的那样，这个示例对有界数据是足够的，但在无界数据的情况下太过限制，因为理论上输入永远不会结束。但它真的不够吗？
- en: Looking at the new streams/tables portion of the diagram, if all we’re doing
    is calculating sums as our final results (and not actually transforming those
    sums in any additional way further downstream within the pipeline), the table
    we created with our grouping operation has our answer sitting right there, evolving
    over time as new data arrive. Why don’t we just read our results from there?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表的新流/表部分来看，如果我们所做的只是计算总和作为我们的最终结果（而不在管道中的下游实际上以任何其他方式转换这些总和），那么我们用分组操作创建的表中就有我们的答案，随着新数据的到来而不断演变。为什么我们不直接从那里读取我们的结果呢？
- en: 'This is exactly the point being made by the folks championing stream processors
    as a database⁸ (primarily the Kafka and Flink crews): anywhere you have a grouping
    operation in your pipeline, you’re creating a table that includes what is effectively
    the output values of that portion of the stage. If those output values happen
    to be the final thing your pipeline is calculating, you don’t need to rematerialize
    them somewhere else if you can read them directly out of that table. Besides providing
    quick and easy access to results as they evolve over time, this approach saves
    on compute resources by not requiring an additional sink stage in the pipeline
    to materialize the outputs, yields disk savings by eliminating redundant data
    storage, and obviates the need for any engineering work building the aforementioned
    sink stages.⁹ The only major caveat is that you need to take care to ensure that
    only the data processing pipeline has the ability to make modifications to the
    table. If the values in the table can change out from under the pipeline due to
    external modification, all bets are off regarding consistency guarantees.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是那些支持流处理器作为数据库的人所要表达的观点⁸（主要是Kafka和Flink团队）：在管道中进行分组操作时，实际上创建了一个包含该阶段输出值的表。如果这些输出值恰好是管道正在计算的最终结果，那么如果可以直接从该表中读取它们，就不需要在其他地方重新生成它们。除了在时间演变过程中提供快速和便捷的结果访问外，这种方法通过不需要在管道中添加额外的接收阶段来节省计算资源，通过消除冗余数据存储来节省磁盘空间，并且消除了构建前述接收阶段的任何工程工作的需要。⁹
    唯一的主要注意事项是，您需要小心确保只有数据处理管道有能力对表进行修改。如果表中的值可以在管道之外由外部修改而发生变化，那么关于一致性保证的所有赌注都将失效。
- en: A number of folks in the industry have been recommending this approach for a
    while now, and it’s being put to great use in a variety of scenarios. We’ve seen
    MillWheel customers within Google do the same thing by serving data directly out
    of their Bigtable-based state tables, and we’re in the process of adding first-class
    support for accessing state from outside of your pipeline in the C++–based Apache
    Beam equivalent we use internally at Google (Google Flume); hopefully those concepts
    will make their way to Apache Beam proper someday soon, as well.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 行业中有许多人一直在推荐这种方法，并且它正在被广泛应用于各种场景中。我们已经看到Google内部的MillWheel客户通过直接从基于Bigtable的状态表中提供数据来做同样的事情，而且我们正在为从Google内部使用的C++-based
    Apache Beam等效版本（Google Flume）中的管道外部访问状态添加一流支持；希望这些概念将来某一天能够真正地传递到Apache Beam。
- en: Now, reading from the state tables is great if the values therein are your final
    results. But, if you have more processing to perform downstream in the pipeline
    (e.g., imagine our pipeline was actually computing the top scoring team), we still
    need some better way to cope with unbounded data, allowing us to transform the
    table back into a stream in a more incremental fashion. For that, we’ll want to
    journey back through the remaining three questions, beginning with windowing,
    expanding into triggering, and finally tying it all together with accumulation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果从状态表中读取值是很好的，如果其中的值是您的最终结果。但是，如果您在管道下游有更多的处理要执行（例如，想象一下我们的管道实际上正在计算得分最高的团队），我们仍然需要一种更好的方式来处理无界数据，允许我们以更增量的方式将表转换回流。为此，我们将希望通过剩下的三个问题的旅程，从窗口化开始，扩展到触发，最后将其与累积结合起来。
- en: '*Where*: Windowing'
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*在哪里*：窗口化'
- en: 'As we know from Chapter 3, windowing tells us *where* in event time grouping
    occurs. Combined with our earlier experiences, we can thus also infer it must
    play a role in stream-to-table conversion because grouping is what drives table
    creation. There are really two aspects of windowing that interact with stream/table
    theory:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从第3章所知，窗口化告诉我们在事件时间中分组发生的位置。结合我们之前的经验，我们也可以推断它必须在流到表转换中起到作用，因为分组是驱动表创建的原因。窗口化有两个方面与流/表理论相互作用：
- en: Window assignment
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口分配
- en: This effectively just means placing a record into one or more windows.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上意味着将记录放入一个或多个窗口中。
- en: Window merging
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口合并
- en: This is the logic that makes dynamic, data-driven types of windows, such as
    sessions, possible.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是使动态的、数据驱动类型的窗口（例如会话）成为可能的逻辑。
- en: The effect of window assignment is quite straightforward. When a record is conceptually
    placed into a window, the definition of the window is essentially combined with
    the user-assigned key for that record to create an implicit composite key used
    at grouping time.¹⁰ Simple.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口分配的效果非常直接。当记录在概念上放置到窗口中时，窗口的定义基本上与该记录的用户分配的键结合起来，以在分组时创建一个隐式的复合键。¹⁰ 简单。
- en: For completeness, let’s take another look at the original windowing example
    from Chapter 3, but from a streams and tables perspective. If you recall, the
    code snippet looked something like Example 6-2 (with parsing *not* elided this
    time).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整起见，让我们再次从第3章的原始窗口化示例中看一看，但从流和表的角度来看。如果你还记得，代码片段看起来有点像示例6-2（这次没有省略解析）。
- en: Example 6-2\. Summation pipeline
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-2。求和管道
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And the original visualization looked like that shown in Figure 6-5.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 原始可视化效果如图6-5所示。
- en: <assets/stsy_0605.mp4>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0605.mp4>
- en: '![Event-time/processing-time view of windowed summation on a batch engine](img/stsy_0605.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![批处理引擎上窗口化求和的事件时间/处理时间视图](img/stsy_0605.png)'
- en: Figure 6-5\. Event-time/processing-time view of windowed summation on a batch
    engine
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5。批处理引擎上窗口化求和的事件时间/处理时间视图
- en: And now, Figure 6-6 shows the streams and tables version.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，图6-6显示了流和表的版本。
- en: <assets/stsy_0606.mp4>
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0606.mp4>
- en: '![Streams and tables view of windowed summation on a batch engine](img/stsy_0606.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![批处理引擎上窗口化求和的流和表视图](img/stsy_0606.png)'
- en: Figure 6-6\. Streams and tables view of windowed summation on a batch engine
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6。批处理引擎上窗口化求和的流和表视图
- en: As you might expect, this looks remarkably similar to Figure 6-4, but with four
    groupings in the table (corresponding to the four windows occupied by the data)
    instead of just one. But as before, we must wait until the end of our bounded
    input is reached before emitting results. We look at how to address this for unbounded
    data in the next section, but first let’s touch briefly on merging windows.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能期望的那样，这看起来与图6-4非常相似，但表中有四个分组（对应数据占据的四个窗口），而不是只有一个。但与以前一样，我们必须等到有界输入结束后才能发出结果。我们将在下一节讨论如何处理无界数据，但首先让我们简要谈一下合并窗口。
- en: Window merging
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 窗口合并
- en: Moving on to merging, we’ll find that the effect of window merging is more complicated
    than window assignment, but still straightforward when you think about the logical
    operations that would need to happen. When grouping a stream into windows that
    can merge, that grouping operation has to take into account all of the windows
    that could possibly merge together. Typically, this is limited to windows whose
    data all have the same key (because we’ve already established that windowing modifies
    grouping to not be just by key, but also key and window). For this reason, the
    system doesn’t really treat the key/window pair as a flat composite key, but rather
    as a hierarchical key, with the user-assigned key as the root, and the window
    a child component of that root. When it comes time to actually group data together,
    the system first groups by the root of the hierarchy (the key assigned by the
    user). After the data have been grouped by key, the system can then proceed with
    grouping by window within that key (using the child components of the hierarchical
    composite keys). This act of grouping by window is where window merging happens.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来讨论合并，我们会发现窗口合并的影响比窗口分配更加复杂，但是当你考虑到需要发生的逻辑操作时，它仍然是直接的。当将流分组到可以合并的窗口时，该分组操作必须考虑到所有可能合并在一起的窗口。通常，这仅限于数据都具有相同键的窗口（因为我们已经确定窗口化修改了分组不仅仅是按键，还有键和窗口）。因此，系统实际上并不将键/窗口对视为一个平面复合键，而是将其视为分层键，用户分配的键是根，窗口是该根的子组件。当实际上将数据分组在一起时，系统首先按分层复合键的根（用户分配的键）进行分组。在按键分组后，系统可以继续在该键内按窗口进行分组（使用分层复合键的子组件）。按窗口进行分组就是窗口合并发生的地方。
- en: What’s interesting from a streams and tables perspective is how this window
    merging changes the mutations that are ultimately applied to a table; that is,
    how it modifies the changelog that dictates the contents of the table over time.
    With nonmerging windows, each new element being grouped results in a single mutation
    to the table (to add that element to the group for the element’s key+window).
    With merging windows, the act of grouping a new element can result in one or more
    existing windows being merged with the new window. So, the merging operation must
    inspect all of the existing windows for the current key, figure out which windows
    can merge with this new window, and then atomically commit deletes for the old
    unmerged windows in conjunction with an insert for the new merged window into
    the table. This is why systems that support merging windows typically define the
    unit of atomicity/parallelization as key, rather than key+window. Otherwise, it
    would be impossible (or at least much more expensive) to provide the strong consistency
    needed for correctness guarantees. When you begin to look at it in this level
    of detail, you can see why it’s so nice to have the system taking care of the
    nasty business of dealing with window merges. For an even closer view of window
    merging semantics, I refer you to section 2.2.2 of [“The Dataflow Model”](http://bit.ly/2sXgVJ3).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从流和表的角度来看，有趣的是窗口合并如何改变最终应用于表的突变；也就是说，它如何修改了随时间指示表内容的更改日志。对于非合并窗口，每个新分组的元素都会导致对表的单个突变（将该元素添加到元素的键+窗口的组中）。对于合并窗口，分组新元素的操作可能导致一个或多个现有窗口与新窗口合并。因此，合并操作必须检查当前键的所有现有窗口，找出哪些窗口可以与新窗口合并，然后原子地删除旧未合并窗口并插入新合并窗口到表中。这就是为什么支持合并窗口的系统通常将原子性/并行性的单位定义为键，而不是键+窗口。否则，要提供正确性保证所需的强一致性将是不可能的（或者至少更加昂贵）。当你开始以这种细节水平来看待它时，你就会明白为什么让系统来处理窗口合并的麻烦事是多么美妙。要更近距离地了解窗口合并语义，我建议你参考[“数据流模型”](http://bit.ly/2sXgVJ3)的2.2.2节。
- en: At the end of the day, windowing is really just a minor alteration to the semantics
    of grouping, which means it’s a minor alteration to the semantics of stream-to-table
    conversion. For window assignment, it’s as simple as incorporating the window
    into an implicit composite key used at grouping time. When window merging becomes
    involved, that composite key is treated more like a hierarchical key, allowing
    the system to handle the nasty business of grouping by key, figuring out window
    merges within that key, and then atomically applying all the necessary mutations
    to the corresponding table for us. Hooray for layers of abstraction!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 归根结底，窗口化实际上只是对分组语义的轻微改变，这意味着它对流到表转换的语义也是轻微的改变。对于窗口分配，就像在分组时将窗口合并到隐式复合键中一样简单。当涉及窗口合并时，这个复合键更像是一个分层键，允许系统处理按键分组，找出该键内的窗口合并，然后原子地应用所有必要的突变到相应的表中。抽象层次的叠加真是太好了！
- en: All that said, we still haven’t actually addressed the problem of converting
    a table to a stream in a more incremental fashion in the case of unbounded data.
    For that, we need to revisit triggers.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们实际上还没有解决将表转换为流的问题，特别是在无界数据的情况下以更增量的方式进行。为此，我们需要重新审视触发器。
- en: '*When*: Triggers'
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*何时*：触发器'
- en: We learned in Chapter 3 that we use triggers to dictate *when* the contents
    of a window will be materialized (with watermarks providing a useful signal of
    input completeness for certain types of triggers). After data have been grouped
    together into a window, we use triggers to dictate when that data should be sent
    downstream. In streams/tables terminology, we understand that grouping means stream-to-table
    conversion. From there, it’s a relatively small leap to see that triggers are
    the complement to grouping; in other words, that “ungrouping” operation we were
    grasping for earlier. Triggers are what drive table-to-stream conversion.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第3章学到，我们使用触发器来决定窗口的内容何时被实现（水印为某些类型的触发器提供了输入完整性的有用信号）。在数据被分组到窗口中之后，我们使用触发器来决定何时将这些数据发送到下游。在流/表术语中，我们了解到分组意味着流到表的转换。从那里，我们可以很容易地看到触发器是分组的补充；换句话说，这是我们之前所探索的“取消分组”操作。触发器是驱动表到流转换的东西。
- en: In streams/tables terminology, triggers are special procedures applied to a
    table that allow for data within that table to be materialized in response to
    relevant events. Stated that way, they actually sound suspiciously similar to
    classic database triggers. And indeed, the choice of name here was no coincidence;
    they are essentially the same thing. When you specify a trigger, you are in effect
    writing code that then is evaluated for every row in the state table as time progresses.
    When that trigger fires, it takes the corresponding data that are currently at
    rest in the table and puts them into motion, yielding a new stream.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在流/表术语中，触发器是应用于表的特殊程序，允许对表中的数据在响应相关事件时进行实现。以这种方式陈述，它们实际上听起来非常类似于经典数据库触发器。事实上，这里选择的名称并非巧合；它们本质上是相同的东西。当您指定触发器时，实际上是在随着时间的推移为状态表中的每一行编写代码。当触发器触发时，它会获取当前静止在表中的相应数据，并将它们置于运动中，产生一个新的流。
- en: Let’s return to our examples. We’ll begin with the simple per-record trigger
    from Chapter 2, which simply emits a new result every time a new record arrives.
    The code and event-time/processing-time visualization for that example is shown
    in Example 6-3. Figure 6-7 presents the results.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的例子。我们将从第2章的简单的每记录触发器开始，该触发器在每次到达新记录时都会发出新的结果。该示例的代码和事件时间/处理时间可视化如示例6-3所示。图6-7呈现了结果。
- en: Example 6-3\. Triggering repeatedly with every record
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-3. 每条记录重复触发
- en: '[PRE3]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: <assets/stsy_0607.mp4>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0607.mp4>
- en: '![Per-record triggering on a streaming engine](img/stsy_0607.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![在流引擎上每条记录触发](img/stsy_0607.png)'
- en: Figure 6-7\. Streams and tables view of windowed summation on a batch engine
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7. 批处理引擎上窗口求和的流和表视图
- en: As before, new results are materialized every time a new record is encountered.
    Rendered in a streams and tables type of view, this diagram would look like Figure 6-8.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '与以前一样，每次遇到新记录时都会实现新的结果。以流和表类型的视图呈现，该图将类似于图6-8。 '
- en: <assets/stsy_0608.mp4>
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0608.mp4>
- en: '![Streams and tables view of windowed summation with per-record triggering
    on a streaming engine](img/stsy_0608.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![在流引擎上每条记录触发的窗口求和的流和表视图](img/stsy_0608.png)'
- en: Figure 6-8\. Streams and tables view of windowed summation with per-record triggering
    on a streaming engine
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8. 流引擎上每条记录触发的窗口求和的流和表视图
- en: An interesting side effect of using per-record triggers is how it somewhat masks
    the effect of data being brought to rest, given that they are then immediately
    put back into motion again by the trigger. Even so, the aggregate artifact from
    the grouping remains at rest in the table, as the ungrouped stream of values flows
    away from it.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用每条记录触发器的一个有趣的副作用是它在某种程度上掩盖了数据被静止的效果，因为它们随后立即被触发器重新置于运动中。即便如此，从分组中产生的聚合物件仍然静止在表中，而未分组的值流则从中流走。
- en: To get a better sense of the at-rest/in-motion relationship, let’s skip forward
    in our triggering examples to the basic watermark completeness streaming example
    from Chapter 2, which simply emitted results when complete (due to the watermark
    passing the end of the window). The code and event-time/processing-time visualization
    for that example are presented in Example 6-4 (note that I’m only showing the
    heuristic watermark version here, for brevity and ease of comparison) and Figure 6-9
    illustrates the results.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解静止/运动关系，让我们跳过我们的触发示例，转到第2章的基本水印完整性流示例，该示例在完成时简单地发出结果（由于水印通过窗口末端）。该示例的代码和事件时间/处理时间可视化如示例6-4所示（请注意，我这里只显示了启发式水印版本，以便简洁和比较），图6-9说明了结果。
- en: Example 6-4\. Watermark completeness trigger
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-4. 水印完整性触发器
- en: '[PRE4]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: <assets/stsy_0609.mp4>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0609.mp4>
- en: '![Event-time/processing-time view of windowed summation with a heuristic watermark
    on a streaming engine](img/stsy_0609.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![带有启发式水印的窗口求和的事件时间/处理时间视图](img/stsy_0609.png)'
- en: Figure 6-9\. Event-time/processing-time view of windowed summation with a heuristic
    watermark on a streaming engine
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-9. 流引擎上带有启发式水印的窗口求和的事件时间/处理时间视图
- en: Thanks to the trigger specified in Example 6-4, which declares that windows
    should be materialized when the watermark passes them, the system is able to emit
    results in a progressive fashion as the otherwise unbounded input to the pipeline
    becomes more and more complete. Looking at the streams and tables version in Figure 6-10,
    it looks as you might expect.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在示例6-4中指定的触发器声明窗口应在水印通过它们时实现，系统能够在管道的无界输入变得越来越完整时以渐进的方式发出结果。在图6-10中的流和表版本中，它看起来就像您所期望的那样。
- en: <assets/stsy_0610.mp4>
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0610.mp4>
- en: '![Streams and tables view of windowed summation with a heuristic watermark
    on a streaming engine](img/stsy_0610.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![带有启发式水印的窗口求和的流和表视图](img/stsy_0610.png)'
- en: Figure 6-10\. Streams and tables view of windowed summation with a heuristic
    watermark on a streaming engine
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-10. 带有启发式水印的窗口求和的流和表视图
- en: In this version, you can see very clearly the ungrouping effect triggers have
    on the state table. As the watermark passes the end of each window, it pulls the
    result for that window out of the table and sets it in motion downstream, separate
    from all the other values in the table. We of course still have the late data
    issue from before, which we can solve again with the more comprehensive trigger
    shown in Example 6-5.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个版本中，您可以非常清楚地看到触发器对状态表的取消分组效果。随着水印通过每个窗口的末尾，它将该窗口的结果从表中取出，并将其与表中的所有其他值分开，向下游传送。当然，我们仍然有之前的迟到数据问题，我们可以再次使用示例6-5中显示的更全面的触发器来解决。
- en: Example 6-5\. Early, on-time, and late firings via the early/on-time/late API
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-5。通过早期/准时/迟API进行早期、准时和迟触发
- en: '[PRE5]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The event-time/processing-time diagram looks like Figure 6-11.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 事件时间/处理时间图看起来像图6-11。
- en: <assets/stsy_0611.mp4>
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0611.mp4>
- en: '![Event-time/processing-time view of windowed summation on a streaming engine
    with early/on-time/late trigger](img/stsy_0611.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![具有早期/准时/迟触发器的流引擎上窗口求和的事件时间/处理时间视图](img/stsy_0611.png)'
- en: Figure 6-11\. Event-time/processing-time view of windowed summation on a streaming
    engine with early/on-time/late trigger
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11。事件时间/处理时间视图的窗口求和在具有早期/准时/迟触发器的流引擎上
- en: Whereas the streams and tables version looks like that shown in Figure 6-12.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 而流和表版本看起来像图6-12所示。
- en: <assets/stsy_0612.mp4>
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/stsy_0612.mp4>
- en: '![Streams and tables view of windowed summation on a streaming engine with
    early/on-time/late trigger](img/stsy_0612.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![具有早期/准时/迟触发器的流引擎上窗口求和的流和表视图](img/stsy_0612.png)'
- en: Figure 6-12\. Streams and tables view of windowed summation on a streaming engine
    with early/on-time/late trigger
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-12。具有早期/准时/迟触发器的流引擎上窗口求和的流和表视图
- en: This version makes even more clear the ungrouping effect triggers have, rendering
    an evolving view of the various independent pieces of the table into a stream,
    as dictated by the triggers specified in Example 6-6.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本更清楚地显示了触发器的取消分组效果，根据示例6-6中指定的触发器，将表的各个独立部分呈现为流的不断变化视图。
- en: The semantics of all the concrete triggers we’ve talked about so far (event-time,
    processing-time, count, composites like early/on-time/late, etc.) are just as
    you would expect when viewed from the streams/tables perspective, so they aren’t
    worth further discussion. However, we haven’t yet spent much time talking about
    what triggers look like in a classic batch processing scenario. Now that we understand
    what the underlying streams/tables topology of a batch pipeline looks like, this
    is worth touching upon briefly.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们谈到的所有具体触发器的语义（事件时间、处理时间、计数、早期/准时/迟等复合触发器等）都符合我们从流/表视角看到的预期，因此不值得进一步讨论。然而，我们还没有花太多时间讨论触发器在经典批处理场景中的样子。现在我们了解了批处理管道的底层流/表拓扑结构是什么样子，这值得简要提及。
- en: 'At the end of the day, there’s really only one type of trigger used in classic
    batch scenarios: one that fires when the input is complete. For the initial MapRead
    stage of the MapReduce job we looked at earlier, that trigger would conceptually
    fire for all of the data in the input table as soon as the pipeline launched,
    given that the input for a batch job is assumed to be complete from the get go.¹¹
    That input source table would thus be converted into a stream of individual elements,
    after which the Map stage could begin processing them.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 归根结底，在经典批处理场景中实际上只有一种类型的触发器：当输入完成时触发。对于我们之前看过的MapReduce作业的初始MapRead阶段，该触发器在概念上会立即为输入表中的所有数据触发，因为批处理作业的输入被假定为从一开始就是完整的。¹¹因此，该输入源表将被转换为单个元素的流，之后Map阶段可以开始处理它们。
- en: For table-to-stream conversions in the middle of the pipeline, such as the ReduceRead
    stage in our example, the same type of trigger is used. In this case, however,
    the trigger must actually wait for all of the data in the table to be complete
    (i.e., what is more commonly referred to as all of the data being written to the
    shuffle), much as our example batch pipelines in Figures 6-4 and 6-6 waited for
    the end of the input before emitting their final results.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在管道中间的表到流转换中，例如我们示例中的ReduceRead阶段，使用相同类型的触发器。然而，在这种情况下，触发器实际上必须等待表中的所有数据完成（即更常见地称为所有数据被写入洗牌），就像我们示例中的批处理管道在图6-4和6-6中等待输入结束之前发出最终结果一样。
- en: 'Given that classic batch processing effectively always makes use of the input-data-complete
    trigger, you might ask what any custom triggers specified by the author of the
    pipeline might mean in a batch scenario. The answer here really is: it depends.
    There are two aspects worth discussing:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于经典批处理实际上总是使用输入数据完成触发器，您可能会问在批处理场景中作者指定的任何自定义触发器可能意味着什么。答案实际上是：这取决于情况。有两个值得讨论的方面：
- en: Trigger guarantees (or lack thereof)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 触发器的保证（或缺乏保证）
- en: Most existing batch processing systems have been designed with this lock-step
    read-process-group-write-repeat sequence in mind. In such circumstances, it’s
    difficult to provide any sort of finer-grained trigger abilities, because the
    only place they would manifest any sort of change would be at the final shuffle
    stage of the pipeline. This doesn’t mean that the triggers specified by the user
    aren’t honored, however; the semantics of triggers are such that it’s possible
    to resort to lower common denominators when appropriate.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现有的批处理系统都是根据这种锁步读取-处理-分组-写入-重复的顺序进行设计的。在这种情况下，很难提供任何更精细的触发能力，因为它们可能会在管道的最终洗牌阶段才会表现出任何变化。然而，这并不意味着用户指定的触发器不会被尊重；触发器的语义是可以在适当的时候采用更低的共同分母。
- en: For example, an `AfterWatermark` trigger is meant to trigger *after* the watermark
    passes the end of a window. It makes no guarantees how *far* beyond the end of
    the window the watermark may be when it fires. Similarly, an `AfterCount(N)` trigger
    only guarantees that *at least N* elements have been processed before triggering;
    *N* might very well be all of the elements in the input set.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`AfterWatermark`触发器意味着在水印通过窗口结束时触发。 它不保证水印在触发时距离窗口结束有多远。 同样，`AfterCount(N)`触发器只保证在触发之前已处理*至少N*个元素；*N*很可能是输入集中的所有元素。
- en: Note that this clever wording of trigger names wasn’t chosen simply to accommodate
    classic batch systems within the model; it’s a very necessary part of the model
    itself, given the natural asynchronicity and nondeterminism of triggering. Even
    in a finely tuned, low-latency, true-streaming system, it’s essentially impossible
    to guarantee that an `AfterWatermark` trigger will fire while the watermark is
    precisely *at* the end of any given window, except perhaps under the most extremely
    limited circumstances (e.g., a single machine processing all of the data for the
    pipeline with a relatively modest load). And even if you could guarantee it, what
    really would be the point? Triggers provide a means of controlling the flow of
    data from a table into a stream, nothing more.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，触发器名称的巧妙措辞并不仅仅是为了适应经典的批处理系统，而是模型本身的一个非常必要的部分，考虑到触发的自然异步性和不确定性。 即使在经过精心调整的低延迟真正流式处理系统中，基本上不可能保证`AfterWatermark`触发器会在水印恰好*在*任何给定窗口的结束时触发，除非在极端有限的情况下（例如，单台机器处理管道的所有数据，并且负载相对较小）。
    即使您可以保证，真的有什么意义吗？ 触发器提供了一种控制数据从表到流的流动的手段，仅此而已。
- en: The blending of batch and streaming
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理和流处理的融合
- en: Given what we’ve learned in this writeup, it should be clear that the main semantic
    difference between batch and streaming systems is the ability to trigger tables
    incrementally. But even that isn’t really a semantic difference, but more of a
    latency/throughput trade-off (because batch systems typically give you higher
    throughput at the cost of higher latency of results).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们在本文中学到的知识，应该清楚批处理和流处理系统之间的主要语义区别是触发表的增量能力。 但即使这也不是真正的语义区别，而更多的是延迟/吞吐量的权衡（因为批处理系统通常以更高的吞吐量换取更高的结果延迟）。
- en: 'This goes back to something I said in “Batch and Streaming Efficiency Differences”:
    there’s really not that much difference between batch and streaming systems today
    except for an efficiency delta (in favor of batch) and a natural ability to deal
    with unbounded data (in favor of streaming). I argued then that much of that efficiency
    delta comes from the combination of larger bundle sizes (an explicit compromise
    of latency in favor of throughput) and more efficient shuffle implementations
    (i.e., stream → table → stream conversions). From that perspective, it should
    be possible to provide a system that seamlessly integrates the best of both worlds:
    one which provides the ability to handle unbounded data naturally but can also
    balance the tensions between latency, throughput, and cost across a broad spectrum
    of use cases by transparently tuning the bundle sizes, shuffle implementations,
    and other such implementation details under the covers.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以追溯到我在“批处理和流处理效率差异”中所说的一些内容：今天批处理和流处理系统之间实际上没有太大的区别，除了效率差异（有利于批处理）和处理无界数据的自然能力（有利于流处理）。
    我当时认为，这种效率差异很大程度上来自于更大的捆绑大小（明确地在延迟和吞吐量之间进行折衷）和更有效的洗牌实现（即，流→表→流转换）。 从这个角度来看，应该可以提供一个系统，它可以无缝地整合两者的优点：既可以自然地处理无界数据，又可以通过透明地调整捆绑大小、洗牌实现和其他实现细节来平衡延迟、吞吐量和成本之间的紧张关系，以满足广泛的用例。
- en: This is precisely what Apache Beam already does at the API level.¹² The argument
    being made here is that there’s room for unification at the execution-engine level,
    as well. In a world like that, batch and streaming will no longer be a thing,
    and we’ll be able to say goodbye to both batch *and* streaming as independent
    concepts once and for all. We’ll just have general data processing systems that
    combine the best ideas from both branches in the family tree to provide an optimal
    experience for the specific use case at hand. Some day.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是 Apache Beam 在 API 级别已经做到的。¹² 这里提出的论点是，在执行引擎级别也有统一的空间。 在这样的世界中，批处理和流处理将不再存在，我们将能够永远告别批处理*和*流处理作为独立的概念。
    我们将只有结合了两者最佳思想的通用数据处理系统，以提供特定用例的最佳体验。 某一天。
- en: 'At this point, we can stick a fork in the trigger section. It’s done. We have
    only one more brief stop on our way to having a holistic view of the relationship
    between the Beam Model and streams-and-tables theory: *accumulation*.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以在触发部分插入叉子。 它完成了。 在我们全面了解 Beam 模型和流和表理论之间关系的过程中，我们只有一个更简短的停留：*累积*。
- en: '*How*: Accumulation'
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*如何*：累积'
- en: 'In Chapter 2, we learned that the three accumulation modes (discarding, accumulating,
    accumulating and retracting¹³) tell us how refinements of results relate when
    a window is triggered multiple times over the course of its life. Fortunately,
    the relationship to streams and tables here is pretty straightforward:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中，我们了解到三种累积模式（丢弃、累积、累积和撤销¹³）告诉我们结果的细化如何与窗口在其生命周期内多次触发相关。 幸运的是，在这里与流和表的关系非常直接：
- en: '*Discarding mode* requires the system to either throw away the previous value
    for the window when triggering or keep around a copy of the previous value and
    compute the delta the next time the window triggers.¹⁴ (This mode might have better
    been called Delta mode.)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*丢弃模式*要求系统在触发时要么丢弃窗口的先前值，要么保留先前值的副本并在下次窗口触发时计算增量¹⁴。（这种模式最好被称为增量模式。）'
- en: '*Accumulating mode* requires no additional work; the current value for the
    window in the table at triggering time is what is emitted. (This mode might have
    better been called Value mode.)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*累积模式*不需要额外的工作；在触发时表中窗口的当前值就会被发出。（这种模式最好被称为值模式。）'
- en: '*Accumulating and retracting mode* requires keeping around copies of all previously
    triggered (but not yet retracted) values for the window. This list of previous
    values can grow quite large in the case of merging windows like sessions, but
    is vital to cleanly reverting the effects of those previous trigger firings in
    cases where the new value cannot simply be used to overwrite a previous value.
    (This mode might have better been called Value and Retractions mode.)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*累积和撤回模式*需要保留窗口中所有先前触发的（但尚未撤回）值的副本。在合并窗口（如会话）的情况下，先前值的列表可能会变得非常大，但对于干净地撤销先前触发的效果是至关重要的，因为新值不能简单地用于覆盖先前的值。（这种模式最好被称为值和撤回模式。）'
- en: The streams-and-tables visualizations of accumulation modes add little additional
    insight into their semantics, so we won’t investigate them here.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 流和表的可视化对累积模式的语义几乎没有额外的洞察力，因此我们不会在这里进行调查。
- en: A Holistic View of Streams and Tables in the Beam Model
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Beam模型中流和表的整体视图
- en: Having addressed the four questions, we can now take a holistic view of streams
    and tables in a Beam Model pipeline. Let’s take our running example (the team
    scores calculation pipeline) and see what its structure looks like at the streams-and-table
    level. The full code for the pipeline might look something like Example 6-6 (repeating
    Example 6-4).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决了这四个问题之后，我们现在可以对Beam模型流水线中的流和表进行整体视图。让我们以我们的运行示例（团队得分计算流水线）为例，看看它在流和表级别的结构是什么样子。流水线的完整代码可能类似于示例6-6（重复示例6-4）。
- en: Example 6-6\. Our full score-parsing pipeline
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-6。我们完整的分数解析流水线
- en: '[PRE6]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Breaking that apart into stages separated by the intermediate `PCollection`
    types (where I’ve used more semantic “type” names like `Team` and `User Score`
    than real types for clarity of what is happening at each stage), you would arrive
    at something like that depicted in Figure 6-13.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 将其分解为由中间的`PCollection`类型分隔的阶段（我使用了更语义化的“类型”名称，如`Team`和`User Score`，而不是真实类型，以便清楚地说明每个阶段发生了什么），你会得到类似于图6-13所示的东西。
- en: '![](img/stsy_0613.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_0613.png)'
- en: Figure 6-13\. Logical phases of a team score summation pipeline, with intermediate
    PCollection types
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-13。团队得分总和流水线的逻辑阶段，带有中间的PCollection类型
- en: When you actually run this pipeline, it first goes through an optimizer, whose
    job is to convert this logical execution plan into an optimized, physical execution
    plan. Each execution engine is different, so actual physical execution plans will
    vary between runners. But a believable strawperson plan might look something like
    Figure 6-14.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当您实际运行此流水线时，它首先通过优化器，其工作是将此逻辑执行计划转换为经过优化的物理执行计划。每个执行引擎都是不同的，因此实际的物理执行计划将在运行程序之间有所不同。但是一个可信的计划可能看起来像图6-14。
- en: '![](img/stsy_0614.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_0614.png)'
- en: Figure 6-14\. Theoretical physical phases of a team score summation pipeline,
    with intermediate PCollection types
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-14。团队得分总和流水线的理论物理阶段，带有中间的PCollection类型
- en: 'There’s a lot going on here, so let’s walk through all of it. There are three
    main differences between Figures 6-13 and 6-14 that we’ll be discussing:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多事情要做，所以让我们逐一讨论。图6-13和6-14之间有三个主要区别，我们将讨论：
- en: Logical versus physical operations
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑与物理操作
- en: As part of building a physical execution plan, the underlying engine must convert
    the logical operations provided by the user into a sequence of primitive operations
    supported by the engine. In some cases, those physical equivalents look essentially
    the same (e.g., `Parse`), and in others, they’re very different.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 作为构建物理执行计划的一部分，底层引擎必须将用户提供的逻辑操作转换为引擎支持的一系列原始操作。在某些情况下，这些物理等价物看起来基本相同（例如`Parse`），而在其他情况下，它们则非常不同。
- en: Physical stages and fusion
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 物理阶段和融合
- en: It’s often inefficient to execute each logical phase as a fully independent
    physical stage in the pipeline (with attendant serialization, network communication,
    and deserialization overhead between each). As a result, the optimizer will typically
    try to fuse as many physical operations as possible into a single physical stage.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在流水线中，将每个逻辑阶段作为完全独立的物理阶段执行通常是低效的（伴随着每个阶段之间的序列化、网络通信和反序列化开销）。因此，优化器通常会尝试将尽可能多的物理操作融合成单个物理阶段。
- en: Keys, values, windows, and partitioning
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 键、值、窗口和分区
- en: To make it more evident what each physical operation is doing, I’ve annotated
    the intermediate `PCollection`s with the type of key, value, window, and data
    partitioning in effect at each point.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地说明每个物理操作正在做什么，我已经注释了中间的`PCollection`，并注明了每个点的键、值、窗口和数据分区的类型。
- en: 'Let’s now walk through each logical operation in detail and see what it translated
    to in the physical plan and how they all relate to streams and tables:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细地走一遍每个逻辑操作，看看它在物理计划中是如何转换的，以及它们如何与流和表相关联：
- en: '`ReadFromSource`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReadFromSource`'
- en: Other than being fused with the physical operation immediately following it
    (`Parse`), not much interesting happens in translation for `ReadFromSource`. As
    far as the characteristics of our data at this point, because the read is essentially
    consuming raw input bytes, we basically have raw strings with no keys, no windows,
    and no (or random) partitioning. The original data source can be either a table
    (e.g., a Cassandra table) or a stream (e.g., RabbitMQ) or something a little like
    both (e.g., Kafka in log compaction mode). But regardless, the end result of reading
    from the input source is a stream.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 除了与紧随其后的物理操作融合在一起（`Parse`），`ReadFromSource`的翻译中并没有太多有趣的事情发生。就我们目前数据的特征而言，因为读取基本上是消耗原始输入字节，我们基本上有没有键、没有窗口和没有（或随机的）分区的原始字符串。原始数据源可以是表（例如Cassandra表）或流（例如RabbitMQ）或类似两者的东西（例如处于日志压缩模式的Kafka）。但无论如何，从输入源读取的最终结果是一个流。
- en: '`Parse`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`Parse`'
- en: The logical `Parse` operation also translates in a relatively straightforward
    manner to the physical version. `Parse` takes the raw strings and extracts a key
    (team ID) and value (user score) from them. It’s a nongrouping operation, and
    thus the stream it consumed remains a stream on the other side.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑`Parse`操作也以相对直接的方式转换为物理版本。`Parse`从原始字符串中提取键（团队ID）和值（用户得分）。这是一个非分组操作，因此它消耗的流仍然是另一侧的流。
- en: '`Window+Trigger`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`Window+Trigger`'
- en: This logical operation is spread out across a number of distinct physical operations.
    The first is window assignment, in which each element is assigned to a set of
    windows. That happens immediately in the `AssignWindows` operation, which is a
    nongrouping operation that simply annotates each element in the stream with the
    window(s) it now belongs to, yielding another stream on the other side.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这个逻辑操作分布在许多不同的物理操作中。首先是窗口分配，其中每个元素被分配到一组窗口中。这立即发生在`AssignWindows`操作中，这是一个非分组操作，只是用窗口注释流中的每个元素，使其属于的窗口，产生另一个流。
- en: The second is window merging, which we learned earlier in the chapter happens
    as part of the grouping operation. As such, it gets sunk down into the `GroupMergeAndCombine`
    operation later in the pipeline. We discuss that operation when we talk about
    the logical `Sum` operation next.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个是窗口合并，我们在本章前面学到的，作为分组操作的一部分发生。因此，它被沉入管道后面的`GroupMergeAndCombine`操作中。我们在下一个逻辑`Sum`操作时讨论该操作。
- en: And finally, there’s triggering. Triggering happens after grouping and is the
    way that we’ll convert the table created by grouping back into a stream. As such,
    it gets sunk into its own operation, which follows `GroupMergeAndCombine`.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有触发。触发发生在分组之后，是我们将由分组创建的表转换回流的方式。因此，它被沉入自己的操作中，跟随`GroupMergeAndCombine`。
- en: '`Sum`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sum`'
- en: 'Summation is really a composite operation, consisting of a couple pieces: partitioning
    and aggregation. Partitioning is a nongrouping operation that redirects the elements
    in the stream in such a way that elements with the same keys end up going to the
    same physical machine. Another word for partitioning is shuffling, though that
    term is a bit overloaded because “Shuffle” in the MapReduce sense is often used
    to mean both partitioning *and* grouping (*and* sorting, for that matter). Regardless,
    partitioning physically alters the stream in way that makes it groupable but doesn’t
    do anything to actually bring the data to rest. As a result, it’s a nongrouping
    operation that yields another stream on the other side.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 求和实际上是一个复合操作，由几个部分组成：分区和聚合。分区是一个非分组操作，以这样的方式重定向流中的元素，使得具有相同键的元素最终进入同一台物理机。分区的另一个词是洗牌，尽管这个术语有点过载，因为“Shuffle”在MapReduce意义上通常用来表示分区*和*分组（*和*排序，无论如何）。无论如何，分区在物理上改变了流，使其可以分组，但实际上并没有做任何事情来使数据真正停下来。因此，它是一个非分组操作，产生另一个流。
- en: After partitioning comes grouping. Grouping itself is a composite operation.
    First comes grouping by key (enabled by the previous partition-by-key operation).
    Next comes window merging and grouping by window, as we described earlier. And
    finally, because summation is implemented as a `CombineFn` in Beam (essentially
    an incremental aggregation operation), there’s combining, where individual elements
    are summed together as they arrive. The specific details are not terribly important
    for our purposes here. What is important is the fact that, since this is (obviously)
    a grouping operation, our chain of streams now comes to rest in a table containing
    the summed team totals as they evolve over time.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 分区后是分组。分组本身是一个复合操作。首先是按键分组（由先前的按键分区操作启用）。接下来是窗口合并和按窗口分组，正如我们之前描述的那样。最后，因为求和在Beam中是作为`CombineFn`实现的（本质上是一个增量聚合操作），所以有组合，即当单个元素到达时将它们相加。具体细节对我们来说并不是非常重要的。重要的是，由于这显然是一个分组操作，我们的流链现在最终停留在一个包含随着时间演变的团队总分的表中。
- en: '`WriteToSink`'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`WriteToSink`'
- en: Lastly, we have the write operation, which takes the stream yielded by triggering
    (which was sunk below the `GroupMergeAndCombine` operation, as you might recall)
    and writes it out to our output data sink. That data itself can be either a table
    or stream. If it’s a table, `WriteToSink` will need to perform some sort of grouping
    operation as part of writing the data into the table. If it’s a stream, no grouping
    will be necessary (though partitioning might still be desired; for example, when
    writing into something like Kafka).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有写操作，它接收触发产生的流（你可能还记得它被沉入`GroupMergeAndCombine`操作下面），并将其写入到我们的输出数据汇中。数据本身可以是表或流。如果是表，`WriteToSink`将需要执行某种分组操作来将数据写入表中。如果是流，就不需要分组（尽管可能仍然需要分区；例如，当写入类似Kafka的东西时）。
- en: 'The big takeaway here is not so much the precise details of everything that’s
    going on in the physical plan, but more the overall relationship of the Beam Model
    to the world of streams and tables. We saw three types of operations: nongrouping
    (e.g., `Parse`), grouping (e.g., `GroupMergeAndCombine`), and ungrouping (e.g.,
    `Trigger`). The nongrouping operations always consumed streams and produced streams
    on the other side. The grouping operations always consumed streams and yielded
    tables. And the ungrouping operations consumed tables and yielded streams. These
    insights, along with everything else we’ve learned along the way, are enough for
    us to formulate a more general theory about the relationship of the Beam Model
    to streams and tables.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重点不是物理计划中正在进行的一切的精确细节，而是Beam模型与流和表世界的整体关系。我们看到了三种类型的操作：非分组（例如，`解析`），分组（例如，`GroupMergeAndCombine`），和取消分组（例如，`触发`）。非分组操作总是消耗流并在另一侧产生流。分组操作总是消耗流并产生表。取消分组操作消耗表并产生流。这些见解以及我们一路学到的一切足以让我们制定关于Beam模型与流和表关系的更一般理论。
- en: A General Theory of Stream and Table Relativity
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流和表相对论的一般理论
- en: Having surveyed how stream processing, batch processing, the four *what*/*where*/*when*/*how*
    questions, and the Beam Model as a whole relate to stream and table theory, let’s
    now attempt to articulate a more general definition of stream and table relativity.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在调查了流处理、批处理、四个*什么*/*哪里*/*何时*/*如何*问题以及整个Beam模型与流和表理论的关系之后，现在让我们试图阐明流和表相对论的更一般定义。
- en: '*A general theory of stream and table relativity​*:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 流和表相对论的一般理论：
- en: '*Data processing pipelines* (both batch and streaming) consist of *tables*,
    *streams*, and *operations* upon those tables and streams.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据处理管道*（批处理和流处理）包括对这些表和流进行的*操作*。'
- en: '*Tables* are *data at rest*, and act as a container for data to accumulate
    and be observed over time.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*表*是*静止的数据*，作为数据积累和随时间观察的容器。'
- en: '*Streams* are *data in motion*, and encode a discretized view of the evolution
    of a table over time.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*流*是*运动中的数据*，并编码了随时间演变的表的离散视图。'
- en: '*Operations* act upon a stream or table and yield a new stream or table. They
    are categorized as follows:'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*操作*作用于流或表并产生新的流或表。它们被分类如下：'
- en: 'stream → stream: Nongrouping (element-wise) operations'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流 → 流：非分组（逐元素）操作
- en: Applying *nongrouping* operations to a stream alters the data in the stream
    while leaving them in motion, yielding a new stream with possibly different cardinality.
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对流应用*非分组*操作会改变流中的数据，同时保持它们在运动中，产生一个可能具有不同基数的新流。
- en: 'stream → table: Grouping operations'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流 → 表：分组操作
- en: '*Grouping* data within a stream brings those data to rest, yielding a *table*
    that evolves over time.'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在流中对数据进行*分组*会使这些数据静止下来，产生一个随时间演变的*表*。
- en: '*Windowing* incorporates the dimension of event time into such groupings.'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*窗口化*将事件时间维度纳入这样的分组中。'
- en: '*Merging windows* dynamically combine over time, allowing them to reshape themselves
    in response to the data observed and dictating that key remain the unit of atomicity/parallelization,
    with window being a child component of grouping within that key.'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*合并窗口*会随时间动态组合，使它们能够根据观察到的数据重新塑造自己，并决定键保持原子性/并行化的单位，窗口作为该键内分组的子组件。'
- en: 'table → stream: Ungrouping (triggering) operations'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表 → 流：取消分组（触发）操作
- en: '*Triggering* data within a table ungroups them into motion, yielding a *stream*
    that captures a view of the table’s evolution over time.'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在表中*触发*数据会将它们取消分组并投入运动，产生一个捕获表随时间演变的*流*。
- en: '*Watermarks* provide a notion of input completeness relative to event time,
    which is a useful reference point when triggering event-timestamped data, particularly
    data grouped into event-time windows from unbounded streams.'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*水印*提供了相对于事件时间的输入完整性概念，这是一个有用的参考点，特别是在触发事件时间戳数据时，特别是从无界流中分组的数据。'
- en: The *accumulation mode* for the trigger determines the nature of the stream,
    dictating whether it contains deltas or values, and whether retractions for previous
    deltas/values are provided.
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 触发器的*累积模式*决定了流的性质，决定它是否包含增量或值，以及是否提供先前增量/值的撤销。
- en: 'table → table: (none)'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表 → 表：（无）
- en: There are no operations that consume a table and yield a table, because it’s
    not possible for data to go from rest and back to rest without being put into
    motion. As a result, all modifications to a table are via conversion to a stream
    and back again.
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 没有操作可以消耗表并产生表，因为数据不可能在不被投入运动的情况下从静止到静止。因此，对表的所有修改都是通过转换为流，然后再转换回来。
- en: 'What I love about these rules is that they just make sense. They have a very
    natural and intuitive feeling about them, and as a result they make it so much
    easier to understand how data flow (or don’t) through a sequence of operations.
    They codify the fact that data exist in one of two constitutions at any given
    time (streams or tables), and they provide simple rules for reasoning about the
    transitions between those states. They demystify windowing by showing how it’s
    just a slight modification of a thing everyone already innately understands: grouping.
    They highlight why grouping operations in general are always such a sticking point
    for streaming (because they bring data in streams to rest as tables) but also
    make it very clear what sorts of operations are needed to get things unstuck (triggers;
    i.e., ungrouping operations). And they underscore just how unified batch and stream
    processing really are, at a conceptual level.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢这些规则的原因是它们很有道理。它们给人一种非常自然和直观的感觉，因此使人更容易理解数据如何通过一系列操作流动（或不流动）。它们规范了这样一个事实：在任何给定时间，数据存在于两种状态中的一种（流或表格），并且它们提供了简单的规则来推理这些状态之间的转换。它们通过展示窗口化只是每个人都本能理解的分组的轻微修改来揭示窗口化的神秘。它们强调了为什么分组操作通常是流处理中的一个难点（因为它们将流中的数据转化为表格），但也非常清楚地表明了需要哪些操作来解决这个问题（触发器；即非分组操作）。它们强调了在概念上批处理和流处理实际上是多么统一。
- en: When I set out to write this chapter, I wasn’t entirely sure what I was going
    to end up with, but the end result was much more satisfying than I’d imagined
    it might be. In the chapters to come, we use this theory of stream and table relativity
    again and again to help guide our analyses. And every time, its application will
    bring clarity and insight that would otherwise have been much harder to gain.
    Streams and tables are the best.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始写这一章时，我并不完全确定最终会得到什么，但最终的结果比我想象的要令人满意得多。在接下来的章节中，我们将再次使用这种流和表格相对论的理论来指导我们的分析。每一次应用都会带来清晰和洞察力，否则这些洞察力将会更难获得。流和表格是最好的。
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we first established the basics of stream and table theory.
    We first defined streams and tables relatively:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们首先建立了流和表格理论的基础。我们首先相对地定义了流和表格：
- en: streams → tables
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 流 → 表格
- en: The aggregation of a stream of updates over time yields a table.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，对更新流的聚合会产生一个表格。
- en: tables → streams
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 → 流
- en: The observation of changes to a table over time yields a stream.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移观察表格的变化会产生一个流。
- en: 'We next defined them independently:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们独立定义它们：
- en: Tables are data *at rest*.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格是数据*静止*的。
- en: Streams are data *in motion*.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流是数据*在运动*中。
- en: 'We then assessed the classic MapReduce model of batch computation from a streams
    and tables perspective and came to the conclusion that the following four steps
    describe batch processing from that perspective:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从流和表格的角度评估了经典的MapReduce批处理计算模型，并得出结论，以下四个步骤描述了从这个角度进行的批处理：
- en: Tables are read in their entirety to become streams.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 表格被完整地读取以成为流。
- en: Streams are processed into new streams until a grouping operation is hit.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流被处理成新的流，直到遇到分组操作。
- en: Grouping turns the stream into a table.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分组将流转换为表格。
- en: Steps 1 through 3 repeat until you run out of operations in the pipeline.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤1到3重复，直到管道中的操作用尽。
- en: From this analysis, we were able to see that streams are just as much a part
    of batch processing as they are stream processing, and also that the idea of data
    being a stream is an orthogonal one from whether the data in question are bounded
    or unbounded.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种分析，我们能够看到流在批处理中和流处理中同样重要，以及数据是流的想法与所讨论的数据是有界还是无界是无关的。
- en: 'Next, we spent a good deal of time considering the relationship between streams
    and tables and the robust, out-of-order stream processing semantics afforded by
    the Beam Model, ultimately arriving at the general theory of stream and table
    relativity we enumerated in the previous section. In addition to the basic definitions
    of streams and tables, the key insight in that theory is that there are four (really,
    just three) types of operations in a data processing pipeline:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们花了很多时间考虑流和表格之间的关系，以及Beam模型提供的强大的、无序的流处理语义，最终得出了我们在前一节中列举的流和表格相对论的一般理论。除了流和表格的基本定义之外，该理论的关键见解是数据处理管道中有四（实际上只有三）种操作类型：
- en: stream → stream
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 流 → 流
- en: Nongrouping (element-wise) operations
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 非分组（逐元素）操作
- en: stream → table
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 流 → 表格
- en: Grouping operations
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 分组操作
- en: table → stream
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 → 流
- en: Ungrouping (triggering) operations
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 非分组（触发）操作
- en: table → table
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 → 表格
- en: (nonexistent)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: （不存在）
- en: By classifying operations in this way, it becomes trivial to understand how
    data flow through (and linger within) a given pipeline over time.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式对操作进行分类，可以轻松地理解数据如何随着时间在给定的管道中流动（或停留）。
- en: 'Finally, and perhaps most important of all, we learned this: when you look
    at things from the streams-and-tables point of view, it becomes abundantly clear
    how batch and streaming really are just the same thing conceptually. Bounded or
    unbounded, it doesn’t matter. It’s streams and tables from top to bottom.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，也许最重要的是，我们学到了这一点：当你从流和表格的角度看问题时，批处理和流处理在概念上实际上是一样的。有界或无界都无所谓。从头到尾都是流和表格。
- en: </bad-physics-jokes>
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: </bad-physics-jokes>
- en: ¹ If you didn’t go to college for computer science and you’ve made it this far
    in the book, you are likely either 1) my parents, 2) masochistic, or 3) very smart
    (and for the record, I’m not implying these groups are necessarily mutually exclusive;
    figure that one out if you can, Mom and Dad! <winky-smiley/>).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 如果你不是为了计算机科学而上大学，但你已经读到了这本书的这一部分，你很可能是1）我的父母，2）受虐狂，或者3）非常聪明（就记录而言，我并不意味着这些群体必然是互相排斥的；如果你能理解这一点，妈妈和爸爸，就自己想想吧！<winky-smiley/>）。
- en: ² And note that in some cases, the tables themselves can accept time as a query
    parameter, allowing you to peer backward in time to snapshots of the table as
    it existed in the past.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 2 请注意，在某些情况下，表本身可以接受时间作为查询参数，允许您向过去查看表的快照。
- en: ³ Note that no guarantees are made about the keys of two successive records
    observed by a single mapper, because no key-grouping has occurred yet. The existence
    of the key here is really just to allow keyed datasets to be consumed in a natural
    way, and if there are no obvious keys for the input data, they’ll all just share
    what is effectively a global null key.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 3 请注意，对于单个mapper观察到的两个连续记录的键，没有任何保证，因为尚未进行键分组。这里的键实际上只是为了让带键数据集以一种自然的方式被消费，如果输入数据没有明显的键，它们实际上都将共享一个全局的空键。
- en: ⁴ Calling the inputs to a batch job “static” might be a bit strong. In reality,
    the dataset being consumed can be constantly changing as it’s processed; that
    is, if you’re reading directly from an HBase/Bigtable table within a timestamp
    range in which the data aren’t guaranteed to be immutable. But in most cases,
    the recommended approach is to ensure that you’re somehow processing a static
    snapshot of the input data, and any deviation from that assumption is at your
    own peril.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 4 将批处理作业的输入称为“静态”可能有点过分。实际上，被消费的数据集在处理过程中可能会不断变化；也就是说，如果你直接从HBase/Bigtable表中读取在时间戳范围内的数据，这些数据并不保证是不可变的。但在大多数情况下，建议的方法是确保你以某种方式处理了输入数据的静态快照，任何偏离这一假设的情况都是自己的风险。
- en: ⁵ Note that grouping a stream by key is importantly distinct from simply *partitioning*
    that stream by key, which ensures that all records with the same key end up being
    processed by the same machine but doesn’t do anything to put the records to rest.
    They instead remain in motion and thus continue on as a stream. A grouping operation
    is more like a partition-by-key followed by a write to the appropriate group for
    that partition, which is what puts them to rest and turns the stream into a table.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 5 请注意，按键对流进行分组与简单地按键对流进行*分区*是有重要区别的，后者确保具有相同键的所有记录最终由同一台机器处理，但并不会使记录停止。它们仍然保持运动，因此继续作为流进行。分组操作更像是按键分区后写入适当分区的组，这是使它们停止并将流转换为表的原因。
- en: ⁶ One giant difference, from an implementation perspective at least, being that
    ReduceWrite, knowing that keys have already been grouped together by MapWrite,
    and further knowing that Reduce is unable to alter keys for the case in which
    its outputs remain keyed, can simply accumulate the outputs generated by reducing
    the values for a single key in order to group them together, which is much simpler
    than the full-blown shuffle implementation required for a MapWrite phase.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 6 一个巨大的区别，至少从实现的角度来看，是ReduceWrite知道键已经被MapWrite分组在一起，进一步知道Reduce无法改变键，因此它可以简单地累积减少值生成的输出，以便将它们分组在一起，这比MapWrite阶段所需的完整洗牌实现要简单得多。
- en: '⁷ Another way of looking at it is that there are two types of tables: updateable
    and appendable; this is the way the Flink folks have framed it for their Table
    API. But even though that’s a great intuitive way of capturing the observed semantics
    of the two situations, I think it obscures the underlying nature of what’s actually
    happening that causes a stream to come to rest as a table; that is, grouping.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 7 另一种看待这个问题的方式是，有两种类型的表：可更新的和可追加的；这是Flink团队为他们的Table API所构建的方式。但即使这是捕捉到两种情况的观察语义的一个很好的直观方式，我认为它掩盖了实际发生的导致流变成表的基本本质；也就是分组。
- en: ⁸ Though as we can clearly see from this example, it’s not just a streaming
    thing; you can get the same effect with a batch system if its state tables are
    world readable.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 8 尽管从这个例子中我们可以清楚地看到，这不仅仅是一个流处理的问题；如果批处理系统的状态表是全局可读的，你也可以得到相同的效果。
- en: ⁹ This is particularly painful if a sink for your storage system of choice doesn’t
    exist yet; building proper sinks that can uphold consistency guarantees is a surprisingly
    subtle and difficult task.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 9 如果你的存储系统中还没有适合的接收器，这将特别痛苦；构建能够保证一致性的适当接收器是一个令人惊讶地微妙和困难的任务。
- en: ¹⁰ This also means that if you place a value into multiple windows—for example,
    sliding windows—the value must conceptually be duplicated into multiple, independent
    records, one per window. Even so, it’s possible in some cases for the underlying
    system to be smart about how it treats certain types of overlapping windows, thus
    optimize away the need for actually duplicating the value. Spark, for example,
    does this for sliding windows.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 10 这也意味着，如果你将一个值放入多个窗口——例如滑动窗口——这个值在概念上必须被复制成多个独立的记录，每个窗口一个。即便如此，在某些情况下，底层系统可以智能地处理某些类型的重叠窗口，从而优化掉实际复制值的需要。例如，Spark就为滑动窗口做到了这一点。
- en: ¹¹ Note that this high-level conceptual view of how things work in batch pipelines
    belies the complexity of efficiently triggering an entire table of data at once,
    particularly when that table is sizeable enough to require a plurality of machines
    to process. The [SplittableDoFn API](https://s.apache.org/splittable-do-fn) recently
    added to Beam provides some insight into the mechanics involved.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 11 请注意，批处理管道中事物工作的这种高层概念视图掩盖了有效触发整个数据表的复杂性，特别是当该表足够大以至于需要多台机器来处理时。Beam最近添加的SplittableDoFn
    API提供了一些关于涉及的机制的见解。
- en: ¹² And yes, if you blend batch and streaming together you get Beam, which is
    where that name came from originally. For reals.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 12 是的，如果你将批处理和流处理混合在一起，你就会得到Beam，这也是这个名字最初的由来。真的。
- en: ¹³ This is why you should always use an Oxford comma.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 13 这就是为什么你应该始终使用牛津逗号。
- en: ¹⁴ Note that in the case of merging windows, in addition to merging the current
    values for the two windows to yield a merged current value, the previous values
    for those two windows would need to be merged, as well, to allow for the later
    calculation of a merged delta come triggering time.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ¹⁴请注意，在合并窗口的情况下，除了合并两个窗口的当前值以得到合并后的当前值之外，还需要合并这两个窗口的先前值，以便在触发时间后进行合并增量的计算。
