- en: 23  Logistic Regression II
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 23  逻辑回归 II
- en: 原文：[https://ds100.org/course-notes/logistic_regression_2/logistic_reg_2.html](https://ds100.org/course-notes/logistic_regression_2/logistic_reg_2.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://ds100.org/course-notes/logistic_regression_2/logistic_reg_2.html](https://ds100.org/course-notes/logistic_regression_2/logistic_reg_2.html)
- en: '*Learning Outcomes* ***   Apply decision rules to make a classification'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习成果* ***   应用决策规则进行分类'
- en: Learn when logistic regression works well and when it does not
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解逻辑回归何时运行良好，何时不运行良好
- en: Introduce new metrics for model performance**  **Today, we will continue studying
    the Logistic Regression model. We’ll discuss decision boundaries that help inform
    the classification of a particular prediction. Then, we’ll pick up from last lecture’s
    discussion of cross-entropy loss, study a few of its pitfalls, and learn potential
    remedies. We will also provide an implementation of `sklearn`’s logistic regression
    model. Lastly, we’ll return to decision rules and discuss metrics that allow us
    to determine our model’s performance in different scenarios.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入模型性能的新指标**  **今天，我们将继续学习逻辑回归模型。我们将讨论决策边界，以帮助确定特定预测的分类。然后，我们将从上次讲座关于交叉熵损失的讨论中继续学习一些问题，并学习潜在的解决方法。我们还将提供`sklearn`逻辑回归模型的实现。最后，我们将回到决策规则，并讨论允许我们在不同情况下确定模型性能的指标。
- en: This will introduce us to the process of **thresholding** – a technique used
    to *classify* data from our model’s predicted probabilities, or \(P(Y=1|x)\).
    In doing so, we’ll focus on how these thresholding decisions affect the behavior
    of our model. We will learn various evaluation metrics useful for binary classification,
    and apply them to our study of logistic regression.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们了解**阈值处理**的过程 - 一种用于从模型预测的概率或\(P(Y=1|x)\)对数据进行*分类*的技术。在这样做时，我们将重点关注这些阈值决策如何影响我们模型的行为。我们将学习对二元分类有用的各种评估指标，并将它们应用于我们对逻辑回归的研究中。
- en: '![tpr_fpr](../Images/d0e7b2f534671da8cdaf49fa1f294461.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![tpr_fpr](../Images/d0e7b2f534671da8cdaf49fa1f294461.png)'
- en: 23.1 Decision Boundaries
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.1 决策边界
- en: In logistic regression, we model the *probability* that a datapoint belongs
    to Class 1\. Last week, we developed the logistic regression model to predict
    that probability, but we never actually made any *classifications* for whether
    our prediction \(y\) belongs in Class 0 or Class 1.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，我们建模数据点属于类别1的*概率*。上周，我们开发了逻辑回归模型来预测该概率，但我们实际上并没有对我们的预测\(y\)属于类别0还是类别1进行任何*分类*。
- en: \[ p = P(Y=1 | x) = \frac{1}{1 + e^{-x^T\theta}}\]
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p = P(Y=1 | x) = \frac{1}{1 + e^{-x^T\theta}}\]
- en: A **decision rule** tells us how to interpret the output of the model to make
    a decision on how to classify a datapoint. We commonly make decision rules by
    specifying a **threshold**, \(T\). If the predicted probability is greater than
    or equal to \(T\), predict Class 1\. Otherwise, predict Class 0.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策规则**告诉我们如何解释模型的输出，以便对数据点进行分类。我们通常通过指定**阈值**\(T\)来制定决策规则。如果预测概率大于或等于\(T\)，则预测为类别1。否则，预测为类别0。'
- en: \[\hat y = \text{classify}(x) = \begin{cases} 1, & P(Y=1|x) \ge T\\ 0, & \text{otherwise
    } \end{cases}\]
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat y = \text{classify}(x) = \begin{cases} 1, & P(Y=1|x) \ge T\\ 0, & \text{otherwise
    } \end{cases}\]
- en: The threshold is often set to \(T = 0.5\), but *not always*. We’ll discuss why
    we might want to use other thresholds \(T \neq 0.5\) later in this lecture.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值通常设置为\(T = 0.5\)，但*并非总是如此*。我们将讨论为什么我们可能希望在本讲座后期使用其他阈值\(T \neq 0.5\)。
- en: 'Using our decision rule, we can define a **decision boundary** as the “line”
    that splits the data into classes based on its features. For logistic regression,
    the decision boundary is a **hyperplane** – a linear combination of the features
    in p-dimensions – and we can recover it from the final logistic regression model.
    For example, if we have a model with 2 features (2D), we have \(\theta = [\theta_0,
    \theta_1, \theta_2]\) including the intercept term, and we can solve for the decision
    boundary like so:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的决策规则，我们可以将**决策边界**定义为根据其特征将数据分成类的“线”。对于逻辑回归，决策边界是一个**超平面** - 特征在p维中的线性组合
    - 我们可以从最终的逻辑回归模型中恢复它。例如，如果我们有一个具有2个特征（2D）的模型，我们有\(\theta = [\theta_0, \theta_1,
    \theta_2]\) 包括截距项，并且我们可以这样解决决策边界：
- en: \[ \begin{align} T &= \frac{1}{1 + e^{-(\theta_0 + \theta_1 * \text{feature1}
    + \theta_2 * \text{feature2})}} \\ 1 + e^{-(\theta_0 + \theta_1 \cdot \text{feature1}
    + \theta_2 \cdot \text{feature2})} &= \frac{1}{T} \\ e^{-(\theta_0 + \theta_1
    \cdot \text{feature1} + \theta_2 \cdot \text{feature2})} &= \frac{1}{T} - 1 \\
    \theta_0 + \theta_1 \cdot \text{feature1} + \theta_2 \cdot \text{feature2} &=
    -\log(\frac{1}{T} - 1) \end{align} \]
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align} T &= \frac{1}{1 + e^{-(\theta_0 + \theta_1 * \text{feature1}
    + \theta_2 * \text{feature2})}} \\ 1 + e^{-(\theta_0 + \theta_1 \cdot \text{feature1}
    + \theta_2 \cdot \text{feature2})} &= \frac{1}{T} \\ e^{-(\theta_0 + \theta_1
    \cdot \text{feature1} + \theta_2 \cdot \text{feature2})} &= \frac{1}{T} - 1 \\
    \theta_0 + \theta_1 \cdot \text{feature1} + \theta_2 \cdot \text{feature2} &=
    -\log(\frac{1}{T} - 1) \end{align} \]
- en: For a model with 2 features, the decision boundary is a line in terms of its
    features. To make it easier to visualize, we’ve included an example of a 1-dimensional
    and a 2-dimensional decision boundary below. Notice how the decision boundary
    predicted by our logistic regression model perfectly separates the points into
    two classes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有2个特征的模型，决策边界是根据其特征的一条线。为了更容易可视化，我们在下面包括了一个一维和一个二维决策边界的示例。请注意，我们的逻辑回归模型预测的决策边界完美地将点分成了两类。
- en: '![varying_threshold](../Images/3b3402e98558f7ee144887442f57c83a.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![varying_threshold](../Images/3b3402e98558f7ee144887442f57c83a.png)'
- en: 'In real life, however, that is often not the case, and we often see some overlap
    between points of different classes across the decision boundary. The *true* classes
    of the 2D data are shown below:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在现实生活中，情况通常并非如此，我们经常看到不同类别的点在决策边界上有一些重叠。2D数据的*真实*类如下所示：
- en: '![varying_threshold](../Images/f2d80f04ac76d5da2e7da332ba8231e5.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![varying_threshold](../Images/f2d80f04ac76d5da2e7da332ba8231e5.png)'
- en: As you can see, the decision boundary predicted by our logistic regression does
    not perfectly separate the two classes. There’s a “muddled” region near the decision
    boundary where our classifier predicts the wrong class. What would the data have
    to look like for the classifier to make perfect predictions?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们的逻辑回归预测的决策边界并不能完全将两个类别分开。在决策边界附近有一个“混乱”的区域，我们的分类器会预测错误的类别。数据看起来会是什么样子，使得分类器能够做出完美的预测呢？
- en: 23.2 Linear Separability and Regularization
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.2 线性可分性和正则化
- en: A classification dataset is said to be **linearly separable** if there exists
    a hyperplane among input features \(x\) that separates the two classes \(y\).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在一个超平面可以将输入特征\(x\)分开两个类别\(y\)，那么分类数据集就被称为**线性可分**。
- en: Linear separability in 1D can be found with a rugplot of a single feature. For
    example, notice how the plot on the bottom left is linearly separable along the
    vertical line \(x=0\). However, no such line perfectly separates the two classes
    on the bottom right.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在1D中的线性可分性可以通过单个特征的rugplot来找到。例如，注意左下角的图是沿着垂直线\(x=0\)线性可分的。然而，在右下角，没有这样的线可以完美地将两个类别分开。
- en: '![linear_separability_1D](../Images/5769c23917e03c757c21888f12d0c243.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![linear_separability_1D](../Images/5769c23917e03c757c21888f12d0c243.png)'
- en: This same definition holds in higher dimensions. If there are two features,
    the separating hyperplane must exist in two dimensions (any line of the form \(y=mx+b\)).
    We can visualize this using a scatter plot.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义在更高的维度中也是成立的。如果有两个特征，分离的超平面必须存在于两个维度中（任何形式为\(y=mx+b\)的直线）。我们可以使用散点图来可视化这一点。
- en: '![linear_separability_1D](../Images/b897847684c9ec3ef2d23f3bce249fe6.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![linear_separability_1D](../Images/b897847684c9ec3ef2d23f3bce249fe6.png)'
- en: 'This sounds great! When the dataset is linearly separable, a logistic regression
    classifier can perfectly assign datapoints into classes. However, (unexpected)
    complications may arise. Consider the `toy` dataset with 2 points and only a single
    feature \(x\):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很棒！当数据集是线性可分的时候，逻辑回归分类器可以完美地将数据点分配到类别中。然而，（意想不到的）复杂性可能会出现。考虑具有2个点和仅一个特征\(x\)的“玩具”数据集：
- en: '![toy_linear_separability](../Images/fbcd463658750f02a26700a4b8254acb.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![toy_linear_separability](../Images/fbcd463658750f02a26700a4b8254acb.png)'
- en: The optimal \(\theta\) value that minimizes loss pushes the predicted probabilities
    of the data points to their true class.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化损失的最佳\(\theta\)值将数据点的预测概率推向其真实类别。
- en: \(P(Y = 1|x = -1) = \frac{1}{1 + e^\theta} \rightarrow 1\)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(P(Y = 1|x = -1) = \frac{1}{1 + e^\theta} \rightarrow 1\)
- en: \(P(Y = 1|x = 1) = \frac{1}{1 + e^{-\theta}} \rightarrow 0\)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(P(Y = 1|x = 1) = \frac{1}{1 + e^{-\theta}} \rightarrow 0\)
- en: This happens when \(\theta = -\infty\). When \(\theta = -\infty\), we observe
    the following behavior for any input \(x\).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当\(\theta = -\infty\)时会发生这种情况。当\(\theta = -\infty\)时，我们观察到对于任何输入\(x\)的以下行为。
- en: \[P(Y=1|x) = \sigma(\theta x) \rightarrow \begin{cases} 1, \text{if } x < 0\\
    0, \text{if } x \ge 0 \end{cases}\]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: \[P(Y=1|x) = \sigma(\theta x) \rightarrow \begin{cases} 1, \text{if } x < 0\\
    0, \text{if } x \ge 0 \end{cases}\]
- en: The diverging weights cause the model to be overconfident. For example, consider
    the new point \((x, y) = (0.5, 1)\). Following the behavior above, our model will
    incorrectly predict \(p=0\), and thus, \(\hat y = 0\).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的发散会导致模型过度自信。例如，考虑新的点\((x, y) = (0.5, 1)\)。根据上面的行为，我们的模型会错误地预测\(p=0\)，因此\(\hat
    y = 0\)。
- en: '![toy_linear_separability](../Images/4827cc0f6c0200d882c8aab8ca18d177.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![toy_linear_separability](../Images/4827cc0f6c0200d882c8aab8ca18d177.png)'
- en: The loss incurred by this misclassified point is infinite.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个错误分类点所产生的损失是无穷大的。
- en: \[-(y\text{ log}(p) + (1-y)\text{ log}(1-p))=1\text{log}(0)\]
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: \[-(y\text{ log}(p) + (1-y)\text{ log}(1-p))=1\text{log}(0)\]
- en: Thus, diverging weights (\(|\theta| \rightarrow \infty\)) occur with **lineary
    separable** data. “Overconfidence” is a particularly dangerous version of overfitting.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，权重发散（\(|\theta| \rightarrow \infty\)）会出现在**线性可分**数据中。“过度自信”是过度拟合的一个特别危险的版本。
- en: Consider the loss function with respect to the parameter \(\theta\).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑损失函数关于参数\(\theta\)的情况。
- en: '![unreg_loss](../Images/93a9aa7eda282f4b8a98861dc30beaa7.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![unreg_loss](../Images/93a9aa7eda282f4b8a98861dc30beaa7.png)'
- en: Though it’s very difficult to see, the plateau for negative values of \(\theta\)
    is slightly tilted downwards, meaning the loss approaches \(0\) as \(\theta\)
    decreases and approaches \(-\infty\).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然很难看到，但是对于\(\theta\)的负值，损失函数的平台略微向下倾斜，这意味着当\(\theta\)减小并趋近于\(-\infty\)时，损失趋近于\(0\)。
- en: 23.2.1 Regularized Logistic Regression
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 23.2.1 正则化逻辑回归
- en: To avoid large weights and infinite loss (particularly on linearly separable
    data), we use regularization. The same principles apply as with linear regression
    - make sure to standardize your features first.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免大的权重和无穷大的损失（特别是在线性可分的数据上），我们使用正则化。与线性回归一样，同样的原则适用-首先确保标准化你的特征。
- en: 'For example, \(L2\) (Ridge) Logistic Regression takes on the form:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，\(L2\)（Ridge）逻辑回归的形式如下：
- en: \[\min_{\theta} -\frac{1}{n} \sum_{i=1}^{n} (y_i \text{log}(\sigma(x_i^T\theta))
    + (1-y_i)\text{log}(1-\sigma(x_i^T\theta))) + \lambda \sum_{i=1}^{d} \theta_j^2\]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \[\min_{\theta} -\frac{1}{n} \sum_{i=1}^{n} (y_i \text{log}(\sigma(x_i^T\theta))
    + (1-y_i)\text{log}(1-\sigma(x_i^T\theta))) + \lambda \sum_{i=1}^{d} \theta_j^2\]
- en: Now, let us compare the loss functions of un-regularized and regularized logistic
    regression.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们比较未正则化和正则化逻辑回归的损失函数。
- en: '![unreg_loss](../Images/93a9aa7eda282f4b8a98861dc30beaa7.png)![reg_loss](../Images/aa9003f626a427c51e054f5f2d79876f.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![unreg_loss](../Images/93a9aa7eda282f4b8a98861dc30beaa7.png)![reg_loss](../Images/aa9003f626a427c51e054f5f2d79876f.png)'
- en: As we can see, \(L2\) regularization helps us prevent diverging weights and
    deters against “overconfidence.”
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，\(L2\)正则化有助于防止权重发散，并且防止“过度自信”。
- en: '`sklearn`’s logistic regression defaults to L2 regularization and `C=1.0`;
    `C` is the inverse of \(\lambda\): \(C = \frac{1}{\lambda}\). Setting `C` to a
    large value, for example, `C=300.0`, results in minimal regularization.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`的逻辑回归默认使用L2正则化和`C=1.0`；`C`是\(\lambda\)的倒数：\(C = \frac{1}{\lambda}\)。将`C`设置为一个大的值，例如`C=300.0`，会导致最小的正则化。'
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that in Data 100, we only use `sklearn` to fit logistic regression models.
    There is no closed-form solution to the optimal theta vector, and the gradient
    is a little messy (see the bonus section below for details).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在Data 100中，我们只使用`sklearn`来拟合逻辑回归模型。对于最优的theta向量，没有封闭形式的解，梯度有点混乱（有关详细信息，请参见下面的奖励部分）。
- en: From here, the `.predict` function returns the predicted class \(\hat y\) of
    the point. In the simple binary case,
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，`.predict` 函数返回点的预测类别 \(\hat y\)。在简单的二进制情况下，
- en: \[\hat y = \begin{cases} 1, & P(Y=1|x) \ge 0.5\\ 0, & \text{otherwise } \end{cases}\]
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat y = \begin{cases} 1, & P(Y=1|x) \ge 0.5\\ 0, & \text{否则 } \end{cases}\]
- en: 23.3 Performance Metrics
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.3 性能指标
- en: You might be thinking, if we’ve already introduced cross-entropy loss, why do
    we need additional ways of assessing how well our models perform? In linear regression,
    we made numerical predictions and used a loss function to determine how “good”
    these predictions were. In logistic regression, our ultimate goal is to classify
    data – we are much more concerned with whether or not each datapoint was assigned
    the correct class using the decision rule. As such, we are interested in the *quality*
    of classifications, not the predicted probabilities.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，如果我们已经引入了交叉熵损失，为什么我们还需要额外的方法来评估我们的模型表现如何呢？在线性回归中，我们进行了数值预测，并使用损失函数来确定这些预测的“好坏”。在逻辑回归中，我们的最终目标是对数据进行分类
    - 我们更关心每个数据点是否使用决策规则分配了正确的类。因此，我们对分类的*质量*感兴趣，而不是预测的概率。
- en: The most basic evaluation metric is **accuracy**, that is, the proportion of
    correctly classified points.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的评估指标是**准确率**，即被正确分类的点的比例。
- en: \[\text{accuracy} = \frac{\# \text{ of points classified correctly}}{\# \text{
    of total points}}\]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{准确率} = \frac{\# \text{分类正确的点}}{\# \text{总点数}}\]
- en: 'Translated to code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译成代码：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: However, accuracy is not always a great metric for classification. To understand
    why, let’s consider a classification problem with 100 emails where only 5 are
    truly spam, and the remaining 95 are truly ham. We’ll investigate two models where
    accuracy is a poor metric.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，准确率并不总是分类的一个很好的指标。要理解为什么，让我们考虑一个分类问题，有100封电子邮件，其中只有5封是真正的垃圾邮件，其余95封是真正的非垃圾邮件。我们将研究两个准确率不佳的模型。
- en: '**Model 1**: Our first model classifies every email as non-spam. The model’s
    accuracy is high (\(\frac{95}{100} = 0.95\)), but it doesn’t detect any spam emails.
    Despite the high accuracy, this is a bad model.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型1**：我们的第一个模型将每封电子邮件都分类为非垃圾邮件。模型的准确率很高（\(\frac{95}{100} = 0.95\))，但它没有检测到任何垃圾邮件。尽管准确率很高，但这是一个糟糕的模型。'
- en: '**Model 2**: The second model classifies every email as spam. The accuracy
    is low (\(\frac{5}{100} = 0.05\)), but the model correctly labels every spam email.
    Unfortunately, it also misclassifies every non-spam email.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型2**：第二个模型将每封电子邮件都分类为垃圾邮件。准确率低（\(\frac{5}{100} = 0.05\))，但模型正确标记了每封垃圾邮件。不幸的是，它也错误分类了每封非垃圾邮件。'
- en: As this example illustrates, accuracy is not always a good metric for classification,
    particularly when your data could exhibit class imbalance (e.g., very few 1’s
    compared to 0’s).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个例子所说明的，准确率并不总是分类的一个很好的指标，特别是当你的数据可能表现出类别不平衡时（例如，1的数量很少相对于0）。
- en: 23.3.1 Types of Classification
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 23.3.1 分类类型
- en: 'There are 4 different different classifications that our model might make:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型可能做出4种不同的分类：
- en: '**True positive**: correctly classify a positive point as being positive (\(y=1\)
    and \(\hat{y}=1\))'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**真正例**：将正点正确分类为正（\(y=1\) 和 \(\hat{y}=1\)）'
- en: '**True negative**: correctly classify a negative point as being negative (\(y=0\)
    and \(\hat{y}=0\))'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**真负例**：将负点正确分类为负（\(y=0\) 和 \(\hat{y}=0\)）'
- en: '**False positive**: incorrectly classify a negative point as being positive
    (\(y=0\) and \(\hat{y}=1\))'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**假正例**：将负点错误分类为正（\(y=0\) 和 \(\hat{y}=1\)）'
- en: '**False negative**: incorrectly classify a positive point as being negative
    (\(y=1\) and \(\hat{y}=0\))'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**假负例**：将正点错误分类为负（\(y=1\) 和 \(\hat{y}=0\)）'
- en: These classifications can be concisely summarized in a **confusion matrix**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分类可以简洁地总结在一个**混淆矩阵**中。
- en: '![confusion_matrix](../Images/a373f62056bb3d8153ce917aa83d14a4.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![混淆矩阵](../Images/a373f62056bb3d8153ce917aa83d14a4.png)'
- en: 'An easy way to remember this terminology is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这个术语的一个简单方法如下：
- en: Look at the second word in the phrase. *Positive* means a prediction of 1\.
    *Negative* means a prediction of 0.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看短语中的第二个词。*正* 意味着预测为1。*负* 意味着预测为0。
- en: Look at the first word in the phrase. *True* means our prediction was correct.
    *False* means it was incorrect.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看短语中的第一个词。*真* 意味着我们的预测是正确的。*假* 意味着它是错误的。
- en: We can now write the accuracy calculation as \[\text{accuracy} = \frac{TP +
    TN}{n}\]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将准确率计算写成 \[\text{准确率} = \frac{TP + TN}{n}\]
- en: In `sklearn`, we use the following syntax
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在`sklearn`中，我们使用以下语法
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![confusion_matrix](../Images/2544bb4eaec9736cde724e2c7b700531.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![混淆矩阵](../Images/2544bb4eaec9736cde724e2c7b700531.png)'
- en: 23.3.2 Accuracy, Precision, and Recall
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 23.3.2 准确率、精度和召回率
- en: The purpose of our discussion of the confusion matrix was to motivate better
    performance metrics for classification problems with class imbalance - namely,
    precision and recall.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论混淆矩阵的目的是激发对于具有类别不平衡的分类问题的更好的性能指标 - 即精度和召回率。
- en: '**Precision** is defined as'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**精度** 定义为'
- en: \[\text{precision} = \frac{\text{TP}}{\text{TP + FP}}\]
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{精度} = \frac{\text{TP}}{\text{TP + FP}}\]
- en: 'Precision answers the question: “Of all observations that were predicted to
    be \(1\), what proportion was actually \(1\)?” It measures how accurate the classifier
    is when its predictions are positive.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 精度回答了这个问题：“在所有被预测为 \(1\) 的观察中，有多少比例实际上是 \(1\)？”它衡量了分类器在其预测为正时的准确性。
- en: '**Recall** (or **sensitivity**) is defined as'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**召回率**（或**敏感度**）定义为'
- en: \[\text{recall} = \frac{\text{TP}}{\text{TP + FN}}\]
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{召回率} = \frac{\text{TP}}{\text{TP + FN}}\]
- en: 'Recall aims to answer: “Of all observations that were actually \(1\), what
    proportion was predicted to be \(1\)?” It measures how many positive predictions
    were missed.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 召回旨在回答：“实际上是\(1\)的所有观察中，有多少被预测为\(1\)？”它衡量了有多少积极的预测被忽略了。
- en: Here’s a helpful graphic that summarizes our discussion above.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个有用的图表，总结了我们上面的讨论。
- en: '![confusion_matrix](../Images/71b37a68997e30ee93daf37ce6951a4f.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![混淆矩阵](../Images/71b37a68997e30ee93daf37ce6951a4f.png)'
- en: 23.3.3 Example Calculation
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 23.3.3示例计算
- en: 'In this section, we will calculate the accuracy, precision, and recall performance
    metrics for our earlier spam classification example. As a reminder, we had 100
    emails, 5 of which were spam. We designed two models:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将计算我们之前的垃圾邮件分类示例的准确性、精度和召回率性能指标。作为提醒，我们有100封电子邮件，其中有5封是垃圾邮件。我们设计了两个模型：
- en: 'Model 1: Predict that every email is *non-spam*'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型1：预测每封电子邮件都是*非垃圾邮件*
- en: 'Model 2: Predict that every email is *spam*'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型2：预测每封电子邮件都是*垃圾邮件*
- en: 23.3.3.1 Model 1
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 23.3.3.1模型1
- en: First, let’s begin by creating the confusion matrix.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从创建混淆矩阵开始。
- en: '|  | 0 | 1 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 |'
- en: '| --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | True Negative: 95 | False Positive: 0 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 真阴性：95 | 假阳性：0 |'
- en: '| 1 | False Negative: 5 | True Positive: 0 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 假阴性：5 | 真阳性：0 |'
- en: Convince yourself of why our confusion matrix looks like so.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '说服自己为什么我们的混淆矩阵看起来像这样。 '
- en: \[\text{accuracy} = \frac{95}{100} = 0.95\] \[\text{precision} = \frac{0}{0
    + 0} = \text{undefined}\] \[\text{recall} = \frac{0}{0 + 5} = 0\]
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{准确性} = \frac{95}{100} = 0.95\] \[\text{精度} = \frac{0}{0 + 0} = \text{未定义}\]
    \[\text{召回率} = \frac{0}{0 + 5} = 0\]
- en: Notice how our precision is undefined because we never predicted class \(1\).
    Our recall is 0 for the same reason – the numerator is 0 (we had no positive predictions).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们的精度是未定义的，因为我们从未预测过类\(1\)。由于相同的原因，我们的召回率为0-分子为0（我们没有积极的预测）。
- en: 23.3.3.2 Model 2
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 23.3.3.2模型2
- en: Our confusion matrix for Model 2 looks like so.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型2的混淆矩阵如下。
- en: '|  | 0 | 1 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 |'
- en: '| --- | --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | True Negative: 0 | False Positive: 95 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 真阴性：0 | 假阳性：95 |'
- en: '| 1 | False Negative: 0 | True Positive: 5 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 假阴性：0 | 真阳性：5 |'
- en: \[\text{accuracy} = \frac{5}{100} = 0.05\] \[\text{precision} = \frac{5}{5 +
    95} = 0.05\] \[\text{recall} = \frac{5}{5 + 0} = 1\]
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{准确性} = \frac{5}{100} = 0.05\] \[\text{精度} = \frac{5}{5 + 95} = 0.05\]
    \[\text{召回率} = \frac{5}{5 + 0} = 1\]
- en: Our precision is low because we have many false positives, and our recall is
    perfect - we correctly classified all spam emails (we never predicted class \(0\)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的精度很低，因为我们有很多假阳性，而我们的召回率是完美的-我们正确分类了所有的垃圾邮件（我们从未预测过类\(0\)）。
- en: 23.3.4 Precision vs. Recall
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 23.3.4精度与召回率
- en: Precision (\(\frac{\text{TP}}{\text{TP} + \textbf{ FP}}\)) penalizes false positives,
    while recall (\(\frac{\text{TP}}{\text{TP} + \textbf{ FN}}\)) penalizes false
    negatives.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 精度（\(\frac{\text{TP}}{\text{TP} + \textbf{ FP}}\))惩罚假阳性，而召回率（\(\frac{\text{TP}}{\text{TP}
    + \textbf{ FN}}\))惩罚假阴性。
- en: In fact, precision and recall are *inversely related*. This is evident in our
    second model – we observed a high recall and low precision. Usually, there is
    a tradeoff in these two (most models can either minimize the number of FP or FN;
    and in rare cases, both).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，精度和召回率是*相互关联*的。这在我们的第二个模型中是明显的-我们观察到召回率高，精度低。通常，这两者之间存在权衡（大多数模型可以最小化FP或FN的数量；在少数情况下，两者都可以）。
- en: The specific performance metric(s) to prioritize depends on the context. In
    many medical settings, there might be a much higher cost to missing positive cases.
    For instance, in our breast cancer example, it is more costly to misclassify malignant
    tumors (false negatives) than it is to incorrectly classify a benign tumor as
    malignant (false positives). In the case of the latter, pathologists can conduct
    further studies to verify malignant tumors. As such, we should minimize the number
    of false negatives. This is equivalent to maximizing recall.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要优先考虑的具体性能指标取决于上下文。在许多医疗环境中，错过阳性病例可能会带来更高的成本。例如，在我们的乳腺癌示例中，误将恶性肿瘤分类为良性肿瘤（假阳性）的成本更高，而不是将良性肿瘤错误地分类为恶性肿瘤（假阴性）。在后一种情况下，病理学家可以进行进一步的研究以验证恶性肿瘤。因此，我们应该最小化假阴性的数量。这等同于最大化召回率。
- en: 23.3.5 Two More Metrics
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 23.3.5另外两个指标
- en: The **True Positive Rate (TPR)** is defined as
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**真阳性率（TPR）**定义为'
- en: \[\text{true positive rate} = \frac{\text{TP}}{\text{TP + FN}}\]
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{真阳性率} = \frac{\text{TP}}{\text{TP + FN}}\]
- en: 'You’ll notice this is equivalent to *recall*. In the context of our spam email
    classifier, it answers the question: “What proportion of spam did I mark correctly?”.
    We’d like this to be close to \(1\)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到这等同于*召回率*。在我们的垃圾邮件分类器的背景下，它回答了一个问题：“我标记了多少垃圾邮件是正确的？”。我们希望这个数字接近\(1\)
- en: The **False Positive Rate (FPR)** is defined as
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**假阳性率（FPR）**定义为'
- en: \[\text{false positive rate} = \frac{\text{FP}}{\text{FP + TN}}\]
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{假阳性率} = \frac{\text{FP}}{\text{FP + TN}}\]
- en: 'Another word for FPR is *specificity*. This answers the question: “What proportion
    of regular email did I mark as spam?”. We’d like this to be close to \(0\)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: FPR的另一个词是*特异性*。这回答了一个问题：“我将多少常规邮件标记为垃圾邮件？”。我们希望这个数字接近\(0\)
- en: As we increase threshold \(T\), both TPR and FPR decrease. We’ve plotted this
    relationship below for some model on a `toy` dataset.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 随着阈值\(T\)的增加，TPR和FPR都会减少。我们在下面为某个`toy`数据集上的某个模型绘制了这种关系。
- en: '![tpr_fpr](../Images/d6bb02acac0fafdd96508a568d416c07.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![tpr_fpr](../Images/d6bb02acac0fafdd96508a568d416c07.png)'
- en: 23.4 Adjusting the Classification Threshold
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.4调整分类阈值
- en: One way to minimize the number of FP vs. FN (equivalently, maximizing precision
    vs. recall) is by adjusting the classification threshold \(T\).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化FP与FN的数量（等效地，最大化精度与召回率）的一种方法是调整分类阈值\(T\)。
- en: \[\hat y = \begin{cases} 1, & P(Y=1|x) \ge T\\ 0, & \text{otherwise } \end{cases}\]
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \[\hat y = \begin{cases} 1, & P(Y=1|x) \ge T\\ 0, & \text{otherwise } \end{cases}\]
- en: The default threshold in `sklearn` is \(T = 0.5\). As we increase the threshold
    \(T\), we “raise the standard” of how confident our classifier needs to be to
    predict 1 (i.e., “positive”).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`中的默认阈值为\(T = 0.5\)。当我们增加阈值\(T\)时，我们“提高了标准”，即我们的分类器需要预测1（即“积极”）的信心有多大。'
- en: '![varying_threshold](../Images/ea1260924b5b01e2191bd62bc139ded6.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![varying_threshold](../Images/ea1260924b5b01e2191bd62bc139ded6.png)'
- en: As you may notice, the choice of threshold \(T\) impacts our classifier’s performance.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能注意到的，阈值\(T\)的选择会影响我们的分类器的性能。
- en: 'High \(T\): Most predictions are \(0\).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高\(T\)：大多数预测为\(0\)。
- en: Lots of false negatives
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多的假阴性
- en: Fewer false positives
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更少的假阳性
- en: 'Low \(T\): Most predictions are \(1\).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低\(T\)：大多数预测为\(1\)。
- en: Lots of false positives
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多的假阳性
- en: Fewer false negatives
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更少的假阴性
- en: In fact, we can choose a threshold \(T\) based on our desired number, or proportion,
    of false positives and false negatives. We can do so using a few different tools.
    We’ll touch on two of the most important ones in Data 100.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们可以根据我们期望的假阳性和假阴性的数量或比例选择一个阈值\(T\)。我们可以使用一些不同的工具来做到这一点。我们将在Data 100中介绍其中两个最重要的工具。
- en: Precision-Recall Curve (PR Curve)
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 精确-召回曲线（PR曲线）
- en: “Receiver Operating Characteristic” Curve (ROC Curve)
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “受试者工作特征”曲线（ROC曲线）
- en: 23.4.1 Precision-Recall Curves
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 23.4.1 精确-召回曲线
- en: A **Precision-Recall Curve (PR Curve)** is an alternative to the ROC curve that
    displays the relationship between precision and recall for various threshold values.
    It is constructed in a similar way as with the ROC curve.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确-召回曲线（PR曲线）**是ROC曲线的替代品，显示了不同阈值的精确度和召回率之间的关系。它的构造方式与ROC曲线类似。'
- en: Let’s first consider how precision and recall change as a function of the threshold
    \(T\). We know this quite well from earlier – precision will generally increase,
    and recall will decrease.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先考虑精确度和召回率如何随阈值\(T\)的变化而变化。我们从前面很清楚 - 精确度通常会增加，召回率会减少。
- en: '![precision-recall-thresh](../Images/0fe061843191f4ea78ebbc1182b1a6ec.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![precision-recall-thresh](../Images/0fe061843191f4ea78ebbc1182b1a6ec.png)'
- en: Displayed below is the PR Curve for the same `toy` dataset. Notice how threshold
    values increase as we move to the left.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示的是相同`toy`数据集的PR曲线。注意随着我们向左移动，阈值值会增加。
- en: '![pr_curve_thresholds](../Images/2bcb0be590b6d2c5aa6e5dc24009743e.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![pr_curve_thresholds](../Images/2bcb0be590b6d2c5aa6e5dc24009743e.png)'
- en: Once again, the perfect classifier will resemble the orange curve, this time,
    facing the opposite direction.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，完美的分类器将类似于橙色曲线，这次朝着相反的方向。
- en: '![pr_curve_perfect](../Images/431d5547efa2eb3a07fcccc1692b46b9.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![pr_curve_perfect](../Images/431d5547efa2eb3a07fcccc1692b46b9.png)'
- en: We want our PR curve to be as close to the “top right” of this graph as possible.
    Again, we use the AUC to determine “closeness”, with the perfect classifier exhibiting
    an AUC = 1 (and the worst with an AUC = 0.5).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的PR曲线尽可能接近这张图的“右上角”。同样，我们使用AUC来确定“接近度”，完美的分类器表现为AUC = 1（最差的为AUC = 0.5）。
- en: 23.4.2 The ROC Curve
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 23.4.2 ROC曲线
- en: The “Receiver Operating Characteristic” Curve (**ROC Curve**) plots the tradeoff
    between FPR and TPR. Notice how the far-left of the curve corresponds to higher
    threshold \(T\) values.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: “受试者工作特征”曲线（**ROC曲线**）绘制了FPR和TPR之间的权衡。注意曲线的最左侧对应于较高的阈值\(T\)值。
- en: '![roc_curve](../Images/d5a1bc0b83b4c0de290a7c81534caafc.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![roc_curve](../Images/d5a1bc0b83b4c0de290a7c81534caafc.png)'
- en: The “perfect” classifier is the one that has a TPR of 1, and FPR of 0\. This
    is achieved at the top-left of the plot below. More generally, it’s ROC curve
    resembles the curve in orange.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: “完美”的分类器是具有TPR = 1和FPR = 0的分类器。这是在下面图中的左上角实现的。更一般地，它的ROC曲线类似于橙色曲线。
- en: '![roc_curve_perfect](../Images/2fb704a6804215df62b78eb7c6cfd21f.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![roc_curve_perfect](../Images/2fb704a6804215df62b78eb7c6cfd21f.png)'
- en: We want our model to be as close to this orange curve as possible. How do we
    quantify “closeness”?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的模型尽可能接近这条橙色曲线。我们如何量化“接近度”？
- en: We can compute the **area under curve (AUC)** of the ROC curve. Notice how the
    perfect classifier has an AUC = 1\. The closer our model’s AUC is to 1, the better
    it is.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算ROC曲线下的**曲线下面积（AUC）**。注意完美分类器的AUC = 1。我们的模型的AUC越接近1，它就越好。
- en: 23.4.2.1 (Bonus) What is the “worst” AUC, and why is it 0.5?
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 23.4.2.1（奖励）什么是“最差”的AUC，为什么是0.5？
- en: On the other hand, a terrible model will have an AUC closer to 0.5\. Random
    predictors randomly predict \(P(Y = 1 | x)\) to be uniformly between 0 and 1\.
    This indicates the classifier is not able to distinguish between positive and
    negative classes, and thus, randomly predicts one of the two.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，糟糕的模型的AUC将更接近0.5。随机预测器随机预测\(P(Y = 1 | x)\)在0和1之间均匀分布。这表明分类器无法区分正类和负类，因此随机预测其中之一。
- en: '![roc_curve_worst_predictor](../Images/c1fbaa1039e2a8eb86449a84517167ca.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![roc_curve_worst_predictor](../Images/c1fbaa1039e2a8eb86449a84517167ca.png)'
- en: 23.5 (Bonus) Gradient Descent for Logistic Regression
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.5（奖励）逻辑回归的梯度下降
- en: 'Let’s define the following: \[ t_i = \phi(x_i)^T \theta \\ p_i = \sigma(t_i)
    \\ t_i = \log(\frac{p_i}{1 - p_i}) \\ 1 - \sigma(t_i) = \sigma(-t_i) \\ \frac{d}{dt}
    \sigma(t) = \sigma(t) \sigma(-t) \]'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义以下：\[ t_i = \phi(x_i)^T \theta \\ p_i = \sigma(t_i) \\ t_i = \log(\frac{p_i}{1
    - p_i}) \\ 1 - \sigma(t_i) = \sigma(-t_i) \\ \frac{d}{dt} \sigma(t) = \sigma(t)
    \sigma(-t) \]
- en: Now, we can simplify the cross-entropy loss \[ \begin{align} y_i \log(p_i) +
    (1 - y_i) \log(1 - p_i) &= y_i \log(\frac{p_i}{1 - p_i}) + \log(1 - p_i) \\ &=
    y_i \phi(x_i)^T + \log(\sigma(-\phi(x_i)^T \theta)) \end{align} \]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以简化交叉熵损失\[ \begin{align} y_i \log(p_i) + (1 - y_i) \log(1 - p_i) &= y_i
    \log(\frac{p_i}{1 - p_i}) + \log(1 - p_i) \\ &= y_i \phi(x_i)^T + \log(\sigma(-\phi(x_i)^T
    \theta)) \end{align} \]
- en: Hence, the optimal \(\hat{\theta}\) is \[\text{argmin}_{\theta} - \frac{1}{n}
    \sum_{i=1}^n (y_i \phi(x_i)^T + \log(\sigma(-\phi(x_i)^T \theta)))\]
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最佳的\(\hat{\theta}\)是\[\text{argmin}_{\theta} - \frac{1}{n} \sum_{i=1}^n (y_i
    \phi(x_i)^T + \log(\sigma(-\phi(x_i)^T \theta)))\]
- en: We want to minimize \[L(\theta) = - \frac{1}{n} \sum_{i=1}^n (y_i \phi(x_i)^T
    + \log(\sigma(-\phi(x_i)^T \theta)))\]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望最小化\[L(\theta) = - \frac{1}{n} \sum_{i=1}^n (y_i \phi(x_i)^T + \log(\sigma(-\phi(x_i)^T
    \theta)))\]
- en: So we take the derivative \[ \begin{align} \triangledown_{\theta} L(\theta)
    &= - \frac{1}{n} \sum_{i=1}^n \triangledown_{\theta} y_i \phi(x_i)^T + \triangledown_{\theta}
    \log(\sigma(-\phi(x_i)^T \theta)) \\ &= - \frac{1}{n} \sum_{i=1}^n y_i \phi(x_i)
    + \triangledown_{\theta} \log(\sigma(-\phi(x_i)^T \theta)) \\ &= - \frac{1}{n}
    \sum_{i=1}^n y_i \phi(x_i) + \frac{1}{\sigma(-\phi(x_i)^T \theta)} \triangledown_{\theta}
    \sigma(-\phi(x_i)^T \theta) \\ &= - \frac{1}{n} \sum_{i=1}^n y_i \phi(x_i) + \frac{\sigma(-\phi(x_i)^T
    \theta)}{\sigma(-\phi(x_i)^T \theta)} \sigma(\phi(x_i)^T \theta)\triangledown_{\theta}
    \sigma(-\phi(x_i)^T \theta) \\ &= - \frac{1}{n} \sum_{i=1}^n (y_i - \sigma(\phi(x_i)^T
    \theta)\phi(x_i)) \end{align} \]
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们对导数进行如下处理\[ \begin{align} \triangledown_{\theta} L(\theta) &= - \frac{1}{n}
    \sum_{i=1}^n \triangledown_{\theta} y_i \phi(x_i)^T + \triangledown_{\theta} \log(\sigma(-\phi(x_i)^T
    \theta)) \\ &= - \frac{1}{n} \sum_{i=1}^n y_i \phi(x_i) + \triangledown_{\theta}
    \log(\sigma(-\phi(x_i)^T \theta)) \\ &= - \frac{1}{n} \sum_{i=1}^n y_i \phi(x_i)
    + \frac{1}{\sigma(-\phi(x_i)^T \theta)} \triangledown_{\theta} \sigma(-\phi(x_i)^T
    \theta) \\ &= - \frac{1}{n} \sum_{i=1}^n y_i \phi(x_i) + \frac{\sigma(-\phi(x_i)^T
    \theta)}{\sigma(-\phi(x_i)^T \theta)} \sigma(\phi(x_i)^T \theta)\triangledown_{\theta}
    \sigma(-\phi(x_i)^T \theta) \\ &= - \frac{1}{n} \sum_{i=1}^n (y_i - \sigma(\phi(x_i)^T
    \theta)\phi(x_i)) \end{align} \]
- en: Setting the derivative equal to 0 and solving for \(\hat{\theta}\), we find
    that there’s no general analytic solution. Therefore, we must solve using numeric
    methods.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 将导数设置为0并解出\(\hat{\theta}\)，我们发现没有一般的解析解。因此，我们必须使用数值方法来解决。
- en: 23.5.1 Gradient Descent Update Rule
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 23.5.1 梯度下降更新规则
- en: \[\theta^{(0)} \leftarrow \text{initial vector (random, zeros, ...)} \]
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: \[\theta^{(0)} \leftarrow \text{初始向量（随机，零，...）} \]
- en: 'For \(\tau\) from 0 to convergence: \[ \theta^{(\tau + 1)} \leftarrow \theta^{(\tau)}
    + \rho(\tau)\left( \frac{1}{n} \sum_{i=1}^n \triangledown_{\theta} L_i(\theta)
    \mid_{\theta = \theta^{(\tau)}}\right) \]'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从0到收敛的\(\tau\)：\[ \theta^{(\tau + 1)} \leftarrow \theta^{(\tau)} + \rho(\tau)\left(
    \frac{1}{n} \sum_{i=1}^n \triangledown_{\theta} L_i(\theta) \mid_{\theta = \theta^{(\tau)}}\right)
    \]
- en: 23.5.2 Stochastic Gradient Descent Update Rule
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 23.5.2 随机梯度下降更新规则
- en: \[\theta^{(0)} \leftarrow \text{initial vector (random, zeros, ...)} \]
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: \[\theta^{(0)} \leftarrow \text{初始向量（随机，零，...）} \]
- en: For \(\tau\) from 0 to convergence, let \(B\) ~ \(\text{Random subset of indices}\).
    \[ \theta^{(\tau + 1)} \leftarrow \theta^{(\tau)} + \rho(\tau)\left( \frac{1}{|B|}
    \sum_{i \in B} \triangledown_{\theta} L_i(\theta) \mid_{\theta = \theta^{(\tau)}}\right)
    \]**
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从0到收敛的\(\tau\)，令\(B\) ~ \(\text{索引的随机子集}\)。\[ \theta^{(\tau + 1)} \leftarrow
    \theta^{(\tau)} + \rho(\tau)\left( \frac{1}{|B|} \sum_{i \in B} \triangledown_{\theta}
    L_i(\theta) \mid_{\theta = \theta^{(\tau)}}\right) \]
