- en: Chapter 2\. Downloading Apache Spark and Getting Started
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。下载 Apache Spark 并开始
- en: In this chapter, we will get you set up with Spark and walk through three simple
    steps you can take to get started writing your first standalone application.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将帮助你设置 Spark，并通过三个简单的步骤来开始编写你的第一个独立应用程序。
- en: We will use local mode, where all the processing is done on a single machine
    in a Spark shell—this is an easy way to learn the framework, providing a quick
    feedback loop for iteratively performing Spark operations. Using a Spark shell,
    you can prototype Spark operations with small data sets before writing a complex
    Spark application, but for large data sets or real work where you want to reap
    the benefits of distributed execution, local mode is not suitable—you’ll want
    to use the YARN or Kubernetes deployment modes instead.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用本地模式，在 Spark shell 中所有处理都在单台机器上完成——这是学习框架的简单方法，提供了迭代执行 Spark 操作的快速反馈循环。使用
    Spark shell，你可以在小数据集上原型化 Spark 操作，然后再编写复杂的 Spark 应用程序，但对于大数据集或实际工作中希望利用分布式执行优势的场景，本地模式不适用——你应该使用
    YARN 或 Kubernetes 部署模式。
- en: While the Spark shell only supports Scala, Python, and R, you can write a Spark
    application in any of the supported languages (including Java) and issue queries
    in Spark SQL. We do expect you to have some familiarity with the language of your
    choice.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Spark shell 只支持 Scala、Python 和 R，但你可以使用任何支持的语言（包括 Java）编写 Spark 应用程序，并在 Spark
    SQL 中发出查询。我们期望你对所选择的语言有一定的熟悉度。
- en: 'Step 1: Downloading Apache Spark'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 1 步：下载 Apache Spark
- en: To get started, go to the [Spark download page](https://oreil.ly/tbKY2), select
    “Pre-built for Apache Hadoop 2.7” from the drop-down menu in step 2, and click
    the “Download Spark” link in step 3 ([Figure 2-1](#the_apache_spark_download_page)).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，请访问 [Spark 下载页面](https://oreil.ly/tbKY2)，在第 2 步的下拉菜单中选择 “Pre-built for Apache
    Hadoop 2.7”，然后在第 3 步点击 “Download Spark” 链接（见 [图 2-1](#the_apache_spark_download_page)）。
- en: '![The Apache Spark download page](assets/lesp_0201.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark 下载页面](assets/lesp_0201.png)'
- en: Figure 2-1\. The Apache Spark download page
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1。Apache Spark 下载页面
- en: This will download the tarball *spark-3.0.0-preview2-bin-hadoop2.7.tgz*, which
    contains all the Hadoop-related binaries you will need to run Spark in local mode
    on your laptop. Alternatively, if you’re going to install it on an existing HDFS
    or Hadoop installation, you can select the matching Hadoop version from the drop-down
    menu. How to build from source is beyond the scope of this book, but you can read
    more about it in the [documentation](https://oreil.ly/fOyIN).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载 *spark-3.0.0-preview2-bin-hadoop2.7.tgz* 压缩包，其中包含在笔记本电脑上以本地模式运行 Spark 所需的所有与
    Hadoop 相关的二进制文件。或者，如果你要安装到现有的 HDFS 或 Hadoop 安装中，可以从下拉菜单中选择匹配的 Hadoop 版本。如何从源代码构建超出了本书的范围，但你可以在
    [文档](https://oreil.ly/fOyIN) 中了解更多。
- en: Note
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: At the time this book went to press Apache Spark 3.0 was still in preview mode,
    but you can download the latest Spark 3.0 using the same download method and instructions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书印刷时，Apache Spark 3.0 仍处于预览模式，但你可以使用相同的下载方法和说明下载最新的 Spark 3.0 版本。
- en: Since the release of Apache Spark 2.2, developers who only care about learning
    Spark in Python have the option of installing PySpark from the [PyPI repository](https://oreil.ly/gyAi8).
    If you only program in Python, you don’t have to install all the other libraries
    necessary to run Scala, Java, or R; this makes the binary smaller. To install
    PySpark from PyPI, just run `pip install pyspark`.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自 Apache Spark 2.2 发布以来，仅关心在 Python 中学习 Spark 的开发人员可以选择从 [PyPI 仓库](https://oreil.ly/gyAi8)
    安装 PySpark。如果你只使用 Python 编程，就无需安装运行 Scala、Java 或 R 所需的所有其他库；这使得二进制文件更小。要从 PyPI
    安装 PySpark，只需运行 `pip install pyspark`。
- en: There are some extra dependencies that can be installed for SQL, ML, and MLlib,
    via `pip install pyspark[sql,ml,mllib]` (or `pip install pyspark[sql]` if you
    only want the SQL dependencies).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SQL、ML 和 MLlib，可以通过 `pip install pyspark[sql,ml,mllib]` 安装一些额外的依赖项（或者如果只需要
    SQL 依赖项，可以使用 `pip install pyspark[sql]`）。
- en: Note
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You will need to install Java 8 or above on your machine and set the `JAVA_HOME`
    environment variable. See the [documentation](https://oreil.ly/c19W9) for instructions
    on how to download and install Java.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在计算机上安装 Java 8 或更高版本，并设置 `JAVA_HOME` 环境变量。请参阅 [文档](https://oreil.ly/c19W9)
    以获取有关如何下载和安装 Java 的说明。
- en: If you want to run R in an interpretive shell mode, you must [install R](https://www.r-project.org)
    and then run `sparkR`. To do distributed computing with R, you can also use the
    open source project [`sparklyr`](https://github.com/sparklyr/sparklyr), created
    by the R community.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在交互式shell模式下运行R，您必须先[安装 R](https://www.r-project.org)，然后运行 `sparkR`。要使用R进行分布式计算，还可以使用由R社区创建的开源项目[`sparklyr`](https://github.com/sparklyr/sparklyr)。
- en: Spark’s Directories and Files
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 的目录和文件
- en: 'We assume that you are running a version of the Linux or macOS operating system
    on your laptop or cluster, and all the commands and instructions in this book
    will be in that flavor. Once you have finished downloading the tarball, `cd` to
    the downloaded directory, extract the tarball contents with `tar -xf spark-3.0.0-preview2-bin-hadoop2.7.tgz`,
    and `cd` into that directory and take a look at the contents:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您在您的笔记本电脑或集群上运行Linux或macOS操作系统的某个版本，并且本书中的所有命令和说明都将使用这种风格。一旦您完成下载tarball，`cd`到下载的目录，使用
    `tar -xf spark-3.0.0-preview2-bin-hadoop2.7.tgz` 提取tarball内容，然后`cd`到该目录并查看内容：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s briefly summarize the intent and purpose of some of these files and directories.
    New items were added in Spark 2.x and 3.0, and the contents of some of the existing
    files and directories were changed too:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要总结一下这些文件和目录的意图和目的。在Spark 2.x和3.0中添加了新项，并且某些现有文件和目录的内容也发生了变化：
- en: README.md
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: README.md
- en: This file contains new detailed instructions on how to use Spark shells, build
    Spark from source, run standalone Spark examples, peruse links to Spark documentation
    and configuration guides, and contribute to Spark.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件包含了如何使用Spark shell、从源代码构建Spark、运行独立Spark示例、查阅Spark文档和配置指南以及为Spark做贡献的详细说明。
- en: bin
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: bin
- en: This directory, as the name suggests, contains most of the scripts you’ll employ
    to interact with Spark, `including` the `Spark shells` (`spark-sql`, `pyspark`,
    `spark-shell`, and `sparkR`). We will use these shells and executables in this
    directory later in this chapter to submit a standalone Spark application using
    `spark-submit`, and write a script that builds and pushes Docker images when running
    Spark with Kubernetes support.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，这个目录包含大多数用于与Spark交互的脚本，包括`Spark shells`（`spark-sql`、`pyspark`、`spark-shell`和`sparkR`）。我们将在本章后面使用这些shell和可执行文件提交一个独立的Spark应用程序，使用`spark-submit`，并编写一个脚本，在支持Kubernetes的Spark上运行时构建和推送Docker镜像。
- en: sbin
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: sbin
- en: Most of the scripts in this directory are administrative in purpose, for starting
    and stopping Spark components in the cluster in its various deployment modes.
    For details on the deployment modes, see the cheat sheet in [Table 1-1](ch01.html#cheat_sheet_for_spark_deployment_modes)
    in [Chapter 1](ch01.html#introduction_to_apache_spark_a_unified_a).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目录中的大多数脚本都是用于在集群中以各种部署模式启动和停止Spark组件的管理目的。有关部署模式的详细信息，请参阅[表 1-1](ch01.html#cheat_sheet_for_spark_deployment_modes)中的速查表，位于[第 1
    章](ch01.html#introduction_to_apache_spark_a_unified_a)中。
- en: kubernetes
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: kubernetes
- en: Since the release of Spark 2.4, this directory contains Dockerfiles for creating
    Docker images for your Spark distribution on a Kubernetes cluster. It also contains
    a file providing instructions on how to build the Spark distribution before building
    your Docker images.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自Spark 2.4发布以来，这个目录包含用于在Kubernetes集群上为您的Spark分发创建Docker镜像的Dockerfile。它还包含一个文件，提供了在构建您的Docker镜像之前如何构建Spark分发的说明。
- en: data
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: data
- en: 'This directory is populated with **.txt* files that serve as input for Spark’s
    components: MLlib, Structured Streaming, and GraphX.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目录中填充了作为Spark组件输入的**.txt*文件：MLlib、Structured Streaming 和 GraphX。
- en: examples
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: examples
- en: For any developer, two imperatives that ease the journey to learning any new
    platform are loads of “how-to” code examples and comprehensive documentation.
    Spark provides examples for Java, Python, R, and Scala, and you’ll want to employ
    them when learning the framework. We will allude to some of these examples in
    this and subsequent chapters.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何开发者，简化学习任何新平台之旅的两个关键点是大量的“如何”代码示例和全面的文档。Spark 提供了Java、Python、R 和 Scala 的示例，当学习框架时，您会想要使用它们。我们将在本章和后续章节中提到一些这些示例。
- en: 'Step 2: Using the Scala or PySpark Shell'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 步：使用 Scala 或 PySpark Shell
- en: 'As mentioned earlier, Spark comes with four widely used interpreters that act
    like interactive “shells” and enable ad hoc data analysis: `pyspark`, `spark-shell`,
    `spark-sql`, and `sparkR`. In many ways, their interactivity imitates shells you’ll
    already be familiar with if you have experience with Python, Scala, R, SQL, or
    Unix operating system shells such as bash or the Bourne shell.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Spark带有四种广泛使用的解释器，它们像交互式的“shell”一样，支持临时数据分析：`pyspark`，`spark-shell`，`spark-sql`和`sparkR`。在很多方面，它们的交互性模拟了您可能已经熟悉的Python、Scala、R、SQL或Unix操作系统的shell，如bash或Bourne
    shell。
- en: These shells have been augmented to support connecting to the cluster and to
    allow you to load distributed data into Spark workers’ memory. Whether you are
    dealing with gigabytes of data or small data sets, Spark shells are conducive
    to learning Spark quickly.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些shell已经增强，支持连接到集群并允许您将分布式数据加载到Spark工作节点的内存中。无论您处理的是几千兆字节的数据还是小数据集，Spark shell都有助于快速学习Spark。
- en: 'To start PySpark, `cd` to the *bin* directory and launch a shell by typing
    `**pyspark**`. If you have installed PySpark from PyPI, then just typing `**pyspark**`
    will suffice:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动PySpark，请`cd`到*bin*目录，并键入`**pyspark**`以启动shell。如果您已从PyPI安装了PySpark，则仅需键入`**pyspark**`即可：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To start a similar Spark shell with Scala, `cd` to the *bin* directory and
    type `**spark-shell**`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动一个类似的Scala Spark shell，`cd`到*bin*目录，然后键入`**spark-shell**`：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Using the Local Machine
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用本地机器
- en: Now that you’ve downloaded and installed Spark on your local machine, for the
    remainder of this chapter you’ll be using Spark interpretive shells locally. That
    is, Spark will be running in local mode.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经在本地机器上下载并安装了Spark，在本章的其余部分中，您将使用本地的Spark解释器shell。也就是说，Spark将在本地模式下运行。
- en: Note
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Refer to [Table 1-1](ch01.html#cheat_sheet_for_spark_deployment_modes) in [Chapter 1](ch01.html#introduction_to_apache_spark_a_unified_a)
    for a reminder of which components run where in local mode.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[Table 1-1](ch01.html#cheat_sheet_for_spark_deployment_modes)在[Chapter 1](ch01.html#introduction_to_apache_spark_a_unified_a)中，以提醒您本地模式下哪些组件运行在哪里。
- en: As noted in the previous chapter, Spark computations are expressed as operations.
    These operations are then converted into low-level RDD-based bytecode as tasks,
    which are distributed to Spark’s executors for execution.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，Spark计算被表达为操作。这些操作然后被转换为基于低级RDD的字节码作为任务，并分布到Spark的执行器进行执行。
- en: 'Let’s look at a short example where we read in a text file as a DataFrame,
    show a sample of the strings read, and count the total number of lines in the
    file. This simple example illustrates the use of the high-level Structured APIs,
    which we will cover in the next chapter. The `show(10, false)` operation on the
    DataFrame only displays the first 10 lines without truncating; by default the
    `truncate` Boolean flag is `true`. Here’s what this looks like in the Scala shell:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简短的示例，在此示例中，我们将文本文件读取为DataFrame，展示读取的字符串样本，并计算文件中的总行数。这个简单的示例演示了高级结构化API的使用，我们将在下一章中介绍。在Scala
    shell中，`show(10, false)`操作在DataFrame上只显示前10行而不截断；默认情况下，`truncate`布尔标志为`true`。下面是在Scala
    shell中的示例：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Quite simple. Let’s look at a similar example using the Python interpretive
    shell, `pyspark`:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 相当简单。让我们看一个使用Python解释器`pyspark`的类似示例：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To exit any of the Spark shells, press Ctrl-D. As you can see, this rapid interactivity
    with Spark shells is conducive not only to rapid learning but to rapid prototyping,
    too.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要退出任何Spark shell，请按Ctrl-D。正如您所看到的，与Spark shell的快速交互不仅有助于快速学习，也有助于快速原型开发。
- en: In the preceding examples, notice the API syntax and signature parity across
    both Scala and Python. Throughout Spark’s evolution from 1.x, that has been one
    (among many) of the enduring improvements.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，请注意Scala和Python之间的API语法和签名的一致性。在Spark从1.x版本到现在的演变过程中，这是持久改进之一。
- en: Also note that we used the high-level Structured APIs to read a text file into
    a Spark DataFrame rather than an RDD. Throughout the book, we will focus more
    on these Structured APIs; since Spark 2.x, RDDs are now consigned to low-level
    APIs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 还请注意，我们使用了高级结构化API来将文本文件读取为Spark DataFrame，而不是RDD。在本书中，我们将更多地关注这些结构化API；自Spark
    2.x以来，RDD已被归类为低级API。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Every computation expressed in high-level Structured APIs is decomposed into
    low-level optimized and generated RDD operations and then converted into Scala
    bytecode for the executors’ JVMs. This generated RDD operation code is not accessible
    to users, nor is it the same as the user-facing RDD APIs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在高级结构化API中表达的每个计算都被分解为低级优化和生成的RDD操作，然后转换为执行器的JVM的Scala字节码。生成的RDD操作代码对用户不可访问，也与用户面向的RDD
    API不同。
- en: 'Step 3: Understanding Spark Application Concepts'
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '步骤 3: 理解 Spark 应用概念'
- en: Now that you have downloaded Spark, installed it on your laptop in standalone
    mode, launched a Spark shell, and executed some short code examples interactively,
    you’re ready to take the final step.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经下载了 Spark，在您的笔记本电脑上以独立模式安装了它，在 Spark shell 中启动了它，并且交互地执行了一些简短的代码示例，您已经准备好迈出最后一步。
- en: 'To understand what’s happening under the hood with our sample code, you’ll
    need to be familiar with some of the key concepts of a Spark application and how
    the code is transformed and executed as tasks across the Spark executors. We’ll
    begin by defining some important terms:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解我们示例代码底层发生的情况，您需要熟悉一些关键的 Spark 应用概念，以及代码如何作为跨 Spark 执行器的任务转换和执行。我们将从定义一些重要术语开始：
- en: Application
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 应用
- en: A user program built on Spark using its APIs. It consists of a driver program
    and executors on the cluster.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用其API构建的用户程序的一部分。它包括群集上的驱动程序和执行器。
- en: '`SparkSession`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession`'
- en: An object that provides a point of entry to interact with underlying Spark functionality
    and allows programming Spark with its APIs. In an interactive Spark shell, the
    Spark driver instantiates a `SparkSession` for you, while in a Spark application,
    you create a `SparkSession` object yourself.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一个提供与底层 Spark 功能交互的入口点，并允许使用其API编程 Spark 的对象。在交互式 Spark shell 中，Spark 驱动程序为您实例化一个`SparkSession`，而在
    Spark 应用程序中，您自己创建一个`SparkSession`对象。
- en: Job
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 作业
- en: A parallel computation consisting of multiple tasks that gets spawned in response
    to a Spark action (e.g., `save()`, `collect()`).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作为响应于 Spark 动作（如`save()`、`collect()`）而生成的多个任务的并行计算。
- en: Stage
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段
- en: Each job gets divided into smaller sets of tasks called stages that depend on
    each other.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每个作业被划分为称为阶段的较小任务集，这些阶段彼此依赖。
- en: Task
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 任务
- en: A single unit of work or execution that will be sent to a Spark executor.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将被发送到 Spark 执行器的工作或执行的单元。
- en: Let’s dig into these concepts in a little more detail.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探讨这些概念。
- en: Spark Application and SparkSession
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 应用程序和 SparkSession
- en: At the core of every Spark application is the Spark driver program, which creates
    a `SparkSession` object. When you’re working with a Spark shell, the driver is
    part of the shell and the `SparkSession` object (accessible via the variable `spark`)
    is created for you, as you saw in the earlier examples when you launched the shells.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Spark 应用程序的核心是 Spark 驱动程序程序，它创建一个`SparkSession`对象。当您使用 Spark shell 时，驱动程序是
    shell 的一部分，并且`SparkSession`对象（通过变量`spark`访问）在您启动 shell 时已经为您创建，就像在之前的示例中看到的那样。
- en: In those examples, because you launched the Spark shell locally on your laptop,
    all the operations ran locally, in a single JVM. But you can just as easily launch
    a Spark shell to analyze data in parallel on a cluster as in local mode. The commands
    `spark-shell --help` or `pyspark --help` will show you how to connect to the Spark
    cluster manager. [Figure 2-2](#spark_components_communicate_through_the) shows
    how Spark executes on a cluster once you’ve done this.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些示例中，因为您在本地笔记本电脑上启动了 Spark shell，所有操作都在单个 JVM 中本地运行。但是，您可以轻松地启动一个 Spark shell，在集群上并行分析数据，就像在本地模式下一样。命令`spark-shell
    --help`或`pyspark --help`将向您展示如何连接到 Spark 群集管理器。[图 2-2](#spark_components_communicate_through_the)展示了在此操作后
    Spark 在集群上执行的方式。
- en: '![Spark components communicate through the Spark driver in Spark’s distributed
    architecture](assets/lesp_0202.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 组件通过 Spark 驱动程序在 Spark 的分布式架构中进行通信](assets/lesp_0202.png)'
- en: Figure 2-2\. Spark components communicate through the Spark driver in Spark’s
    distributed architecture
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. Spark 组件通过 Spark 驱动程序在 Spark 的分布式架构中进行通信。
- en: Once you have a `SparkSession`, you can [program Spark using the APIs](https://oreil.ly/2r5Xo)
    to perform Spark operations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您拥有了`SparkSession`，您就可以使用[API编程Spark](https://oreil.ly/2r5Xo)来执行Spark操作。
- en: Spark Jobs
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 作业
- en: During interactive sessions with Spark shells, the driver converts your Spark
    application into one or more Spark jobs ([Figure 2-3](#spark_driver_creating_one_or_more_spark)).
    It then transforms each job into a DAG. This, in essence, is Spark’s execution
    plan, where each node within a DAG could be a single or multiple Spark stages.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在与 Spark shell 的交互会话中，驱动程序将您的 Spark 应用程序转换为一个或多个 Spark 作业（[图 2-3](#spark_driver_creating_one_or_more_spark)）。然后将每个作业转换为
    DAG。这本质上就是 Spark 的执行计划，其中 DAG 中的每个节点可以是单个或多个 Spark 阶段。
- en: '![Spark driver creating one or more Spark jobs](assets/lesp_0203.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 驱动程序创建一个或多个 Spark 作业](assets/lesp_0203.png)'
- en: Figure 2-3\. Spark driver creating one or more Spark jobs
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. Spark 驱动程序创建一个或多个 Spark 作业
- en: Spark Stages
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 阶段
- en: As part of the DAG nodes, stages are created based on what operations can be
    performed serially or in parallel ([Figure 2-4](#spark_job_creating_one_or_more_stages)).
    Not all Spark operations can happen in a single stage, so they may be divided
    into multiple stages. Often stages are delineated on the operator’s computation
    boundaries, where they dictate data transfer among Spark executors.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 DAG 节点的一部分，阶段基于可以串行或并行执行的操作创建（[图 2-4](#spark_job_creating_one_or_more_stages)）。并非所有
    Spark 操作都可以在单个阶段中进行，因此它们可能会分成多个阶段。通常，阶段在运算符的计算边界上界定，它们指示 Spark 执行器之间的数据传输。
- en: '![Spark job creating one or more stages](assets/lesp_0204.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 作业创建一个或多个阶段](assets/lesp_0204.png)'
- en: Figure 2-4\. Spark job creating one or more stages
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. Spark 作业创建一个或多个阶段
- en: Spark Tasks
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 任务
- en: Each stage is comprised of Spark tasks (a unit of execution), which are then
    federated across each Spark executor; each task maps to a single core and works
    on a single partition of data ([Figure 2-5](#spark_stage_creating_one_or_more_tasks_t)).
    As such, an executor with 16 cores can have 16 or more tasks working on 16 or
    more partitions in parallel, making the execution of Spark’s tasks exceedingly
    parallel!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个阶段由 Spark 任务（执行的单位）组成，然后在每个 Spark 执行器上进行联合；每个任务映射到单个核心并在单个数据分区上工作（[图 2-5](#spark_stage_creating_one_or_more_tasks_t)）。因此，具有
    16 个核心的执行器可以并行处理 16 个或更多任务，从而使 Spark 任务的执行极为并行化！
- en: '![Spark stage creating one or more tasks to be distributed to executors](assets/lesp_0205.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 阶段创建一个或多个任务以分配给执行器](assets/lesp_0205.png)'
- en: Figure 2-5\. Spark stage creating one or more tasks to be distributed to executors
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. Spark 阶段创建一个或多个任务以分配给执行器
- en: Transformations, Actions, and Lazy Evaluation
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换、动作和惰性评估
- en: 'Spark operations on distributed data can be classified into two types: *transformations*
    and *actions*. Transformations, as the name suggests, transform a Spark DataFrame
    into a new DataFrame without altering the original data, giving it the property
    of immutability. Put another way, an operation such as `select()` or `filter()`
    will not change the original DataFrame; instead, it will return the transformed
    results of the operation as a new DataFrame.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 对分布式数据的操作可以分为两种类型：*转换* 和 *动作*。转换顾名思义，将 Spark DataFrame 转换为新的 DataFrame
    而不改变原始数据，具有不可变性的属性。换句话说，诸如 `select()` 或 `filter()` 的操作不会改变原始 DataFrame；相反，它将返回操作的转换结果作为新的
    DataFrame。
- en: All transformations are evaluated lazily. That is, their results are not computed
    immediately, but they are recorded or remembered as a *lineage*. A recorded lineage
    allows Spark, at a later time in its execution plan, to rearrange certain transformations,
    coalesce them, or optimize transformations into stages for more efficient execution.
    Lazy evaluation is Spark’s strategy for delaying execution until an action is
    invoked or data is “touched” (read from or written to disk).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 所有转换都是惰性评估的。也就是说，它们的结果不会立即计算，而是被记录或记忆为 *血统*。记录的血统允许 Spark 在执行计划的稍后时间重新排列某些转换，将其合并，或者优化转换为更有效的阶段。惰性评估是
    Spark 延迟执行的策略，直到调用动作或数据被“触及”（从磁盘读取或写入）。
- en: An action triggers the lazy evaluation of all the recorded transformations.
    In [Figure 2-6](#transformationscomma_actionscomma_and_la), all transformations
    T are recorded until the action A is invoked. Each transformation T produces a
    new DataFrame.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 动作触发了所有记录转换的惰性评估。在 [图 2-6](#transformationscomma_actionscomma_and_la) 中，所有转换
    T 被记录，直到调用动作 A。每个转换 T 产生一个新的 DataFrame。
- en: '![Lazy transformations and eager actions](assets/lesp_0206.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![懒惰的转换和渴望的动作](assets/lesp_0206.png)'
- en: Figure 2-6\. Lazy transformations and eager actions
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 懒惰的转换和渴望的动作
- en: While lazy evaluation allows Spark to optimize your queries by peeking into
    your chained transformations, lineage and data immutability provide fault tolerance.
    Because Spark records each transformation in its lineage and the DataFrames are
    immutable between transformations, it can reproduce its original state by simply
    replaying the recorded lineage, giving it resiliency in the event of failures.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 惰性评估使得 Spark 能够通过查看链式转换来优化查询，而血统和数据不可变性则提供了容错能力。由于 Spark 记录了每个转换的血统，并且在转换之间的
    DataFrame 是不可变的，它可以通过简单地重放记录的血统来恢复其原始状态，从而在发生故障时具有容错能力。
- en: '[Table 2-1](#transformations_and_actions_as_spark_ope) lists some examples
    of transformations and actions.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2-1](#transformations_and_actions_as_spark_ope) 列出了一些转换和操作的示例。'
- en: Table 2-1\. Transformations and actions as Spark operations
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-1\. Spark 操作的转换和操作
- en: '| Transformations | Actions |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 转换 | 操作 |'
- en: '| --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `orderBy()` | `show()` |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `orderBy()` | `show()` |'
- en: '| `groupBy()` | `take()` |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `groupBy()` | `take()` |'
- en: '| `filter()` | `count()` |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `filter()` | `count()` |'
- en: '| `select()` | `collect()` |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `select()` | `collect()` |'
- en: '| `join()` | `save()` |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `join()` | `save()` |'
- en: 'The actions and transformations contribute to a Spark query plan, which we
    will cover in the next chapter. Nothing in a query plan is executed until an action
    is invoked. The following example, shown both in Python and Scala, has two transformations—`read()`
    and `filter()`—and one action—`count()`. The action is what triggers the execution
    of all transformations recorded as part of the query execution plan. In this example,
    nothing happens until `filtered.count()` is executed in the shell:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 操作和转换贡献到了一个 Spark 查询计划中，我们将在下一章中讨论它。在执行查询计划之前，不会执行任何操作。下面的示例，以 Python 和 Scala
    两种语言显示，有两个转换——`read()` 和 `filter()`——以及一个操作——`count()`。操作触发了作为查询执行计划的一部分记录的所有转换的执行。在这个示例中，直到在
    shell 中执行 `filtered.count()` 之前，什么都不会发生：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Narrow and Wide Transformations
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 窄转换和宽转换
- en: As noted, transformations are operations that Spark evaluates lazily. A huge
    advantage of the lazy evaluation scheme is that Spark can inspect your computational
    query and ascertain how it can optimize it. This optimization can be done by either
    joining or pipelining some operations and assigning them to a stage, or breaking
    them into stages by determining which operations require a shuffle or exchange
    of data across clusters.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，转换是 Spark 惰性评估的操作。惰性评估方案的一个巨大优势在于，Spark 能够检查您的计算查询并确定如何优化它。此优化可以通过连接或流水线一些操作并将它们分配到一个阶段，或者通过确定哪些操作需要在集群之间进行数据洗牌或交换来进行。这将这些操作分解成阶段。
- en: Transformations can be classified as having either *narrow dependencies* or
    *wide dependencies*. Any transformation where a single output partition can be
    computed from a single input partition is a *narrow* transformation. For example,
    in the previous code snippet, `filter()` and `contains()` represent narrow transformations
    because they can operate on a single partition and produce the resulting output
    partition without any exchange of data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 转换可以被分类为具有 *窄依赖* 或 *宽依赖*。任何一个可以从单个输入分区计算单个输出分区的转换都是 *窄* 转换。例如，在前面的代码片段中，`filter()`
    和 `contains()` 表示窄转换，因为它们可以在单个分区上操作并产生结果输出分区而无需交换任何数据。
- en: However, `groupBy()` or `orderBy()` instruct Spark to perform *wide* transformations,
    where data from other partitions is read in, combined, and written to disk. Since
    each partition will have its own count of the word that contains the “Spark” word
    in its row of data, a count (`groupBy()`) will force a shuffle of data from each
    of the executor’s partitions across the cluster. In this transformation, `orderBy()`
    requires output from other partitions to compute the final aggregation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`groupBy()` 或 `orderBy()` 指示 Spark 执行 *宽* 转换，从而读取来自其他分区的数据，将其组合并写入磁盘。由于每个分区将有其自己的包含
    “Spark” 词的单词计数的数据行，一个计数 (`groupBy()`) 将会强制进行来自执行器分区的数据在集群中的洗牌。在这个转换中，`orderBy()`
    需要来自其他分区的输出来计算最终的聚合。
- en: '[Figure 2-7](#narrow_versus_wide_transformations) illustrates the two types
    of dependencies.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-7](#narrow_versus_wide_transformations) 描述了两种依赖关系的类型。'
- en: '![Narrow versus wide transformations](assets/lesp_0207.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![窄转换与宽转换](assets/lesp_0207.png)'
- en: Figure 2-7\. Narrow versus wide transformations
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7\. 窄转换与宽转换
- en: The Spark UI
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark UI
- en: 'Spark includes a [graphical user interface](https://oreil.ly/AXg5h) that you
    can use to inspect or monitor Spark applications in their various stages of decomposition—that
    is jobs, stages, and tasks. Depending on how Spark is deployed, the driver launches
    a web UI, running by default on port 4040, where you can view metrics and details
    such as:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 包含一个[图形用户界面](https://oreil.ly/AXg5h)，您可以使用它来检查或监视 Spark 应用程序在其不同分解阶段（即作业、阶段和任务）的情况。根据
    Spark 的部署方式，驱动程序会启动一个 Web UI，默认运行在 4040 端口，您可以查看诸如以下详细信息和指标：
- en: A list of scheduler stages and tasks
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度器阶段和任务的列表
- en: A summary of RDD sizes and memory usage
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD 大小和内存使用情况的摘要
- en: Information about the environment
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于环境的信息
- en: Information about the running executors
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行执行器的信息
- en: All the Spark SQL queries
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有的 Spark SQL 查询
- en: In local mode, you can access this interface at *http://<localhost>:4040* in
    a web browser.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地模式下，您可以在 Web 浏览器中访问 *http://<localhost>:4040* 来访问此界面。
- en: Note
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When you launch `spark-shell`, part of the output shows the localhost URL to
    access at port 4040.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当您启动 `spark-shell` 时，输出的一部分显示了本地主机的 URL，以访问 4040 端口。
- en: Let’s inspect how the Python example from the previous section translates into
    jobs, stages, and tasks. To view what the DAG looks like, click on “DAG Visualization”
    in the web UI. As [Figure 2-8](#the_dag_for_our_simple_python_example) shows,
    the driver created a single job and a single stage.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查前一节中 Python 示例如何转换为作业、阶段和任务。要查看 DAG 的外观，请在 Web UI 中点击“DAG 可视化”。正如[图 2-8](#the_dag_for_our_simple_python_example)所示，驱动程序创建了一个作业和一个阶段。
- en: '![The DAG for our simple Python example](assets/lesp_0208.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![我们简单 Python 示例的 DAG](assets/lesp_0208.png)'
- en: Figure 2-8\. The DAG for our simple Python example
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-8\. 我们简单 Python 示例的 DAG
- en: Notice that there is no `Exchange`, where data is exchanged between executors,
    required because there is only a single stage. The individual operations of the
    stage are shown in blue boxes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于只有一个阶段，不存在需要在执行器之间交换数据的“交换”操作。阶段的各个操作显示为蓝色框。
- en: Stage 0 is comprised of one task. If you have multiple tasks, they will be executed
    in parallel. You can view the details of each stage in the Stages tab, as shown
    in [Figure 2-9](#details_of_stage_0).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段 0 由一个任务组成。如果有多个任务，它们将并行执行。您可以在“阶段”选项卡中查看每个阶段的详细信息，如[图 2-9](#details_of_stage_0)所示。
- en: '![Details of stage 0](assets/lesp_0209.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![阶段 0 的详细信息](assets/lesp_0209.png)'
- en: Figure 2-9\. Details of stage 0
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-9\. 阶段 0 的详细信息
- en: We will cover the Spark UI in more detail in [Chapter 7](ch07.html#optimizing_and_tuning_spark_applications).
    For now, just note that the UI provides a microscopic lens into Spark’s internal
    workings as a tool for debugging and inspecting.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第 7 章](ch07.html#optimizing_and_tuning_spark_applications)中更详细地介绍 Spark
    UI。现在，只需注意 UI 提供了一种深入了解 Spark 内部工作的微观镜头，作为调试和检查工具。
- en: Your First Standalone Application
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您的第一个独立应用程序
- en: To facilitate learning and exploring, the Spark distribution comes with a set
    of sample applications for each of Spark’s components. You are welcome to peruse
    the *examples* directory in your installation location to get an idea of what’s
    available.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于学习和探索，Spark 发行版附带了每个 Spark 组件的一组示例应用程序。您可以浏览安装位置中的 *examples* 目录，以了解可用内容。
- en: 'From the installation directory on your local machine, you can run one of the
    several Java or Scala sample programs that are provided using the command `bin/run-example
    *<class> [params]*`. For example:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从本地机器上的安装目录，您可以使用命令 `bin/run-example *<class> [params]*` 运行提供的几个 Java 或 Scala
    示例程序之一。例如：
- en: '[PRE7]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This will spew out `INFO` messages on your console along with a list of each
    word in the *README.md* file and its count (counting words is the “Hello, World”
    of distributed computing).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在控制台上输出 `INFO` 消息，以及 *README.md* 文件中每个单词及其计数的列表（计数单词是分布式计算的“Hello, World”）。
- en: Counting M&Ms for the Cookie Monster
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为 Cookie Monster 计算 M&M 的数量
- en: In the previous example, we counted words in a file. If the file were huge,
    it would be distributed across a cluster partitioned into small chunks of data,
    and our Spark program would distribute the task of counting each word in each
    partition and return us the final aggregated count. But that example has become
    a bit of a cliche.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们统计了文件中的单词。如果文件很大，它将分布在由数据的小块组成的集群中，我们的 Spark 程序将分发任务，计算每个分区中每个单词的计数，并返回最终聚合的计数。但是，这个示例已经变成了一个陈词滥调。
- en: Let’s solve a similar problem, but with a larger data set and using more of
    Spark’s distribution functionality and DataFrame APIs. We will cover the APIs
    used in this program in later chapters, but for now bear with us.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解决一个类似的问题，但使用更大的数据集和更多 Spark 分布功能和 DataFrame APIs。我们将在后面的章节中介绍本程序中使用的 APIs，但现在请稍等片刻。
- en: Among the authors of this book is a data scientist who loves to bake cookies
    with M&Ms in them, and she rewards her students in the US states where she frequently
    teaches machine learning and data science courses with batches of those cookies.
    But she’s data-driven, obviously, and wants to ensure that she gets the right
    colors of M&Ms in the cookies for students in the different states ([Figure 2-11](#distribution_of_mampersandms_by_color_le)).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的作者中有一位数据科学家，她喜欢在烤饼干时加入 M&M，并且在她经常教授机器学习和数据科学课程的美国州份给她的学生批量提供这些饼干作为奖励。但显然她是数据驱动的，她希望确保在不同州份的学生得到正确颜色的
    M&M 饼干（见 [Figure 2-11](#distribution_of_mampersandms_by_color_le)）。
- en: '![Distribution of M&Ms by color (source: https://oreil.ly/mhWIT)](assets/lesp_0211.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![M&M 按颜色分布（来源：https://oreil.ly/mhWIT）](assets/lesp_0211.png)'
- en: 'Figure 2-11\. Distribution of M&Ms by color (source: [*https://oreil.ly/mhWIT*](https://oreil.ly/mhWIT))'
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-11\. M&M 按颜色分布（来源：[*https://oreil.ly/mhWIT*](https://oreil.ly/mhWIT)）
- en: Let’s write a Spark program that reads a file with over 100,000 entries (where
    each row or line has a `<*state*, *mnm_color*, *count*>`) and computes and aggregates
    the counts for each color and state. These aggregated counts tell us the colors
    of M&Ms favored by students in each state. The complete Python listing is provided
    in [Example 2-1](#counting_and_aggregating_mampersandms_l).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个 Spark 程序，读取一个包含超过 100,000 条记录的文件（每行或行有一个 `<*state*, *mnm_color*, *count*>`），并计算并聚合每种颜色和州的计数。这些聚合计数告诉我们每个州学生喜欢的
    M&M 颜色。详细的 Python 代码清单见 [Example 2-1](#counting_and_aggregating_mampersandms_l)。
- en: Example 2-1\. Counting and aggregating M&Ms (Python version)
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 2-1\. 计数和聚合 M&M（Python 版本）
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can enter this code into a Python file called *mnmcount.py* using your favorite
    editor, download the *mnn_dataset.csv* file from this book’s [GitHub repo](https://github.com/databricks/LearningSparkV2),
    and submit it as a Spark job using the `submit-spark` script in the installation’s
    *bin* directory. Set your `SPARK_HOME` environment variable to the root-level
    directory where you installed Spark on your local machine.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用你喜欢的编辑器将此代码输入到名为 *mnmcount.py* 的 Python 文件中，从本书的 [GitHub repo](https://github.com/databricks/LearningSparkV2)
    下载 *mnn_dataset.csv* 文件，并使用安装目录中的 *bin* 目录中的 `submit-spark` 脚本将其作为 Spark 作业提交。将你的
    `SPARK_HOME` 环境变量设置为你在本地机器上安装 Spark 的根级目录。
- en: Note
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding code uses the DataFrame API, which reads like high-level DSL queries.
    We will cover this and the other APIs in the next chapter; for now, note the clarity
    and simplicity with which you can instruct Spark what to do, not how to do it,
    unlike with the RDD API. Cool stuff!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用了 DataFrame API，读起来像高级 DSL 查询。我们将在下一章中介绍这个和其他 API；现在请注意，你可以清晰简洁地告诉 Spark
    要做什么，而不是如何做，与 RDD API 不同。非常酷的东西！
- en: To avoid having verbose `INFO` messages printed to the console, copy the *log4j.properties.template*
    file to *log4j.properties* and set `log4j.rootCategory=WARN` in the *conf/log4j.properties*
    file.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在控制台上打印冗长的`INFO`消息，请将 *log4j.properties.template* 文件复制到 *log4j.properties*，并在
    *conf/log4j.properties* 文件中设置 `log4j.rootCategory=WARN`。
- en: 'Let’s submit our first Spark job using the Python APIs (for an explanation
    of what the code does, please read the inline comments in [Example 2-1](#counting_and_aggregating_mampersandms_l)):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Python APIs 提交我们的第一个 Spark 作业（关于代码功能的解释，请阅读 [Example 2-1](#counting_and_aggregating_mampersandms_l)
    中的内联注释）：
- en: '[PRE9]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: First we see all the aggregations for each M&M color for each state, followed
    by those only for CA (where the preferred color is yellow).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们看到每个州每种 M&M 颜色的所有聚合结果，然后是仅限于 CA 的那些（那里偏爱黄色）。
- en: What if you want to use a Scala version of this same Spark program? The APIs
    are similar; in Spark, parity is well preserved across the supported languages,
    with minor syntax differences. [Example 2-2](#counting_and_aggregating_mampersandms_le)
    is the Scala version of the program. Take a look, and in the next section we’ll
    show you how to build and run the application.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要使用 Scala 版本的同一个 Spark 程序怎么办？这些 APIs 是相似的；在 Spark 中，跨支持的语言保持了良好的一致性，只有少量语法差异。
    [Example 2-2](#counting_and_aggregating_mampersandms_le) 是该程序的 Scala 版本。看一看，在下一节中我们将展示如何构建和运行应用程序。
- en: Example 2-2\. Counting and aggregating M&Ms (Scala version)
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 2-2\. 计数和聚合 M&M（Scala 版本）
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Building Standalone Applications in Scala
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Scala 中构建独立应用程序
- en: We will now show you how to build your first Scala Spark program, using the
    [Scala Build Tool (sbt)](https://www.scala-sbt.org).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将展示如何使用 [Scala Build Tool (sbt)](https://www.scala-sbt.org) 构建你的第一个 Scala
    Spark 程序。
- en: Note
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Because Python is an interpreted language and there is no such step as compiling
    first (though it’s possible to compile your Python code into bytecode in *.pyc*),
    we will not go into this step here. For details on how to use Maven to build Java
    Spark programs, we refer you to the [guide](https://oreil.ly/1qMlG) on the Apache
    Spark website. For brevity in this book, we cover examples mainly in Python and
    Scala.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 Python 是一种解释性语言，并且没有像编译成字节码那样的编译步骤（尽管可以将 Python 代码编译成 *.pyc* 字节码），我们不会在这里详细介绍这一步骤。有关如何使用
    Maven 构建 Java Spark 程序的详细信息，请参阅 Apache Spark 网站上的 [指南](https://oreil.ly/1qMlG)。为了书写简洁，在本书中，我们主要以
    Python 和 Scala 示例为主。
- en: '*build.sbt* is the specification file that, like a makefile, describes and
    instructs the Scala compiler to build your Scala-related tasks, such as jars,
    packages, what dependencies to resolve, and where to look for them. In our case,
    we have a simple sbt file for our M&M code ([Example 2-3](#sbt_build_file)).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*build.sbt* 是类似于 makefile 的规范文件，描述和指导 Scala 编译器如何构建你的 Scala 相关任务，例如 jars、packages、需要解决的依赖以及查找它们的位置。在我们的例子中，我们有一个简单的
    sbt 文件用于我们的 M&M 代码 ([示例 2-3](#sbt_build_file))。'
- en: Example 2-3\. sbt build file
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-3\. sbt 构建文件
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Assuming that you have the [Java Development Kit (JDK)](https://oreil.ly/AfpMz)
    and sbt installed and `JAVA_HOME` and `SPARK_HOME` set, with a single command,
    you can build your Spark application:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经安装了 [Java 开发工具包 (JDK)](https://oreil.ly/AfpMz) 和 sbt，并设置了 `JAVA_HOME` 和
    `SPARK_HOME`，通过一条命令，你可以构建你的 Spark 应用程序：
- en: '[PRE12]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After a successful build, you can run the Scala version of the M&M count example
    as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 成功构建后，你可以像下面这样运行 M&M 计数示例的 Scala 版本：
- en: '[PRE13]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output is the same as for the Python run. Try it!
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 输出与 Python 运行相同。试一试吧！
- en: There you have it—our data scientist author will be more than happy to use this
    data to decide what colors of M&Ms to use in the cookies she bakes for her classes
    in any of the states she teaches in.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样——我们的数据科学家作者将非常乐意使用这些数据来决定在她所教的任何州的课程中要使用什么颜色的 M&M 饼干。
- en: Summary
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we covered the three simple steps you need to take to get
    started with Apache Spark: downloading the framework, familiarizing yourself with
    the Scala or PySpark interactive shell, and getting to grips with high-level Spark
    application concepts and terms. We gave a quick overview of the process by which
    you can use transformations and actions to write a Spark application, and we briefly
    introduced using the Spark UI to examine the jobs, stages, and tasks created.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了开始使用 Apache Spark 的三个简单步骤：下载框架、熟悉 Scala 或 PySpark 交互式 shell，并了解高级
    Spark 应用程序的概念和术语。我们快速概述了使用转换和动作编写 Spark 应用程序的过程，并简要介绍了使用 Spark UI 来检查创建的作业、阶段和任务。
- en: Finally, through a short example, we showed you how you can use the high-level
    Structured APIs to tell Spark what to do—which brings us to the next chapter,
    where we examine those APIs in more detail.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过一个简短的示例，我们向你展示了如何使用高级结构化 API 告诉 Spark 做什么——这将引导我们进入下一章，更详细地探讨这些 API。
