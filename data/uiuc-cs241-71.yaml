- en: 'File System, Part 7: Scalable and Reliable Filesystems'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件系统，第7部分：可扩展和可靠的文件系统
- en: Reliable Single Disk Filesystems
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠的单磁盘文件系统
- en: How and why does the kernel cache the filesystem?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内核如何以及为什么缓存文件系统？
- en: 'Most filesystems cache significant amounts of disk data in physical memory.
    Linux, in this respect, is particularly extreme: All unused memory is used as
    a giant disk cache.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数文件系统在物理内存中缓存大量磁盘数据。在这方面，Linux特别极端：所有未使用的内存都被用作巨大的磁盘缓存。
- en: The disk cache can have significant impact on overall system performance because
    disk I/O is slow. This is especially true for random access requests on spinning
    disks where the disk read-write latency is dominated by the seek time required
    to move the read-write disk head to the correct position.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘缓存可能会对整个系统性能产生重大影响，因为磁盘I/O速度很慢。这对于旋转磁盘上的随机访问请求尤其如此，其中磁盘读写延迟由移动读写磁盘头到正确位置所需的寻道时间主导。
- en: 'For efficiency, the kernel caches recently used disk blocks. For writing we
    have to choose a trade-off between performance and reliability: Disk writes can
    also be cached ("Write-back cache") where modified disk blocks are stored in memory
    until evicted. Alternatively a ''write-through cache'' policy can be employed
    where disk writes are sent immediately to the disk. The latter is safer (as filesystem
    modifications are quickly stored to persistent media) but slower than a write-back
    cache; If writes are cached then they can be delayed and efficiently scheduled
    based on the physical position of each disk block.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高效率，内核会缓存最近使用的磁盘块。对于写入，我们必须在性能和可靠性之间进行权衡：磁盘写入也可以被缓存（“写回缓存”），其中修改后的磁盘块存储在内存中直到被驱逐。或者可以采用“写穿缓存”策略，其中磁盘写入立即发送到磁盘。后者比写回缓存更安全（因为文件系统修改会快速存储到持久介质），但比写回缓存慢；如果写入被缓存，那么它们可以被延迟，并且可以根据每个磁盘块的物理位置进行高效调度。
- en: Note this is a simplified description because solid state drives (SSDs) can
    be used as a secondary write-back cache.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是一个简化的描述，因为固态硬盘（SSD）可以用作辅助写回缓存。
- en: Both solid state disks (SSD) and spinning disks have improved performance when
    reading or writing sequential data. Thus operating system can often use a read-ahead
    strategy to amortize the read-request costs (e.g. time cost for a spinning disk)
    and request several contiguous disk blocks per request. By issuing an I/O request
    for the next disk block before the user application requires the next disk block,
    the apparent disk I/O latency can be reduced.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是固态硬盘（SSD）还是旋转硬盘，在读取或写入顺序数据时都具有改进的性能。因此，操作系统通常可以使用预读策略来分摊读取请求成本（例如旋转硬盘的时间成本），并请求每个请求的几个连续磁盘块。通过在用户应用程序需要下一个磁盘块之前发出下一个磁盘块的I/O请求，可以减少表面磁盘I/O延迟。
- en: My data is important! Can I force the disk writes to be saved to the physical
    media and wait for it to complete?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我的数据很重要！我可以强制磁盘写入保存到物理介质并等待完成吗？
- en: Yes (almost). Call `sync` to request that a filesystem changes be written (flushed)
    to disk. However, not all operating systems honor this request and even if the
    data is evicted from the kernel buffers the disk firmware use an internal on-disk
    cache or may not yet have finished changing the physical media.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 是的（几乎）。调用`sync`请求将文件系统更改写入（刷新）到磁盘。但并非所有操作系统都会遵守此请求，即使数据已从内核缓冲区中驱逐，磁盘固件也会使用内部磁盘缓存，或者可能尚未完成更改物理介质。
- en: Note you can also request that all changes associated with a particular file
    descriptor are flushed to disk using `fsync(int fd)`
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您还可以使用`fsync（int fd）`请求将与特定文件描述符相关的所有更改刷新到磁盘。
- en: What if my disk fails in the middle of an important operation?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如果我的磁盘在重要操作中失败怎么办？
- en: Don't worry most modern file systems do something called **journalling** that
    work around this. What the file system does is before it completes a potentially
    expensive operation, is that it writes what it is going to do down in a journal.
    In the case of a crash or failure, one can step through the journal and see which
    files are corrupt and fix them. This is a way to salvage hard disks in cases there
    is critical data and there is no apparent backup.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心，大多数现代文件系统都有一种称为**日志**的东西来解决这个问题。文件系统在完成潜在昂贵的操作之前，会将其要做的事情写在日志中。在崩溃或故障的情况下，可以逐步查看日志并查看哪些文件损坏并修复它们。这是一种在关键数据存在且没有明显备份的情况下挽救硬盘的方法。
- en: How likely is a disk failure?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 磁盘故障的可能性有多大？
- en: Disk failures are measured using "Mean-Time-Failure". For large arrays, the
    mean failure time can be surprisingly short. For example if the MTTF(single disk)
    = 30,000 hours, then the MTTF(100 disks)= 30000/100=300 hours ie about 12 days!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘故障是用“平均故障时间”来衡量的。对于大型数组，平均故障时间可能会非常短。例如，如果MTTF（单个磁盘）= 30,000小时，则MTTF（100个磁盘）=
    30000/100 = 300小时，即约12天！
- en: Redundancy
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 冗余
- en: How do I protect my data from disk failure?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何保护我的数据免受磁盘故障？
- en: Easy! Store the data twice! This is the main principle of a "RAID-1" disk array.
    RAID is short for redundant array of inexpensive disks. By duplicating the writes
    to a disk with writes to another (backup disk) there are exactly two copies of
    the data. If one disk fails, the other disk serves as the only copy until it can
    be re-cloned. Reading data is faster (since data can be requested from either
    disk) but writes are potentially twice as slow (now two write commands need to
    be issued for every disk block write) and, compared to using a single disk, the
    cost of storage per byte has doubled.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单！数据存储两次！这是“RAID-1”磁盘阵列的主要原则。RAID是廉价磁盘冗余阵列的缩写。通过将写入复制到一个磁盘并将写入复制到另一个磁盘（备份磁盘），数据恰好有两份副本。如果一个磁盘故障，另一个磁盘将作为唯一副本，直到可以重新克隆。读取数据更快（因为数据可以从任一磁盘请求），但写入可能会慢两倍（现在每个磁盘块写入需要发出两个写命令），并且与使用单个磁盘相比，每字节存储成本翻了一番。
- en: Another common RAID scheme is RAID-0, meaning that a file could be split up
    amoung two disks, but if any one of the disks fail then the files are irrecoverable.
    This has the benefit of halving write times because one part of the file could
    be writing to hard disk one and another part to hard disk two.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的RAID方案是RAID-0，意味着文件可以分割在两个磁盘中，但如果任何一个磁盘故障，那么文件将无法恢复。这样做的好处是可以将写入时间减半，因为文件的一部分可以写入硬盘一，另一部分可以写入硬盘二。
- en: It is also common to combine these systems. If you have a lot of hard disks,
    consider RAID-10\. This is where you have two systems of RAID-1, but the systems
    are hooked up in RAID-0 to each other. This means you would get roughly the same
    speed from the slowdowns but now any one disk can fail and you can recover that
    disk. (If two disks from opposing raid partitions fail there is a chance that
    recover can happen though we don't could on it most of the time).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 还常常将这些系统结合在一起。如果你有很多硬盘，考虑RAID-10。这是指有两个RAID-1系统，但这些系统在彼此之间以RAID-0连接。这意味着你可以从减速中获得大致相同的速度，但现在任何一个磁盘都可以故障，你可以恢复该磁盘。（如果来自相对RAID分区的两个磁盘故障，有可能进行恢复，尽管我们大多数时候不依赖它）。
- en: What is RAID-3?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAID-3是什么？
- en: RAID-3 uses parity codes instead of mirroring the data. For each N-bits written
    we will write one extra bit, the 'Parity bit' that ensures the total number of
    1s written is even. The parity bit is written to an additional disk. If any one
    disk (including the parity disk) is lost, then its contents can still be computed
    using the contents of the other disks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: RAID-3使用奇偶校验码而不是镜像数据。对于每N位写入，我们将写入一个额外的位，即“奇偶校验位”，以确保写入的1的总数是偶数。奇偶校验位被写入到额外的磁盘上。如果任何一个磁盘（包括奇偶校验磁盘）丢失，那么它的内容仍然可以使用其他磁盘的内容计算出来。
- en: '![](../Images/1d72e84109674f2e5db6da917167668b.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d72e84109674f2e5db6da917167668b.jpg)'
- en: One disadvantage of RAID-3 is that whenever a disk block is written, the parity
    block will always be written too. This means that there is effectively a bottleneck
    in a separate disk. In practice, this is more likely to cause a failure because
    one disk is being used 100% of the time and once that disk fails then the other
    disks are more prone to failure.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: RAID-3的一个缺点是每当写入一个磁盘块时，奇偶校验块也总是会被写入。这意味着实际上有一个单独的磁盘瓶颈。实际上，这更有可能导致故障，因为一个磁盘被100%使用，一旦该磁盘故障，其他磁盘更容易发生故障。
- en: How secure is RAID-3 to data-loss?
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAID-3对数据丢失有多安全？
- en: A single disk failure will not result in data loss (because there is sufficient
    data to rebuild the array from the remaining disks). Data-loss will occur when
    a two disks are unusable because there is no longer sufficient data to rebuild
    the array. We can calculate the probability of a two disk failure based on the
    repair time which includes not just the time to insert a new disk but the time
    required to rebuild the entire contents of the array.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 单个磁盘故障不会导致数据丢失（因为有足够的数据可以从剩余的磁盘重建阵列）。当两个磁盘不可用时，由于不再有足够的数据来重建阵列，数据丢失将发生。我们可以根据修复时间计算两个磁盘故障的概率，这不仅包括插入新磁盘的时间，还包括重建整个阵列内容所需的时间。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using typical numbers (MTTR=1day, MTTF=1000days, N-1 = 9,, p=0.009
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用典型数字（MTTR=1天，MTTF=1000天，N-1=9，p=0.009
- en: There is a 1% chance that another drive will fail during the rebuild process
    (at that point you had better hope you still have an accessible backup of your
    original data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在重建过程中，另一块驱动器出现故障的概率为1%（在这一点上，你最好希望你仍然有原始数据的可访问备份）。
- en: In practice the probability of a second failure during the repair process is
    likely higher because rebuilding the array is I/O-intensive (and on top of normal
    I/O request activity). This higher I/O load will also stress the disk array
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，修复过程中第二次故障的概率可能更高，因为重建阵列是I/O密集型的（并且在正常I/O请求活动之上）。这种更高的I/O负载也会对磁盘阵列造成压力
- en: What is RAID-5?
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAID-5是什么？
- en: RAID-5 is similar to RAID-3 except that the check block (parity information)
    is assigned to different disks for different blocks. The check-block is 'rotated'
    through the disk array. RAID-5 provides better read and write performance than
    RAID-3 because there is no longer the bottleneck of the single parity disk. The
    one drawback is that you need more disks to have this setup and there are more
    complicated algorithms need to be used
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RAID-5类似于RAID-3，只是检查块（奇偶校验信息）分配给不同的磁盘用于不同的块。检查块在磁盘阵列中“旋转”。RAID-5提供比RAID-3更好的读写性能，因为不再有单个奇偶校验磁盘的瓶颈。唯一的缺点是你需要更多的磁盘来设置这个，并且需要使用更复杂的算法。
- en: '![](../Images/d69ff523ab899f8909888c58907d4ca8.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d69ff523ab899f8909888c58907d4ca8.jpg)'
- en: Distributed storage
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式存储
- en: Failure is the common case Google reports 2-10% of disks fail per year Now multiply
    that by 60,000+ disks in a single warehouse... Must survive failure of not just
    a disk, but a rack of servers or a whole data center
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 故障是常见情况，谷歌报告称每年有2-10%的磁盘故障，现在将这个数字乘以单个仓库中的60,000多个磁盘...必须经受住不仅是磁盘的故障，还有服务器机架或整个数据中心的故障
- en: 'Solutions Simple redundancy (2 or 3 copies of each file) e.g., Google GFS (2001)
    More efficient redundancy (analogous to RAID 3++) e.g., [Google Colossus filesystem](http://goo.gl/LwFIy)
    (~2010): customizable replication including Reed-Solomon codes with 1.5x redundancy'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案简单冗余（每个文件有2或3个副本），例如，谷歌GFS（2001年）更有效的冗余（类似于RAID 3++），例如，[Google Colossus文件系统](http://goo.gl/LwFIy)（约2010年）：可定制的复制，包括带有1.5倍冗余的Reed-Solomon编码
