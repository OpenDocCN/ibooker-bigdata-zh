- en: Chapter 21\. Time-Based Stream Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 21 章。基于时间的流处理
- en: As we have hinted at previously, and as we have shown in previous transformations,
    Spark Streaming offers the capability of building time-based aggregates of data.
    In contrast with Structured Streaming, the out-of-the-box capabilities of Spark
    Streaming in this area are limited to processing time, which, if you recall from
    [“The Effect of Time”](ch02.xhtml#event-time-intro), is the time when the streaming
    engine processes the events.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，并且如我们在之前的转换中展示的，Spark Streaming 提供了构建基于时间的数据聚合的能力。与结构化流不同，Spark Streaming
    在这一领域的开箱即用能力仅限于处理时间，如果您还记得来自 [“时间的影响”](ch02.xhtml#event-time-intro) 的话，那是流引擎处理事件的时间。
- en: In this chapter, we are going to look into the different aggregation capabilities
    of Spark Streaming. Although they are constrained to the processing-time domain,
    they provide rich semantics and can be helpful to process data in a scalable and
    resource-constrained way.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨 Spark Streaming 的不同聚合能力。虽然它们受限于处理时间域，但它们提供丰富的语义，可以帮助以可伸缩和资源受限的方式处理数据。
- en: Window Aggregations
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口聚合
- en: Aggregation is a frequent pattern in stream data processing, reflecting the
    difference in concerns from the producers of the data (at the input) and the consumers
    of data (at the output).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合在流数据处理中是一个频繁出现的模式，反映了数据生产者（在输入端）和数据消费者（在输出端）关注点的差异。
- en: As discussed in [“Window Aggregations”](ch02.xhtml#windows-discussion), the
    concept of a *window* of data over time can help us to create aggregates that
    span large periods of time. The Spark Streaming API offers definitions for the
    two generic window concepts presented in that section, *tumbling* and *sliding
    windows*, and provides specialized reduce functions that operate over windows
    to limit the amount of intermediate memory required to execute a given aggregation
    over a period of time.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [“Window Aggregations”](ch02.xhtml#windows-discussion) 中所讨论的，通过时间的数据*窗口*的概念可以帮助我们创建跨越较长时间段的聚合。Spark
    Streaming API 提供了在该部分介绍的两个通用窗口概念的定义，*滚动*和*滑动窗口*，并提供了专门的减少函数，用于在一段时间内执行给定聚合所需的中间内存的限制。
- en: 'In the next pages, we are going to explore the windowing capabilities of Spark
    Streaming:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的页面中，我们将探索 Spark Streaming 的窗口能力：
- en: Tumbling windows
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动窗口
- en: Sliding windows
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滑动窗口
- en: Window-based reductions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于窗口的减少
- en: Tumbling Windows
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tumbling 窗口
- en: In Spark Streaming, the most basic window definition is the `window(<time>)`
    operation on DStreams. This DStream transformation creates a new *windowed* DStream
    that can further transform to implement our desired logic.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark Streaming 中，最基本的窗口定义是对 DStreams 进行的 `window(<time>)` 操作。这个 DStream 转换创建了一个新的*窗口化*
    DStream，可以进一步转换以实现我们想要的逻辑。
- en: 'Assuming a DStream of hashtags, we can do this with tumbling windows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个 hashtags 的 DStream，我们可以使用滚动窗口来实现这一点：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the `window` operation, before the `map` and `reduceByKey` steps—that we
    now know do a simple counting—we are reprogramming the segmentation of our DStream
    into RDDs. The original stream, `hashTags`, follows a strict segmentation according
    to the batch interval: one RDD per batch.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `window` 操作中，在 `map` 和 `reduceByKey` 步骤之前—我们现在知道它们只是简单地计数—我们正在重新编程我们的 DStream
    的分段为 RDD。原始流 `hashTags` 严格按照批处理间隔进行分段：每批一个 RDD。
- en: 'In this case, we are configuring the new DStream, `hashTags.window(Seconds(60))`,
    to contain one RDD per 60 seconds. Every time the clock ticks 60 seconds, a new
    RDD is created on the cluster’s resources, independently of the previous elements
    of the same windowed DStream. In that sense, the window is *tumbling* as explained
    in [“Tumbling Windows”](ch02.xhtml#tumbling_window_definition): every RDD is 100%
    composed of new “fresh” elements read on the wire.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们正在配置新的 DStream，`hashTags.window(Seconds(60))`，每 60 秒包含一个 RDD。每当时钟滴答
    60 秒，集群资源上都会创建一个新的 RDD，与同一窗口化 DStream 的先前元素无关。从这个意义上说，窗口是*滚动*的，如 [“Tumbling Windows”](ch02.xhtml#tumbling_window_definition)
    中所解释的：每个 RDD 都是由新的“新鲜”元素组成的。
- en: Window Length Versus Batch Interval
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 窗口长度与批处理间隔
- en: Because the creation of a windowed stream is obtained by coalescing the information
    of several RDDs of the original stream into a single one for the windowed stream,
    the window interval *must be* a multiple of the batch interval.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因为通过将原始流的几个 RDD 的信息合并为窗口流的单个 RDD 来创建窗口化流，窗口间隔*必须是*批处理间隔的倍数。
- en: Naturally, any window length that’s a multiple of the initial `batch interval`
    can be passed as an argument. Hence, this kind of grouped stream allows the user
    to ask questions of the last minute, 15 minutes, or hour of data—more precisely,
    of the *k*-th interval of the windowed streaming computation runtime.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，任何初始`批处理间隔`的倍数都可以作为参数传递。因此，这种分组流使用户可以查询最近一分钟、15分钟或一小时的数据——更准确地说，是窗口化流计算运行时的第*k*个间隔。
- en: 'One important observation that takes new users by surprise: the window interval
    is aligned with the start of the streaming application. For example, given a 30-minute
    window over a DStream with a batch interval of two minutes, if the streaming job
    starts at 10:11, the windowed intervals will be computed at 10:41, 11:11, 11:41,
    12:11, 12:41, and so on.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个让新用户感到惊讶的重要观察：窗口间隔与流式应用程序的启动时间对齐。例如，给定一个30分钟的窗口，批处理间隔为2分钟的DStream，如果流处理作业从10:11开始，则窗口间隔将在10:41、11:11、11:41、12:11、12:41等时刻计算。
- en: Sliding Windows
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滑动窗口
- en: Although the information of “what were the most popular hashtags each 10 minutes
    during a famous sporting event” is interesting for forensics or future predictions,
    it is not the kind of question one usually asks during that event. Neither is
    a tumbling window the relevant time frame when detecting anomalies. In that case,
    aggregates are usually necessary because the values being observed have frequent
    but often small, and therefore meaningless fluctuations, that require the context
    provided by additional data points for analysis. The price of a stock or the temperature
    of a component have small fluctuations that should not be observed individually.
    The actual trends become visible by observing a series of recent events.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管“在一场著名体育赛事中，每10分钟最受欢迎的标签是什么”这样的信息对法医学或未来预测很有趣，但在该事件期间通常不会提出这种问题。检测异常时，也不会使用滚动窗口作为相关的时间框架。在这种情况下，通常需要聚合，因为所观察的值经常有但通常很小的无意义波动，需要通过附加数据点提供的上下文进行分析。股票价格或组件温度具有不应单独观察的小波动。通过观察一系列最近事件，实际趋势变得可见。
- en: 'It is often useful to look at a different type of aggregate that presents data
    over a relatively large period, while keeping part of it fresh—sliding windows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在保持部分内容新鲜的情况下，通常有必要查看一种不同类型的聚合，它在相对较长的时间段内呈现数据——滑动窗口：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this example, we are describing a different type of DStream to compute the
    most frequent hashtags on: this time, a new RDD is produced every 10 seconds.
    In this alternative of the `window` function, the first argument, named `windowDuration`,
    determines the length of the window, whereas the second argument, called `slideDuration`,
    dictates how often we want to observe a new window of data.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们描述了一种不同类型的DStream，用于计算最频繁的标签：这次，每10秒产生一个新的RDD。在`window`函数的这种替代版本中，第一个参数名为`windowDuration`，确定窗口的长度，而第二个参数名为`slideDuration`，确定我们希望多久观察一次新数据窗口。
- en: Coming back to the example, the resulting windowed DStream will contain RDDs
    that present the result of the computation over the latest 60 seconds of data,
    produced every 10 seconds.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 回到这个例子，结果窗口化的DStream将包含在最新60秒数据上的计算结果的RDD，每10秒产生一次。
- en: This sliding view makes it possible to implement a monitoring application, and
    it is not infrequent to see the data produced by such a stream be sent to a dashboard
    after processing. In that case, naturally, the refresh rate of the dashboard is
    linked to the slide interval.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种滑动视图使得实现监控应用程序成为可能，而通常在处理后，会看到此类流产生的数据被发送到仪表盘。在这种情况下，仪表盘的刷新率自然与滑动间隔相关联。
- en: Sliding Windows Versus Batch Interval
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滑动窗口与批处理间隔对比
- en: Again, because the RDDs of the output function are obtained by coalescing the
    input RDDs of the original DStream, it is clear that the slide interval needs
    to be a multiple of the batch interval, and that the window interval needs to
    be a multiple of the slide interval.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，由于输出函数的RDD是通过合并原始DStream的输入RDD获得的，因此很明显，滑动间隔需要是批处理间隔的倍数，而窗口间隔需要是滑动间隔的倍数。
- en: For example, using a *streaming context* with a batch interval of 5 seconds,
    and a base DStream called `stream` , the expression `stream.window(30, 9)` is
    illegal because the slide interval is not a multiple of the batch interval. A
    correct window specification would be `stream.window(30, 10)`. The expression
    `stream.window(Seconds(40), Seconds(25))` is equally invalid, despite the window
    duration and slide interval being a multiple of the batch interval. That’s because
    the window interval must be a multiple of the slide interval. In this case, `stream.window(Seconds(50),
    Seconds(25))` is a correct window duration and sliding interval definition.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用批处理间隔为5秒的*流上下文*，并且一个名为`stream`的基本DStream，表达式`stream.window(30, 9)`是不合法的，因为滑动间隔不是批处理间隔的倍数。正确的窗口规范应该是`stream.window(30,
    10)`。表达式`stream.window(Seconds(40), Seconds(25))`同样无效，尽管窗口持续时间和滑动间隔是批处理间隔的倍数。这是因为窗口间隔必须是滑动间隔的倍数。在这种情况下，`stream.window(Seconds(50),
    Seconds(25))`是正确的窗口持续时间和滑动间隔的定义。
- en: In a nutshell, the batch interval can be seen as an “indivisible atom” of the
    time interval of windowed DStreams.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，批处理间隔可以看作是窗口化DStreams时间间隔的“不可分割原子”。
- en: Finally, note that the sliding window needs to be smaller than the window length
    for the computation to make sense. Spark will output runtime errors if one of
    these constraints is not respected.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，滑动窗口的长度必须小于窗口长度，以便计算有意义。如果不遵守这些约束之一，Spark 将输出运行时错误。
- en: Sliding Windows Versus Tumbling Windows
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滑动窗口与翻滚窗口的区别
- en: The trivial case of a sliding window is that one in which the slide interval
    is equal to the window length. In this case, you’ll notice that the stream is
    the equivalent to the tumbling windows case presented earlier, in which the `window`
    function has only the `windowDuration` argument. This corresponds precisely to
    the semantics implemented internally by the `window` function.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 滑动窗口的特殊情况是滑动间隔等于窗口长度的情况。在这种情况下，您会注意到流与前面介绍的翻滚窗口情况相当，其中`window`函数只有`windowDuration`参数。这恰好对应于`window`函数在内部实现的语义。
- en: Using Windows Versus Longer Batch Intervals
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用窗口化与更长批处理间隔
- en: 'You could wonder, looking at the simple tumbling windows, why it would be necessary
    to use windowed streams in tumbling mode rather than simply increase the batch
    interval: after all, if a user wants the data aggregated per minute, isn’t that
    exactly what the batch interval is made for? There are several counterarguments
    to this approach:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想，在简单的翻滚窗口中，为什么需要使用窗口化流以滚动模式，而不是简单地增加批处理间隔：毕竟，如果用户希望按分钟汇总数据，这不正是批处理间隔的用途吗？这种方法有几个反驳的观点：
- en: Multiple aggregation needs
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 多重聚合需求
- en: 'It sometimes happens that users want to see data computed upon in different
    increments, which would require extracting two particular frequencies of looking
    at the data. In that case, because the batch interval is indivisible and is the
    source of the aggregation for other windows, it is best set to the smallest of
    the necessary latencies. In mathematical terms, if we want multiple windows of
    our data, say of durations *x, y, and z*, we want to set our `batch interval`
    to the *greatest common divisor* (`gcd`) of them: `gcd(x,y,z)`.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有时用户希望看到以不同增量计算的数据，这需要提取查看数据的两个特定频率。在这种情况下，因为批处理间隔是不可分割的，并且是其他窗口聚合的来源，最好将其设置为所需最小延迟的最大公约数（`gcd`）。在数学上，如果我们希望多个窗口的数据，比如持续时间为*x,
    y 和 z*，我们希望将我们的`batch interval`设置为它们的*最大公约数*（`gcd(x,y,z)`）。
- en: Safety and locality
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性和局部性
- en: The batch interval is not only the source of the division of a DStream into
    RDDs. For receiver-based sources, the batch interval also plays a role in how
    its data is replicated and moved across the network. If, for example, we have
    a cluster of eight machines, each with four cores, because we might want to have
    on the order of two partitions per core, we want to set the block interval so
    that there are 32 block intervals per batch. It is the block interval that determines
    the clock tick at which data received in Spark is considered for replication by
    the block manager. Hence, when the batch interval grows, the block interval, which
    should be configured to a fraction of the batch interval, can grow, as well, and
    would make the system more susceptible to a crash that would compromise data ingestion
    (e.g., if a receiver machine dies). For example, with a large batch interval of
    one hour, and a block interval of two minutes, the potential data loss in case
    of a receiver crash is of maximum two minutes, which depending on the data frequency,
    might be inappropriate. We can mitigate this risk by using reliable receivers
    that use the write-ahead log (WAL) to avoid data loss, yet that comes at the cost
    of extra overhead and storage.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理间隔不仅仅是将 DStream 划分为 RDD 的来源。对于基于接收器的数据源，批处理间隔还影响数据的复制和跨网络传输的方式。例如，如果我们有一个包含八台机器的集群，每台机器有四个核心，因为我们可能希望每个核心有大约两个分区，所以我们希望设置块间隔使得每个批次有
    32 个块间隔。块间隔决定了 Spark 中接收到的数据在何时被块管理器视为需要复制的时钟周期。因此，当批处理间隔增长时，块间隔也应该相应增长，这样会使系统更容易受到崩溃的影响，可能会影响数据的接收（例如，如果接收机器宕机）。例如，如果批处理间隔为一小时，块间隔为两分钟，那么在接收器崩溃的情况下可能会最多损失两分钟的数据，这取决于数据的频率，可能是不合适的。我们可以通过使用可靠的接收器来减轻这种风险，这些接收器使用预写日志（WAL）来避免数据丢失，但这会增加额外的开销和存储成本。
- en: For the case in which we would want an aggregation of one hour, a tumbling window
    of one hour, based on a source `DStream` with batch intervals of five minutes,
    gives a block interval a little bit under 10 seconds, which would make for a lesser
    potential data loss.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要一小时聚合的情况，基于每五分钟批处理间隔的源 `DStream` 使用的滚动窗口为大约 10 秒不到的块间隔，这将降低潜在的数据损失。
- en: In sum, keeping a batch interval of a reasonable size increases the resiliency
    of the cluster’s setup.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，保持合理大小的批处理间隔可以增加集群设置的弹性。
- en: Window Reductions
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口缩减
- en: Toward the end of building a complex pipeline, we often want to see indicators
    of our data that are inherently something that depends on various notions of time.
    We expect to see, for example, the count of visitors on our website or cars passing
    through a crossroad during the past 15 minutes, during the past hour, and during
    the previous day.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建复杂的管道的末尾，我们经常希望看到数据的指标，这些指标本质上是依赖于不同时间概念的内容。例如，我们期望看到网站访客数量或者穿过十字路口的汽车数量，分别是过去
    15 分钟、过去一小时和前一天的情况。
- en: Those three pieces of information can all be computed based on counts on a windowed
    DStream, which we have already seen in this chapter. Although the window-based
    functions provide us with the primitives that we need to produce aggregates over
    different periods of time, they also require us to maintain all of the data for
    the specified period. For example, to produce a 24-hour aggregation, the window
    functions we know so far would require us to keep 24 hours’ worth of data in storage
    (memory and/or disk—depending on the DStream configuration).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个信息都可以基于窗口化的 DStream 的计数进行计算，我们在本章已经见过了这种情况。尽管基于窗口的函数为我们提供了在不同时间段内生成聚合的基本功能，但它们也要求我们保留指定时间段内的所有数据。例如，要生成
    24 小时的聚合，到目前为止我们所知道的窗口函数需要在存储（内存和/或磁盘，具体取决于 DStream 的配置）中保留 24 小时的数据。
- en: Imagine that we want the total of user visits to our site during that 24-hour
    period. We don’t really need to keep each individual record for 24 hours to then
    count. Instead, we can have a running count and add new data as it comes in. This
    is the intuition behind window-based reductions. The provided function—which is
    assumed to be associative—is applied to each new microbatch of data and then,
    the result is added to the aggregation maintained by the window DStream. Instead
    of keeping potentially large amounts of data around, we aggregate it as it comes
    into the system, providing a scalable aggregation that uses minimal memory resources.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们想要在24小时内计算用户访问我们网站的总数。我们不需要保留每个单独的记录24小时然后再计数。相反，我们可以使用一个运行计数，并在新数据到来时添加。这就是基于窗口的减少的直觉。假设的函数（假定为可结合的）应用于每个新的微批次数据，然后结果添加到窗口DStream维护的聚合中。与保留大量数据不同，我们将其作为系统进入的方式进行聚合，提供一种使用最小内存资源的可伸缩聚合。
- en: 'The windowed reducers family of functions in Spark Streaming take a reducer
    function together with the parameters for a window definition that we learned
    earlier. The following sections discuss a few such reducers:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming中的窗口化减少函数族结合了一个减少函数和我们之前学习的窗口定义参数。接下来的部分讨论了几种这样的减少函数：
- en: reduceByWindow
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: reduceByWindow
- en: '`reduceByWindow` takes a reduce function, the window duration, and the slide
    duration. The reduce function must combine two elements of the original DStream
    and produce a new combined element of the same type. `slideDuration` can be omitted
    to produce a tumbling window of `windowDuration` length:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByWindow`采用一个减少函数，窗口持续时间和滑动持续时间。减少函数必须组合原始DStream的两个元素，并产生相同类型的新组合元素。可以省略`slideDuration`以生成长度为`windowDuration`的滚动窗口：'
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: reduceByKeyAndWindow
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: reduceByKeyAndWindow
- en: '`reduceByKeyAndWindow` is defined only in pair DStreams—a DStream of `(Key,Value)`
    tuples. It requires similar parameters as `reduceByWindow` but the reduce function
    applies to the values of the DStream. This operation is potentially more useful
    than its sibling `reduceByWindow` because we can have keys to indicate what values
    we are dealing with:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKeyAndWindow`仅在成对的DStream中定义——一个`(Key, Value)`元组的DStream。它需要与`reduceByWindow`类似的参数，但减少函数应用于DStream的值。这个操作可能比它的兄弟`reduceByWindow`更有用，因为我们可以使用键来指示我们正在处理哪些值：'
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Coming back to our example of hashtags, we could implement a daily aggregate
    of hashtag frequency by using the `reduceByKeyAndWindow` function:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回到标签的例子，我们可以使用`reduceByKeyAndWindow`函数实现每日标签频率的累加：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: countByWindow
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: countByWindow
- en: '`countByWindow` is a specialized form of `reduceByWindow` in which we are interested
    only in the count of elements in the DStream over a window of time. It answers
    the question: *how many events were received in the given window?*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`countByWindow`是`reduceByWindow`的一种特殊形式，我们只关心在时间窗口内元素的计数。它回答了这个问题：在给定的窗口中收到了多少事件？'
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`countByWindow` uses the same `windowDuration` and `slideDuration` parameters
    that we saw defined in [“Sliding Windows”](#sps-sliding-windows).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`countByWindow`使用我们在[“滑动窗口”](#sps-sliding-windows)中定义的`windowDuration`和`slideDuration`参数。'
- en: countByValueAndWindow
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: countByValueAndWindow
- en: '`countByValueAndWindow` is a grouped variant of the `countByWindow` operation
    just mentioned.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`countByValueAndWindow`是刚才提到的`countByWindow`操作的分组变体。'
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`countByValueAndWindow` uses the values in the original DStream as keys for
    counting. It makes our hashtag example rather trivial:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`countByValueAndWindow`将原始DStream中的值作为键来计数。这使得我们的标签例子变得相当简单：'
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Internally, it is doing similar steps to our earlier example: creating tuples
    with the form `(value, 1L)` and then using `reduceByKeyAndWindow` over the resulting
    DStream. As the name gives away, we use `countByValueAndWindow` to count the occurrences
    of each value in the original DStream for each window specified.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，它执行类似于我们之前示例的步骤：创建形如`(value, 1L)`的元组，然后在生成的DStream上使用`reduceByKeyAndWindow`。顾名思义，我们使用`countByValueAndWindow`来计算原始DStream中每个值的出现次数。
- en: It uses the same `windowDuration` and `slideDuration` parameters that we saw
    defined in [“Sliding Windows”](#sps-sliding-windows)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用我们在[“滑动窗口”](#sps-sliding-windows)中定义的`windowDuration`和`slideDuration`参数。
- en: Invertible Window Aggregations
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可逆窗口聚合
- en: The `reduceByWindow` and `reduceByKeyAndWindow` functions contain an additional
    fourth argument as an optional parameter. That argument is called the *inverse
    reduce function*. It matters only if you happen to use an aggregation function
    that is invertible, meaning you can “take away” an element from the aggregate.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByWindow`和`reduceByKeyAndWindow`函数包含一个额外的第四个参数作为可选参数。该参数称为*逆减少函数*。仅当您使用可逆的聚合函数时才重要，这意味着您可以从聚合中“减去”一个元素。'
- en: 'Formally, the inverse function `invReduceFunc` is such that for any accumulated
    value y, and an element x: `invReduceFunc(reduceFunc(x, y), x) = y`.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，反函数`invReduceFunc`满足对于任何累积值y和元素x：`invReduceFunc(reduceFunc(x, y), x) = y`。
- en: Behind the scenes, this invertible notion lets Spark simplify the computation
    of our aggregates by letting Spark compute over the elements of a couple of slide
    intervals on each new sliding window rather than of the full contents of the window.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，这种可逆概念使得Spark可以简化我们的聚合计算，使Spark在每个新滑动窗口上计算几个滑动间隔的元素，而不是整个窗口的全部内容。
- en: For example, suppose that we aggregate integer counts that we see with a batch
    interval of one minute, over a window of 15 minutes, sliding every minute. If
    you are not specifying an inverse reduce function, you need to add every new count
    over 15 minutes for summarizing the data that you have seen on your DStream. We
    sketch this process in [Figure 21-1](#non-invertible-reduce).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们每分钟以批处理间隔聚合整数计数，窗口为15分钟，每分钟滑动一次。如果您未指定逆减少函数，则需要在DStream上看到的数据上添加每15分钟的新计数以汇总数据。我们在[图 21-1](#non-invertible-reduce)中概述了这个过程。
- en: '![spas 2101](Images/spas_2101.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2101](Images/spas_2101.png)'
- en: Figure 21-1\. The aggregation of a reduceByWindow with a noninvertible function
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 21-1\. 使用非可逆函数的reduceByWindow的聚合
- en: This works, but if we see 100,000 elements in a minute, we are summing 1.5 million
    data points in the window for every minute—and, more important, we need to store
    those 1.5 million elements in memory. Can we do better?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有效，但是如果我们每分钟看到100,000个元素，则每分钟在窗口内求和1.5百万数据点——更重要的是，我们需要将这1.5百万元素存储在内存中。我们可以做得更好吗？
- en: We could remember the count over the previous 15 minutes and consider that we
    have a new minute of data incoming. Taking the count over those previous 15 minutes,
    we could subtract the count for the oldest minute of that 15-minute aggregate,
    because a counting (summation) function is invertible. We subtract the count for
    the oldest minute, resulting in a 14-minute aggregate, to which we just need to
    add the newest one minute of data, giving us the last 15 minutes of data. [Figure 21-2](#invertible-reduce)
    shows this process at work.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以记住过去15分钟的计数，并考虑我们有新的一分钟数据进入。取得这些前15分钟的计数，我们可以减去该15分钟聚合中最旧一分钟的计数，因为计数（求和）函数是可逆的。我们减去最旧一分钟的计数，得到14分钟的聚合值，然后我们只需添加最新的一分钟数据，就得到了最近15分钟的数据。[图 21-2](#invertible-reduce)展示了这一过程的工作方式。
- en: '![spas 2102](Images/spas_2102.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2102](Images/spas_2102.png)'
- en: Figure 21-2\. The aggregation of a reduceByWindow with an invertible function
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 21-2\. 使用可逆函数的reduceByWindow的聚合
- en: The interesting part of this is that we do not need to store 1.5 million data
    points; rather, we only need the intermediate counts for every minute—that is,
    15 values.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这件事的有趣之处在于，我们不需要存储1.5百万数据点；相反，我们只需要每分钟的中间计数值——也就是说，15个值。
- en: As you can see, having an invertible function for reduction can be tremendously
    useful in the case of windowed aggregations. It is also something that we can
    make the generation of various DStreams created by `reduceByWindow` inexpensive,
    giving us an excellent way to share information and aggregates on the values that
    we want to consider when analyzing our stream even over long aggregation periods.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，对于窗口聚合来说，拥有可逆的减少函数可以非常有用。这也是我们可以使由`reduceByWindow`创建的各种DStream生成廉价的方法之一，使我们有一个出色的方式来在分析我们的流时共享信息和聚合值，即使是在长时间的聚合期间也是如此。
- en: Caution
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The inverse of the aggregation function is useless for tumbling windows, since
    every aggregation interval is entirement disjoint from the others. It is therefore
    not worth bothering with this option if you do not use a slide interval!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于翻滚窗口，聚合函数的反函数是无用的，因为每个聚合间隔完全不重叠。因此，如果您不使用滑动间隔，这个选项就不值得麻烦了！
- en: Slicing Streams
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流分片
- en: Finally, note Spark’s DStreams also have a selection function called `slice`,
    which returns the particular subpart of a DStream that was included between two
    bounds. You can specify the bounds using a beginning and end `Time`, which corresponds
    to Spark’s `org.apache.spark.streaming.Time` or as an `org.apache.spark.streaming.Interval`.
    Both are simple reimplementations of time arithmetic using milliseconds as the
    base unit—leaving the user with a decent expressivity.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意 Spark 的 DStreams 还具有一个名为 `slice` 的选择函数，它返回在两个边界之间包括的 DStream 的特定子部分。您可以使用开始和结束
    `Time` 指定边界，这对应于 Spark 的 `org.apache.spark.streaming.Time` 或 `org.apache.spark.streaming.Interval`。两者都是使用毫秒作为基本单位的时间算术的简单重新实现，为用户留下了相当高的表达能力。
- en: Spark will produce sliced DStreams by letting through the elements that carry
    the correct timestamp. Note also that `slice` produces as many RDDs as there are
    batch intervals between the two boundaries of the DStream.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 将通过让带有正确时间戳的元素通过来生成切片的 DStreams。还要注意，`slice` 会生成与 DStream 两个边界之间批处理间隔数相同的
    RDD。
- en: What If Your Slice Specification Doesn’t Fit Neatly into Batches?
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果您的切片规范不完全适合批处理怎么办？
- en: 'The timing of an RDD’s original batch and the output of the slice might not
    exactly match if the beginning and end time are not aligned with the original
    DStream’s batch interval *ticks*. Any change in timings will be reflected in logs
    at the `INFO` level:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 RDD 的原始批次时间和切片的输出时间不完全匹配，那么如果开始和结束时间与原始 DStream 的批次间隔 *ticks* 不对齐，时间的任何变化将在
    `INFO` 级别的日志中反映出来：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Summary
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we looked at the capabilities of Spark Streaming to create
    and process windows of data from a DStream. You are now able to do the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了 Spark Streaming 的能力，可以创建和处理来自 DStream 的数据窗口。现在您可以做到以下几点：
- en: Express a tumbling and a sliding window using the DStream API
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DStream API 表达翻滚窗口和滑动窗口
- en: Count elements in a window, including using keys in the data to group the counts
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算窗口中的元素数量，包括使用数据中的键来分组计数
- en: Create window-based counts and aggregations
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建基于窗口的计数和聚合
- en: Use the optimized `reduce` versions that exploit function inversibility to drastically
    reduce the amount of internal memory used
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用优化的 `reduce` 版本，利用函数的反可逆性大幅减少内部内存使用量
- en: Window aggregations let us observe trends in the data as it unfolds over periods
    of time much longer than a batch interval. The tools you just learned empower
    you to apply these techniques in Spark Streaming.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口聚合允许我们观察数据在远远超过批处理间隔的时间段内的趋势。您刚学到的工具使您能够在 Spark Streaming 中应用这些技术。
