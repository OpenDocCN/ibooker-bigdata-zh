- en: Chapter 1\. Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章·介绍
- en: You know nothing, Jon Snow.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你什么也不知道，琼·雪诺。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Ygritte
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ——伊格丽特
- en: With information growing at exponential rates, it’s no surprise that historians
    are referring to this period of history as the Information Age. The increasing
    speed at which data is being collected has created new opportunities and is certainly
    poised to create even more. This chapter presents the tools that have been used
    to solve large-scale data challenges. First, it introduces Apache Spark as a leading
    tool that is democratizing our ability to process large datasets. With this as
    a backdrop, we introduce the R computing language, which was specifically designed
    to simplify data analysis. Finally, this leads us to introduce `sparklyr`, a project
    merging R and Spark into a powerful tool that is easily accessible to all.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随着信息以指数速度增长，历史学家们将这段历史称为信息时代并不足为奇。数据收集速度的增加创造了新的机会，并肯定会创造更多。本章介绍了用于解决大规模数据挑战的工具。首先，它介绍了Apache
    Spark作为领先的工具，使我们能够处理大型数据集。在此背景下，我们介绍了专门设计简化数据分析的R计算语言。最后，我们介绍了`sparklyr`，这是一个将R和Spark合并为一个强大工具的项目，非常易于所有人使用。
- en: '[Chapter 2, *Getting Started*](ch02.html#starting) presents the prerequisites,
    tools, and steps you need to perform to get Spark and R working on your personal
    computer. You will learn how to install and initialize Spark, get introduced to
    common operations, and get your very first data processing and modeling task done.
    It is the goal of that chapter to help anyone grasp the concepts and tools required
    to start tackling large-scale data challenges which, until recently, were accessible
    to just a few organizations.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[第2章，*入门*](ch02.html#starting)介绍了在个人计算机上使Spark和R工作所需的先决条件、工具和步骤。您将学习如何安装和初始化Spark，介绍常见操作，并完成您的第一个数据处理和建模任务。该章的目标是帮助任何人掌握开始解决大规模数据挑战所需的概念和工具，这些挑战直到最近只有少数组织才能访问。'
- en: You then move into learning how to analyze large-scale data, followed by building
    models capable of predicting trends and discover information hidden in vast amounts
    of information. At which point, you will have the tools required to perform data
    analysis and modeling at scale. Subsequent chapters help you move away from your
    local computer into computing clusters required to solve many real world problems.
    The last chapters present additional topics, like real-time data processing and
    graph analysis, which you will need to truly master the art of analyzing data
    at any scale. The last chapter of this book provides you with tools and inspiration
    to consider contributing back to the Spark and R communities.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将进入学习如何分析大规模数据，接着是构建能够预测趋势并发现大量信息中隐藏信息的模型。到那时，您将拥有在规模上执行数据分析和建模所需的工具。随后的章节将帮助您摆脱本地计算机，转向解决许多实际问题所需的计算集群。最后几章介绍了额外的主题，如实时数据处理和图分析，这些是您真正掌握在任何规模上分析数据的艺术所需的工具。本书的最后一章为您提供了考虑回馈Spark和R社区的工具和灵感。
- en: We hope that this is a journey you will enjoy, that will help you to solve problems
    in your professional career, and to nudge the world into making better decisions
    that can benefit us all.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这是一段让你享受的旅程，它将帮助你解决职业生涯中的问题，并推动世界做出更好的决策，从而造福我们所有人。
- en: Overview
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'As humans, we have been storing, retrieving, manipulating, and communicating
    information since the Sumerians in Mesopotamia developed writing around 3000 BC.
    Based on the storage and processing technologies employed, it is possible to distinguish
    four distinct phases of development: premechanical (3000 BC to 1450 AD), mechanical
    (1450–1840), electromechanical (1840–1940), and electronic (1940–present).^([1](ch01.html#idm46099155685336))'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，自从苏美尔人在公元前3000年左右发展出文字以来，我们一直在存储、检索、操纵和传播信息。根据所采用的存储和处理技术，可以区分出四个不同的发展阶段：前机械时代（公元前3000年至公元1450年）、机械时代（1450年至1840年）、电机械时代（1840年至1940年）和电子时代（1940年至今）。^([1](ch01.html#idm46099155685336))
- en: Mathematician George Stibitz used the word *digital* to describe fast electric
    pulses back in 1942,^([2](ch01.html#idm46099155156792)) and to this day, we describe
    information stored electronically as digital information. In contrast, *analog*
    information represents everything we have stored by any nonelectronic means such
    as handwritten notes, books, newspapers, and so on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数学家乔治·斯蒂比茨在1942年首次使用 *digital* 这个词来描述快速的电脉冲，^([2](ch01.html#idm46099155156792))
    至今，我们仍将以电子方式存储的信息称为数字信息。相比之下，*analog* 信息代表我们以非电子方式存储的所有内容，如手写笔记、书籍、报纸等。
- en: The World Bank report on digital development provides an estimate of digital
    and analog information stored over the past decades.^([3](ch01.html#idm46099135878424))
    This report noted that digital information surpassed analog information around
    2003\. At that time, there were about 10 million terabytes of digital information,
    which is roughly about 10 million storage drives today. However, a more relevant
    finding from this report was that our footprint of digital information is growing
    at exponential rates. [Figure 1-1](#intro-store-capacity) shows the findings of
    this report; notice that every other year, the world’s information has grown tenfold.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 世界银行关于数字发展的报告提供了过去几十年存储的数字和模拟信息的估计。^([3](ch01.html#idm46099135878424)) 报告指出，数字信息在2003年左右超过了模拟信息。那时，大约有1000万
    TB 的数字信息，大致相当于今天的1000万个存储驱动器。然而，报告中更重要的发现是，我们的数字信息足迹正以指数速率增长。[图 1-1](#intro-store-capacity)
    展示了这份报告的发现；请注意，每隔两年，世界的信息量增长了十倍。
- en: With the ambition to provide tools capable of searching all of this new digital
    information, many companies attempted to provide such functionality with what
    we know today as search engines, used when searching the web. Given the vast amount
    of digital information, managing information at this scale was a challenging problem.
    Search engines were unable to store all of the web page information required to
    support web searches in a single computer. This meant that they had to split information
    into several files and store them across many machines. This approach became known
    as the *Google File System*, and was presented in a research paper published in
    2003 by Google.^([4](ch01.html#idm46099159614776))
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司怀揣着提供工具的雄心，可以搜索所有这些新的数字信息，而今天我们称之为搜索引擎，在搜索网页时使用。鉴于大量的数字信息，管理这种规模的信息是一个具有挑战性的问题。搜索引擎无法在单台计算机上存储支持网页搜索所需的所有网页信息。这意味着它们必须将信息分割成多个文件并存储在许多计算机上。这种方法被称为*Google
    文件系统*，并在谷歌于2003年发表的研究论文中提出。^([4](ch01.html#idm46099159614776))
- en: '![World’s capacity to store information](assets/mswr_0101.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![全球信息存储能力](assets/mswr_0101.png)'
- en: Figure 1-1\. World’s capacity to store information
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-1\. 全球信息存储能力
- en: Hadoop
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop
- en: 'One year later, Google published a new paper describing how to perform operations
    across the Google File System, an approach that came to be known as *MapReduce*.^([5](ch01.html#idm46099155625112))
    As you would expect, there are two operations in MapReduce: map and reduce. The
    *map operation* provides an arbitrary way to transform each file into a new file,
    whereas the *reduce operation* combines two files. Both operations require custom
    computer code, but the MapReduce framework takes care of automatically executing
    them across many computers at once. These two operations are sufficient to process
    all the data available on the web, while also providing enough flexibility to
    extract meaningful information from it.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一年后，谷歌发布了一篇新论文，描述如何在谷歌文件系统上执行操作，这种方法后来被称为*MapReduce*。^([5](ch01.html#idm46099155625112))
    如您所预料，MapReduce 中有两种操作：map 和 reduce。*Map 操作* 提供了一种任意转换每个文件为新文件的方式，而*reduce 操作*
    则将两个文件合并。这两种操作都需要定制的计算机代码，但 MapReduce 框架会自动在许多计算机上执行它们。这两种操作足以处理网页上的所有数据，并且提供足够的灵活性从中提取有意义的信息。
- en: For example, as illustrated in [Figure 1-2](#intro-mapreduce-example), we can
    use MapReduce to count words in two different text files stored in different machines.
    The map operation splits each word in the original file and outputs a new word-counting
    file with a mapping of words and counts. The reduce operation can be defined to
    take two word-counting files and combine them by aggregating the totals for each
    word; this last file will contain a list of word counts across all the original
    files.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，正如在[图 1-2](#intro-mapreduce-example)中所示，我们可以使用MapReduce来统计存储在不同机器上的两个不同文本文件中的单词。映射操作将原始文件中的每个单词拆分，并输出一个新的单词计数文件，其中包含单词和计数的映射。Reduce操作可以定义为获取两个单词计数文件并通过聚合每个单词的总数来组合它们；最后生成的文件将包含所有原始文件中单词的计数列表。
- en: Counting words is often the most basic MapReduce example, but we can also use
    MapReduce for much more sophisticated and interesting applications. For instance,
    we can use it to rank web pages in Google’s *PageRank* algorithm, which assigns
    ranks to web pages based on the count of hyperlinks linking to a web page and
    the rank of the page linking to it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 计算单词通常是最基本的MapReduce示例，但我们也可以将MapReduce用于更复杂和有趣的应用。例如，我们可以用它来在Google的*PageRank*算法中对网页进行排名，该算法根据指向某个网页的超链接数以及指向它的页面的等级来分配网页排名。
- en: '![MapReduce example counting words across files](assets/mswr_0102.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![跨文件计算单词的MapReduce示例](assets/mswr_0102.png)'
- en: Figure 1-2\. MapReduce example counting words across files
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-2\. 跨文件计算单词的MapReduce示例
- en: After these papers were released by Google, a team at Yahoo worked on implementing
    the Google File System and MapReduce as a single open source project. This project
    was released in 2006 as *Hadoop*, with the Google File System implemented as the
    *Hadoop Distributed File System* (HDFS). The Hadoop project made distributed file-based
    computing accessible to a wider range of users and organizations, making MapReduce
    useful beyond web data processing.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在Google发布这些论文后，Yahoo团队致力于实现Google文件系统和MapReduce作为一个开源项目。这个项目于2006年发布为*Hadoop*，其中Google文件系统实现为*Hadoop分布式文件系统*（HDFS）。Hadoop项目使得分布式基于文件的计算对更广泛的用户和组织可用，使得MapReduce在Web数据处理之外也变得有用。
- en: Although Hadoop provided support to perform MapReduce operations over a distributed
    file system, it still required MapReduce operations to be written with code every
    time a data analysis was run. To improve upon this tedious process, the *Hive*
    project, released in 2008 by Facebook, brought *Structured Query Language* (SQL)
    support to Hadoop. This meant that data analysis could now be performed at large
    scale without the need to write code for each MapReduce operation; instead, one
    could write generic data analysis statements in SQL, which are much easier to
    understand and write.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Hadoop支持在分布式文件系统上执行MapReduce操作，但仍需要每次运行数据分析时编写MapReduce操作的代码。为了改善这一繁琐的过程，Facebook在2008年发布了*Hive*项目，为Hadoop引入了*结构化查询语言*（SQL）支持。这意味着现在可以在大规模上执行数据分析，而无需为每个MapReduce操作编写代码；相反，可以使用SQL编写通用的数据分析语句，这样更易于理解和编写。
- en: Spark
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark
- en: In 2009, *Apache Spark* began as a research project at UC Berkeley’s AMPLab
    to improve on MapReduce. Specifically, Spark provided a richer set of verbs beyond
    MapReduce to facilitate optimizing code running in multiple machines. Spark also
    loaded data in-memory, making operations much faster than Hadoop’s on-disk storage.
    One of the earliest results showed that running *logistic regression*, a data
    modeling technique that we will introduce in [Chapter 4](ch04.html#modeling),
    allowed Spark to run 10 times faster than Hadoop by making use of in-memory datasets.^([6](ch01.html#idm46099155863336))
    A chart similar to [Figure 1-3](#intro-spark-logistic-regression) was presented
    in the original research publication.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 2009年，*Apache Spark*作为加州大学伯克利分校AMPLab的一个研究项目开始，旨在改进MapReduce。具体来说，Spark提供了比MapReduce更丰富的动词集，以便于优化在多台机器上运行的代码。Spark还将数据加载到内存中，使操作比Hadoop在磁盘存储中更快。最早的结果之一显示，运行*逻辑回归*（一种我们将在[第四章](ch04.html#modeling)介绍的数据建模技术）时，Spark的运行速度比Hadoop快了10倍，通过利用内存数据集实现了这一点。^([6](ch01.html#idm46099155863336))
    原始研究出版物中也呈现了类似于[图 1-3](#intro-spark-logistic-regression)的图表。
- en: '![Logistic regression performance in Hadoop and Spark](assets/mswr_0103.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![在Hadoop和Spark中的逻辑回归性能](assets/mswr_0103.png)'
- en: Figure 1-3\. Logistic regression performance in Hadoop and Spark
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-3\. 在Hadoop和Spark中的逻辑回归性能
- en: Even though Spark is well known for its in-memory performance, it was designed
    to be a general execution engine that works both in-memory and on-disk. For instance,
    Spark has set [new records in large-scale sorting](http://sortbenchmark.org/),
    for which data was not loaded in-memory; rather, Spark made improvements in network
    serialization, network shuffling, and efficient use of the CPU’s cache to dramatically
    enhance performance. If you needed to sort large amounts of data, there was no
    other system in the world faster than Spark.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Spark 以其内存性能而闻名，但它设计成通用执行引擎，既可在内存中运行，也可在磁盘上运行。例如，Spark 在大规模排序方面创造了[新纪录](http://sortbenchmark.org/)，数据并非加载到内存中；相反，Spark
    通过改进网络序列化、网络洗牌和有效利用 CPU 缓存显著提升了性能。如果您需要对大量数据进行排序，没有比 Spark 更快的系统了。
- en: To give you a sense of how much faster and efficient Spark is, it takes 72 minutes
    and 2,100 computers to sort 100 terabytes of data using Hadoop, but only 23 minutes
    and 206 computers [using Spark](https://oreil.ly/Duram). In addition, Spark holds
    the [cloud sorting record](https://oreil.ly/lHMjg), which makes it the most cost-effective
    solution for sorting large-datasets in the cloud.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您感受到 Spark 更快、更高效的程度，使用 Hadoop 对 100 TB 数据进行排序需要 72 分钟和 2,100 台计算机，而使用 [Spark](https://oreil.ly/Duram)
    仅需 23 分钟和 206 台计算机。此外，Spark 保持了[云端排序记录](https://oreil.ly/lHMjg)，使其成为处理大数据集的最具成本效益的解决方案。
- en: '|  | Hadoop record | Spark record |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | Hadoop 记录 | Spark 记录 |'
- en: '| --- | --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Data size | 102.5 TB | 100 TB |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 数据大小 | 102.5 TB | 100 TB |'
- en: '| Elapsed time | 72 mins | 23 mins |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 经过时间 | 72 分钟 | 23 分钟 |'
- en: '| Nodes | 2,100 | 206 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 节点 | 2,100 | 206 |'
- en: '| Cores | 50,400 | 6,592 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 核心数 | 50,400 | 6,592 |'
- en: '| Disk | 3,150 GB/s | 618 GB/s |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 磁盘 | 3,150 GB/s | 618 GB/s |'
- en: '| Network | 10 GB/s | 10 GB/s |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 网络 | 10 GB/s | 10 GB/s |'
- en: '| Sort rate | 1.42 TB/min | 4.27 TB/min |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 排序速率 | 1.42 TB/min | 4.27 TB/min |'
- en: '| Sort rate/node | 0.67 GB/min | 20.7 GB/min |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 每节点排序速率 | 0.67 GB/min | 20.7 GB/min |'
- en: Spark is also easier to use than Hadoop; for instance, the word-counting MapReduce
    example takes about 50 lines of code in Hadoop, but it takes only 2 lines of code
    in Spark. As you can see, Spark is much faster, more efficient, and easier to
    use than Hadoop.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 也比 Hadoop 更易于使用；例如，Hadoop 中的单词计数 MapReduce 示例大约需要 50 行代码，而在 Spark 中仅需
    2 行代码。正如您所见，Spark 比 Hadoop 更快、更高效，更易于使用。
- en: In 2010, Spark was released as an open source project and then donated to the
    Apache Software Foundation in 2013\. Spark is licensed under [Apache 2.0](https://oreil.ly/cNH5p),
    which allows you to freely use, modify, and distribute it. Spark then reached
    more than 1,000 contributors, making it one of the most active projects in the
    Apache Software Foundation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 2010 年，Spark 作为一个开源项目发布，然后在 2013 年捐赠给 Apache 软件基金会。Spark 使用[Apache 2.0](https://oreil.ly/cNH5p)许可证，允许您自由使用、修改和分发它。Spark
    随后达到了超过 1,000 名贡献者，使其成为 Apache 软件基金会中最活跃的项目之一。
- en: 'This gives an overview of how Spark came to be, which we can now use to formally
    introduce Apache Spark as defined on the project’s [website](http://spark.apache.org):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了 Spark 发展历程的概述，现在我们可以正式介绍 Apache Spark，如其在项目的[网站](http://spark.apache.org)定义的那样：
- en: Apache Spark is a unified analytics engine for large-scale data processing.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Apache Spark 是用于大规模数据处理的统一分析引擎。
- en: 'To help us understand this definition of Apache Spark, we break it down as
    follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们理解 Apache Spark 的这一定义，我们将其分解如下：
- en: Unified
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 统一的
- en: Spark supports many libraries, cluster technologies, and storage systems.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 支持许多库、集群技术和存储系统。
- en: Analytics
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 分析
- en: Analytics is the discovery and interpretation of data to produce and communicate
    information.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 分析是发现和解释数据以产生和传达信息的过程。
- en: Engine
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 引擎
- en: Spark is expected to be efficient and generic.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 预计将是高效且通用的。
- en: Large-scale
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模
- en: You can interpret large-scale as *cluster*-scale, a set of connected computers
    working together.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将大规模解释为*集群*规模，一组连接在一起工作的计算机。
- en: Spark is described as an *engine* because it’s generic and efficient. It’s generic
    because it optimizes and executes generic code; that is, there are no restrictions
    as to what type of code you can write in Spark. It is efficient, because, as we
    mentioned earlier, Spark much faster than other technologies by making efficient
    use of memory, network, and CPUs to speed data processing algorithms in computing
    clusters.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 被描述为一个*引擎*，因为它是通用且高效的。它是通用的，因为它优化和执行通用代码；也就是说，您可以在 Spark 中编写任何类型的代码而没有限制。它是高效的，因为正如我们之前提到的，Spark
    通过有效利用内存、网络和 CPU 加速数据处理算法，比其他技术快得多。
- en: This makes Spark ideal in many *analytics* projects like [ranking movies at
    Netflix](http://spark.apache.org), [aligning protein sequences](https://bit.ly/2KUZEdb),
    or [analyzing high-energy physics at CERN](https://bit.ly/2KoTGlc).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得Spark在许多*分析*项目中成为理想选择，例如像[Netflix电影排名](http://spark.apache.org)，[蛋白质序列对齐](https://bit.ly/2KUZEdb)，或[分析CERN的高能物理](https://bit.ly/2KoTGlc)。
- en: As a *unified* platform, Spark is expected to support many cluster technologies
    and multiple data sources, which you learn about in [Chapter 6](ch06.html#clusters)
    and [Chapter 8](ch08.html#data), respectively. It is also expected to support
    many different libraries like Spark SQL, MLlib, GraphX, and Spark Streaming; libraries
    that you can use for analysis, modeling, graph processing, and real-time data
    processing, respectively. In summary, Spark is a platform providing access to
    clusters, data sources, and libraries for large-scale computing, as illustrated
    in [Figure 1-4](#intro-spark-unified).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作为*统一*平台，Spark预期支持许多集群技术和多个数据源，这些你可以在[第6章](ch06.html#clusters)和[第8章](ch08.html#data)学习。它还预期支持许多不同的库，如Spark
    SQL、MLlib、GraphX和Spark Streaming；这些库可用于分析、建模、图处理和实时数据处理，分别介绍在[图1-4](#intro-spark-unified)中。总之，Spark是一个平台，提供访问集群、数据源和库，用于大规模计算。
- en: '![Spark as a unified analytics engine for large-scale data processing](assets/mswr_0104.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![Spark作为大规模数据处理的统一分析引擎](assets/mswr_0104.png)'
- en: Figure 1-4\. Spark as a unified analytics engine for large-scale data processing
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4\. Spark作为大规模数据处理的统一分析引擎
- en: Describing Spark as *large scale* implies that a good use case for Spark is
    tackling problems that can be solved with multiple machines. For instance, when
    data does not fit on a single disk drive or into memory, Spark is a good candidate
    to consider. However, you can also consider it for problems that might not be
    large scale, but for which using multiple computers could speed up computation.
    For instance, CPU-intensive models and scientific simulations also benefit from
    running in Spark.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 描述Spark为*大规模*意味着Spark适合解决需要多台机器处理的问题。例如，当数据无法容纳在单个磁盘驱动器或内存中时，Spark是一个值得考虑的良好选择。然而，你也可以考虑将其应用于可能不是大规模的问题，但使用多台计算机可以加速计算的情况。例如，CPU密集型模型和科学仿真也受益于在Spark中运行。
- en: Therefore, Spark is good at tackling large-scale data-processing problems, usually
    known as [*big data*](https://bit.ly/2XnLHec) (datasets that are more voluminous
    and complex than traditional ones) but it is also good at tackling large-scale
    computation problems, known as [*big compute*](http://bit.ly/2OVzHOc) (tools and
    approaches using a large amount of CPU and memory resources in a coordinated way).
    Big data often requires big compute, but [big compute does not necessarily require
    big data](https://bit.ly/2FhjStV).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Spark擅长处理大规模数据处理问题，通常称为[*大数据*](https://bit.ly/2XnLHec)（比传统数据集更庞大和复杂的数据集），但它也擅长处理大规模计算问题，称为[*大计算*](http://bit.ly/2OVzHOc)（使用大量CPU和内存资源以协调方式的工具和方法）。大数据通常需要大计算，但[大计算并不一定需要大数据](https://bit.ly/2FhjStV)。
- en: Big data and big compute problems are usually easy to spot—if the data does
    not fit into a single machine, you might have a big data problem; if the data
    fits into a single machine but processing it takes days, weeks, or even months
    to compute, you might have a big compute problem.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据和大计算问题通常很容易识别——如果数据无法容纳在单台机器中，你可能面临大数据问题；如果数据适合单台机器但处理时间需要数天、数周或甚至数月，你可能面临大计算问题。
- en: 'However, there is also a third problem space for which neither data nor compute
    is necessarily large scale and yet there are significant benefits to using cluster
    computing frameworks like Spark. For this third problem space, there are a few
    use cases:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有第三个问题领域，既不需要大规模数据也不需要大规模计算，但使用像Spark这样的集群计算框架仍然能带来显著的好处。对于这第三个问题领域，有几个应用案例：
- en: Velocity
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 速度
- en: Suppose that you have a dataset of 10 GB and a process that takes 30 minutes
    to run over this data—this is neither big compute nor big data by any means. However,
    if you happen to be researching ways to improve the accuracy of your models, reducing
    the runtime down to three minutes is a significant improvement, which can lead
    to meaningful advances and productivity gains by increasing the velocity at which
    you can analyze data. Alternatively, you might need to process data faster—for
    stock trading, for instance. Whereas three minutes could seem fast enough, it
    can be far too slow for real-time data processing, for which you might need to
    process data in a few seconds—or even a few milliseconds.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个10 GB的数据集，并且一个处理这些数据需要30分钟运行的流程——从任何角度来看，这既不是大计算也不是大数据。然而，如果你正在研究如何提高模型准确性的方法，将运行时间缩短到三分钟是一项显著的改进，这可以通过增加数据分析速度来实现有意义的进展和生产力增益。或者，你可能需要更快地处理数据——比如股票交易。尽管三分钟可能看起来足够快，但对于实时数据处理来说可能太慢了，你可能需要在几秒钟甚至几毫秒内处理数据。
- en: Variety
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性
- en: You could also have an efficient process to collect data from many sources into
    a single location, usually a database; this process could be already running efficiently
    and close to real time. Such processes are known as *Extract, Transform, Load*
    (ETL); data is extracted from multiple sources, transformed to the required format,
    and loaded into a single data store. Although this has worked for years, the trade-off
    from this approach is that adding a new data source is expensive. Because the
    system is centralized and tightly controlled, making changes could cause the entire
    process to halt; therefore, adding new data source usually takes too long to be
    implemented. Instead, you can store all data in its natural format and process
    it as needed using cluster computing, an architecture known as a *data lake*.
    In addition, storing data in its raw format allows you to process a variety of
    new file formats like images, audio, and video without having to figure out how
    to fit them into conventional structured storage systems.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以有一个高效的流程，从多个来源收集数据到一个单一的位置，通常是一个数据库；这个过程可能已经在高效地运行，并且接近实时。这些过程被称为*提取、转换、加载*（ETL）；数据从多个来源提取，转换为所需格式，然后加载到单一数据存储中。尽管这种方法多年来一直有效，但其缺点是添加新数据源的成本较高。因为系统是集中管理和严格控制的，进行更改可能会导致整个过程停止；因此，添加新数据源通常需要太长时间来实施。相反，你可以以其自然格式存储所有数据，并根据需要使用集群计算处理它，这种架构被称为*数据湖*。此外，以原始格式存储数据使你能够处理各种新文件格式，如图像、音频和视频，而无需考虑如何将它们适应传统的结构化存储系统中。
- en: Veracity
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 真实性
- en: When using many data sources, you might find the data quality varies greatly
    between them, which requires special analysis methods to improve their accuracy.
    For instance, suppose that you have a table of cities with values like San Francisco,
    Seattle, and Boston. What happens when data contains a misspelled entry like “Bston”?
    In a relational database, this invalid entry might be dropped. However, dropping
    values is not necessarily the best approach in all cases; you might want to correct
    this field by making use of geocodes, cross-referencing data sources, or attempting
    a best-effort match. Therefore, understanding and improving the veracity of the
    original data source can lead to more accurate results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用许多数据源时，你可能会发现它们的数据质量差异很大，这需要特殊的分析方法来提高其准确性。例如，假设你有一个包含类似于旧金山、西雅图和波士顿等值的城市表。当数据包含类似于“Bston”的拼写错误条目时会发生什么？在关系数据库中，这个无效条目可能会被删除。然而，在所有情况下，删除值并不一定是最好的方法；你可能希望通过使用地理编码、交叉引用数据源或尝试最佳匹配来更正此字段。因此，理解和改善原始数据源的真实性可以带来更准确的结果。
- en: If we include “volume” as a synonym for big data, you get the mnemonic people
    refer to as [the four Vs of big data](http://bit.ly/2MkF1sp); others have expanded
    this to [five](https://oreil.ly/gU9rP) or even [10 Vs of big data](http://bit.ly/2KBOGbM).
    Mnemonics aside, cluster computing is being used today in more innovative ways,
    and is not uncommon to see organizations experimenting with new workflows and
    a variety of tasks that were traditionally uncommon for cluster computing. Much
    of the hype attributed to big data falls into this space where, strictly speaking,
    you’re not handling big data, but there are still benefits from using tools designed
    for big data and big compute.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将“volume”作为大数据的同义词，您将得到人们称之为[大数据的四个V](http://bit.ly/2MkF1sp)的记忆法；其他人将其扩展为[五个V](https://oreil.ly/gU9rP)，甚至[十个V的大数据](http://bit.ly/2KBOGbM)。除了记忆法之外，集群计算正在以更加创新的方式使用，不少组织正在实验新的工作流程和传统上不常见的各种任务。大数据所带来的大部分炒作都集中在这个领域，严格来说，并不是在处理大数据，但仍然可以从使用为大数据和大计算设计的工具中获益。
- en: Our hope is that this book will help you to understand the opportunities and
    limitations of cluster computing and, specifically, the opportunities and limitations
    of using Apache Spark with R.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望本书能帮助您了解集群计算的机会和限制，特别是使用 Apache Spark 进行 R 编程的机会和限制。
- en: R
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R
- en: The R computing language has its origins in the S language, which was created
    at Bell Laboratories. [Rick Becker explained in useR 2016](https://bit.ly/2MSTm0j)
    that at that time in Bell Labs, computing was done by calling subroutines written
    in the Fortran language, which, apparently, were not pleasant to deal with. The
    S computing language was designed as an interface language to solve particular
    problems without having to worry about other languages, such as Fortran. The creator
    of S, [John Chambers](http://bit.ly/2Z5QygX), shows in [Figure 1-5](#intro-r-diagram)
    how S was designed to provide an interface that simplifies data processing; his
    co-creator presented this during *useR! 2016* as the original diagram that inspired
    the creation of S.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: R 计算语言源自于贝尔实验室创造的 S 语言。[Rick Becker 在 useR 2016 中解释过](https://bit.ly/2MSTm0j)，当时在贝尔实验室，计算是通过调用用
    Fortran 语言编写的子程序来完成的，显然这种方式并不理想。S 计算语言被设计为一个接口语言，用来解决特定问题，而不需要担心其他语言（如 Fortran）。S
    的创造者之一，[John Chambers](http://bit.ly/2Z5QygX)，在[图1-5](#intro-r-diagram)中展示了 S
    的设计，旨在提供简化数据处理的接口；他的合作者在 *useR! 2016* 中展示了这幅图，这幅图也是创造 S 的灵感来源。
- en: '![Interface language diagram by John Chambers (Rick Becker useR 2016](assets/mswr_0105.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![John Chambers 的接口语言图表（Rick Becker 在 useR 2016](assets/mswr_0105.png)'
- en: Figure 1-5\. Interface language diagram by John Chambers (Rick Becker useR 2016)
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-5\. John Chambers 的接口语言图表（Rick Becker 在 useR 2016
- en: 'R is a modern and free implementation of S. Specifically, according to the
    [R Project for Statistical Computing](https://www.r-project.org):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: R 是 S 的现代和免费实现。根据[统计计算的 R 项目](https://www.r-project.org)：
- en: R is a programming language and free software environment for statistical computing
    and graphics.
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: R 是用于统计计算和绘图的编程语言和自由软件环境。
- en: 'While working with data, we believe there are two strong arguments for using
    R:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据时，我们认为使用 R 有两个强有力的理由：
- en: R language
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: R 语言
- en: R was designed by statisticians for statisticians, meaning that this is one
    of the few successful languages designed for nonprogrammers, so learning R will
    probably feel more natural. Additionally, because the R language was designed
    to be an interface to other tools and languages, R allows you to focus more on
    understanding data and less on the particulars of computer science and engineering.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: R 是由统计学家为统计学家设计的，这意味着这是为非程序员设计的少数几种成功语言之一，因此学习 R 可能会感觉更自然。此外，因为 R 语言被设计为其他工具和语言的接口，所以可以更专注于理解数据，而不是计算机科学和工程学的细节。
- en: R community
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: R 社区
- en: 'The R community provides a rich package archive provided by the [Comprehensive
    R Archive Network](https://cran.r-project.org/) (CRAN), which allows you to install
    ready-to-use packages to perform many tasks—most notably high-quality data manipulation,
    visualization, and statistical models, many of which are available only in R.
    In addition, the R community is a welcoming and active group of talented individuals
    motivated to help you succeed. Many packages provided by the R community make
    R, by far, the best option for statistical computing. Some of the most downloaded
    R packages include: [`dplyr`](http://bit.ly/2YS3PP3) to manipulate data, [`cluster`](http://bit.ly/307Tuv7)
    to analyze clusters, and [`ggplot2`](http://bit.ly/2ZaWMjY) to visualize data.
    [Figure 1-6](#intro-cran-downloads) quantifies the growth of the R community by
    plotting daily downloads of R packages in CRAN.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: R 社区提供了由[全面的 R 存档网络](https://cran.r-project.org/) (CRAN) 提供的丰富的软件包存档，允许您安装用于执行许多任务的即用软件包，其中包括高质量的数据操作、可视化和统计模型，其中许多仅在
    R 中提供。此外，R 社区是一个热情活跃的群体，由才华横溢的个体组成，致力于帮助您成功。R 社区提供的许多软件包使得 R 显然成为统计计算的最佳选择。一些最常下载的
    R 软件包包括：[`dplyr`](http://bit.ly/2YS3PP3) 用于数据操作，[`cluster`](http://bit.ly/307Tuv7)
    用于分析聚类，以及 [`ggplot2`](http://bit.ly/2ZaWMjY) 用于数据可视化。[图 1-6](#intro-cran-downloads)
    通过绘制 CRAN 中 R 软件包的每日下载量来量化 R 社区的增长。
- en: '![Daily downloads of CRAN packages](assets/mswr_0106.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![CRAN 软件包的每日下载量](assets/mswr_0106.png)'
- en: Figure 1-6\. Daily downloads of CRAN packages
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-6\. CRAN 包的每日下载量
- en: 'Aside from statistics, R is also used in many other fields. The following areas
    are particularly relevant to this book:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 除了统计学，R 还在许多其他领域中使用。以下领域特别与本书相关：
- en: Data science
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学
- en: Data science is based on knowledge and practices from statistics and computer
    science that turn raw data into understanding^([7](ch01.html#idm46099155417416))
    by using data analysis and modeling techniques. Statistical methods provide a
    solid foundation to understand the world and perform predictions, while the automation
    provided by computing methods allows us to simplify statistical analysis and make
    it much more accessible. Some have advocated that statistics should be renamed
    data science;^([8](ch01.html#idm46099155681976)) however, data science goes beyond
    statistics by also incorporating advances in computing.^([9](ch01.html#idm46099155681176))
    This book presents analysis and modeling techniques common in statistics but applied
    to large datasets, which requires incorporating advances in distributed computing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学基于统计学和计算机科学的知识和实践，通过使用数据分析和建模技术将原始数据转化为理解^([7](ch01.html#idm46099155417416))。统计方法为理解世界和进行预测提供了坚实的基础，而计算方法提供的自动化则使我们能够简化统计分析并使其更加易于接触。一些人主张应将统计学重新命名为数据科学；^([8](ch01.html#idm46099155681976))
    然而，数据科学不仅仅局限于统计学，还融入了计算科学的进展。^([9](ch01.html#idm46099155681176)) 本书介绍了统计学中常见的分析和建模技术，但应用于大数据集，这需要融合分布式计算的进展。
- en: Machine learning
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习
- en: Machine learning uses practices from statistics and computer science; however,
    it is heavily focused on automation and prediction. For instance, Arthur Samuel
    coined the term *machine learning* while automating a computer program to play
    checkers.^([10](ch01.html#idm46099155677432)) Although we could perform data science
    on particular games, writing a program to play checkers requires us to automate
    the entire process. Therefore, this falls in the realm of machine learning, not
    data science. Machine learning makes it possible for many users to take advantage
    of statistical methods without being aware of using them. One of the first important
    applications of machine learning was to filter spam emails. In this case, it’s
    just not feasible to perform data analysis and modeling over each email account;
    therefore, machine learning automates the entire process of finding spam and filtering
    it out without having to involve users at all. This book presents the methods
    to transition data science workflows into fully automated machine learning methods—for
    instance, by providing support to build and export Spark pipelines that can be
    easily reused in automated environments.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习使用统计学和计算机科学中的实践，但它更加专注于自动化和预测。例如，Arthur Samuel 在自动化一个计算机程序以玩跳棋时创造了术语*机器学习*。^([10](ch01.html#idm46099155677432))
    虽然我们可以对特定游戏执行数据科学，但编写一个玩跳棋的程序需要我们自动化整个过程。因此，这属于机器学习的范畴，而不是数据科学。机器学习使得许多用户能够利用统计方法而无需意识到使用它们。机器学习的一个最早的重要应用是过滤垃圾邮件。在这种情况下，对每个电子邮件账户进行数据分析和建模是不可行的；因此，机器学习自动化了发现垃圾邮件并过滤掉它们的整个过程，而无需完全依赖用户。本书介绍了将数据科学工作流程转换为完全自动化机器学习方法的方法——例如，通过支持构建和导出可以在自动化环境中轻松重用的
    Spark 流水线。
- en: Deep learning
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习
- en: Deep learning builds on knowledge of statistics, data science, and machine learning
    to define models loosely inspired by biological nervous systems. Deep learning
    models evolved from neural network models after the vanishing-gradient problem
    was resolved by training one layer at a time,^([11](ch01.html#idm46099155463784))
    and have proven useful in image and speech recognition tasks. For instance, in
    voice assistants like Siri, Alexa, Cortana, or Google Assistant, the model performing
    the audio-to-text conversion is most likely based on deep learning models. Although
    Graphic Processing Units (GPUs) have been successfully used to train deep learning
    models,^([12](ch01.html#idm46099155462248)) some datasets cannot be processed
    in a single GPU. It is also the case that deep learning models require huge amounts
    of data, which needs to be preprocessed across many machines before it can be
    fed into a single GPU for training. This book doesn’t make any direct references
    to deep learning models; however, you can use the methods we present in this book
    to prepare data for deep learning and, in the years to come, using deep learning
    with large-scale computing will become a common practice. In fact, recent versions
    of Spark have already introduced execution models optimized for training deep
    learning in Spark.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习建立在统计学、数据科学和机器学习的知识基础上，定义了部分受生物神经系统启发的模型。深度学习模型是在解决消失梯度问题后，通过逐层训练逐步演变而来的神经网络模型，^([11](ch01.html#idm46099155463784))
    并且已经被证明在图像和语音识别任务中非常有用。例如，在像 Siri、Alexa、Cortana 或 Google Assistant 这样的语音助手中，执行音频转文本的模型很可能基于深度学习模型。尽管图形处理单元（GPU）已成功用于训练深度学习模型，^([12](ch01.html#idm46099155462248))
    但有些数据集不能在单个 GPU 上处理。此外，深度学习模型需要大量数据，在进入单个 GPU 进行训练之前，需要在许多机器上对其进行预处理。本书并未直接涉及深度学习模型；然而，您可以使用本书中介绍的方法准备深度学习数据，并且在未来几年内，使用深度学习进行大规模计算将成为常见做法。事实上，最新版本的
    Spark 已经引入了为在 Spark 中训练深度学习优化的执行模型。
- en: 'When working in any of the previous fields, you will be faced with increasingly
    large datasets or increasingly complex computations that are slow to execute or
    at times even impossible to process in a single computer. However, it is important
    to understand that Spark does not need to be the answer to all our computations
    problems; instead, when faced with computing challenges in R, using the following
    techniques can be as effective:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在从事前述任何领域时，您将面临越来越大的数据集或越来越复杂的计算，这些计算执行起来速度缓慢，有时甚至在单台计算机上处理起来不可能。然而，重要的是要理解，Spark
    并不需要成为所有计算问题的答案；相反，在面对 R 中的计算挑战时，使用以下技术可能同样有效：
- en: Sampling
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样
- en: A first approach to try is to reduce the amount of data being handled, through
    sampling. However, we must sample the data properly by applying sound statistical
    principles. For instance, selecting the top results is not sufficient in sorted
    datasets; with simple random sampling, there might be underrepresented groups,
    which we could overcome with stratified sampling, which in turn adds complexity
    to properly select categories. It’s beyond the scope of this book to teach how
    to properly perform statistical sampling, but many resources are available on
    this topic.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先尝试的方法是通过采样来减少处理的数据量。然而，我们必须通过应用 sound 统计原则正确地对数据进行采样。例如，在排序数据集中选择前几个结果是不够的；通过简单随机抽样，可能会存在代表不足的组；我们可以通过分层抽样来克服这些问题，但这又增加了正确选择类别的复杂性。本书的范围不包括教授如何正确进行统计抽样，但是关于这个主题有很多资源可供参考。
- en: Profiling
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 分析
- en: You can try to understand why a computation is slow and make the necessary improvements.
    A profiler is a tool capable of inspecting code execution to help identify bottlenecks.
    In R, the R profiler, the [`profvis`](http://bit.ly/2OXGabw) R package, and [RStudio
    profiler feature](https://bit.ly/2RqJPw8) allow you to easily to retrieve and
    visualize a profile; however, it’s not always trivial to optimize.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试理解为什么计算速度慢，并进行必要的改进。分析器是一种检查代码执行以帮助识别瓶颈的工具。在R中，R分析器，[`profvis`](http://bit.ly/2OXGabw)
    R包和[RStudio分析器功能](https://bit.ly/2RqJPw8)允许您轻松检索和可视化配置文件；然而，优化并不总是简单的。
- en: Scaling up
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展
- en: Speeding up computation is usually possible by buying faster or more capable
    hardware (say, increasing your machine memory, upgrading your hard drive, or procuring
    a machine with many more CPUs); this approach is known as *scaling up*. However,
    there are usually hard limits as to how much a single computer can scale up, and
    even with significant CPUs, you need to find frameworks that parallelize computation
    efficiently.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通常可以通过购买更快或更强大的硬件（例如增加机器内存，升级硬盘驱动器，或者购买具有更多CPU的机器）来加速计算，这种方法称为*扩展*。然而，单台计算机能够扩展的限制通常是很严格的，即使具有大量CPU，您也需要找到能够有效并行化计算的框架。
- en: Scaling out
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展出
- en: Finally, we can consider spreading computation and storage across multiple machines.
    This approach provides the highest degree of scalability because you can potentially
    use an arbitrary number of machines to perform a computation. This approach is
    commonly known as *scaling out*. However, spreading computation effectively across
    many machines is a complex endeavor, especially without using specialized tools
    and frameworks like Apache Spark.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以考虑将计算和存储分布到多台机器上。这种方法提供了最高程度的可扩展性，因为您可以潜在地使用任意数量的机器来执行计算。这种方法通常称为*扩展出*。然而，有效地跨多台机器分布计算是一项复杂的工作，特别是在没有使用专门的工具和框架（如Apache
    Spark）的情况下。
- en: This last point brings us closer to the purpose of this book, which is to bring
    the power of distributed computing systems provided by Apache Spark to solve meaningful
    computation problems in data science and related fields, using R.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后一点让我们更接近这本书的目的，即利用Apache Spark提供的分布式计算系统的能力来解决数据科学及相关领域中的有意义的计算问题，使用R。
- en: sparklyr
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: sparklyr
- en: 'When you think of the computation power that Spark provides and the ease of
    use of the R language, it is natural to want them to work together, seamlessly.
    This is also what the R community expected: an R package that would provide an
    interface to Spark that was easy to use, compatible with other R packages, and
    available in CRAN. With this goal, we started developing `sparklyr`. The first
    version, [`sparklyr 0.4`](http://bit.ly/2Zhgevy), was released during the *useR!
    2016* conference. This first version included support for `dplyr`, `DBI`, modeling
    with `MLlib`, and an extensible API that enabled extensions like [H2O](https://www.h2o.ai/)’s
    [`rsparkling`](http://bit.ly/2z348qO) package. Since then, many new features and
    improvements have been made available through [`sparklyr` `0.5`](http://bit.ly/sparklyr05),
    [`0.6`](http://bit.ly/sparklyr06), [`0.7`](http://bit.ly/sparklyr07), [`0.8`](http://bit.ly/sparklyr08),
    [`0.9`](http://bit.ly/2TBnKMt) and [`1.0`](http://bit.ly/sparklyr10).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当你考虑到Spark提供的计算能力和R语言的易用性时，自然而然地希望它们可以无缝地一起工作。这也是R社区的期望：一个能够提供与其他R包兼容、易于使用，并且可以在CRAN上获取的接口，用于连接Spark的R包。基于这个目标，我们开始开发`sparklyr`。第一个版本，[`sparklyr
    0.4`](http://bit.ly/2Zhgevy)，是在*useR! 2016*会议期间发布的。这个版本包括对`dplyr`、`DBI`、使用`MLlib`进行建模的支持，以及一个可扩展的API，支持像[H2O](https://www.h2o.ai/)的[`rsparkling`](http://bit.ly/2z348qO)包等扩展。从那时起，通过[`sparklyr`
    `0.5`](http://bit.ly/sparklyr05)、[`0.6`](http://bit.ly/sparklyr06)、[`0.7`](http://bit.ly/sparklyr07)、[`0.8`](http://bit.ly/sparklyr08)、[`0.9`](http://bit.ly/2TBnKMt)和[`1.0`](http://bit.ly/sparklyr10)发布了许多新功能和改进。
- en: Officially, `sparklyr` is an R interface for Apache Spark. It’s available in
    CRAN and works like any other CRAN package, meaning that it’s agnostic to Spark
    versions, it’s easy to install, it serves the R community, it embraces other packages
    and practices from the R community, and so on. It’s hosted in [GitHub](http://bit.ly/30b5NGT)
    and licensed under Apache 2.0, which allows you to clone, modify, and contribute
    back to this project.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地说，`sparklyr`是Apache Spark的R接口。它可以在CRAN上获取，工作方式与其他CRAN包相同，这意味着它与Spark版本无关，安装简便，服务于R社区，支持R社区的其他包和实践等等。它托管在[GitHub](http://bit.ly/30b5NGT)，根据Apache
    2.0许可，允许您克隆、修改并贡献回这个项目。
- en: 'When thinking of who should use `sparklyr`, the following roles come to mind:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑到谁应该使用`sparklyr`时，以下角色值得一提：
- en: New users
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 新用户
- en: For new users, it is our belief that `sparklyr` provides the easiest way to
    get started with Spark. Our hope is that the early chapters of this book will
    get you up and running with ease and set you up for long-term success.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新用户，我们相信`sparklyr`提供了使用Spark的最简单方式。我们希望本书的前几章能够让你轻松上手，并为你长期的成功打下基础。
- en: Data scientists
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家
- en: For data scientists who already use and love R, `sparklyr` integrates with many
    other R practices and packages like `dplyr`, `magrittr`, `broom`, `DBI`, `tibble`,
    `rlang`, and many others, which will make you feel at home while working with
    Spark. For those new to R and Spark, the combination of high-level workflows available
    in `sparklyr` and low-level extensibility mechanisms make it a productive environment
    to match the needs and skills of every data scientist.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于已经喜欢并使用R的数据科学家，`sparklyr`与许多其他R实践和包集成，如`dplyr`、`magrittr`、`broom`、`DBI`、`tibble`、`rlang`等，使您在使用Spark时感到如同在家。对于新接触R和Spark的人来说，`sparklyr`提供的高级工作流和低级可扩展性机制相结合，使其成为满足每位数据科学家需求和技能的高效环境。
- en: Expert users
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 专家用户
- en: For those users who are already immersed in Spark and can write code natively
    in Scala, consider making your Spark libraries available as an R package to the
    R community, a diverse and skilled community that can put your contributions to
    good use while moving [open science](http://bit.ly/2yZLrVd) forward.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些已经深入研究Spark并能够在Scala中编写代码的用户，考虑将您的Spark库作为R包提供给R社区。R社区是一个多样化且技能娴熟的社区，能够很好地利用您的贡献，同时推动[开放科学](http://bit.ly/2yZLrVd)的发展。
- en: 'We wrote this book to describe and teach the exciting overlap between Apache
    Spark and R. `sparklyr` is the R package that brings together these communities,
    expectations, future directions, packages, and package extensions. We believe
    that there is an opportunity to use this book to bridge the R and Spark communities:
    to present to the R community why Spark is exciting, and to the Spark community
    what makes R great. Both communities are solving very similar problems with a
    set of different skills and backgrounds; therefore, it is our hope that `sparklyr`
    can be a fertile ground for innovation, a welcoming place for newcomers, a productive
    environment for experienced data scientists, and an open community where cluster
    computing, data science, and machine learning can come together.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们撰写本书的目的是描述和教授 Apache Spark 与 R 之间令人兴奋的重叠部分。`sparklyr` 是一个将这两个社区、期望、未来方向、包和包扩展汇集在一起的
    R 包。我们相信，通过本书，可以将 R 和 Spark 社区连接起来：向 R 社区展示 Spark 为何令人兴奋，向 Spark 社区展示 R 的优秀之处。这两个社区都在用一套不同的技能和背景解决非常相似的问题；因此，我们希望
    `sparklyr` 能成为创新的沃土，对新手来说是一个友好的地方，对经验丰富的数据科学家来说是一个高效的环境，并且是一个能够让集群计算、数据科学和机器学习相结合的开放社区。
- en: Recap
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter presented Spark as a modern and powerful computing platform, R
    as an easy-to-use computing language with solid foundations in statistical methods,
    and `sparklyr` as a project bridging both technologies and communities. In a world
    in which the total amount of information is growing exponentially, learning how
    to analyze data at scale will help you to tackle the problems and opportunities
    humanity is facing today. However, before we start analyzing data, [Chapter 2](ch02.html#starting)
    will equip you with the tools you will need throughout the rest of this book.
    Be sure to follow each step carefully and take the time to install the recommended
    tools, which we hope will become familiar resources that you use and love.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将 Spark 描述为一个现代而强大的计算平台，R 作为一个易于使用且在统计方法方面有坚实基础的计算语言，而 `sparklyr` 则是一个连接这两种技术和社区的项目。在信息总量呈指数增长的今天，学会如何进行规模化数据分析将帮助您解决人类面临的问题和机遇。然而，在开始分析数据之前，[第二章](ch02.html#starting)
    将为您提供本书其余部分所需的工具。请务必仔细跟随每一个步骤，并花时间安装推荐的工具，我们希望这些工具能成为您熟悉和喜爱的资源。
- en: '^([1](ch01.html#idm46099155685336-marker)) Laudon KC, Traver CG, Laudon JP
    (1996). “Information technology and systems.” *Cambridge, MA: Course Technology*.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch01.html#idm46099155685336-marker)) Laudon KC, Traver CG, Laudon JP
    (1996). “信息技术与系统。” *Cambridge, MA: Course Technology*.'
- en: '^([2](ch01.html#idm46099155156792-marker)) Ceruzzi PE (2012). *Computing: a
    concise history*. MIT Press.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch01.html#idm46099155156792-marker)) Ceruzzi PE (2012). *计算：简明历史*. MIT
    Press.
- en: ^([3](ch01.html#idm46099135878424-marker)) Group WB (2016). *The Data Revolution*.
    World Bank Publications.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch01.html#idm46099135878424-marker)) 世界银行小组 (2016). *数据革命*. World Bank
    Publications.
- en: ^([4](ch01.html#idm46099159614776-marker)) Ghemawat S, Gobioff H, Leung S (2003).
    “The Google File System.” In *Proceedings of the Nineteenth ACM Symposium on Operating
    Systems Principles*. ISBN 1-58113-757-5.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch01.html#idm46099159614776-marker)) Ghemawat S, Gobioff H, Leung S (2003).
    “Google文件系统。” 在 *第十九届ACM操作系统原理研讨会 (OSDI)* 中的论文集。 ISBN 1-58113-757-5.
- en: '^([5](ch01.html#idm46099155625112-marker)) Dean J, Ghemawat S (2004). “MapReduce:
    Simplified data processing on large clusters.” In *USENIX Symposium on Operating
    System Design and Implementation (OSDI)*.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch01.html#idm46099155625112-marker)) Dean J, Ghemawat S (2004). “MapReduce：简化大型集群上的数据处理。”
    在 *USENIX操作系统设计与实现研讨会 (OSDI)* 中。
- en: '^([6](ch01.html#idm46099155863336-marker)) Zaharia M, Chowdhury M, Franklin
    MJ, Shenker S, Stoica I (2010). “Spark: Cluster computing with working sets.”
    *HotCloud*, 10(10-10), 95.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '^([6](ch01.html#idm46099155863336-marker)) Zaharia M, Chowdhury M, Franklin
    MJ, Shenker S, Stoica I (2010). “Spark: 使用工作集的集群计算。” *HotCloud*, 10(10-10), 95.'
- en: '^([7](ch01.html#idm46099155417416-marker)) Wickham H, Grolemund G (2016). *R
    for data science: import, tidy, transform, visualize, and model data*. O’Reilly
    Media, Inc.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch01.html#idm46099155417416-marker)) Wickham H, Grolemund G (2016). *数据科学中的
    R：导入、整理、转换、可视化和建模数据*. O’Reilly Media, Inc.
- en: ^([8](ch01.html#idm46099155681976-marker)) Wu CJ (1997). “Statistics = Data
    Science?”
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch01.html#idm46099155681976-marker)) 吴CJ (1997). “统计学 = 数据科学？”
- en: '^([9](ch01.html#idm46099155681176-marker)) Cleveland WS (2001). “Data Science:
    An Action Plan for Expanding the Technical Areas of the Field of Statistics?”'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch01.html#idm46099155681176-marker)) 克利夫兰 WS (2001). “数据科学：扩展统计学领域的行动计划？”
- en: ^([10](ch01.html#idm46099155677432-marker)) Samuel AL (1959). “Some studies
    in machine learning using the game of checkers.” *IBM Journal of research and
    development*, 3(3), 210–229.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch01.html#idm46099155677432-marker)) Samuel AL (1959). “使用跳棋游戏进行机器学习的一些研究。”
    *IBM研究与发展杂志*, 3(3), 210–229.
- en: ^([11](ch01.html#idm46099155463784-marker)) Hinton GE, Osindero S, Teh Y (2006).
    “A fast learning algorithm for deep belief nets.” *Neural computation*, 18(7),
    1527–1554.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch01.html#idm46099155463784-marker)) Hinton GE, Osindero S, Teh Y (2006).
    “深度信念网络的快速学习算法。” *神经计算*, 18(7), 1527–1554.
- en: ^([12](ch01.html#idm46099155462248-marker)) Krizhevsky A, Sutskever I, Hinton
    GE (2012). “Imagenet classification with deep convolutional neural networks.”
    In *Advances in neural information processing systems*, 1097–1105.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch01.html#idm46099155462248-marker)) Krizhevsky A, Sutskever I, Hinton
    GE (2012). “使用深度卷积神经网络的Imagenet分类。” 在 *神经信息处理系统进展* 中，1097–1105.
