- en: Chapter 11\. Managing, Deploying, and Scaling Machine Learning Pipelines with
    Apache Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。使用Apache Spark管理、部署和扩展机器学习管道
- en: In the previous chapter, we covered how to build machine learning pipelines
    with MLlib. This chapter will focus on how to manage and deploy the models you
    train. By the end of this chapter, you will be able to use MLflow to track, reproduce,
    and deploy your MLlib models, discuss the difficulties of and trade-offs among
    various model deployment scenarios, and architect scalable machine learning solutions.
    But before we discuss deploying models, let’s first discuss some best practices
    for model management to get your models ready for deployment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了如何使用MLlib构建机器学习管道。本章将专注于如何管理和部署你训练的模型。通过本章的学习，你将能够使用MLflow跟踪、复现和部署你的MLlib模型，讨论各种模型部署场景中的困难和权衡，并设计可扩展的机器学习解决方案。但在讨论部署模型之前，让我们首先讨论一些模型管理的最佳实践，为你的模型部署做好准备。
- en: Model Management
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型管理
- en: 'Before you deploy your machine learning model, you should ensure that you can
    reproduce and track the model’s performance. For us, end-to-end reproducibility
    of machine learning solutions means that we need to be able to reproduce the code
    that generated a model, the environment used in training, the data it was trained
    on, and the model itself. Every data scientist loves to remind you to set your
    seeds so you can reproduce your experiments (e.g., for the train/test split, when
    using models with inherent randomness such as random forests). However, there
    are many more aspects that contribute to reproducibility than just setting seeds,
    and some of them are much more subtle. Here are a few examples:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署机器学习模型之前，你应该确保能够复现和跟踪模型的性能。对于我们来说，端到端的机器学习解决方案的可复现性意味着我们需要能够复现生成模型的代码，训练时使用的环境，训练模型的数据以及模型本身。每个数据科学家都喜欢提醒你设置种子，这样你就可以复现你的实验（例如，在使用具有内在随机性的模型（如随机森林）时进行的训练/测试分割）。然而，影响可复现性的因素远不止设置种子，其中一些因素要微妙得多。以下是一些例子：
- en: Library versioning
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 库版本管理
- en: When a data scientist hands you their code, they may or may not mention the
    dependent libraries. While you are able to figure out which libraries are required
    by going through the error messages, you won’t be certain which library versions
    they used, so you’ll likely install the latest ones. But if their code was built
    on a previous version of a library, which may be taking advantage of some default
    behavior that differs from the version you installed, using the latest version
    can cause the code to break or the results to differ (for example, consider how
    [XGBoost](https://xgboost.readthedocs.io/en/latest) changed [how it handles missing
    values](https://oreil.ly/frAKS) in v0.90).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当一位数据科学家交给你他们的代码时，他们可能会或者不会提到依赖的库。虽然你可以通过错误消息找出需要的库，但你不确定他们使用的库版本，所以你可能会安装最新的版本。但如果他们的代码是建立在之前的库版本上的，这些版本可能利用了一些与你安装的最新版本不同的默认行为，使用最新版本可能会导致代码出错或结果不同（例如，考虑[XGBoost](https://xgboost.readthedocs.io/en/latest)在v0.90中如何改变了处理缺失值的方式）。
- en: Data evolution
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据演变
- en: Suppose you build a model on June 1, 2020, and keep track of all your hyperparameters,
    libraries, etc. You then try to reproduce the same model on July 1, 2020—but the
    pipeline breaks or the results differ because the underlying data has changed,
    which could happen if someone added an extra column or an order of magnitude more
    data after the initial build.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在2020年6月1日构建了一个模型，并跟踪了所有的超参数、库等信息。然后你试图在2020年7月1日复现同样的模型——但是管道中断或结果不同，因为底层数据发生了变化，这可能是因为某人在初始构建之后添加了额外的列或数量级更多的数据。
- en: Order of execution
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 执行顺序
- en: If a data scientist hands you their code, you should be able to run it top-to-bottom
    without error. However, data scientists are notorious for running things out of
    order, or running the same stateful cell multiple times, making their results
    very difficult to reproduce. (They might also check in a copy of the code with
    different hyperparameters than those used to train the final model!)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一位数据科学家交给你他们的代码，你应该能够顺利地从头到尾运行它。然而，数据科学家以非按顺序运行事物而著称，或者多次运行同一个有状态的单元格，使得他们的结果非常难以复现。（他们可能还会检查代码的副本，其中的超参数与用于训练最终模型的超参数不同！）
- en: Parallel operations
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 并行操作
- en: 'To maximize throughput, GPUs will run many operations in parallel. However,
    the order of execution is not always guaranteed, which can lead to nondeterministic
    outputs. This is a known problem with functions like [`tf.reduce_sum()`](https://oreil.ly/FxNt2)
    and when aggregating floating-point numbers (which have limited precision): the
    order in which you add them may generate slightly different results, which can
    be exacerbated across many iterations.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化吞吐量，GPU将并行运行许多操作。然而，并不总是能保证执行的顺序，这可能导致不确定性的输出。这是使用诸如 [`tf.reduce_sum()`](https://oreil.ly/FxNt2)
    和聚合浮点数（具有有限精度）时的已知问题：添加它们的顺序可能会生成略有不同的结果，这在许多迭代中可能会恶化。
- en: An inability to reproduce your experiments can often be a blocker in getting
    business units to adopt your model or put it into production. While you could
    build your own in-house tools for tracking your models, data, dependency versions,
    etc., they may become obsolete, brittle, and take significant development effort
    to maintain. Equally important is having industry-wide standards for managing
    models so that they can be easily shared with partners. There are both open source
    and proprietary tools that can help us with reproducing our machine learning experiments
    by abstracting away many of these common difficulties. This section will focus
    on MLflow, as it has the tightest integration with MLlib of the currently available
    open source model management tools.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 无法重现你的实验通常会成为业务部门接受你的模型或将其投入生产的障碍。虽然你可以构建自己的内部工具来追踪模型、数据、依赖版本等，但它们可能会变得过时、脆弱，并需要大量的开发工作来维护。同样重要的是，具有行业标准的模型管理工具，可以让模型轻松共享给合作伙伴。有开源和专有工具可以帮助我们通过抽象掉许多常见困难来重现我们的机器学习实验。本节将重点介绍MLflow，因为它与当前可用的开源模型管理工具中的MLlib集成最紧密。
- en: MLflow
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLflow
- en: '[MLflow](https://mlflow.org) is an open source platform that helps developers
    reproduce and share experiments, manage models, and much more. It provides interfaces
    in Python, R, and Java/Scala, as well as a REST API. As shown in [Figure 11-1](#mlflow_components),
    MLflow has four main components:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[MLflow](https://mlflow.org) 是一个开源平台，帮助开发者重现和分享实验，管理模型等等。它提供了Python、R和Java/Scala的接口，以及一个REST
    API。如图 [11-1](#mlflow_components) 所示，MLflow 主要包括四个组件：'
- en: Tracking
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 追踪（Tracking）
- en: Provides APIs to record parameters, metrics, code versions, models, and artifacts
    such as plots, and text.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 提供API来记录参数、度量、代码版本、模型和诸如图表和文本等工件。
- en: Projects
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 项目（Projects）
- en: A standardized format to package your data science projects and their dependencies
    to run on other platforms. It helps you manage the model training process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个标准化的格式，用于打包你的数据科学项目及其依赖项，以在其他平台上运行。它帮助你管理模型训练过程。
- en: Models
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 模型（Models）
- en: A standardized format to package models to deploy to diverse execution environments.
    It provides a consistent API for loading and applying models, regardless of the
    algorithm or library used to build the model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个标准化的格式，用于打包模型以部署到不同的执行环境。它提供了一致的API来加载和应用模型，无论使用何种算法或库来构建模型。
- en: Registry
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注册（Registry）
- en: A repository to keep track of model lineage, model versions, stage transitions,
    and annotations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个存储模型谱系、模型版本、阶段转换和注释的库。
- en: '![MLflow components](assets/lesp_1101.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![MLflow 组件](assets/lesp_1101.png)'
- en: Figure 11-1\. MLflow components
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-1\. MLflow 组件
- en: Let’s track the MLlib model experiments we ran in [Chapter 10](ch10.html#machine_learning_with_mllib)
    for reproducibility. We will then see how the other components of MLflow come
    into play when we discuss model deployment. To get started with MLflow, simply
    run `pip install mlflow` on your local host.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们追踪MLlib模型实验，在[第10章](ch10.html#machine_learning_with_mllib)中运行，以便重现。然后我们将讨论模型部署时MLflow的其他组件如何发挥作用。要开始使用MLflow，只需在本地主机上运行
    `pip install mlflow`。
- en: Tracking
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 追踪（Tracking）
- en: MLflow Tracking is a logging API that is agnostic to the libraries and environments
    that actually do the training. It is organized around the concept of *runs*, which
    are executions of data science code. Runs are aggregated into *experiments*, such
    that many runs can be part of a given experiment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow Tracking 是一个日志记录API，它对实际执行训练的库和环境是不可知的。它围绕数据科学代码执行的概念组织，称为 *runs*。Runs
    被聚合成 *experiments*，因此许多 runs 可以成为给定实验的一部分。
- en: The MLflow tracking server can host many experiments. You can log to the tracking
    server using a notebook, local app, or cloud job, as shown in [Figure 11-2](#mlflow_tracking_server).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow跟踪服务器可以托管多个实验。您可以使用笔记本、本地应用程序或云作业将日志记录到跟踪服务器，如[图11-2](#mlflow_tracking_server)所示。
- en: '![MLflow tracking server](assets/lesp_1102.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![MLflow跟踪服务器](assets/lesp_1102.png)'
- en: Figure 11-2\. MLflow tracking server
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2\. MLflow跟踪服务器
- en: 'Let’s examine a few things that can be logged to the tracking server:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一些可以记录到跟踪服务器的内容：
- en: Parameters
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: Key/value inputs to your code—e.g., hyperparameters like `num_trees` or `max_depth`
    in your random forest
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的键/值输入，例如随机森林中的超参数`num_trees`或`max_depth`
- en: Metrics
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 指标
- en: Numeric values (can update over time)—e.g., RMSE or accuracy values
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数值（随时间更新）例如RMSE或准确率数值
- en: Artifacts
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 艺术品
- en: Files, data, and models—e.g., `matplotlib` images, or Parquet files
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 文件、数据和模型，例如`matplotlib`图像或Parquet文件
- en: Metadata
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据
- en: Information about the run, such as the source code that executed the run or
    the version of the code (e.g., the Git commit hash string for the code version)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 运行信息，比如执行了运行的源代码或代码版本（例如，代码版本的Git提交哈希字符串）
- en: Models
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 模型
- en: The model(s) you trained
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您训练的模型（们）
- en: 'By default, the tracking server records everything to the filesystem, but you
    can [specify a database](https://oreil.ly/awTsZ) for faster querying, such as
    for the parameters and metrics. Let’s add MLflow tracking to our random forest
    code from [Chapter 10](ch10.html#machine_learning_with_mllib):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，跟踪服务器将所有内容记录到文件系统，但您可以[指定一个数据库](https://oreil.ly/awTsZ)以进行更快的查询，例如参数和指标。让我们为我们的随机森林代码从[第10章](ch10.html#machine_learning_with_mllib)添加MLflow跟踪：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To start logging with MLflow, you will need to start a run using `mlflow.start_run()`.
    Instead of explicitly calling `mlflow.end_run()`, the examples in this chapter
    will use a `with` clause to automatically end the run at the end of the `with`
    block:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用`mlflow.start_run()`记录日志，您需要启动一个运行。本章节的示例将使用`with`子句来自动结束`with`块时自动结束运行，而不是显式调用`mlflow.end_run()`：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s examine the MLflow UI, which you can access by running `mlflow ui` in
    your terminal and navigating to *http://localhost:5000/*. [Figure 11-3](#the_mlflow_ui)
    shows a screenshot of the UI.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查MLflow用户界面，您可以通过在终端中运行`mlflow ui`并导航至*http://localhost:5000/*来访问。[图11-3](#the_mlflow_ui)显示了用户界面的截图。
- en: '![The MLflow UI](assets/lesp_1103.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![MLflow用户界面](assets/lesp_1103.png)'
- en: Figure 11-3\. The MLflow UI
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-3\. MLflow用户界面
- en: The UI stores all the runs for a given experiment. You can search across all
    the runs, filter for those that meet particular criteria, compare runs side by
    side, etc. If you wish, you can also export the contents as a CSV file to analyze
    locally. Click on the run in the UI named `"random-forest"`. You should see a
    screen like [Figure 11-4](#random_forest_run).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 用户界面存储了给定实验的所有运行。您可以搜索所有运行，按特定条件过滤运行，比较运行并排查等。如果希望，您还可以将内容导出为CSV文件进行本地分析。在UI中点击名为`"random-forest"`的运行。您应该看到类似[图11-4](#random_forest_run)的屏幕。
- en: '![Random forest run](assets/lesp_1104.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林运行](assets/lesp_1104.png)'
- en: Figure 11-4\. Random forest run
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-4\. 随机森林运行
- en: You’ll notice that it keeps track of the source code used for this MLflow run,
    as well as storing all the corresponding parameters, metrics, etc. You can add
    notes about this run in free text, as well as tags. You cannot modify the parameters
    or metrics after the run has finished.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到它记录了用于此MLflow运行的源代码，并存储了所有相应的参数、指标等。您可以添加关于此运行的自由文本注释，以及标签。运行完成后无法修改参数或指标。
- en: 'You can also query the tracking server using the `MlflowClient` or REST API:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`MlflowClient`或REST API查询跟踪服务器：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This produces the following output:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We have hosted this code as an [MLflow project](https://oreil.ly/PleOQ) in
    the [GitHub repo](https://github.com/databricks/LearningSparkV2) for this book,
    so you can experiment running it with different hyperparameter values for `max_depth`
    and `num_trees`. The YAML file inside the MLflow project specifies the library
    dependencies so this code can be run in other environments:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这段代码作为[MLflow项目](https://oreil.ly/PleOQ)托管在本书的[GitHub存储库](https://github.com/databricks/LearningSparkV2)中，因此您可以尝试使用不同的超参数值（例如`max_depth`和`num_trees`）运行它。MLflow项目内的YAML文件指定了库依赖关系，因此此代码可以在其他环境中运行：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that you have tracked and reproduced your experiments, let’s discuss the
    various deployment options available for your MLlib models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经跟踪并复现了您的实验，让我们讨论可用于MLlib模型的各种部署选项。
- en: Model Deployment Options with MLlib
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MLlib 的模型部署选项
- en: Deploying machine learning models means something different for every organization
    and use case. Business constraints will impose different requirements for latency,
    throughput, cost, etc., which dictate which mode of model deployment is suitable
    for the task at hand—be it batch, streaming, real-time, or mobile/embedded. Deploying
    models on mobile/embedded systems is outside the scope of this book, so we will
    focus primarily on the other options. [Table 11-1](#batchcomma_streamingcomma_and_real_time)
    shows the [throughput](https://oreil.ly/qp1nZ) and [latency](https://oreil.ly/R7fzj)
    trade-offs for these three deployment options for generating predictions. We care
    about both the number of concurrent requests and the size of those requests, and
    the resulting solutions will look quite different.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 部署机器学习模型对每个组织和用例来说意味着不同的事情。业务约束将对延迟、吞吐量、成本等施加不同的要求，这些要求决定了哪种模型部署模式适合当前任务——无论是批处理、流式处理、实时处理还是移动/嵌入式。在本书之外的范围内，将模型部署到移动/嵌入式系统，因此我们将主要关注其他选项。[表 11-1](#batchcomma_streamingcomma_and_real_time)显示了生成预测的这三种部署选项的[吞吐量](https://oreil.ly/qp1nZ)和[延迟](https://oreil.ly/R7fzj)权衡。我们关心同时请求的数量和请求的大小，所得到的解决方案将会有很大的不同。
- en: Table 11-1\. Batch, streaming, and real-time comparison
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11-1\. 批处理、流式处理和实时处理比较
- en: '|  | Throughput | Latency | Example application |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | 吞吐量 | 延迟 | 示例应用程序 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Batch** | High | High (hours to days) | Customer churn prediction |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| **批处理** | 高 | 高（几小时到几天） | 客户流失预测 |'
- en: '| **Streaming** | Medium | Medium (seconds to minutes) | Dynamic pricing |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **流式处理** | 中等 | 中等（秒到分钟） | 动态定价 |'
- en: '| **Real-time** | Low | Low (milliseconds) | Online ad bidding |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **实时处理** | 低 | 低（毫秒级） | 在线广告竞价 |'
- en: Batch processing generates predictions on a regular schedule and writes the
    results out to persistent storage to be served elsewhere. It is typically the
    cheapest and easiest deployment option as you only need to pay for compute during
    your scheduled run. Batch processing is much more efficient per data point because
    you accumulate less overhead when amortized across all predictions made. This
    is particularly the case with Spark, because of the overhead of communicating
    back and forth between the driver and the executors—you wouldn’t want to make
    predictions one data point at a time! However, its main drawback is latency, as
    it is typically scheduled with a period of hours or days to generate the next
    batch of predictions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理会定期生成预测并将结果写入持久存储以供其他地方提供服务。通常它是最便宜和最简单的部署选项，因为您只需在计划运行期间支付计算费用。批处理每个数据点的效率更高，因为在所有进行的预测中摊销更少的开销。这在
    Spark 中特别明显，因为在驱动程序和执行器之间来回通信的开销很大——您不希望逐个数据点地进行预测！然而，它的主要缺点是延迟，因为通常会安排几小时或几天的时间来生成下一批预测。
- en: Streaming provides a nice trade-off between throughput and latency. You will
    continuously make predictions on micro-batches of data and get your predictions
    in seconds to minutes. If you are using Structured Streaming, almost all of your
    code will look identical to the batch use case, making it easy to go back and
    forth between these two options. With streaming, you will have to pay for the
    VMs or computing resources you use to continually stay up and running, and ensure
    that you have configured the stream properly to be fault tolerant and provide
    buffering if there are spikes in the incoming data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 流式处理在吞吐量和延迟之间提供了一个很好的权衡。您将持续对数据的微批次进行预测，并在几秒钟到几分钟内获取预测结果。如果您使用结构化流处理，几乎所有代码看起来都与批处理用例相同，这使得在这两个选项之间来回切换变得容易。使用流式处理，您将不得不为使用的虚拟机或计算资源支付费用，以确保系统能够持续运行，并确保正确配置流以实现容错和在数据传入出现峰值时提供缓冲。
- en: Real-time deployment prioritizes latency over throughput and generates predictions
    in a few milliseconds. Your infrastructure will need to support load balancing
    and be able to scale to many concurrent requests if there is a large spike in
    demand (e.g., for online retailers around the holidays). Sometimes when people
    say “real-time deployment” they mean extracting precomputed predictions in real
    time, but here we’re referring to *generating* model predictions in real time.
    Real-time deployment is the only option that Spark cannot meet the latency requirements
    for, so to use it you will need to export your model outside of Spark. For example,
    if you intend to use a REST endpoint for real-time model inference (say, computing
    predictions in under 50 ms), MLlib does not meet the latency requirements necessary
    for this application, as shown in [Figure 11-5](#deployment_options_for_mllib).
    You will need to get your feature preparation and model out of Spark, which can
    be time-consuming and difficult.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 实时部署优先考虑延迟而非吞吐量，并在几毫秒内生成预测。如果需求激增（例如，在假期期间的在线零售商），您的基础架构需要支持负载平衡，并能够扩展到许多并发请求。有时，当人们提到“实时部署”时，他们指的是实时提取预先计算的预测，但在这里，我们指的是实时生成模型预测。实时部署是Spark无法满足延迟要求的唯一选项，因此如果要使用它，您需要在Spark之外导出您的模型。例如，如果您打算使用REST端点进行实时模型推断（例如，在50毫秒内计算预测），MLlib无法满足此应用所需的延迟要求，如[图11-5](#deployment_options_for_mllib)所示。您需要将特征准备和模型移出Spark，这可能是耗时且困难的过程。
- en: '![Deployment options for MLlib](assets/lesp_1105.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![MLlib的部署选项](assets/lesp_1105.png)'
- en: Figure 11-5\. Deployment options for MLlib
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-5. MLlib的部署选项
- en: Before you begin the modeling process, you need to define your model deployment
    requirements. MLlib and Spark are just a few tools in your toolbox, and you need
    to understand when and where they should be applied. The remainder of this section
    discusses the deployment options for MLlib in more depth, and then we’ll consider
    the deployment options with Spark for non-MLlib models.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始建模过程之前，您需要定义您的模型部署要求。MLlib和Spark只是您工具箱中的几个工具，您需要理解何时以及在何处应用它们。本节的其余部分将更深入地讨论MLlib的部署选项，然后我们将考虑非MLlib模型的Spark部署选项。
- en: Batch
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量
- en: 'Batch deployments represent the majority of use cases for deploying machine
    learning models, and this is arguably the easiest option to implement. You will
    run a regular job to generate predictions, and save the results to a table, database,
    data lake, etc. for downstream consumption. In fact, you have already seen how
    to generate batch predictions in [Chapter 10](ch10.html#machine_learning_with_mllib)
    with MLlib. MLlib’s `model.transform()` will apply the model in parallel to all
    partitions of your DataFrame:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 批量部署代表了部署机器学习模型的大多数用例，并且这可能是最容易实现的选项。您将运行定期作业来生成预测，并将结果保存到表格、数据库、数据湖等以供下游消费。事实上，您已经在[第10章](ch10.html#machine_learning_with_mllib)中看到了如何使用MLlib生成批量预测。MLlib的`model.transform()`将并行地将模型应用于DataFrame的所有分区：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A few things to keep in mind with batch deployments are:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 批量部署需要注意以下几点：
- en: How frequently will you generate predictions?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您将多频繁地生成预测？
- en: There is a trade-off between latency and throughput. You will get higher throughput
    batching many predictions together, but then the time it takes to receive any
    individual predictions will be much longer, delaying your ability to act on these
    predictions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 存在延迟和吞吐量之间的权衡。批量将多个预测批次在一起可以获得更高的吞吐量，但接收任何单个预测结果的时间会更长，延迟您对这些预测的行动能力。
- en: How often will you retrain the model?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您会多频繁地重新训练模型？
- en: Unlike libraries like sklearn or TensorFlow, MLlib does not support online updates
    or warm starts. If you’d like to retrain your model to incorporate the latest
    data, you’ll have to retrain the entire model from scratch, rather than getting
    to leverage the existing parameters. In terms of the frequency of retraining,
    some people will set up a regular job to retrain the model (e.g., once a month),
    while others will actively [monitor the model drift](https://oreil.ly/aX4dT) to
    identify when they need to retrain.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 不像像sklearn或TensorFlow这样的库，MLlib不支持在线更新或热启动。如果您想重新训练模型以整合最新数据，您需要从头开始重新训练整个模型，而不是利用现有参数。关于重新训练的频率，有些人会设置定期作业来重新训练模型（例如，每月一次），而其他人则会积极[监控模型漂移](https://oreil.ly/aX4dT)以确定何时需要重新训练。
- en: How will you version the model?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 您将如何对模型进行版本管理？
- en: You can use the [MLflow Model Registry](https://oreil.ly/D5LR6) to keep track
    of the models you are using and control how they are transitioned to/from staging,
    production, and archived. You can see a screenshot of the Model Registry in [Figure 11-6](#mlflow_model_registry).
    You can use the Model Registry with the other deployment options too.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用[MLflow模型注册表](https://oreil.ly/D5LR6)来跟踪您正在使用的模型，并控制它们如何在暂存、生产和归档之间过渡。您可以在[图 11-6](#mlflow_model_registry)中查看模型注册表的截图。您还可以将模型注册表与其他部署选项一起使用。
- en: '![MLflow Model Registry](assets/lesp_1106.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![MLflow模型注册表](assets/lesp_1106.png)'
- en: Figure 11-6\. MLflow Model Registry
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-6\. MLflow模型注册表
- en: 'In addition to using the MLflow UI to manage your models, you can also manage
    them programmatically. For example, once you have registered your production model,
    it has a consistent URI that you can use to retrieve the latest version:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用MLflow UI来管理您的模型外，您还可以以编程方式管理它们。例如，一旦您注册了生产模型，它就有一个一致的URI，您可以使用它来检索最新版本：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Streaming
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式处理
- en: Instead of waiting for an hourly or nightly job to process your data and generate
    predictions, Structured Streaming can continuously perform inference on incoming
    data. While this approach is more costly than a batch solution as you have to
    continually pay for compute time (and get lower throughput), you get the added
    benefit of generating predictions more frequently so you can act on them sooner.
    Streaming solutions in general are more complicated to maintain and monitor than
    batch solutions, but they offer lower latency.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 不必等待每小时或每夜的作业来处理数据并生成预测，结构化流处理可以连续对传入数据进行推断。虽然这种方法比批处理解决方案更昂贵，因为您必须持续支付计算时间（并且获得较低的吞吐量），但您可以更频繁地生成预测，以便更早地采取行动。总体而言，流处理解决方案比批处理解决方案更复杂，需要更多的维护和监控工作，但提供更低的延迟。
- en: 'With Spark it’s very easy to convert your batch predictions to streaming predictions,
    and practically all of the code is the same. The only difference is that when
    you read in the data, you need to use `spark.readStream()` rather than `spark.read()`
    and change the source of the data. In the following example we are going to simulate
    reading in streaming data by streaming in a directory of Parquet files. You’ll
    notice that we are specifying a `schema` even though we are working with Parquet
    files. This is because we need to define the schema a priori when working with
    streaming data. In this example, we will use the random forest model trained on
    our Airbnb data set from the previous chapter to perform these streaming predictions.
    We will load in the saved model using MLflow. We have partitioned the source file
    into one hundred small Parquet files so you can see the output changing at every
    trigger interval:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark，将批量预测转换为流式预测非常容易，实际上所有的代码都是一样的。唯一的区别在于，在读取数据时，您需要使用`spark.readStream()`而不是`spark.read()`并更改数据源。在下面的示例中，我们将通过流式处理一组Parquet文件的方式模拟读取流数据。您会注意到，尽管我们使用Parquet文件，但我们仍需要定义模式，因为在处理流数据时需要事先定义模式。在此示例中，我们将使用在前一章节中基于我们的Airbnb数据集训练的随机森林模型来执行这些流式预测。我们将使用MLflow加载保存的模型。我们将源文件分成了一百个小的Parquet文件，以便您可以在每个触发间隔看到输出变化：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: After you generate these predictions, you can write them out to any target location
    for retrieval later (refer to [Chapter 8](ch08.html#structured_streaming) for
    Structured Streaming tips). As you can see, the code is virtually unchanged between
    the batch and streaming scenarios, making MLlib a great solution for both. However,
    depending on the latency demands of your task, MLlib may not be the best choice.
    With Spark there is significant overhead involved in generating the query plan
    and communicating the task and results between the driver and the worker. Thus,
    if you need really low-latency predictions, you’ll need to export your model out
    of Spark.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成这些预测之后，您可以将它们写入任何目标位置，以便稍后检索（请参阅[第 8章](ch08.html#structured_streaming)中关于结构化流处理的提示）。正如您所见，批处理和流处理场景之间的代码几乎没有变化，这使得MLlib成为两者的良好解决方案。然而，根据任务对延迟的需求不同，MLlib可能不是最佳选择。使用Spark在生成查询计划和在驱动程序与工作节点之间传输任务和结果时会涉及显著的开销。因此，如果您需要真正低延迟的预测，您需要将模型导出到Spark之外。
- en: Model Export Patterns for Real-Time Inference
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时推理的模型导出模式
- en: There are some domains where real-time inference is required, including fraud
    detection, ad recommendation, and the like. While making predictions with a small
    number of records may achieve the low latency required for real-time inference,
    you will need to contend with load balancing (handling many concurrent requests)
    as well as geolocation in latency-critical tasks. There are popular managed solutions,
    such as [AWS SageMaker](https://aws.amazon.com/sagemaker) and [Azure ML](https://oreil.ly/OzEnY),
    that provide low-latency model serving solutions. In this section we’ll show you
    how to export your MLlib models so they can be deployed to those services.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些领域需要实时推断，包括欺诈检测、广告推荐等。尽管使用少量记录进行预测可以达到实时推断所需的低延迟，但您将需要处理负载平衡（处理许多并发请求）以及延迟关键任务中的地理位置。有一些流行的托管解决方案，例如
    [AWS SageMaker](https://aws.amazon.com/sagemaker) 和 [Azure ML](https://oreil.ly/OzEnY)，提供了低延迟模型服务解决方案。在本节中，我们将展示如何导出您的
    MLlib 模型，以便部署到这些服务中。
- en: One way to export your model out of Spark is to reimplement the model natively
    in Python, C, etc. While it may seem simple to extract the coefficients of the
    model, exporting all the feature engineering and preprocessing steps along with
    them (`OneHotEncoder`, `VectorAssembler`, etc.) quickly gets troublesome and is
    very error-prone. There are a few open source libraries, such as [MLeap](https://mleap-docs.combust.ml)
    and [ONNX](https://onnx.ai), that can help you automatically export a supported
    subset of the MLlib models to remove their dependency on Spark. However, as of
    the time of this writing the company that developed MLeap is no longer supporting
    it. Nor does MLeap yet support Scala 2.12/Spark 3.0.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型从 Spark 导出的一种方法是在 Python、C 等中本地重新实现模型。虽然提取模型的系数似乎很简单，但连同特征工程和预处理步骤（`OneHotEncoder`、`VectorAssembler`
    等）导出时会很麻烦，并且容易出错。有一些开源库，如 [MLeap](https://mleap-docs.combust.ml) 和 [ONNX](https://onnx.ai)，可以帮助您自动导出
    MLlib 模型的支持子集，以消除它们对 Spark 的依赖。然而，到撰写本文时，开发 MLeap 的公司已不再支持它。MLeap 也尚不支持 Scala
    2.12/Spark 3.0。
- en: ONNX (Open Neural Network Exchange), on the other hand, has become the de facto
    open standard for machine learning interoperability. Some of you might recall
    other ML interoperability formats, like PMML (Predictive Model Markup Language),
    but those never gained quite the same traction as ONNX has now. ONNX is very popular
    in the deep learning community as a tool that allows developers to easily switch
    between libraries and languages, and at the time of this writing it has experimental
    support for MLlib.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，ONNX（开放神经网络交换）已成为机器学习互操作性的事实开放标准。您可能还记得其他机器学习互操作格式，如 PMML（预测模型标记语言），但它们从未像现在的
    ONNX 那样广受欢迎。在深度学习社区中，ONNX 作为一种工具非常受欢迎，它允许开发人员轻松在不同库和语言之间切换，并在撰写本文时，它已经具有对 MLlib
    的实验性支持。
- en: Instead of exporting MLlib models, there are other third-party libraries that
    integrate with Spark that are convenient to deploy in real-time scenarios, such
    as [XGBoost](https://oreil.ly/7-iZJ) and H2O.ai’s [Sparkling Water](https://oreil.ly/yhKP9)
    (whose name is derived from a combination of H2O and Spark).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与导出 MLlib 模型不同，还有其他第三方库与 Spark 集成，方便在实时场景中部署，例如 [XGBoost](https://oreil.ly/7-iZJ)
    和 H2O.ai 的 [Sparkling Water](https://oreil.ly/yhKP9)（其名称源自 H2O 和 Spark 的结合）。
- en: 'XGBoost is one of [the most successful algorithms](https://oreil.ly/iReUA)
    in [Kaggle competitions](https://www.kaggle.com) for structured data problems,
    and it’s a very popular library among data scientists. Although XGBoost is not
    technically part of MLlib, the [XGBoost4J-Spark library](https://oreil.ly/XGg5c)
    allows you to integrate distributed XGBoost into your MLlib pipelines. A benefit
    of XGBoost is the ease of deployment: after you train your MLlib pipeline, you
    can extract the XGBoost model and save it as a non-Spark model for serving in
    Python, as demonstrated here:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 是结构化数据问题中 [最成功的算法之一](https://oreil.ly/iReUA) 在 [Kaggle 竞赛](https://www.kaggle.com)
    中，它是数据科学家中非常流行的库。虽然 XGBoost 技术上不属于 MLlib 的一部分，但 [XGBoost4J-Spark 库](https://oreil.ly/XGg5c)
    允许您将分布式 XGBoost 集成到 MLlib 流水线中。XGBoost 的一个好处是部署的简易性：在训练 MLlib 流水线后，您可以提取 XGBoost
    模型并保存为非 Spark 模型，用于 Python 服务，如此展示：
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of this writing, the distributed XGBoost API is only available in
    Java/Scala. A full example is included in the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 到撰写本文时，分布式 XGBoost API 仅在 Java/Scala 中可用。书中的示例可以在 [GitHub 仓库](https://github.com/databricks/LearningSparkV2)
    找到。
- en: Now that you have learned about the different ways of exporting MLlib models
    for use in real-time serving environments, let’s discuss how we can leverage Spark
    for non-MLlib models.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了不同的导出 MLlib 模型以在实时服务环境中使用的方式，请讨论我们如何利用 Spark 对非 MLlib 模型进行优化。
- en: Leveraging Spark for Non-MLlib Models
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用 Spark 进行非 MLlib 模型
- en: As mentioned previously, MLlib isn’t always the best solution for your machine
    learning needs. It may not meet super low-latency inference requirements or have
    built-in support for the algorithm you’d like to use. For these cases, you can
    still leverage Spark, but not MLlib. In this section, we will discuss how you
    can use Spark to perform distributed inference of single-node models using Pandas
    UDFs, perform hyperparameter tuning, and scale feature engineering.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，MLlib 并不总是满足您的机器学习需求的最佳解决方案。它可能无法满足超低延迟推断要求，或者没有内置支持您想要使用的算法。对于这些情况，您仍然可以利用
    Spark，但不使用 MLlib。在本节中，我们将讨论如何使用 Spark 执行单节点模型的分布式推断，使用 Pandas UDFs 进行超参数调整和特征工程的扩展。
- en: Pandas UDFs
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pandas UDFs
- en: While MLlib is fantastic for distributed training of models, you are not limited
    to just using MLlib for making batch or streaming predictions with Spark—you can
    create custom functions to apply your pretrained models at scale, known as user-defined
    functions (UDFs, covered in [Chapter 5](ch05.html#spark_sql_and_dataframes_interacting_wit)).
    A common use case is to build a scikit-learn or TensorFlow model on a single machine,
    perhaps on a subset of your data, but perform distributed inference on the entire
    data set using Spark.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 MLlib 在分布式训练模型方面非常出色，但您不仅限于仅使用 MLlib 在 Spark 中进行批处理或流式预测，您可以创建自定义函数以在规模化时应用您预先训练的模型，称为用户定义函数（UDFs，在
    [第 5 章](ch05.html#spark_sql_and_dataframes_interacting_wit) 中有介绍）。一个常见的用例是在单台机器上构建
    scikit-learn 或 TensorFlow 模型，可能在数据子集上，但使用 Spark 在整个数据集上进行分布式推断。
- en: If you define your own UDF to apply a model to each record of your DataFrame
    in Python, opt for [pandas UDFs](https://oreil.ly/ww2_S) for optimized serialization
    and deserialization, as discussed in [Chapter 5](ch05.html#spark_sql_and_dataframes_interacting_wit).
    However, if your model is very large, then there is high overhead for the Pandas
    UDF to repeatedly load the same model for every batch in the same Python worker
    process. In Spark 3.0, Pandas UDFs can accept an iterator of `pandas.Series` or
    `pandas.DataFrame` so that you can load the model only once instead of loading
    it for every series in the iterator. For more details on what’s new in Apache
    Spark 3.0 with Pandas UDFs, see [Chapter 12](ch12.html#epilogue_apache_spark_3dot0).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 Python 中定义自己的 UDF 来将模型应用于 DataFrame 的每条记录，请选择 [pandas UDFs](https://oreil.ly/ww2_S)
    进行优化的序列化和反序列化，如 [第 5 章](ch05.html#spark_sql_and_dataframes_interacting_wit) 中所讨论的。但是，如果您的模型非常庞大，则
    Pandas UDF 对于每个批次重复加载相同模型会有很高的开销。在 Spark 3.0 中，Pandas UDF 可以接受 `pandas.Series`
    或 `pandas.DataFrame` 的迭代器，因此您只需加载一次模型，而不是在迭代器中的每个系列中重复加载。有关 Apache Spark 3.0 中
    Pandas UDF 的新功能详情，请参阅 [第 12 章](ch12.html#epilogue_apache_spark_3dot0)。
- en: Note
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If the workers cached the model weights after loading it for the first time,
    subsequent calls of the same UDF with the same model loading will become significantly
    faster.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果工作节点在第一次加载后缓存了模型权重，使用相同模型加载的同一个 UDF 的后续调用将变得显著更快。
- en: 'In the following example, we will use `mapInPandas()`, introduced in Spark
    3.0, to apply a `scikit-learn` model to our Airbnb data set. `mapInPandas()` takes
    an iterator of `pandas.DataFrame` as input, and outputs another iterator of `pandas.DataFrame`.
    It’s flexible and easy to use if your model requires all of your columns as input,
    but it requires serialization/deserialization of the whole DataFrame (as it is
    passed to its input). You can control the size of each `pandas.DataFrame` with
    the `spark.sql.execution.arrow.maxRecordsPerBatch` config. A full copy of the
    code to generate the model is available in this book’s [GitHub repo](https://github.com/databricks/LearningSparkV2),
    but here we will just focus on loading the saved `scikit-learn` model from MLflow
    and applying it to our Spark DataFrame:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们将使用 Spark 3.0 中引入的 `mapInPandas()` 来将 `scikit-learn` 模型应用于我们的 Airbnb
    数据集。`mapInPandas()` 接受 `pandas.DataFrame` 的迭代器作为输入，并输出另一个 `pandas.DataFrame` 的迭代器。如果您的模型需要所有列作为输入，它非常灵活且易于使用，但需要整个
    DataFrame 的序列化/反序列化（因为它传递给其输入）。您可以通过 `spark.sql.execution.arrow.maxRecordsPerBatch`
    配置来控制每个 `pandas.DataFrame` 的大小。可以在本书的 [GitHub 仓库](https://github.com/databricks/LearningSparkV2)
    中找到生成模型的完整代码，但在这里我们只关注从 MLflow 加载保存的 `scikit-learn` 模型并将其应用于我们的 Spark DataFrame：
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In addition to applying models at scale using a Pandas UDF, you can also use
    them to parallelize the process of building many models. For example, you might
    want to build a model for each IoT device type to predict time to failure. You
    can use `pyspark.sql.GroupedData.applyInPandas()` (introduced in Spark 3.0) for
    this task. The function takes a `pandas.DataFrame` and returns another `pandas.DataFrame`.
    The book’s GitHub repo contains a full example of the code to build a model per
    IoT device type and track the individual models with MLflow; just a snippet is
    included here for brevity:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用 Pandas UDF 扩展规模应用模型之外，你还可以使用它们来并行化构建多个模型的过程。例如，你可能希望为每种 IoT 设备类型构建一个模型来预测故障时间。你可以使用
    `pyspark.sql.GroupedData.applyInPandas()`（在 Spark 3.0 中引入）来完成此任务。该函数接受一个 `pandas.DataFrame`
    并返回另一个 `pandas.DataFrame`。本书的 GitHub 存储库包含了构建每种 IoT 设备类型模型的完整代码示例，并使用 MLflow 跟踪各个模型；这里只包含了一个简短的片段以保持简洁：
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `groupBy()` will cause a full shuffle of your data set, and you need to
    ensure that your model and the data for each group can fit on a single machine.
    Some of you might be familiar with `pyspark.sql.GroupedData.apply()` (e.g., `df.groupBy("device_id").apply(build_model`)),
    but that API will be deprecated in future releases of Spark in favor of `pyspark.sql.GroupedData.applyInPandas()`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupBy()` 会导致数据集完全洗牌，你需要确保每个组的模型和数据都能在一台机器上适合。你们中的一些人可能熟悉 `pyspark.sql.GroupedData.apply()`（例如，`df.groupBy("device_id").apply(build_model)`），但该
    API 将在未来的 Spark 版本中弃用，推荐使用 `pyspark.sql.GroupedData.applyInPandas()`。'
- en: Now that you have seen how to apply UDFs to perform distributed inference and
    parallelize model building, let’s look at how to use Spark for distributed hyperparameter
    tuning.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到如何应用 UDF 进行分布式推断和并行化模型构建，让我们看看如何使用 Spark 进行分布式超参数调整。
- en: Spark for Distributed Hyperparameter Tuning
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 用于分布式超参数调整
- en: 'Even if you do not intend to do distributed inference or do not need MLlib’s
    distributed training capabilities, you can still leverage Spark for distributed
    hyperparameter tuning. This section will cover two open source libraries in particular:
    Joblib and Hyperopt.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你不打算进行分布式推断或不需要 MLlib 的分布式训练能力，你仍然可以利用 Spark 进行分布式超参数调整。本节将特别介绍两个开源库：Joblib
    和 Hyperopt。
- en: Joblib
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Joblib
- en: According to its documentation, [Joblib](https://github.com/joblib/joblib) is
    “a set of tools to provide lightweight pipelining in Python.” It has a Spark backend
    to distribute tasks on a Spark cluster. Joblib can be used for hyperparameter
    tuning as it automatically broadcasts a copy of your data to all of your workers,
    which then create their own models with different hyperparameters on their copies
    of the data. This allows you to train and evaluate multiple models in parallel.
    You still have the fundamental limitation that a single model and all the data
    have to fit on a single machine, but you can trivially parallelize the hyperparameter
    search, as shown in [Figure 11-7](#distributed_hyperparameter_search).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其文档，[Joblib](https://github.com/joblib/joblib) 是“提供 Python 中轻量级管道处理的一组工具”。它具有
    Spark 后端，可在 Spark 集群上分发任务。Joblib 可用于超参数调整，因为它会自动将数据的副本广播到所有工作节点，然后在数据的各自副本上使用不同的超参数创建自己的模型。这允许你并行训练和评估多个模型。但你仍然有一个基本限制，即单个模型和所有数据必须适合一台机器，但你可以轻松地并行化超参数搜索，正如
    [图 11-7](#distributed_hyperparameter_search) 中所示。
- en: '![Distributed hyperparameter search](assets/lesp_1107.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![分布式超参数搜索](assets/lesp_1107.png)'
- en: Figure 11-7\. Distributed hyperparameter search
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-7\. 分布式超参数搜索
- en: 'To use Joblib, install it via `pip install joblibspark`. Ensure you are using
    `scikit-learn` version 0.21 or later and `pyspark` version 2.4.4 or later. An
    example of how to do distributed cross-validation is shown here, and the same
    approach will work for distributed hyperparameter tuning as well:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Joblib，请通过 `pip install joblibspark` 进行安装。确保你使用的是 `scikit-learn` 版本 0.21
    或更高以及 `pyspark` 版本 2.4.4 或更高。这里展示了一个分布式交叉验证的示例，相同的方法也适用于分布式超参数调整：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: See the `scikit-learn` [GridSearchCV documentation](https://oreil.ly/zjuSD)
    for an explanation of the parameters returned from the cross-validator.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 `scikit-learn` [GridSearchCV 文档](https://oreil.ly/zjuSD)，了解交叉验证器返回的参数解释。
- en: Hyperopt
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hyperopt
- en: '[Hyperopt](https://oreil.ly/N9TVh) is a Python library for “serial and parallel
    optimization over awkward search spaces, which may include real-valued, discrete,
    and conditional dimensions.” You can install it via `pip install hyperopt`. There
    are two main ways to [scale Hyperopt with Apache Spark](https://oreil.ly/D07fV):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[Hyperopt](https://oreil.ly/N9TVh) 是一个 Python 库，用于“在棘手的搜索空间上进行串行和并行优化，这些空间可能包括实值、离散和条件维度”。您可以通过
    `pip install hyperopt` 安装它。有两种主要方法可以使用 [Apache Spark 扩展 Hyperopt](https://oreil.ly/D07fV)：'
- en: Using single-machine Hyperopt with a distributed training algorithm (e.g., MLlib)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单机 Hyperopt 与分布式训练算法（例如 MLlib）
- en: Using distributed Hyperopt with single-machine training algorithms with the
    `SparkTrials` class
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `SparkTrials` 类的分布式 Hyperopt 与单机训练算法
- en: 'For the former case, there is nothing special you need to configure to use
    MLlib with Hyperopt versus any other library. So, let’s take a look at the latter
    case: distributed Hyperopt with single-node models. Unfortunately, you can’t combine
    distributed hyperparameter evaluation with distributed training models at the
    time of this writing. The full code example for parallelizing the hyperparameter
    search for a [Keras](https://oreil.ly/XbHSG) model can be found in the book’s
    [GitHub repo](https://github.com/databricks/LearningSparkV2); just a snippet is
    included here to illustrate the key components of Hyperopt:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前一种情况，与使用 Hyperopt 与任何其他库相比，您无需特别配置 MLlib。所以，让我们看看后一种情况：单节点模型的分布式 Hyperopt。不幸的是，您目前无法将分布式超参数评估与分布式训练模型结合起来。有关为
    [Keras](https://oreil.ly/XbHSG) 模型并行化超参数搜索的完整代码示例可以在该书的 [GitHub 仓库](https://github.com/databricks/LearningSparkV2)
    中找到；这里仅包含一个片段，以说明 Hyperopt 的关键组成部分：
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`fmin()` generates new hyperparameter configurations to use for your `training_function`
    and passes them to `SparkTrials`. `SparkTrials` runs batches of these training
    tasks in parallel as a single-task Spark job on each Spark executor. When the
    Spark task is done, it returns the results and the corresponding loss to the driver.
    Hyperopt uses these new results to compute better hyperparameter configurations
    for future tasks. This allows for massive scale-out of hyperparameter tuning.
    MLflow also integrates with Hyperopt, so you can track the results of all the
    models you’ve trained as part of your hyperparameter tuning.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`fmin()` 生成新的超参数配置，用于您的 `training_function` 并传递给 `SparkTrials`。 `SparkTrials`
    将这些训练任务的批次作为单个 Spark 作业并行运行在每个 Spark 执行器上。当 Spark 任务完成时，它将结果和相应的损失返回给驱动程序。Hyperopt
    使用这些新结果来计算未来任务的更好超参数配置。这允许进行大规模超参数调整。MLflow 也与 Hyperopt 集成，因此您可以跟踪您作为超参数调整一部分训练的所有模型的结果。'
- en: An important parameter for `SparkTrials` is `parallelism`. This determines the
    maximum number of trials to evaluate concurrently. If `parallelism=1`, then you
    are training each model sequentially, but you might get better models by making
    full use of adaptive algorithms. If you set `parallelism=max_evals` (the total
    number of models to train), then you are just doing a random search. Any number
    between `1` and `max_evals` allows you to have a trade-off between scalability
    and adaptiveness. By default, `parallelism` is set to the number of Spark executors.
    You can also specify a `timeout` to limit the maximum number of seconds that `fmin()`
    is allowed to take.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkTrials` 的一个重要参数是 `parallelism`。这确定同时评估的最大试验数。如果 `parallelism=1`，则您是按顺序训练每个模型，但通过充分利用自适应算法，您可能会获得更好的模型。如果设置
    `parallelism=max_evals`（要训练的总模型数），那么您只是在进行随机搜索。在 `1` 到 `max_evals` 之间的任何数字允许您在可扩展性和适应性之间进行权衡。默认情况下，`parallelism`
    设置为 Spark 执行器的数量。您还可以指定 `timeout` 来限制 `fmin()` 允许花费的最大秒数。'
- en: Even if MLlib isn’t suitable for your problem, hopefully you can see the value
    of using Spark in any of your machine learning tasks.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 MLlib 对您的问题不合适，希望您能看到在任何机器学习任务中使用 Spark 的价值。
- en: Summary
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered a variety of best practices for managing and deploying
    machine learning pipelines. You saw how MLflow can help you track and reproduce
    experiments and package your code and its dependencies to deploy elsewhere. We
    also discussed the main deployment options—batch, streaming, and real-time—and
    their associated trade-offs. MLlib is a fantastic solution for large-scale model
    training and batch/streaming use cases, but it won’t beat a single-node model
    for real-time inference on small data sets. Your deployment requirements directly
    impact the types of models and frameworks that you can use, and it is critical
    to discuss these requirements before you begin your model building process.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了多种管理和部署机器学习流水线的最佳实践。您看到了 MLflow 如何帮助您跟踪和重现实验，并将您的代码及其依赖打包以便在其他地方部署。我们还讨论了主要的部署选项——批处理、流处理和实时处理——及其相关的权衡取舍。MLlib
    是大规模模型训练和批处理/流处理用例的绝佳解决方案，但在小数据集上进行实时推断时，它无法与单节点模型相提并论。您的部署需求直接影响您可以使用的模型和框架类型，因此在开始模型构建过程之前讨论这些需求至关重要。
- en: In the next chapter, we will highlight a handful of key new features in Spark
    3.0 and how you can incorporate them into your Spark workloads.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重点介绍 Spark 3.0 的几个关键新功能，以及如何将它们整合到您的 Spark 工作负载中。
