- en: 26  Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 26  聚类
- en: 原文：[https://ds100.org/course-notes/clustering/clustering.html](https://ds100.org/course-notes/clustering/clustering.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://ds100.org/course-notes/clustering/clustering.html](https://ds100.org/course-notes/clustering/clustering.html)
- en: '*Learning Outcomes* ***   Introduction to clustering'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习成果* ***   聚类介绍'
- en: Assessing the taxonomy of clustering approaches
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估聚类方法的分类法
- en: K-Means clustering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值聚类
- en: 'Clustering with no explicit loss function: minimizing inertia'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有明确损失函数的聚类：最小化惯性
- en: Hierarchical Agglomerative Clustering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次凝聚聚类
- en: 'Picking K: a hyperparameter**  **Last time, we began our journey into unsupervised
    learning by discussing Principal Component Analysis (PCA).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择K：一个超参数** **上次，我们通过讨论主成分分析（PCA）开始了我们对无监督学习的探讨。
- en: 'In this lecture, we will explore another very popular unsupervised learning
    concept: clustering. Clustering allows us to “group” similar datapoints together
    without being given labels of what “class” or where each point explicitly comes
    from. We will discuss two clustering algorithms: K-Means clustering and hierarchical
    agglomerative clustering, and we’ll examine the assumptions, strengths, and drawbacks
    of each one.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本讲座中，我们将探讨另一个非常流行的无监督学习概念：聚类。聚类允许我们在没有给出“类”或每个点明确来自何处的标签的情况下将相似的数据点“分组”在一起。我们将讨论两种聚类算法：K均值聚类和层次凝聚聚类，并且我们将检查每种算法的假设、优势和局限性。
- en: '26.1 Review: Taxonomy of Machine Learning'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.1回顾：机器学习分类法
- en: 26.1.1 Supervised Learning
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 26.1.1 监督学习
- en: 'In “Supervised Learning”, our goal is to create a function that maps inputs
    to outputs. Each model is learned from example input/output pairs (training set),
    validated using input/output pairs, and eventually tested on more input/output
    pairs. Each pair consists of:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在“监督学习”中，我们的目标是创建一个将输入映射到输出的函数。每个模型都是从示例输入/输出对（训练集）中学习的，使用输入/输出对进行验证，并最终在更多的输入/输出对上进行测试。每对由以下组成：
- en: Input vector
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入向量
- en: Output value (**label**)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出值（**标签**）
- en: In regression, our output value is quantitative, and in classification, our
    output value is categorical.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归中，我们的输出值是定量的，在分类中，我们的输出值是分类的。
- en: '![](../Images/391792c08b3ee8123e1f155b3e61bb85.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/391792c08b3ee8123e1f155b3e61bb85.png)'
- en: ML taxonomy
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ML分类法
- en: 26.1.2 Unsupervised Learning
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 26.1.2 无监督学习
- en: In unsupervised learning, our goal is to identify patterns in unlabeled data.
    In this type of learning, we do not have input/output pairs. Sometimes we may
    have labels but choose to ignore them (e.g. PCA on labeled data). Instead, we
    are more interested in the inherent structure of the data we have rather than
    trying to simply predict a label using that structure of data. For example, if
    we are interested in dimensionality reduction, we can use PCA to reduce our data
    to a lower dimension.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，我们的目标是识别无标签数据中的模式。在这种类型的学习中，我们没有输入/输出对。有时我们可能有标签，但选择忽略它们（例如在标记数据上进行PCA）。相反，我们更感兴趣的是我们所拥有的数据的固有结构，而不是仅仅使用该数据的结构来预测标签。例如，如果我们对降维感兴趣，我们可以使用PCA将我们的数据降低到较低的维度。
- en: 'Now let us consider a new problem: clustering.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一个新问题：聚类。
- en: 26.1.3 Clustering Examples
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 26.1.3 聚类示例
- en: 26.1.3.1 Example 1
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 26.1.3.1 示例1
- en: Consider this figure from Fall 2019 Midterm 2\. The original dataset was 8 dimensions,
    but we have used PCA to reduce our data down to 2 dimensions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下2019年秋季期中考试2的图。原始数据集有8个维度，但我们已经使用PCA将我们的数据减少到2个维度。
- en: '![blobs](../Images/133fb8bf5b322178ee7da509d449ff3c.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![blobs](../Images/133fb8bf5b322178ee7da509d449ff3c.png)'
- en: 'Each point represents the 1st and 2nd principal component of how much time
    patrons spent at 8 different zoo exhibits. Visually and intuitively, we could
    potentially guess that this data belongs to 3 groups: one for each cluster. The
    goal of clustering is now to assign each point (in the 2 dimensional PCA representation)
    to a cluster.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每个点表示游客在8个不同动物园展品上花费的时间的第一和第二主成分。从视觉和直觉上，我们可能会猜测这些数据属于3个组：每个集群一个。现在聚类的目标是将每个点（在2维PCA表示中）分配到一个集群中。
- en: '![clusters_ex1](../Images/df2edb2c090f355cbf5844d5699ed6cd.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![clusters_ex1](../Images/df2edb2c090f355cbf5844d5699ed6cd.png)'
- en: 'This is an unsupervised task, as:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个无监督的任务，因为：
- en: We don’t have labels for each visitor.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有每个访客的标签。
- en: Want to infer patterns even without labels.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 希望推断出模式，即使没有标签。
- en: '26.1.3.2 Example 2: Netflix'
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 26.1.3.2 示例2：Netflix
- en: 'Now suppose you’re Netflix and are looking at information on customer viewing
    habits. Clustering can come in handy here. We can assign each person or show to
    a “cluster”. (Note: while we don’t know for sure that Netflix actually uses ML
    clustering to identify these categories, they could in principle.)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设你是Netflix，正在查看有关客户观看习惯的信息。聚类在这里很有用。我们可以将每个人或节目分配到一个“集群”。（注意：虽然我们不能确定Netflix是否实际使用ML聚类来识别这些类别，但原则上他们可以这样做。）
- en: Keep in mind that with clustering, we don’t need to define clusters in advance.
    This marks one of the key differences between clustering and classification, whereas
    with classification, we have to decide on labels in advance whereas clustering
    discovers groups automatically.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，对于聚类，我们不需要提前定义聚类。这标志着聚类和分类之间的一个关键区别，而在分类中，我们必须提前决定标签，而聚类会自动发现组。
- en: '26.1.3.3 Example 3: Education'
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 26.1.3.3 示例3：教育
- en: 'Let’s say we’re working with student-generated materials and pass them into
    the S-BERT module to extract sentence embeddings. Features from clusters are extracted
    to:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在处理学生生成的材料，并将其传递到S-BERT模块以提取句子嵌入。从集群中提取特征：
- en: Detect anomalies in group activities
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测组活动中的异常
- en: Predict the group’s median quiz grade
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测组的中位数测验成绩
- en: '![outline-ex3](../Images/ecef84878fa938531a5c13bb6f36a8ab.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![outline-ex3](../Images/ecef84878fa938531a5c13bb6f36a8ab.png)'
- en: 'Here we can see the outline of the anomaly detection module. It consists of:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到异常检测模块的大纲。它包括：
- en: S-BERT feature extraction
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S-BERT特征提取
- en: Topic extraction
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题提取
- en: Feature extraction
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取
- en: 16D \(\rightarrow\) 2D PCA dimensionality reduction and 2D \(\rightarrow\) 16D
    reconstruction
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 16D \(\rightarrow\) 2D PCA降维和2D \(\rightarrow\) 16D重构
- en: Anomaly detection based on reconstruction error
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于重构误差的异常检测
- en: Looking more closely at our clustering, we can better understand the different
    components as represented by centers. Below we have two examples.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 更仔细地观察我们的聚类，我们可以更好地理解中心所代表的不同组件。下面我们有两个例子。
- en: '![components](../Images/1f3721159f657f1a6cab42998e082f6d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![components](../Images/1f3721159f657f1a6cab42998e082f6d.png)'
- en: Note that the details for this example are not in scope.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此示例的详细信息不在范围内。
- en: '26.1.3.4 Example 4: Reverse Engineering Biology'
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 26.1.3.4 例4：逆向工程生物学
- en: 'Now, consider the plot below:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑下面的图：
- en: '![genes](../Images/a1671628abc7a9075416ac2772e7ad2f.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![genes](../Images/a1671628abc7a9075416ac2772e7ad2f.png)'
- en: 'The rows of this plot are conditions (e.g. a row might be: “poured acid on
    the cells”) and the columns are genes. The green coloration indicates that the
    gene was “off” (red indicates the gene was “on”). For example, the ~9 genes in
    the top left corner of the plot were all turned off by the 6 experiments (rows)
    at the top.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图的行是条件（例如一个行可能是：“在细胞上倒酸”），列是基因。绿色表示基因“关闭”（红色表示基因“开启”）。例如，图的左上角的~9个基因都被顶部的6个实验（行）关闭。
- en: In a clustering lens, we might be interested in clustering similar observations
    together based on the reactions (on/off) to certain experiments.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类的视角下，我们可能对基于对某些实验的反应（开/关）相似的观察结果进行聚类感兴趣。
- en: For example, here is a look at our data before and after clustering.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这是我们在聚类之前和之后的数据。
- en: '![beforeandafter4](../Images/d4d52f2a25d0b561df202348aa553f8d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![beforeandafter4](../Images/d4d52f2a25d0b561df202348aa553f8d.png)'
- en: 'Note: apologies if you can’t differentiate red from green by eye! Historical
    visualizations are not always the best.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你无法通过眼睛区分红色和绿色，我很抱歉！历史可视化并不总是最好的。
- en: 26.2 Taxonomy of Clustering Approaches
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.2 聚类方法的分类
- en: '![taxonomy](../Images/5079f981ec119e0d2d853e3652aab8e2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![taxonomy](../Images/5079f981ec119e0d2d853e3652aab8e2.png)'
- en: 'There are many types of clustering algorithms, and they all have strengths,
    inherent weaknesses, and different use cases. We will first focus on a partitional
    approach: K-Means clustering.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多类型的聚类算法，它们都有优势、固有的弱点和不同的用例。我们首先将专注于分区方法：K-Means聚类。
- en: 26.3 K-Means Clustering
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.3 K-Means聚类
- en: 'The most popular clustering approach is K-Means. The algorithm itself entails
    the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的聚类方法是K-Means。算法本身包括以下内容：
- en: Pick an arbitrary \(K\), and randomly place \(K\) “centers”, each a different
    color.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个任意的\(K\)，并随机放置\(K\)个不同颜色的“中心”。
- en: 'Repeat until convergence:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复直到收敛：
- en: Color points according to the closest center.
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据最接近的中心对点进行着色。
- en: Move the center for each color to the center of points with that color.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每种颜色的中心移动到具有该颜色的点的中心。
- en: 'Consider the following data with an arbitrary \(K = 2\) and randomly placed
    “centers” denoted by the different colors (blue, orange):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下具有任意\(K = 2\)和随机放置的不同颜色（蓝色、橙色）“中心”的数据：
- en: '![init_cluster](../Images/b7c30842e244dc5e1d8e91a040ef3485.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![init_cluster](../Images/b7c30842e244dc5e1d8e91a040ef3485.png)'
- en: 'Now, we will follow the rest of the algorithm. First, let us color each point
    according to the closest center:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将遵循算法的其余部分。首先，让我们根据最接近的中心对每个点进行着色：
- en: '![cluster_class](../Images/34a3a269cf265a6ad18940e2a585ee66.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![cluster_class](../Images/34a3a269cf265a6ad18940e2a585ee66.png)'
- en: Next, we will move the center for each color to the center of points with that
    color. Notice how the centers are generally well-centered amongst the data that
    shares its color.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把每种颜色的中心移动到具有该颜色的点的中心。请注意，中心通常在其颜色共享的数据中心位置。
- en: '![cluster_iter1](../Images/9ddbd899a5c47b0334efc498168414dd.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![cluster_iter1](../Images/9ddbd899a5c47b0334efc498168414dd.png)'
- en: Assume this process (re-color, re-set centers) repeats for a few more iterations,
    and we’ve ended up at this state.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这个过程（重新着色，重新设置中心）重复了几次迭代，我们最终达到了这个状态。
- en: '![cluster_iter5](../Images/4935b296ab804dcf8e83beceb4429ed6.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![cluster_iter5](../Images/4935b296ab804dcf8e83beceb4429ed6.png)'
- en: After this iteration, the center stays still and does not move at all. Thus,
    we have converged, and the clustering is complete!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次迭代之后，中心保持不动，根本不移动。因此，我们已经收敛，聚类完成了！
- en: 26.3.0.1 Note
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 26.3.0.1 注意
- en: 'A quick note: K-Means is a totally different algorithm than K-Nearest Neighbors.
    K-means is used for *clustering*, where each point is assigned to one of \(K\)
    clusters. On the other hand, K-Nearest Neighbors is used for *classification*
    (or less often, regression), and the predicted value is typically the most common
    class among the \(K\)-nearest data points in the training set. The names may be
    similar, but there isn’t really anything in common.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速的说明：K-Means是一种完全不同的算法，与K-最近邻算法完全不同。K-Means用于*聚类*，其中每个点被分配到\(K\)个簇中的一个。另一方面，K-最近邻算法用于*分类*（或更少见的回归），预测值通常是训练集中\(K\)个最近数据点中最常见的类。这些名称可能相似，但实际上没有任何共同之处。
- en: 26.4 Minimizing Inertia
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.4 最小化惯性
- en: 'Consider the following example where \(K = 4\):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下\(K = 4\)的例子：
- en: '![four_cluster](../Images/0857b53972165c73d48824d715d38e5f.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![four_cluster](../Images/0857b53972165c73d48824d715d38e5f.png)'
- en: Due to the randomness of where the \(K\) centers initialize/start, you will
    get a different output/clustering every time you run K-Means. Consider three possible
    K-Means outputs; the algorithm has converged, and the colors denote the final
    cluster they are clustered as.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于\(K\)中心初始化/开始的随机性，每次运行K-Means都会得到不同的输出/聚类。考虑三种可能的K-Means输出；算法已经收敛，颜色表示它们被聚类为的最终簇。
- en: '![random_outputs](../Images/876f001bd5554a490fe7bc980e5f2d13.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![random_outputs](../Images/876f001bd5554a490fe7bc980e5f2d13.png)'
- en: Which clustering output is the best? To evaluate different clustering results,
    we need a loss function.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 哪种聚类输出是最好的？要评估不同的聚类结果，我们需要一个损失函数。
- en: 'The two common loss functions are:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 两种常见的损失函数是：
- en: '**Inertia**: Sum of squared distances from each data point to its center.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**惯性**：每个数据点到其中心的平方距离的总和。'
- en: '**Distortion**: Weighted sum of squared distances from each data point to its
    center.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**畸变**：每个数据点到其中心的平方距离的加权和。'
- en: '![inertia_distortion](../Images/d97a8411a1be08d445fa08f97c3646ca.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![inertia_distortion](../Images/d97a8411a1be08d445fa08f97c3646ca.png)'
- en: 'In the example above:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中：
- en: 'Calculated inertia: \(0.47^2 + 0.19^2 + 0.34^2 + 0.25^2 + 0.58^2 + 0.36^2 +
    0.44^2\)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算的惯性：\(0.47^2 + 0.19^2 + 0.34^2 + 0.25^2 + 0.58^2 + 0.36^2 + 0.44^2\)
- en: 'Calculated distortion: \(\frac{0.47^2 + 0.19^2 + 0.34^2}{3} + \frac{0.25^2
    + 0.58^2 + 0.36^2 + 0.44^2}{4}\)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算失真：\(\frac{0.47^2 + 0.19^2 + 0.34^2}{3} + \frac{0.25^2 + 0.58^2 + 0.36^2 +
    0.44^2}{4}\)
- en: Switching back to the four-cluster example at the beginning of this section,
    `random.seed(25)` had an inertia of `44.96`, `random.seed(29)` had an inertia
    of `45.95`, and `random.seed(40)` had an inertia of `54.35`. It seems that the
    best clustering output was `random.seed(25)` with an inertia of `44.96`!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 回到本节开头的四簇示例，`random.seed(25)`的惯性为`44.96`，`random.seed(29)`的惯性为`45.95`，`random.seed(40)`的惯性为`54.35`。看来最佳的聚类输出是`random.seed(25)`，惯性为`44.96`！
- en: It turns out that the function K-Means is trying to minimize is inertia, but
    often fails to find global optimum. Why does this happen? We can think of K-means
    as a pair of optimizers that take turns. The first optimizer holds *center positions*
    constant and optimizes *data colors*. The second optimizers holds *data colors*
    constant and optimizes *center positions*. Neither optimizer gets full control!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，K-Means试图最小化的函数是惯性，但通常无法找到全局最优解。为什么会这样？我们可以将K-means看作是一对轮流进行优化的优化器。第一个优化器保持*中心位置*恒定，并优化*数据颜色*。第二个优化器保持*数据颜色*恒定，并优化*中心位置*。两个优化器都没有完全控制！
- en: 'This is a hard problem: give an algorithm that optimizes inertia FOR A GIVEN
    \(K\); \(K\) is picked in advance. Your algorithm should return the EXACT best
    centers and colors, but you don’t need to worry about runtime.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个困难的问题：给出一个优化给定\(K\)的惯性的算法；\(K\)是预先选择的。你的算法应该返回确切的最佳中心和颜色，但你不需要担心运行时间。
- en: '*Note: This is a bit of a CS61B/CS70/CS170 problem, so do not worry about completely
    understanding the tricky predicament we are in too much!*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：这是一个有点CS61B/CS70/CS170问题，所以不要太担心完全理解我们所处的棘手困境！*'
- en: 'A potential algorithm:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个潜在的算法：
- en: 'For all possible \(k^n\) colorings:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有可能的k^n种着色：
- en: Compute the \(k\) centers for that coloring.
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算该着色的k个中心。
- en: Compute the inertia for the \(k\) centers.
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算\(k\)个中心的惯性。
- en: If current inertia is better than best known, write down the current centers
    and coloring and call that the new best known.
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果当前的惯性比已知的最佳值更好，那么写下当前的中心和着色，并将其称为新的已知最佳值。
- en: No better algorithm has been found for solving the problem of minimizing inertia
    exactly.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 尚未找到更好的算法来解决最小化惯性的问题。
- en: 26.5 Hierarchical Agglomerative Clustering
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.5 分层聚类
- en: Now, let us consider hierarchical agglomerative clustering.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑分层聚类。
- en: '![hierarchical_approach](../Images/457fb1337b9f5b42604fa3626c5650a1.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![hierarchical_approach](../Images/457fb1337b9f5b42604fa3626c5650a1.png)'
- en: 'Consider the following results of two K-Means clustering outputs:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个K-Means聚类输出的结果：
- en: '![clustering_comparison](../Images/a27ed33eff2dfe97ef61c0d32ebe5b0f.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![clustering_comparison](../Images/a27ed33eff2dfe97ef61c0d32ebe5b0f.png)'
- en: 'Which clustering result do you like better? It seems K-Means likes the one
    on the right better because it has lower inertia (the sum of squared distances
    from each data point to its center), but this raises some questions:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你更喜欢哪个聚类结果？看起来K-Means更喜欢右边的结果，因为它的惯性更低（每个数据点到其中心的平方距离之和），但这引发了一些问题：
- en: Why is the inertia on the right lower? K-Means optimizes for distance, not “blobbiness”.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么右边的惯性更低？K-Means优化距离，而不是“blobbiness”。
- en: Is clustering on the right “wrong”? Good question!
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右边的聚类“错误”吗？好问题！
- en: Now, let us introduce Hierarchical Agglomerative Clustering! We start with every
    data point in a separate cluster, and we’ll keep merging the most similar pairs
    of data points/clusters until we have one big cluster left. This is called a **bottom-up**
    or **agglomerative method**.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们介绍分层聚类！我们从每个数据点在一个单独的簇开始，然后我们将继续合并最相似的数据点/簇，直到最后只剩下一个大簇。这被称为**自下而上**或**聚合方法**。
- en: 'There are various ways to decide the order of combining clusters called **Linkage
    Criterion**:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种方法来决定合并簇的顺序，称为**链接标准**：
- en: '**Single linkage** (similarity of the most similar): the distance between two
    clusters as the **minimum** distance between a point in the first cluster and
    a point in the second.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单链接**（最相似的相似性）：两个簇之间的距离是第一个簇中的一个点与第二个簇中的一个点之间的**最小**距离。'
- en: '**Complete linkage** (similarity of the least similar): the distance between
    two clusters as the **maximum** distance between a point in the first cluster
    and a point in the second.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全链接**（最不相似的相似性）：两个簇之间的距离是第一个簇中的一个点与第二个簇中的一个点之间的**最大**距离。'
- en: '**Average linkage**: **average** similarity of pairs of points in clusters.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均链接**：簇中两个点的**平均**相似性。'
- en: '![linkage](../Images/89a08b95f3fc4368d1c3172824c01ca9.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![linkage](../Images/89a08b95f3fc4368d1c3172824c01ca9.png)'
- en: When the algorithm starts, every data point is in its own cluster. In the plot
    below, there are 12 data points, so the algorithm starts with 12 clusters. As
    the clustering begins, it begins to assess which clusters are the closest together.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当算法开始时，每个数据点都在自己的簇中。在下面的图中，有12个数据点，所以算法从12个簇开始。随着聚类的开始，它开始评估哪些簇彼此最接近。
- en: '![agg1](../Images/524e159e69a72393e13b3a5e03bd444c.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![agg1](../Images/524e159e69a72393e13b3a5e03bd444c.png)'
- en: The closest clusters are 10 and 11, so they are merged together.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最接近的簇是10和11，所以它们被合并在一起。
- en: '![agg2](../Images/ec520ca9f40add06d73b27f6c027f66c.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![agg2](../Images/ec520ca9f40add06d73b27f6c027f66c.png)'
- en: Next, points 0 and 4 are merged together because they are closest.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，点0和4被合并在一起，因为它们最接近。
- en: '![agg3](../Images/9601019a66c695fe7ae4f204f9c30eca.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![agg3](../Images/9601019a66c695fe7ae4f204f9c30eca.png)'
- en: 'At this point, we have 10 clusters: 8 with a single point (clusters 1, 2, 3,
    4, 5, 6, 7, 8, and 9) and 2 with 2 points (clusters 0 and 10).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们有10个簇：8个单点（簇1、2、3、4、5、6、7、8和9）和2个双点（簇0和10）。
- en: 'Although clusters 0 and 3 are not the closest, let us consider if we were trying
    to merge them. A tricky question arises: what is the distance between clusters
    0 and 3? We can use the **Complete-Link** approach that uses the **max** distance
    among all pairs of points between groups.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管簇0和3不是最接近的，但让我们考虑如果我们试图合并它们。一个棘手的问题出现了：簇0和3之间的距离是多少？我们可以使用“完全链接”方法，它使用组之间所有点对中的最大距离。
- en: '![agg4](../Images/87c2dd738346b25cba02f108e934382c.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![agg4](../Images/87c2dd738346b25cba02f108e934382c.png)'
- en: Let us assume the algorithm runs a little longer, and we have reached the following
    state. Clusters 0 and 7 are up next, but why? The **max line between any member
    of 0 and 6** is longer than the **max line between any member of 0 and 7**.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设算法运行时间稍长，并且我们已经达到了以下状态。接下来是簇0和7，但为什么？0和6之间的最大线长于0和7之间的最大线长。
- en: '![agg5](../Images/7abc194445c8ddeb469e721086d5676c.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![agg5](../Images/7abc194445c8ddeb469e721086d5676c.png)'
- en: Thus, 0 and 7 are merged into 0.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，0和7被合并为0。
- en: After more iterations, we finally converge to the plot on the left. There are
    two clusters (0, 1), and the agglomerative algorithm has converged.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 经过更多迭代，我们最终收敛到左侧的图。有两个簇（0，1），凝聚算法已经收敛。
- en: '![agg6](../Images/c0773ca9f07e571febd27b71bebcbd61.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![agg6](../Images/c0773ca9f07e571febd27b71bebcbd61.png)'
- en: Notice that on the full dataset, our agglomerative clustering algorithm achieves
    the more “correct” output.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在完整数据集上，我们的凝聚聚类算法实现了更“正确”的输出。
- en: 26.5.1 Clustering, Dendrograms, and Intuition
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 26.5.1 聚类、树状图和直觉
- en: 'Agglomerative clustering is one form of “hierarchical clustering.” It is interpretable
    because we can keep track of when two clusters got merged (each cluster is a tree),
    and we can visualize merging hierarchy, resulting in a “dendrogram.” Won’t discuss
    this any further for this course, but you might see these in the wild. Here are
    some examples:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合聚类是“分层聚类”的一种形式。它是可以解释的，因为我们可以跟踪两个簇何时被合并（每个簇都是一棵树），并且我们可以可视化合并层次，从而得到“树状图”。我们不会在本课程中进一步讨论这一点，但你可能会在实际应用中看到这些。以下是一些例子：
- en: '![dendro_1](../Images/5dede85d61294e0e804e70705087d3d5.png) ![dendro_2](../Images/761004accdc8935ef02b311b64a07294.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![dendro_1](../Images/5dede85d61294e0e804e70705087d3d5.png) ![dendro_2](../Images/761004accdc8935ef02b311b64a07294.png)'
- en: Some professors use agglomerative clustering for grading bins; if there is a
    big gap between two people, draw a grading threshold there. The idea is that grade
    clustering should be more like the figure below on the left, not the right.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一些教授使用凝聚聚类来进行分级分箱；如果两个人之间有很大的差距，就在那里画一个分级阈值。其想法是，等级聚类应该更像下图左侧的情况，而不是右侧的情况。
- en: '![grading](../Images/639fd3237484053c12024c5437baac5a.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![grading](../Images/639fd3237484053c12024c5437baac5a.png)'
- en: 26.6 Picking K
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.6 选择K
- en: The algorithms we’ve discussed require us to pick a \(K\) before we start. But
    how do we pick \(K\)? Often, the best \(K\) is subjective. For example, consider
    the state plot below.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的算法要求我们在开始之前选择K。但是我们如何选择K？通常，最佳的K是主观的。例如，考虑下面的状态图。
- en: '![states](../Images/dbf2b9a25dae5759aa67f3c97c6c5203.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![states](../Images/dbf2b9a25dae5759aa67f3c97c6c5203.png)'
- en: How many clusters are there here? For K-Means, one approach to determine this
    is to plot inertia versus many different \(K\) values. We’d pick the \(K\) in
    the **elbow**, where we get diminishing returns afterward (note that big, complicated
    data often lacks an elbow, so this method is not foolproof). Here, we would likely
    select \(K = 2\).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有多少个簇？对于K-Means，确定这一点的一种方法是绘制惯性与许多不同的K值。我们会在“拐点”处选择K，在此之后我们会得到递减的回报（请注意，大型复杂数据通常缺乏拐点，因此这种方法并不是百分之百可靠的）。在这里，我们可能会选择K
    = 2。
- en: '![elbow](../Images/b53e24fa870a6aae31e2363140635548.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![elbow](../Images/b53e24fa870a6aae31e2363140635548.png)'
- en: 26.6.1 Silhouette Scores
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 26.6.1 轮廓分数
- en: To evaluate how “well-clustered” a specific data point is, we can use the **silhouette
    score**, a.k.a. the **silhouette width**. A high silhouette score indicates that
    a point is near the other points in its cluster; a low score means that it’s far
    from the other points in its cluster.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估特定数据点的“聚类效果”如何，我们可以使用“轮廓分数”，又称“轮廓宽度”。较高的轮廓分数表示该点接近其簇中的其他点；较低的分数意味着它远离其簇中的其他点。
- en: '![high_low](../Images/4c000257bc4e8bb691b654cfbb2c54a2.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![high_low](../Images/4c000257bc4e8bb691b654cfbb2c54a2.png)'
- en: 'For a data point \(X\), score \(S\) is: \[S =\frac{B - A}{max(A, B)}\] where
    \(A\) is the distance to *other* points in the cluster, and \(B\) is the average
    distance to points in the *closest* cluster.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据点X，得分S是：\[S =\frac{B - A}{max(A, B)}\]其中A是到簇中*其他*点的距离，B是到*最近*簇中点的平均距离。
- en: Consider what the highest possible value of \(S\) is and how that value can
    occur. The highest possible value of \(S\) is 1, which happens if every point
    in \(X\)’s cluster is right on top of \(X\); the average distance to other points
    in \(X\)’s cluster is \(0\), so \(A = 0\). Thus, \(S = \frac{B}{max(0, B)} = \frac{B}{B}
    = 1\). Another case where \(S = 1\) could happen is if \(B\) is *much* greater
    than \(A\) (we denote this as \(B >> A\)).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下S的最大可能值以及该值如何出现。S的最大可能值是1，如果X的簇中的每个点都恰好在X的正上方；到X的簇中其他点的平均距离是0，因此A = 0。因此，S
    = \frac{B}{max(0, B)} = \frac{B}{B} = 1。另一种情况是S = 1，如果B远大于A（我们将其表示为B >> A）。
- en: Can \(S\) be negative? The answer is yes. If the average distance to X’s clustermates
    is larger than the distance to the closest cluster, then this is possible. For
    example, the “low score” point on the right of the image above has \(S = -0.13\)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: S可以是负数吗？答案是可以。如果到X的簇内成员的平均距离大于到最近簇的距离，那么这是可能的。例如，上图右侧的“低分”点具有S = -0.13
- en: 26.6.2 Silhouette Plot
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 26.6.2 轮廓图
- en: We can plot the **silhouette scores** for all of our datapoints. Points with
    large silhouette widths are deeply embedded in their cluster; the red dotted line
    shows the average. Below, we plot the silhouette score for our plot with \(K=2\).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制所有数据点的轮廓分数。轮廓宽度较大的点深深嵌入其簇中；红色虚线显示了平均值。下面，我们绘制了K=2的轮廓分数。
- en: '![dendro_1](../Images/4c000257bc4e8bb691b654cfbb2c54a2.png) ![dendro_2](../Images/85d4c2bb2d4c4a792795f03809c9558d.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![dendro_1](../Images/4c000257bc4e8bb691b654cfbb2c54a2.png) ![dendro_2](../Images/85d4c2bb2d4c4a792795f03809c9558d.png)'
- en: 'Similarly, we can plot the silhouette score for the same dataset but with \(K=3\):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以为相同的数据集绘制 \(K=3\) 的轮廓分数：
- en: '![dendro_1](../Images/c21b541ba41ea8073fa8a12de43bb8c8.png) ![dendro_2](../Images/e58f414249bd74409d992bec11b08258.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![dendro_1](../Images/c21b541ba41ea8073fa8a12de43bb8c8.png) ![dendro_2](../Images/e58f414249bd74409d992bec11b08258.png)'
- en: The average silhouette score is lower with 3 clusters, so \(K=2\) is a better
    choice. This aligns with our visual intuition as well.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 3 个集群的平均轮廓分数较低，因此 \(K=2\) 是更好的选择。这也符合我们的直觉。
- en: '26.6.3 Picking K: Real World Metrics'
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 26.6.3 选择 K：真实世界的指标
- en: Sometimes you can rely on real-world metrics to guide your choice of \(K\).
    For t-shirts, we can either
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你可以依靠真实世界的指标来指导你选择 \(K\)。对于 T 恤，我们可以
- en: Cluster heights and weights of customers with \(K = 3\) to design Small, Medium,
    and Large shirts.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户的集群高度和体重，\(K = 3\) 设计小号、中号和大号衬衫。
- en: Cluster heights and weights of customers with \(K = 5\) to design XS, S, M,
    L, and XL shirts.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户的集群高度和体重，\(K = 5\) 设计 XS、S、M、L 和 XL 衬衫。
- en: To pick \(K\), consider projected costs and sales for the 2 different \(K\)s,
    and pick the one that maximizes profit.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要选择 \(K\)，考虑两种不同 \(K\) 的预期成本和销售，并选择最大化利润的那个。
- en: 26.7 Conclusion
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.7 结论
- en: 'We’ve now discussed a new machine learning goal – clustering – and explored
    two solutions:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在讨论了一个新的机器学习目标 - 聚类 - 并探讨了两种解决方案：
- en: K-Means tries to optimize a loss function called inertia (no known algorithm
    to find the optimal answer in an efficient manner)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-Means 尝试优化一个称为惯性的损失函数（没有已知的算法以有效的方式找到最佳答案）
- en: Hierarchical Agglomerative
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次凝聚
- en: 'Our version of these algorithms required a hyperparameter \(K\). There are
    4 ways to pick \(K\): intuitively, elbow method, silhouette scores, and harnessing
    real-world metrics.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们版本的这些算法需要一个超参数 \(K\)。有 4 种方法可以选择 \(K\)：直觉上、肘部法则、轮廓分数和利用真实世界的指标。
- en: There are many machine learning problems. Each can be addressed by many different
    solution techniques. Each has many metrics for evaluating success / loss. Many
    techniques can be used to solve different problem types. For example, linear models
    can be used for regression and classification.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多机器学习问题。每个问题都可以通过许多不同的解决方案技术来解决。每个问题都有许多用于评估成功/损失的指标。许多技术可以用来解决不同的问题类型。例如，线性模型可以用于回归和分类。
- en: We’ve only scratched the surface and haven’t discussed many important ideas,
    such as neural networks / deep learning. We’ll provide some specific course recommendations
    on how to further explore these topics in the last lecture.**
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是触及了表面，并没有讨论许多重要的想法，比如神经网络/深度学习。我们将在最后一堂课上提供一些关于如何进一步探索这些主题的具体课程建议。**
