- en: 22  Logistic Regression I
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 22  逻辑回归 I
- en: 原文：[https://ds100.org/course-notes/logistic_regression_1/logistic_reg_1.html](https://ds100.org/course-notes/logistic_regression_1/logistic_reg_1.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://ds100.org/course-notes/logistic_regression_1/logistic_reg_1.html](https://ds100.org/course-notes/logistic_regression_1/logistic_reg_1.html)
- en: '*Learning Outcomes* ***   Understand the difference between regression and
    classification'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习成果* ***   了解回归和分类之间的区别'
- en: Derive the logistic regression model for classifying data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推导用于分类数据的逻辑回归模型
- en: Quantify the error of our logistic regression model with cross-entropy loss**  **Up
    until this point in the class , we’ve focused on **regression** tasks - that is,
    predicting a *numerical* quantity from a given dataset. We discussed optimization,
    feature engineering, and regularization all in the context of performing regression
    to predict some quantity.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用交叉熵损失量化我们的逻辑回归模型的错误**  **到目前为止，在课堂上，我们专注于**回归**任务 - 也就是说，从给定数据集中预测一个*数值*数量。我们讨论了优化、特征工程和正则化，都是在执行回归以预测某个数量的情况下。
- en: Now that we have this deep understanding of the modeling process, let’s expand
    our knowledge of possible modeling tasks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对建模任务的可能性有了深入的了解，让我们扩展我们对可能建模任务的了解。
- en: 22.1 Classification
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.1 分类
- en: 'In the next two lectures, we’ll tackle the task of **classification**. A classification
    problem aims to classify data into *categories*. Unlike in regression, where we
    predicted a numeric output, classification involves predicting some **categorical
    variable**, or **response**, \(y\). Examples of classification tasks include:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两次讲座中，我们将解决**分类**的任务。分类问题旨在将数据分类为*类别*。与回归不同，在回归中我们预测了一个数值输出，分类涉及预测一些**分类变量**或**响应**\(y\)。分类任务的例子包括：
- en: Predicting which team won from its turnover percentage
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从失误百分比预测哪支球队赢得比赛
- en: Predicting the day of the week of a meal from the total restaurant bill
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从总餐厅账单预测一餐的星期几
- en: Predicting the model of car from its horsepower
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从其马力预测汽车的型号
- en: 'There are a couple of different types of classification:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同类型的分类：
- en: 'Binary classification: classify data into two classes, and responses \(y\)
    are either 0 or 1'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元分类：将数据分类为两类，响应\(y\)要么是0，要么是1
- en: 'Multiclass classification: classify data into multiple classes (e.g., image
    labeling, next word in a sentence, etc.)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多类分类：将数据分类为多个类别（例如，图像标记，句子中的下一个单词等）
- en: 'Structured prediction tasks: conduct mutliple related classification predictions
    (e.g., translation, voice recognition, etc.)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化预测任务：进行多个相关的分类预测（例如，翻译，语音识别等）
- en: In Data 100, we will mostly deal with **binary classification**, where we are
    attempting to classify data into one of two classes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在Data 100中，我们将主要处理**二元分类**，我们试图将数据分类为两类之一。
- en: 'To build a classification model, we need to modify our modeling workflow slightly.
    Recall that in regression we:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个分类模型，我们需要稍微修改我们的建模工作流程。回想一下，在回归中我们：
- en: Created a design matrix of numeric features
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了一个数值特征的设计矩阵
- en: Defined our model as a linear combination of these numeric features
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的模型定义为这些数值特征的线性组合
- en: Used the model to output numeric predictions
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型输出数值预测
- en: 'In classification, however, we no longer want to output numeric predictions;
    instead, we want to predict the class to which a datapoint belongs. This means
    that we need to update our workflow. To build a classification model, we will:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在分类中，我们不再希望输出数值预测；相反，我们希望预测数据点属于哪个类。这意味着我们需要更新我们的工作流程。为了构建一个分类模型，我们将：
- en: Create a design matrix of numeric features.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数值特征的设计矩阵。
- en: Define our model as a linear combination of these numeric features, transformed
    by a non-linear **sigmoid function**. This outputs a numeric quantity.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的模型定义为这些数值特征的线性组合，通过非线性**sigmoid函数**进行转换。这会输出一个数值数量。
- en: Apply a **decision rule** to interpret the outputted quantity and decide a classification.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用**决策规则**来解释输出的数量并决定分类。
- en: Output a predicted class.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出一个预测的类。
- en: 'There are two key differences: as we’ll soon see, we need to incorporate a
    non-linear transformation to capture non-linear relationships hidden in our data.
    We do so by applying the sigmoid function to a linear combination of the features.
    Secondly, we must apply a decision rule to convert the numeric quantities computed
    by our model into an actual class prediction. This can be as simple as saying
    that any datapoint with a feature greater than some number \(x\) belongs to Class
    1.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个关键的区别：正如我们很快会看到的，我们需要加入一个非线性转换来捕捉我们数据中隐藏的非线性关系。我们通过将sigmoid函数应用于特征的线性组合来实现这一点。其次，我们必须应用一个决策规则，将我们模型计算出的数值数量转换为实际的类别预测。这可以简单地说，任何具有大于某个数字\(x\)的特征的数据点都属于类1。
- en: '![reg](../Images/bb5ea8ccdb67ad2431f86ad01f052317.png)![class](../Images/521b6b90cbe7a363e303548b3524139c.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![reg](../Images/bb5ea8ccdb67ad2431f86ad01f052317.png)![class](../Images/521b6b90cbe7a363e303548b3524139c.png)'
- en: This was a very high-level overview. Let’s walk through the process in detail
    to clarify what we mean.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个非常高层次的概述。让我们详细地走一遍流程，澄清我们的意思。
- en: 22.2 Deriving the Logistic Regression Model
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.2 推导逻辑回归模型
- en: Throughout this lecture, we will work with the `games` dataset, which contains
    information about games played in the NBA basketball league. Our goal will be
    to use a basketball team’s `"GOAL_DIFF"` to predict whether or not a given team
    won their game (`"WON"`). If a team wins their game, we’ll say they belong to
    Class 1\. If they lose, they belong to Class 0.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本讲座中，我们将使用`games`数据集，其中包含NBA篮球联赛中比赛的信息。我们的目标是使用篮球队的“GOAL_DIFF”来预测给定球队是否赢得比赛（“WON”）。如果一支球队赢得比赛，我们将说他们属于类1。如果他们输了，他们属于类0。
- en: For those who are curious, `"GOAL_DIFF"` represents the difference in successful
    field goal percentages between the two competing teams.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些好奇的人，“GOAL_DIFF”代表两支竞争球队之间成功投篮百分比的差异。
- en: <details><summary>Code</summary>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE0]</details>'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE0]</details>'
- en: '|  | GAME_ID | TEAM_NAME | MATCHUP | WON | GOAL_DIFF | AST |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | GAME_ID | TEAM_NAME | MATCHUP | WON | GOAL_DIFF | AST |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 21701216 | Dallas Mavericks | DAL vs. PHX | 0 | -0.251 | 20 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 21701216 | 达拉斯小牛 | DAL vs. PHX | 0 | -0.251 | 20 |'
- en: '| 1 | 21700846 | Phoenix Suns | PHX @ GSW | 0 | -0.237 | 13 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 21700846 | 菲尼克斯太阳 | PHX @ GSW | 0 | -0.237 | 13 |'
- en: '| 2 | 21700071 | San Antonio Spurs | SAS @ ORL | 0 | -0.234 | 19 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 21700071 | 圣安东尼奥马刺 | SAS @ ORL | 0 | -0.234 | 19 |'
- en: '| 3 | 21700221 | New York Knicks | NYK @ TOR | 0 | -0.234 | 17 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 21700221 | 纽约尼克斯 | NYK @ TOR | 0 | -0.234 | 17 |'
- en: '| 4 | 21700306 | Miami Heat | MIA @ NYK | 0 | -0.222 | 21 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 21700306 | 迈阿密热火 | MIA @ NYK | 0 | -0.222 | 21 |'
- en: Let’s visualize the relationship between `"GOAL_DIFF"` and `"WON"` using the
    Seaborn function `sns.stripplot`. A strip plot automatically introduces a small
    amount of random noise to jitter the data. Recall that all values in the `"WON"`
    column are either 1 (won) or 0 (lost) – if we were to directly plot them without
    jittering, we would see severe overplotting.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Seaborn函数`sns.stripplot`来可视化`"GOAL_DIFF"`和`"WON"`之间的关系。条带图会自动引入一小部分随机噪声来使数据发生抖动。回想一下，`"WON"`列中的所有值都是1（赢）或0（输）-
    如果我们直接绘制它们而不进行抖动，我们将看到严重的重叠。
- en: <details><summary>Code</summary>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>Code</summary>
- en: '[PRE1]</details>'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1]</details>'
- en: '![](../Images/80587f2a31abef6ced7b0cb411a3e3cf.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80587f2a31abef6ced7b0cb411a3e3cf.png)'
- en: This dataset is unlike anything we’ve seen before – our target variable contains
    only two unique values! Remember that each y value is either 0 or 1; the plot
    above jitters the y data slightly for ease of reading.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集不像我们以前见过的任何东西-我们的目标变量只包含两个唯一的值！记住，每个y值要么是0，要么是1；上面的图稍微抖动了y数据以便阅读。
- en: The regression models we have worked with always assumed that we were attempting
    to predict a continuous target. If we apply a linear regression model to this
    dataset, something strange happens.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所使用的回归模型总是假设我们试图预测一个连续的目标。如果我们将线性回归模型应用于这个数据集，会发生一些奇怪的事情。
- en: <details><summary>Code</summary>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>Code</summary>
- en: '[PRE2]</details>'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE2]</details>'
- en: '![](../Images/538c168bd9ccecf683162cf5ef6fb912.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/538c168bd9ccecf683162cf5ef6fb912.png)'
- en: 'The linear regression fit follows the data as closely as it can. However, there
    are a few key flaws with this approach:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归拟合尽可能地与数据保持一致。然而，这种方法存在一些关键缺陷：
- en: The predicted output, \(\hat{y}\), can be outside the range of possible classes
    (there are predictions above 1 and below 0)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测输出\(\hat{y}\)可能超出可能类别的范围（有预测值大于1和小于0）
- en: This means that the output can’t always be interpreted (what does it mean to
    predict a class of -2.3?)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这意味着输出并不总是可以解释（预测-2.3的类别意味着什么？）
- en: Our usual linear regression framework won’t work here. Instead, we’ll need to
    get more creative.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常的线性回归框架在这里行不通。相反，我们需要更有创意。
- en: 'Back in [Data 8](https://inferentialthinking.com/chapters/08/1/Applying_a_Function_to_a_Column.html#example-prediction),
    you gradually built up to the concept of linear regression by using the **graph
    of averages**. Before you knew the mathematical underpinnings of the regression
    line, you took a more intuitive approach: you bucketed the \(x\) data into bins
    of common values, then computed the average \(y\) for all datapoints in the same
    bin. The result gave you the insight needed to derive the regression fit.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[Data 8](https://inferentialthinking.com/chapters/08/1/Applying_a_Function_to_a_Column.html#example-prediction)，你通过使用**平均值图**逐渐建立了线性回归的概念。在你了解回归线的数学基础之前，你采取了更直观的方法：将\(x\)数据分成常见值的箱子，然后计算相同箱子中所有数据点的平均\(y\)。结果为你提供了推导回归拟合所需的洞察力。
- en: Let’s take the same approach as we grapple with our new classification task.
    In the cell below, we 1) bucket the `"GOAL_DIFF"` data into bins of similar values
    and 2) compute the average `"WON"` value of all datapoints in a bin.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们采用与我们应对新的分类任务相同的方法。在下面的单元格中，我们1）将`"GOAL_DIFF"`数据分成相似值的箱子，2）计算箱子中所有数据点的平均`"WON"`值。
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/367d8aa8ec627db6729027674bdb4cb9.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/367d8aa8ec627db6729027674bdb4cb9.png)'
- en: 'Interesting: our result is certainly not like the straight line produced by
    finding the graph of averages for a linear relationship. We can make two observations:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是：我们的结果肯定不像通过找到线性关系的平均值图产生的直线。我们可以做两个观察：
- en: All predictions on our line are between 0 and 1
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们线上的所有预测都在0和1之间
- en: The predictions are **non-linear**, following a rough “S” shape
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测是**非线性**的，遵循一个粗略的“S”形状
- en: Let’s think more about what we’ve just done.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地思考一下我们刚刚做的事情。
- en: 'To find the average \(y\) value for each bin, we computed:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到每个箱子的平均\(y\)值，我们计算：
- en: \[\frac{1 \text{(\# Y = 1 in bin)} + 0 \text{(\# Y = 0 in bin)}}{\text{\# datapoints
    in bin}} = \frac{\text{\# Y = 1 in bin}}{\text{\# datapoints in bin}} = P(\text{Y
    = 1} | \text{bin})\]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \[\frac{1 \text{(\# Y = 1 in bin)} + 0 \text{(\# Y = 0 in bin)}}{\text{\# datapoints
    in bin}} = \frac{\text{\# Y = 1 in bin}}{\text{\# datapoints in bin}} = P(\text{Y
    = 1} | \text{bin})\]
- en: 'This is simply the probability of a datapoint in that bin belonging to Class
    1! This aligns with our observation from earlier: all of our predictions lie between
    0 and 1, just as we would expect for a probability.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是该箱中数据点属于类别1的概率！这与我们之前的观察一致：我们所有的预测都在0和1之间，正如我们对概率的预期一样。
- en: Our graph of averages was really modeling the probability, \(p\), that a datapoint
    belongs to Class 1, or essentially that \(\text{Y = 1}\) for a particular value
    of \(\text{x}\).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的平均值图实际上在建模概率\(p\)，即数据点属于类别1的概率，或者本质上是对于特定值的\(\text{x}\)，\(\text{Y = 1}\)的概率。
- en: \[ p = P(Y = 1 | \text{ x} )\]
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p = P(Y = 1 | \text{ x} )\]
- en: In logistic regression, we have a new modeling goal. We want to model the **probability
    that a particular datapoint belongs to Class 1**. To do so, we’ll need to create
    a model that can approximate the S-shaped curve we plotted above.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，我们有一个新的建模目标。我们想要建模**特定数据点属于类别1的概率**。为此，我们需要创建一个可以近似我们上面绘制的S形曲线的模型。
- en: 'Fortunately for us, we’re already well-versed with a technique to model non-linear
    relationships – applying non-linear transformations to linearize the relationship.
    Recall the steps we’ve applied previously:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说幸运的是，我们已经熟悉了一种模拟非线性关系的技术——对线性关系进行非线性转换。回想一下我们之前应用过的步骤：
- en: Transform the variables until we linearize their relationship
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对变量进行转换，直到线性化它们的关系
- en: Fit a linear model to the transformed variables
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对转换后的变量拟合线性模型
- en: “Undo” our transformations to identify the underlying relationship between the
    original variables
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “撤销”我们的转换，以确定原始变量之间的基本关系
- en: In past examples, we used the bulge diagram to help us decide what transformations
    may be useful. The S-shaped curve we saw above, however, looks nothing like any
    relationship we’ve seen in the past. We’ll need to think carefully about what
    transformations will linearize this curve.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的例子中，我们使用了凸起图来帮助我们决定哪些转换可能有用。然而，我们上面看到的S形曲线与我们过去看到的任何关系都不相似。我们需要仔细考虑哪些转换会线性化这条曲线。
- en: 'Let’s consider our eventual goal: determining if we should predict a Class
    of 0 or 1 for each datapoint. Rephrased, we want to decide if it seems more “likely”
    that the datapoint belongs to Class 0 or to Class 1\. One way of deciding this
    is to see which class has the higher predicted probability for a given datapoint.
    The **odds** is defined as the probability of a datapoint belonging to Class 1
    divided by the probability of it belonging to Class 0.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑我们最终的目标：确定我们是否应该预测每个数据点的类别是0还是1。换句话说，我们想要确定数据点更“可能”属于类别0还是类别1。决定这一点的一种方法是看哪个类别对于给定的数据点有更高的预测概率。**赔率**被定义为数据点属于类别1的概率除以它属于类别0的概率。
- en: \[\text{odds} = \frac{P(Y=1|x)}{P(Y=0|x)} = \frac{p}{1-p}\]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{odds} = \frac{P(Y=1|x)}{P(Y=0|x)} = \frac{p}{1-p}\]
- en: If we plot the odds for each input `"GOAL_DIFF"` (\(x\)), we see something that
    looks more promising.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们为每个输入“GOAL_DIFF”(\(x\))绘制赔率，我们会看到一些看起来更有希望的东西。
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/4d327a0385498407fe131e853207d209.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d327a0385498407fe131e853207d209.png)'
- en: It turns out that the relationship between our input `"GOAL_DIFF"` and the odds
    is roughly exponential! Let’s linearize the exponential by taking the logarithm.
    As a reminder, you should assume that any logarithm in Data 100 is the base \(e\)
    natural logarithm unless told otherwise.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，我们的输入“GOAL_DIFF”和赔率之间的关系大致是指数的！让我们通过取对数来线性化指数。作为提醒，你应该假设Data 100中的任何对数都是以\(e\)为底的自然对数，除非另有说明。
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/c233effaecd2ed104ac309dca516e2b0.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c233effaecd2ed104ac309dca516e2b0.png)'
- en: 'We see something promising – the relationship between the log-odds and our
    input feature is approximately linear. This means that we can use a linear model
    to describe the relationship between the log-odds and \(x\). In other words:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了一些有希望的东西——对数几率和我们的输入特征之间的关系大致是线性的。这意味着我们可以使用线性模型来描述对数几率和\(x\)之间的关系。换句话说：
- en: \[\begin{align} \log{(\frac{p}{1-p})} &= \theta_0 + \theta_1 x_i\\ &= x^{\top}
    \theta \end{align}\]
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align} \log{(\frac{p}{1-p})} &= \theta_0 + \theta_1 x_i\\ &= x^{\top}
    \theta \end{align}\]
- en: Here, we use \(x^{\top}\) to represent an observation in our dataset, stored
    as a row vector. You can imagine it as a single row in our design matrix. \(x^{\top}
    \theta\) indicates a linear combination of the features for this observation (just
    as we used in multiple linear regression).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用\(x^{\top}\)来表示数据集中的一个观察值，存储为行向量。你可以想象它是我们设计矩阵中的一行。\(x^{\top} \theta\)表示这个观察的特征的线性组合（就像我们在多元线性回归中使用的那样）。
- en: 'We’re in good shape! We have now derived the following relationship:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做得很好！我们现在得出了以下关系：
- en: \[\log{(\frac{p}{1-p})} = x^{\top} \theta\]
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: \[\log{(\frac{p}{1-p})} = x^{\top} \theta\]
- en: Remember that our goal is to predict the probability of a datapoint belonging
    to Class 1, \(p\). Let’s rearrange this relationship to uncover the original relationship
    between \(p\) and our input data, \(x^{\top}\).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们的目标是预测数据点属于类别1的概率\(p\)。让我们重新排列这个关系，以揭示\(p\)和我们的输入数据\(x^{\top}\)之间的原始关系。
- en: \[\begin{align} \log{(\frac{p}{1-p})} &= x^T \theta\\ \frac{p}{1-p} &= e^{x^T
    \theta}\\ p &= (1-p)e^{x^T \theta}\\ p &= e^{x^T \theta}- p e^{x^T \theta}\\ p(1
    + e^{x^T \theta}) &= e^{x^T \theta} \\ p &= \frac{e^{x^T \theta}}{1+e^{x^T \theta}}\\
    p &= \frac{1}{1+e^{-x^T \theta}}\\ \end{align}\]
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align} \log{(\frac{p}{1-p})} &= x^T \theta\\ \frac{p}{1-p} &= e^{x^T
    \theta}\\ p &= (1-p)e^{x^T \theta}\\ p &= e^{x^T \theta}- p e^{x^T \theta}\\ p(1
    + e^{x^T \theta}) &= e^{x^T \theta} \\ p &= \frac{e^{x^T \theta}}{1+e^{x^T \theta}}\\
    p &= \frac{1}{1+e^{-x^T \theta}}\\ \end{align}\]
- en: Phew, that was a lot of algebra. What we’ve uncovered is the **logistic regression
    model** to predict the probability of a datapoint \(x^{\top}\) belonging to Class
    1\. If we plot this relationship for our data, we see the S-shaped curve from
    earlier!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀，这是很多代数。我们发现的是用于预测数据点\(x^{\top}\)属于类别1的**逻辑回归模型**。如果我们为我们的数据绘制这种关系，我们会看到之前的S形曲线！
- en: <details><summary>Code</summary>
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE6]</details>'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE6]</details>'
- en: '![](../Images/9d18f4e97e8149249a9cb3baaca5e4ec.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d18f4e97e8149249a9cb3baaca5e4ec.png)'
- en: 'To predict a probability using the logistic regression model, we:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用逻辑回归模型来预测概率时，我们：
- en: Compute a linear combination of the features, \(x^{\top}\theta\)
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算特征的线性组合\(x^{\top}\theta\)
- en: Apply the sigmoid activation function, \(\sigma(x^{\top} \theta)\).
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用Sigmoid激活函数，\(\sigma(x^{\top} \theta)\)。
- en: Our predicted probabilities are of the form \(P(Y=1|x) = p = \frac{1}{1+e^{-(\theta_0
    + \theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_p x_p)}}\)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测概率的形式为\(P(Y=1|x) = p = \frac{1}{1+e^{-(\theta_0 + \theta_1 x_1 + \theta_2
    x_2 + \ldots + \theta_p x_p)}}\)
- en: 'An important note: despite its name, logistic regression is used for *classification*
    tasks, not regression tasks. In Data 100, we always apply logistic regression
    with the goal of classifying data.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明：尽管名字是这样，逻辑回归用于*分类*任务，而不是回归任务。在Data 100中，我们总是应用逻辑回归来分类数据。
- en: The S-shaped curve is formally known as the **sigmoid function** and is typically
    denoted by \(\sigma\).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: S形曲线在正式上被称为**Sigmoid函数**，通常用\(\sigma\)表示。
- en: \[\sigma(t) = \frac{1}{1+e^{-t}}\]
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: \[\sigma(t) = \frac{1}{1+e^{-t}}\]
- en: '*Properties of the Sigmoid* **   Reflection/Symmetry: \(1-\sigma(t) = \frac{e^{-t}}{1+e^{-t}}=\sigma(-t)\)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*Sigmoid的属性* **   反射/对称：\(1-\sigma(t) = \frac{e^{-t}}{1+e^{-t}}=\sigma(-t)\)'
- en: 'Inverse: \(t=\sigma^{-1}(p)=\log{(\frac{p}{1-p})}\)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反函数：\(t=\sigma^{-1}(p)=\log{(\frac{p}{1-p})}\)
- en: 'Derivative: \(\frac{d}{dz} \sigma(t) = \sigma(t) (1-\sigma(t))=\sigma(t)\sigma(-t)\)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导数：\(\frac{d}{dz} \sigma(t) = \sigma(t) (1-\sigma(t))=\sigma(t)\sigma(-t)\)
- en: 'Domain: \(-\infty < t < \infty\)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 域：\(-\infty < t < \infty\)
- en: 'Range: \(0 < \sigma(t) < 1\)*  *In the context of our modeling process, the
    sigmoid is considered an **activation function**. It takes in a linear combination
    of the features and applies a non-linear transformation.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 范围：\(0 < \sigma(t) < 1\)*  *在我们建模过程的背景下，Sigmoid被认为是一个**激活函数**。它接收特征的线性组合并应用非线性变换。
- en: Let’s summarize our logistic regression modeling workflow.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下我们的逻辑回归建模工作流程。
- en: '![log_reg](../Images/fcde3443466d23c5bd77063f6d2fced3.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![log_reg](../Images/fcde3443466d23c5bd77063f6d2fced3.png)'
- en: 'Our main takeaways from this section are:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从本节中得出的主要结论是：
- en: Fit the “S” curve as best as possible
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽量最好地拟合“S”曲线
- en: 'The curve models the probability: \(P = (Y=1 | x)\)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线模拟了概率：\(P = (Y=1 | x)\)
- en: Assume log-odds is a linear combination of \(x\) and \(\theta\)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设对数几率是\(x\)和\(\theta\)的线性组合
- en: 'Putting this together, we know that the estimated probability that response
    is 1 given the features \(x\) is equal to the logistic function \(\sigma()\) at
    the value \(x^{\top}\theta\):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 综上所述，我们知道给定特征\(x\)的响应的估计概率等于在值\(x^{\top}\theta\)处的逻辑函数\(\sigma()\)：
- en: \[\begin{align} \hat{P}_{\theta}(Y = 1 | x) = \frac{1}{1 + e^{-x^{\top}\theta}}
    \end{align}\]
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align} \hat{P}_{\theta}(Y = 1 | x) = \frac{1}{1 + e^{-x^{\top}\theta}}
    \end{align}\]
- en: 'More commonly, the logistic regression model is written as:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 更常见的是，逻辑回归模型被写成：
- en: \[\begin{align} \hat{P}_{\theta}(Y = 1 | x) = \sigma(x^{\top}\theta) \end{align}\]
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align} \hat{P}_{\theta}(Y = 1 | x) = \sigma(x^{\top}\theta) \end{align}\]
- en: '*Properties of the Logistic Model* *Consider a logistic regression model with
    one feature and an intercept term:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*逻辑模型的属性* *考虑一个具有一个特征和一个截距项的逻辑回归模型：'
- en: \[\begin{align} p = P(Y = 1 | x) = \frac{1}{1+e^{-(\theta_0 + \theta_1 x)}}
    \end{align}\]
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align} p = P(Y = 1 | x) = \frac{1}{1+e^{-(\theta_0 + \theta_1 x)}}
    \end{align}\]
- en: 'Properties:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 属性：
- en: \(\theta_0\) controls the position of the curve along the horizontal axis
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\theta_0\) 控制曲线沿水平轴的位置
- en: The magnitude of \(\theta_1\) controls the “steepness” of the sigmoid
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\theta_1\)的大小控制了Sigmoid的“陡峭度”
- en: The sign of \(\theta_1\) controls the orientation of the curve* **Example Calculation*
    **Suppose we want to predict the probability that a team wins a game, given `"GOAL_DIFF"`
    (first feature) and the number of free throws (second feature). Let’s say we fit
    a logistic regression model (with no intercept) using the training data and estimate
    the optimal parameters. Now we want to predict the probability that a new team
    will win their game.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\theta_1\)的符号控制曲线的方向* **示例计算* **假设我们想要预测一个球队赢得比赛的概率，给定“GOAL_DIFF”（第一个特征）和罚球次数（第二个特征）。假设我们使用训练数据拟合了一个逻辑回归模型（没有截距），并估计了最优参数。现在我们想要预测一个新球队赢得比赛的概率。
- en: \[\begin{align} \hat{\theta}^{\top} = \begin{matrix}[0.1 & -0.5]\end{matrix}
    \\x^{\top} = \begin{matrix}[15 & 1]\end{matrix} \end{align}\]
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align} \hat{\theta}^{\top} = \begin{matrix}[0.1 & -0.5]\end{matrix}
    \\x^{\top} = \begin{matrix}[15 & 1]\end{matrix} \end{align}\]
- en: \[\begin{align} \hat{P}_{\hat{\theta}}(Y = 1 | x) = \sigma(x^{\top}\hat{\theta})
    = \sigma(0.1 \cdot 15 + (-0.5) \cdot 1) = \sigma(1) = \frac{1}{1+e^{-1}} \approx
    0.7311 \end{align}\]
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align} \hat{P}_{\hat{\theta}}(Y = 1 | x) = \sigma(x^{\top}\hat{\theta})
    = \sigma(0.1 \cdot 15 + (-0.5) \cdot 1) = \sigma(1) = \frac{1}{1+e^{-1}} \approx
    0.7311 \end{align}\]
- en: We see that the response is more likely to be 1 than 0, indicating that a reasonable
    prediction is \(\hat{y} = 1\). We’ll dive deeper into this in the next lecture.****  ****##
    22.3 Cross-Entropy Loss
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到响应更可能是1而不是0，这表明一个合理的预测是\(\hat{y} = 1\)。我们将在下一讲中更深入地讨论这个问题。****  ****## 22.3
    交叉熵损失
- en: To quantify the error of our logistic regression model, we’ll need to define
    a loss function.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化我们的逻辑回归模型的误差，我们需要定义一个损失函数。
- en: 22.3.1 Why Not MSE?
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 22.3.1 为什么不用MSE？
- en: 'You may wonder: why not use our familiar mean squared error? It turns out that
    the MSE is not well suited for logistic regression. To see why, let’s consider
    a simple, artificially generated `toy` dataset (this will be easier to work with
    than the more complicated `games` data).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：为什么不使用我们熟悉的均方误差？事实证明，均方误差不适用于逻辑回归。为了理解原因，让我们考虑一个简单的、人工生成的“玩具”数据集（这比更复杂的“游戏”数据更容易处理）。
- en: <details><summary>Code</summary>
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE7]</details>'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE7]</details>'
- en: '|  | x | y |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | x | y |'
- en: '| --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | -4.0 | 0 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 0 | -4.0 | 0 |'
- en: '| 1 | -2.0 | 0 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 1 | -2.0 | 0 |'
- en: '| 2 | -0.5 | 1 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 2 | -0.5 | 1 |'
- en: '| 3 | 1.0 | 0 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.0 | 0 |'
- en: '| 4 | 3.0 | 1 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3.0 | 1 |'
- en: 'We’ll construct a basic logistic regression model with only one feature and
    no intercept term. Our predicted probabilities take the form:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个只有一个特征和没有截距项的基本逻辑回归模型。我们的预测概率的形式为：
- en: \[p=P(Y=1|x)=\frac{1}{1+e^{-\theta_1 x}}\]
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: \[p=P(Y=1|x)=\frac{1}{1+e^{-\theta_1 x}}\]
- en: In the cell below, we plot the MSE for our model on the data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的单元格中，我们绘制了模型在数据上的MSE。
- en: <details><summary>Code</summary>
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE8]</details>'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE8]</details>'
- en: '![](../Images/80484a7d766ef2d1491be9452ee758c0.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80484a7d766ef2d1491be9452ee758c0.png)'
- en: 'This looks nothing like the parabola we found when plotting the MSE of a linear
    regression model! In particular, we can identify two flaws with using the MSE
    for logistic regression:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们绘制线性回归模型的MSE时，这看起来一点也不像我们找到的抛物线！特别是，我们可以确定使用MSE进行逻辑回归有两个缺陷：
- en: The MSE loss surface is *non-convex*. There is both a global minimum and a (barely
    perceptible) local minimum in the loss surface above. This means that there is
    the risk of gradient descent converging on the local minimum of the loss surface,
    missing the true optimum parameter \(\theta_1\).
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MSE损失曲面是*非凸的*。在上面的损失曲面中既有全局最小值，也有一个（几乎不可察觉的）局部最小值。这意味着梯度下降有可能收敛到损失曲面的局部最小值，错过真正的最优参数\(\theta_1\)。
- en: 'Squared loss is *bounded* for a classification task. Recall that each true
    \(y\) has a value of either 0 or 1\. This means that even if our model makes the
    worst possible prediction (e.g. predicting \(p=0\) for \(y=1\)), the squared loss
    for an observation will be no greater than 1: \[(y-p)^2=(1-0)^2=1\] The MSE does
    not strongly penalize poor predictions.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于分类任务，平方损失是*有界*的。回想一下，每个真实的 \(y\) 的值要么是0，要么是1。这意味着即使我们的模型做出了最糟糕的预测（例如对于 \(y=1\)
    预测 \(p=0\)），一个观察的平方损失也不会大于1：\[(y-p)^2=(1-0)^2=1\] 均方误差并不会严厉地惩罚糟糕的预测。
- en: 22.3.2 Motivating Cross-Entropy Loss
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 22.3.2 激励交叉熵损失
- en: Suffice to say, we don’t want to use the MSE when working with logistic regression.
    Instead, we’ll consider what kind of behavior we would *like* to see in a loss
    function.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，我们不想在逻辑回归中使用均方误差。相反，我们将考虑在损失函数中看到的*希望*看到的行为。
- en: Let \(y\) be the binary label \({0, 1}\), and \(p\) be the model’s predicted
    probability of the label being 1.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让 \(y\) 为二进制标签 \({0, 1}\)，\(p\) 是模型预测标签为1的概率。
- en: When the true \(y\) is 1, we should incur *low* loss when the model predicts
    large \(p\)
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当真实的 \(y\) 为1时，当模型预测大的 \(p\) 时，我们应该承担*低*的损失
- en: When the true \(y\) is 0, we should incur *high* loss when the model predicts
    large \(p\)
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当真实的 \(y\) 为0时，当模型预测大的 \(p\) 时，我们应该承担*高*的损失
- en: In other words, our loss function should behave differently depending on the
    value of the true class, \(y\).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们的损失函数应该根据真实类别 \(y\) 的值而有所不同。
- en: The **cross-entropy loss** incorporates this changing behavior. We will use
    it throughout our work on logistic regression. Below, we write out the cross-entropy
    loss for a *single* datapoint (no averages just yet).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉熵损失**包含了这种变化行为。我们将在逻辑回归的工作中使用它。下面，我们写出了*单个*数据点的交叉熵损失（暂时还没有平均值）。'
- en: \[\text{Cross-Entropy Loss} = \begin{cases} -\log{(p)} & \text{if } y=1 \\ -\log{(1-p)}
    & \text{if } y=0 \end{cases}\]
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{交叉熵损失} = \begin{cases} -\log{(p)} & \text{if } y=1 \\ -\log{(1-p)} &
    \text{if } y=0 \end{cases}\]
- en: Why does this (seemingly convoluted) loss function “work”? Let’s break it down.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这种（看似复杂的）损失函数“有效”？让我们来分解一下。
- en: When \(y=1\)![cross-entropy loss when Y=1](../Images/b7cbbe22bdc1d8a1088407c4517d2fa0.png)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(y=1\)![cross-entropy loss when Y=1](../Images/b7cbbe22bdc1d8a1088407c4517d2fa0.png)
- en: As \(p \rightarrow 0\), loss approches \(\infty\)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 \(p \rightarrow 0\) 时，损失趋近于 \(\infty\)
- en: As \(p \rightarrow 1\), loss approaches 0
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 \(p \rightarrow 1\) 时，损失趋近于0
- en: When \(y=0\)![cross-entropy loss when Y=0](../Images/7b1d9e44eae768997e06feced3d654a1.png)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(y=0\)![cross-entropy loss when Y=0](../Images/7b1d9e44eae768997e06feced3d654a1.png)
- en: As \(p \rightarrow 0\), loss approches 0
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 \(p \rightarrow 0\) 时，损失趋近于0
- en: As \(p \rightarrow 1\), loss approaches \(\infty\)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 \(p \rightarrow 1\) 时，损失趋近于 \(\infty\)
- en: All good – we are seeing the behavior we want for our logistic regression model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一切顺利 - 我们看到了我们希望在逻辑回归模型中看到的行为。
- en: 'The piecewise function we outlined above is difficult to optimize: we don’t
    want to constantly “check” which form of the loss function we should be using
    at each step of choosing the optimal model parameters. We can re-express cross-entropy
    loss in a more convenient way:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上面概述的分段函数很难优化：我们不想在选择最佳模型参数的每一步不断“检查”应该使用损失函数的哪种形式。我们可以以更方便的方式重新表达交叉熵损失：
- en: \[\text{Cross-Entropy Loss} = -\left(y\log{(p)}-(1-y)\log{(1-p)}\right)\]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{交叉熵损失} = -\left(y\log{(p)}-(1-y)\log{(1-p)}\right)\]
- en: By setting \(y\) to 0 or 1, we see that this new form of cross-entropy loss
    gives us the same behavior as the original formulation.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 \(y\) 设置为0或1，我们看到这种新形式的交叉熵损失给了我们与原始公式相同的行为。
- en: 'When \(y=1\):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(y=1\)：
- en: \[\begin{align} \text{CE} &= -\left((1)\log{(p)}-(1-1)\log{(1-p)}\right)\\ &=
    -\log{(p)} \end{align}\]
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align} \text{CE} &= -\left((1)\log{(p)}-(1-1)\log{(1-p)}\right)\\ &=
    -\log{(p)} \end{align}\]
- en: 'When \(y=0\):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(y=0\)：
- en: \[\begin{align} \text{CE} &= -\left((0)\log{(p)}-(1-0)\log{(1-p)}\right)\\ &=
    -\log{(1-p)} \end{align}\]
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align} \text{CE} &= -\left((0)\log{(p)}-(1-0)\log{(1-p)}\right)\\ &=
    -\log{(1-p)} \end{align}\]
- en: The empirical risk of the logistic regression model is then the mean cross-entropy
    loss across all datapoints in the dataset. When fitting the model, we want to
    determine the model parameter \(\theta\) that leads to the lowest mean cross-entropy
    loss possible.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型的经验风险是数据集中所有数据点的平均交叉熵损失。在拟合模型时，我们希望确定导致最低平均交叉熵损失的模型参数 \(\theta\)。
- en: \[R(\theta) = - \frac{1}{n} \sum_{i=1}^n \left(y_i\log{(p_i)}-(1-y_i)\log{(1-p_i)}\right)\]
    \[R(\theta) = - \frac{1}{n} \sum_{i=1}^n \left(y_i\log{(\sigma(X_i^{\top}\theta)}-(1-y_i)\log{(1-\sigma(X_i^{\top}\theta)}\right)\]
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: \[R(\theta) = - \frac{1}{n} \sum_{i=1}^n \left(y_i\log{(p_i)}-(1-y_i)\log{(1-p_i)}\right)\]
    \[R(\theta) = - \frac{1}{n} \sum_{i=1}^n \left(y_i\log{(\sigma(X_i^{\top}\theta)}-(1-y_i)\log{(1-\sigma(X_i^{\top}\theta)}\right)\]
- en: 'The optimization problem is therefore to find the estimate \(\hat{\theta}\)
    that minimizes \(R(\theta)\):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，优化问题是找到最小化 \(R(\theta)\) 的估计 \(\hat{\theta}\)：
- en: \[\begin{align} \hat{\theta} = \underset{\theta}{\arg\min} = - \frac{1}{n} \sum_{i=1}^n
    \left(y_i\log{(\sigma(X_i^{\top}\theta)}-(1-y_i)\log{(1-\sigma(X_i^{\top}\theta)}\right)
    \end{align}\]
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align} \hat{\theta} = \underset{\theta}{\arg\min} = - \frac{1}{n} \sum_{i=1}^n
    \left(y_i\log{(\sigma(X_i^{\top}\theta)}-(1-y_i)\log{(1-\sigma(X_i^{\top}\theta)}\right)
    \end{align}\]
- en: Plotting the cross-entropy loss surface for our `toy` dataset gives us a more
    encouraging result – our loss function is now convex. This means we can optimize
    it using gradient descent. Computing the gradient of the logistic model is fairly
    challenging, so we’ll let `sklearn` take care of this for us. You won’t need to
    compute the gradient of the logistic model in Data 100.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制我们的`toy`数据集的交叉熵损失曲面给了我们一个更令人鼓舞的结果 - 我们的损失函数现在是凸的。这意味着我们可以使用梯度下降来优化它。计算逻辑模型的梯度是相当具有挑战性的，所以我们会让`sklearn`来为我们处理这个问题。在Data
    100中，您不需要计算逻辑模型的梯度。
- en: <details><summary>Code</summary>
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE9]</details>'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE9]</details>'
- en: '![](../Images/b5a24c1c8c57c6f3adc24bb239c462d4.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5a24c1c8c57c6f3adc24bb239c462d4.png)'
- en: 22.4 (Bonus) Maximum Likelihood Estimation
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.4 (奖励) 最大似然估计
- en: It may have seemed like we pulled cross-entropy loss out of thin air. How did
    we know that taking the negative logarithms of our probabilities would work so
    well? It turns out that cross-entropy loss is justified by probability theory.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们是凭空提出了交叉熵损失。我们怎么知道取负对数的概率会这么有效呢？事实证明，交叉熵损失是由概率论证明的。
- en: The following section is out of scope, but is certainly an interesting read!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分超出了范围，但肯定是一个有趣的阅读！
- en: '22.4.1 Building Intuition: The Coin Flip'
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 22.4.1 建立直觉：抛硬币
- en: 'To build some intuition for logistic regression, let’s look at an introductory
    example to classification: the coin flip. Suppose we observe some outcomes of
    a coin flip (1 = Heads, 0 = Tails).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对逻辑回归建立一些直觉，让我们看一个分类的入门例子：抛硬币。假设我们观察到一些抛硬币的结果（1 = 正面，0 = 反面）。
- en: '[PRE10]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: A reasonable model is to assume all flips are IID (independent and identically
    distributed). In other words, each flip has the same probability of returning
    a 1 (or heads). Let’s define a parameter \(\theta\), the probability that the
    next flip is a heads. We will use this parameter to inform our decision for \(\hat
    y\) (predicting either 0 or 1) of the next flip. If \(\theta \ge 0.5, \hat y =
    1, \text{else } \hat y = 0\).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一个合理的模型是假设所有的翻转都是独立同分布的（独立且同分布）。换句话说，每次翻转都有相同的概率返回1（或正面）。让我们定义一个参数\(\theta\)，即下一次翻转是正面的概率。我们将使用这个参数来决定我们对下一次翻转的\(\hat
    y\)（预测为0或1）。如果\(\theta \ge 0.5, \hat y = 1, \text{else } \hat y = 0\)。
- en: You may be inclined to say \(0.5\) is the best choice for \(\theta\). However,
    notice that we made no assumption about the coin itself. The coin may be biased,
    so we should make our decision based only on the data. We know that exactly \(\frac{4}{10}\)
    of the flips were heads, so we might guess \(\hat \theta = 0.4\). In the next
    section, we will mathematically prove why this is the best possible estimate.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会倾向于说\(\theta = 0.5\)是最好的选择。然而，请注意我们对硬币本身没有做任何假设。硬币可能是有偏的，所以我们应该只基于数据做出决定。我们知道确切地有\(\frac{4}{10}\)的翻转是正面，所以我们可能猜测\(\hat
    \theta = 0.4\)。在下一节中，我们将在数学上证明为什么这是最佳估计。
- en: 22.4.2 Likelihood of Data
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 22.4.2 数据的可能性
- en: 'Let’s call the result of the coin flip a random variable \(Y\). This is a Bernoulli
    random variable with two outcomes. \(Y\) has the following distribution:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把硬币翻转的结果称为随机变量\(Y\)。这是一个有两个结果的伯努利随机变量。\(Y\)有以下分布：
- en: \[P(Y = y) = \begin{cases} p, \text{if } y=1\\ 1 - p, \text{if } y=0 \end{cases}
    \]
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: \[P(Y = y) = \begin{cases} p, \text{if } y=1\\ 1 - p, \text{if } y=0 \end{cases}
    \]
- en: \(p\) is unknown to us. But we can find the \(p\) that makes the data we observed
    the most *likely*.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: \(p\)对我们来说是未知的。但我们可以找到使我们观察到的数据最*可能*的\(p\)。
- en: The probability of observing 4 heads and 6 tails follows the binomial distribution.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到4个正面和6个反面的概率遵循二项分布。
- en: \[\binom{10}{4} (p)^4 (1-p)^6\]
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: \[\binom{10}{4} (p)^4 (1-p)^6\]
- en: We define the **likelihood** of obtaining our observed data as a quantity *proportional*
    to the probability above. To find it, simply multiply the probabilities of obtaining
    each coin flip.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义获得我们观察到的数据的**可能性**，作为与上述概率成比例的数量。要找到它，只需将获得每个硬币翻转的概率相乘。
- en: \[(p)^{4} (1-p)^6\]
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: \[(p)^{4} (1-p)^6\]
- en: The technique known as **maximum likelihood estimation** finds the \(p\) that
    maximizes the above likelihood. You can find this maximum by taking the derivative
    of the likelihood, but we’ll provide a more intuitive graphical solution.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 称为**最大似然估计**的技术找到了最大化上述可能性的\(p\)。你可以通过对可能性求导来找到这个最大值，但我们将提供一个更直观的图形解决方案。
- en: '[PRE12]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/3e1244e40fcb1f4487f1ce79862ef3d6.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e1244e40fcb1f4487f1ce79862ef3d6.png)'
- en: 'More generally, the likelihood for some Bernoulli(\(p\)) random variable \(Y\)
    is:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，对于某个伯努利(\(p\))随机变量\(Y\)的可能性是：
- en: \[P(Y = y) = \begin{cases} 1, \text{with probability } p\\ 0, \text{with probability
    } 1 - p \end{cases} \]
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: \[P(Y = y) = \begin{cases} 1, \text{with probability } p\\ 0, \text{with probability
    } 1 - p \end{cases} \]
- en: 'Equivalently, this can be written in a compact way:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 等价地，这可以用一种简洁的方式来写：
- en: \[P(Y=y) = p^y(1-p)^{1-y}\]
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: \[P(Y=y) = p^y(1-p)^{1-y}\]
- en: When \(y = 1\), this reads \(P(Y=y) = p\)
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当\(y = 1\)时，这读作\(P(Y=y) = p\)
- en: When \(y = 0\), this reads \(P(Y=y) = (1-p)\)
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当\(y = 0\)时，这读作\(P(Y=y) = (1-p)\)
- en: In our example, a Bernoulli random variable is analogous to a single data point
    (e.g., one instance of a basketball team winning or losing a game). All together,
    our `games` data consists of many IID Bernoulli(\(p\)) random variables. To find
    the likelihood of independent events in succession, simply multiply their likelihoods.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，伯努利随机变量类似于一个单个数据点（例如，篮球队赢得或输掉一场比赛的一个实例）。所有在一起，我们的`games`数据由许多IID伯努利(\(p\))随机变量组成。要找到连续独立事件的可能性，只需将它们的可能性相乘。
- en: \[\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i}\]
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: \[\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i}\]
- en: As with the coin example, we want to find the parameter \(p\) that maximizes
    this likelihood. Earlier, we gave an intuitive graphical solution, but let’s take
    the derivative of the likelihood to find this maximum.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 与硬币的例子一样，我们想找到最大化这种可能性的参数\(p\)。之前，我们给出了一个直观的图形解决方案，但让我们对可能性的导数进行求解以找到这个最大值。
- en: At a first glance, this derivative will be complicated! We will have to use
    the product rule, followed by the chain rule. Instead, we can make an observation
    that simplifies the problem.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这个导数会很复杂！我们将不得不使用乘法规则，然后是链式法则。相反，我们可以做一个简化问题的观察。
- en: Finding the \(p\) that maximizes \[\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i}\] is
    equivalent to the \(p\) that maximizes \[\text{log}(\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i})\]
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最大化\[\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i}\]的\(p\)等同于最大化\[\text{log}(\prod_{i=1}^{n}
    p^{y_i} (1-p)^{1-y_i})\]
- en: 'This is because \(\text{log}\) is a strictly *increasing* function. It won’t
    change the maximum or minimum of the function it was applied to. From \(\text{log}\)
    properties, \(\text{log}(a*b)\) = \(\text{log}(a) + \text{log}(b)\). We can apply
    this to our equation above to get:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为\(\text{log}\)是一个严格*增加*的函数。它不会改变它所应用的函数的最大值或最小值。根据\(\text{log}\)的性质，\(\text{log}(a*b)\)
    = \(\text{log}(a) + \text{log}(b)\)。我们可以将这应用到我们上面的方程中得到：
- en: \[\underset{p}{\text{argmax}} \sum_{i=1}^{n} \text{log}(p^{y_i} (1-p)^{1-y_i})\]
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: \[=\underset{p}{\text{argmax}} \sum_{i=1}^{n} \text{log}(p^{y_i} (1-p)^{1-y_i})\]
- en: \[= \underset{p}{\text{argmax}} \sum_{i=1}^{n} (\text{log}(p^{y_i}) + \text{log}((1-p)^{1-y_i}))\]
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: \[= \underset{p}{\text{argmax}} \sum_{i=1}^{n} (\text{log}(p^{y_i}) + \text{log}((1-p)^{1-y_i}))\]
- en: \[= \underset{p}{\text{argmax}} \sum_{i=1}^{n} (y_i\text{log}(p) + (1-y_i)\text{log}(1-p))\]
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: \[= \underset{p}{\text{argmax}} \sum_{i=1}^{n} (y_i\text{log}(p) + (1-y_i)\text{log}(1-p))\]
- en: We can add a constant factor of \(\frac{1}{n}\) out front. It won’t affect the
    \(p\) that maximizes our likelihood.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在前面添加一个常数因子\(\frac{1}{n}\)。这不会影响最大化我们的似然性的\(p\)。
- en: \[=\underset{p}{\text{argmax}} \frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(p) +
    (1-y_i)\text{log}(1-p)\]
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \[=\underset{p}{\text{argmax}} \frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(p) +
    (1-y_i)\text{log}(1-p)\]
- en: One last “trick” we can do is change this to a minimization problem by negating
    the result. This works because we are dealing with a *concave* function, which
    can be made *convex*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的最后一个“技巧”是通过取反来将其转换为最小化问题。这是因为我们正在处理一个*凹*函数，可以将其变为*凸*函数。
- en: \[= \underset{p}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(p)
    + (1-y_i)\text{log}(1-p)\]
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: \[= \underset{p}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(p)
    + (1-y_i)\text{log}(1-p)\]
- en: Now let’s say that we have data that are independent with different probability
    \(p_i\). Then, we would want to find the \(p_1, p_2, \dots, p_n\) that maximize
    \[\prod_{i=1}^{n} p_i^{y_i} (1-p_i)^{1-y_i}\]
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们有独立的具有不同概率\(p_i\)的数据。那么，我们希望找到最大化\[\prod_{i=1}^{n} p_i^{y_i} (1-p_i)^{1-y_i}\]的\(p_1,
    p_2, \dots, p_n\)。
- en: 'Setting up and simplifying the optimization problems as we did above, we ultimately
    want to find:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 像我们上面那样设置和简化优化问题，我们最终想要找到：
- en: \[= \underset{p}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(p_i)
    + (1-y_i)\text{log}(1-p_i)\]
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: \[= \underset{p}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(p_i)
    + (1-y_i)\text{log}(1-p_i)\]
- en: 'For logistic regression, \(p_i = \sigma(x^{\top}\theta)\). Plugging that in,
    we get:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于逻辑回归，\(p_i = \sigma(x^{\top}\theta)\)。将其代入，我们得到：
- en: \[= \underset{p}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(\sigma(x^{\top}\theta))
    + (1-y_i)\text{log}(1-\sigma(x^{\top}\theta))\]
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: \[= \underset{p}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(\sigma(x^{\top}\theta))
    + (1-y_i)\text{log}(1-\sigma(x^{\top}\theta))\]
- en: This is exactly our average cross-entropy loss minimization problem from before!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们之前讨论过的平均交叉熵损失最小化问题！
- en: Why did we do all this complicated math? We have shown that *minimizing* cross-entropy
    loss is equivalent to *maximizing* the likelihood of the training data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要做这么复杂的数学？我们已经证明*最小化*交叉熵损失等价于*最大化*训练数据的似然性。
- en: By minimizing cross-entropy loss, we are choosing the model parameters that
    are “most likely” for the data we observed.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过最小化交叉熵损失，我们选择了对我们观察到的数据“最有可能”的模型参数。
- en: Note that this is under the assumption that all data is drawn independently
    from the same logistic regression model with parameter \(\theta\). In fact, many
    of the model + loss combinations we’ve seen can be motivated using MLE (e.g.,
    OLS, Ridge Regression, etc.). In probability and ML classes, you’ll get the chance
    to explore MLE further.******
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是在所有数据都是独立地从相同的逻辑回归模型中抽取的假设下。事实上，我们所见过的许多模型+损失组合都可以通过最大似然估计来解释（例如OLS，岭回归等）。在概率和机器学习课程中，你将有机会进一步探讨最大似然估计。
