- en: Chapter 10\. Machine Learning with MLlib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。MLlib机器学习
- en: Up until this point, we have focused on data engineering workloads with Apache
    Spark. Data engineering is often a precursory step to preparing your data for
    machine learning (ML) tasks, which will be the focus of this chapter. We live
    in an era in which machine learning and artificial intelligence applications are
    an integral part of our lives. Chances are that whether we realize it or not,
    every day we come into contact with ML models for purposes such as online shopping
    recommendations and advertisements, fraud detection, classification, image recognition,
    pattern matching, and more. These ML models drive important business decisions
    for many companies. According to [this McKinsey study](https://oreil.ly/Dxj0A),
    35% of what consumers purchase on Amazon and 75% of what they watch on Netflix
    is driven by machine learning–based product recommendations. Building a model
    that performs well can make or break companies.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 直到此刻，我们一直专注于使用Apache Spark进行数据工程工作负载。数据工程通常是为机器学习（ML）任务准备数据的前奏步骤，这也是本章的重点。我们生活在一个机器学习和人工智能应用成为生活中不可或缺部分的时代。无论我们是否意识到，每天我们都可能接触到用于在线购物推荐和广告、欺诈检测、分类、图像识别、模式匹配等目的的ML模型。这些ML模型对许多公司的重要业务决策起着推动作用。根据[这项麦肯锡研究](https://oreil.ly/Dxj0A)，消费者在亚马逊购买的35%和在Netflix观看的75%产品是基于机器学习的产品推荐。构建一个表现良好的模型可以成败企业。
- en: In this chapter we will get you started building ML models using [MLlib](https://oreil.ly/_XSOs),
    the de facto machine learning library in Apache Spark. We’ll begin with a brief
    introduction to machine learning, then cover best practices for distributed ML
    and feature engineering at scale (if you’re already familiar with machine learning
    fundamentals, you can skip straight to [“Designing Machine Learning Pipelines”](#designing_machine_learning_pipelines)).
    Through the short code snippets presented here and the notebooks available in
    the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2), you’ll
    learn how to build basic ML models and use MLlib.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将带您开始使用[MLlib](https://oreil.ly/_XSOs)，这是Apache Spark中的事实上的机器学习库来构建ML模型。我们将从机器学习的简要介绍开始，然后涵盖在规模上进行分布式ML和特征工程的最佳实践（如果您已经熟悉机器学习基础知识，您可以直接跳转到[“设计机器学习管道”](#designing_machine_learning_pipelines)）。通过本书中呈现的简短代码片段和可在书的[GitHub库](https://github.com/databricks/LearningSparkV2)中找到的笔记本，您将学习如何构建基本的ML模型并使用MLlib。
- en: Note
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This chapter covers the Scala and Python APIs; if you’re interested in using
    R (`sparklyr`) with Spark for machine learning, we invite you to check out [*Mastering
    Spark with R*](http://shop.oreilly.com/product/0636920223764.do) by Javier Luraschi,
    Kevin Kuo, and Edgar Ruiz (O’Reilly).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了Scala和Python的API；如果您有兴趣使用R（`sparklyr`）与Spark进行机器学习，我们建议您查阅[*Mastering Spark
    with R*](http://shop.oreilly.com/product/0636920223764.do)由Javier Luraschi、Kevin
    Kuo和Edgar Ruiz（O’Reilly）编著。
- en: What Is Machine Learning?
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: Machine learning is getting a lot of hype these days—but what is it, exactly?
    Broadly speaking, machine learning is a process for extracting patterns from your
    data, using statistics, linear algebra, and numerical optimization. Machine learning
    can be applied to problems such as predicting power consumption, determining whether
    or not there is a cat in your video, or clustering items with similar characteristics.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当今机器学习受到了很多炒作，但它究竟是什么？广义地说，机器学习是一个从您的数据中提取模式的过程，使用统计学、线性代数和数值优化。机器学习可以应用于诸如预测能耗、确定您的视频中是否有猫，或者对具有相似特征的项目进行聚类等问题。
- en: There are a few types of machine learning, including supervised, semi-supervised,
    unsupervised, and reinforcement learning. This chapter will mainly focus on supervised
    machine learning and just touch upon unsupervised learning. Before we dive in,
    let’s briefly discuss the differences between supervised and unsupervised ML.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习有几种类型，包括监督学习、半监督学习、无监督学习和强化学习。本章将主要关注监督学习，并简要涉及无监督学习。在我们深入讨论之前，让我们简要讨论一下监督学习和无监督学习的区别。
- en: Supervised Learning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**监督学习**'
- en: 'In [supervised machine learning](https://oreil.ly/fVOVL), your data consists
    of a set of input records, each of which has associated labels, and the goal is
    to predict the output label(s) given a new unlabeled input. These output labels
    can either be *discrete* or *continuous*, which brings us to the two types of
    supervised machine learning: *classification* and *regression*.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [监督学习](https://oreil.ly/fVOVL) 中，你的数据包含一组输入记录，每个记录都有相关的标签，目标是在给定新的未标记输入时预测输出标签。这些输出标签可以是*离散*的或*连续*的，这就引出了监督学习的两种类型：*分类*和*回归*。
- en: In a classification problem, the aim is to separate the inputs into a discrete
    set of classes or labels. With *binary* classification, there are two discrete
    labels you want to predict, such as “dog” or “not dog,” as [Figure 10-1](#binary_classification_example_dog_or_not)
    depicts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，目标是将输入分为一组离散的类别或标签。在*二元*分类中，有两个离散的标签需要预测，例如“狗”或“非狗”，正如 [图 10-1](#binary_classification_example_dog_or_not)
    所示。
- en: '![Binary classification example: dog or not dog](assets/lesp_1001.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![二元分类示例：狗或非狗](assets/lesp_1001.png)'
- en: 'Figure 10-1\. Binary classification example: dog or not dog'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 二元分类示例：狗或非狗
- en: With *multiclass*, also known as *multinomial*, classification, there can be
    three or more discrete labels, such as predicting the breed of a dog (e.g., Australian
    shepherd, golden retriever, or poodle, as shown in [Figure 10-2](#multinomial_classification_example_austr)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在*多类*，也称为*多项式*，分类中，可能有三个或更多的离散标签，例如预测狗的品种（例如澳大利亚牧羊犬、金毛寻回犬或贵宾犬，如图 10-2](#multinomial_classification_example_austr)
    所示。
- en: '![Multinomial classification example: Australian shepherd, golden retriever,
    or poodle](assets/lesp_1002.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![多项分类示例：澳大利亚牧羊犬、金毛寻回犬或贵宾犬](assets/lesp_1002.png)'
- en: 'Figure 10-2\. Multinomial classification example: Australian shepherd, golden
    retriever, or poodle'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 多项分类示例：澳大利亚牧羊犬、金毛寻回犬或贵宾犬
- en: In regression problems, the value to predict is a continuous number, not a label.
    This means you might predict values that your model hasn’t seen during training,
    as shown in [Figure 10-3](#regression_example_predicting_ice_cream). For example,
    you might build a model to predict the daily ice cream sales given the temperature.
    Your model might predict the value $77.67, even if none of the input/output pairs
    it was trained on contained that value.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，要预测的值是一个连续的数字，而不是一个标签。这意味着你的模型可能会预测在训练过程中从未见过的值，就像在 [图 10-3](#regression_example_predicting_ice_cream)
    中展示的那样。例如，你可以建立一个模型来预测根据温度每日的冰淇淋销量。即使它在训练时没有看到过这个值的输入/输出对，你的模型可能会预测出值 $77.67$。
- en: '![Regression example: predicting ice cream sales based on temperature](assets/lesp_1003.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![回归示例：根据温度预测冰淇淋销量](assets/lesp_1003.png)'
- en: 'Figure 10-3\. Regression example: predicting ice cream sales based on temperature'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 回归示例：根据温度预测冰淇淋销量
- en: '[Table 10-1](#popular_classification_and_regression_al) lists some commonly
    used supervised ML algorithms that are available in [Spark MLlib](https://oreil.ly/Yt0uu),
    with a note as to whether they can be used for regression, classification, or
    both.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10-1](#popular_classification_and_regression_al) 列出了一些常用的监督机器学习算法，这些算法可以在
    [Spark MLlib](https://oreil.ly/Yt0uu) 中使用，表明它们可以用于回归、分类或两者。'
- en: Table 10-1\. Popular classification and regression algorithms
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-1\. 热门分类和回归算法
- en: '| Algorithm | Typical usage |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 典型用法 |'
- en: '| --- | --- |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Linear regression | Regression |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 线性回归 | 回归 |'
- en: '| Logistic regression | Classification (we know, it has regression in the name!)
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 分类（我们知道它的名字里有回归！） |'
- en: '| Decision trees | Both |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 两者 |'
- en: '| Gradient boosted trees | Both |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 梯度提升树 | 两者 |'
- en: '| Random forests | Both |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 两者 |'
- en: '| Naive Bayes | Classification |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 分类 |'
- en: '| Support vector machines (SVMs) | Classification |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机 (SVMs) | 分类 |'
- en: Unsupervised Learning
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Obtaining the labeled data required by supervised machine learning can be very
    expensive and/or infeasible. This is where [unsupervised machine learning](https://oreil.ly/J80ym)
    comes into play. Instead of predicting a label, unsupervised ML helps you to better
    understand the structure of your data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 获得监督机器学习所需的带标签数据可能非常昂贵和/或不可行。这就是 [无监督学习](https://oreil.ly/J80ym) 发挥作用的地方。与预测标签不同，无监督
    ML 帮助你更好地理解数据的结构。
- en: As an example, consider the original unclustered data on the left in [Figure 10-4](#clustering_example).
    There is no known true label for each of these data points (*x*[1], *x*[2]), but
    by applying unsupervised machine learning to our data we can find the clusters
    that naturally form, as shown on the right.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑左侧原始未聚类数据在[图 10-4](#clustering_example)中。这些数据点（*x*[1]、*x*[2]）没有已知的真实标签，但通过应用无监督机器学习，我们可以找到自然形成的聚类，如右侧所示。
- en: '![Clustering example](assets/lesp_1004.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![聚类示例](assets/lesp_1004.png)'
- en: Figure 10-4\. Clustering example
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 聚类示例
- en: Unsupervised machine learning can be used for outlier detection or as a preprocessing
    step for supervised machine learning—for example, to [reduce the dimensionality](https://oreil.ly/N5JWF)
    (i.e., number of dimensions per datum) of the data set, which is useful for reducing
    storage requirements or simplifying downstream tasks. Some [unsupervised machine
    learning algorithms in MLlib](https://oreil.ly/NLYo6) include *k*-means, Latent
    Dirichlet Allocation (LDA), and Gaussian mixture models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习可用于异常检测或作为监督机器学习的预处理步骤，例如用于[减少数据集的维度](https://oreil.ly/N5JWF)（即每个数据点的维数），这对于减少存储需求或简化下游任务非常有用。MLlib
    中一些[无监督机器学习算法](https://oreil.ly/NLYo6)包括 *k*-means、潜在狄利克雷分配（LDA）和高斯混合模型。
- en: Why Spark for Machine Learning?
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么选择 Spark 进行机器学习？
- en: Spark is a unified analytics engine that provides an ecosystem for data ingestion,
    feature engineering, model training, and deployment. Without Spark, developers
    would need many disparate tools to accomplish this set of tasks, and might still
    struggle with scalability.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个统一的分析引擎，提供了数据摄取、特征工程、模型训练和部署的生态系统。没有 Spark，开发人员需要使用许多不同的工具来完成这些任务，并且可能仍然面临可扩展性问题。
- en: 'Spark has two machine learning packages: [`spark.mllib`](https://oreil.ly/qy-PT)
    and [`spark.ml`](https://oreil.ly/WBGzN). `spark.mllib` is the original machine
    learning API, based on the RDD API (which has been in maintenance mode since Spark
    2.0), while `spark.ml` is the newer API, based on DataFrames. The rest of this
    chapter will focus on using the `spark.ml` package and how to design machine learning
    pipelines in Spark. However, we use “MLlib” as an umbrella term to refer to both
    machine learning library packages in Apache Spark.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 有两个机器学习包：[`spark.mllib`](https://oreil.ly/qy-PT) 和 [`spark.ml`](https://oreil.ly/WBGzN)。`spark.mllib`
    是基于 RDD API 的原始机器学习 API（自 Spark 2.0 起处于维护模式），而 `spark.ml` 是基于 DataFrames 的较新 API。本章的其余部分将重点介绍如何使用
    `spark.ml` 包及如何设计 Spark 中的机器学习管道。然而，我们使用“MLlib”作为一个统称，用来指代 Apache Spark 中的两个机器学习库包。
- en: With `spark.ml`, data scientists can use one ecosystem for their data preparation
    and model building, without the need to downsample their data to fit on a single
    machine. `spark.ml` focuses on O(*n*) scale-out, where the model scales linearly
    with the number of data points you have, so it can scale to massive amounts of
    data. In the following chapter, we will discuss some of the trade-offs involved
    in choosing between a distributed framework such as `spark.ml` and a single-node
    framework like [`scikit-learn`](https://oreil.ly/WSz_8) (`sklearn`). If you have
    previously used `scikit-learn`, many of the APIs in `spark.ml` will feel quite
    familiar, but there are some subtle differences that we will discuss.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `spark.ml`，数据科学家可以在数据准备和模型构建中使用一个生态系统，而无需将数据降采样以适应单台机器。`spark.ml` 专注于 O(*n*)
    的规模扩展，其中模型随着数据点数量线性扩展，因此可以适应大量数据。在接下来的章节中，我们将讨论在选择分布式框架（如 `spark.ml`）和单节点框架（如[`scikit-learn`](https://oreil.ly/WSz_8)
    (`sklearn`)）之间涉及的一些权衡。如果您之前使用过 `scikit-learn`，那么 `spark.ml` 中的许多 API 将感觉非常熟悉，但我们将讨论一些细微的差异。
- en: Designing Machine Learning Pipelines
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计机器学习管道
- en: In this section, we will cover how to create and tune ML pipelines. The concept
    of pipelines is common across many ML frameworks as a way to organize a series
    of operations to apply to your data. In MLlib, the [Pipeline API](https://oreil.ly/FdTA_)
    provides a high-level API built on top of DataFrames to organize your machine
    learning workflow. The Pipeline API is composed of a series of transformers and
    estimators, which we will discuss in-depth later.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何创建和调整 ML 管道。管道的概念在许多 ML 框架中都很常见，作为一种组织一系列操作并应用于数据的方式。在 MLlib 中，[Pipeline
    API](https://oreil.ly/FdTA_) 提供了一个基于 DataFrames 构建的高级 API，用于组织您的机器学习工作流程。Pipeline
    API 由一系列转换器和评估器组成，我们稍后将深入讨论。
- en: Throughout this chapter, we will use the San Francisco housing data set from
    [Inside Airbnb](https://oreil.ly/hBfNj). It contains information about Airbnb
    rentals in San Francisco, such as the number of bedrooms, location, review scores,
    etc., and our goal is to build a model to predict the nightly rental prices for
    listings in that city. This is a regression problem, because price is a continuous
    variable. We will guide you through the workflow a data scientist would go through
    to approach this problem, including feature engineering, building models, hyperparameter
    tuning, and evaluating model performance. This data set is quite messy and can
    be difficult to model (like most real-world data sets!), so if you are experimenting
    on your own, don’t feel bad if your early models aren’t great.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用来自 [Inside Airbnb](https://oreil.ly/hBfNj) 的旧金山住房数据集。它包含有关旧金山 Airbnb
    租赁的信息，如卧室数量、位置、评论得分等，我们的目标是建立一个模型来预测该市房源的每晚租金。这是一个回归问题，因为价格是一个连续变量。我们将引导你完成数据科学家处理这个问题的工作流程，包括特征工程、构建模型、超参数调优和评估模型性能。这个数据集相当凌乱，建模可能会比较困难（就像大多数真实世界的数据集一样！），所以如果你自己在尝试时，早期的模型不理想也不要感到沮丧。
- en: 'The intent of this chapter is not to show you every API in MLlib, but rather
    to equip you with the skills and knowledge to get started with using MLlib to
    build end-to-end pipelines. Before going into the details, let’s define some MLlib
    terminology:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本   本章的目的不是向你展示 MLlib 中的所有 API，而是装备你掌握使用 MLlib 构建端到端管道的技能和知识。在详细讲解之前，让我们先定义一些
    MLlib 的术语：
- en: Transformer
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器
- en: Accepts a DataFrame as input, and returns a new DataFrame with one or more columns
    appended to it. Transformers do not learn any parameters from your data and simply
    apply rule-based transformations to either prepare data for model training or
    generate predictions using a trained MLlib model. They have a `.transform()` method.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接受 DataFrame 作为输入，并返回一个新 DataFrame，附加一个或多个列。转换器不会从数据中学习任何参数，而是简单地应用基于规则的转换，既为模型训练准备数据，又使用训练好的
    MLlib 模型生成预测。它们有一个 `.transform()` 方法。
- en: Estimator
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器
- en: Learns (or “fits”) parameters from your DataFrame via a `.fit()` method and
    returns a `Model`, which is a transformer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从你的 DataFrame 通过 `.fit()` 方法学习（或“拟合”）参数，并返回一个 `Model`，即一个转换器。
- en: Pipeline
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 管道
- en: Organizes a series of transformers and estimators into a single model. While
    pipelines themselves are estimators, the output of `pipeline.fit()` returns a
    `PipelineModel`, a transformer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将一系列转换器和估计器组织成一个单一的模型。虽然管道本身是估计器，但 `pipeline.fit()` 的输出返回一个 `PipelineModel`，即一个转换器。
- en: While these concepts may seem rather abstract right now, the code snippets and
    examples in this chapter will help you understand how they all come together.
    But before we can build our ML models and use transformers, estimators, and pipelines,
    we need to load in our data and perform some data preparation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些概念现在看起来可能相当抽象，但本章中的代码片段和示例将帮助你理解它们是如何结合在一起的。但在我们构建 ML 模型并使用转换器、估计器和管道之前，我们需要加载数据并进行一些数据准备。
- en: Data Ingestion and Exploration
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据获取和探索
- en: We have slightly preprocessed the data in our example data set to remove outliers
    (e.g., Airbnbs posted for $0/night), converted all integers to doubles, and selected
    an informative subset of the more than one hundred fields. Further, for any missing
    numerical values in our data columns, we have imputed the median value and added
    an indicator column (the column name followed by `_na`, such as `bedrooms_na`).
    This way the ML model or human analyst can interpret any value in that column
    as an imputed value, not a true value. You can see the data preparation notebook
    in the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2). Note
    there are many other ways to handle missing values, which are outside the scope
    of this book.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对示例数据集进行了稍微的预处理，去除了异常值（例如，Airbnb 发布的每晚 $0），将所有整数转换为浮点数，并选择了超过一百个字段的有信息量的子集。此外，对于数据列中的任何缺失数值，我们使用中位数值进行了填充，并添加了一个指标列（列名后跟
    `_na`，如 `bedrooms_na`）。这样，ML 模型或人工分析师就可以将该列中的任何值解释为填充值，而不是真实值。你可以在书的 [GitHub 仓库](https://github.com/databricks/LearningSparkV2)
    中查看数据准备笔记本。请注意，还有许多其他处理缺失值的方法，超出了本书的范围。
- en: 'Let’s take a quick peek at the data set and the corresponding schema (with
    the output showing just a subset of the columns):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速浏览一下数据集和相应的模式（输出仅显示部分列）：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Our goal is to predict the price per night for a rental property, given our
    features.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是预测租赁物业每晚的价格，给定我们的特征。
- en: Note
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: Before data scientists can get to model building, they need to explore and understand
    their data. They will often use Spark to group their data, then use data visualization
    libraries such as [matplotlib](https://matplotlib.org) to visualize the data.
    We will leave data exploration as an exercise for the reader.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学家可以开始建模之前，他们需要探索和理解他们的数据。他们经常使用 Spark 对数据进行分组，然后使用诸如 [matplotlib](https://matplotlib.org)
    等数据可视化库来可视化数据。我们将数据探索留给读者作为练习。
- en: Creating Training and Test Data Sets
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练和测试数据集
- en: 'Before we begin feature engineering and modeling, we will divide our data set
    into two groups: *train* and *test*. Depending on the size of your data set, your
    train/test ratio may vary, but many data scientists use 80/20 as a standard train/test
    split. You might be wondering, “Why not use the entire data set to train the model?”
    The problem is that if we built a model on the entire data set, it’s possible
    that the model would memorize or “overfit” to the training data we provided, and
    we would have no more data with which to evaluate how well it generalizes to previously
    unseen data. The model’s performance on the test set is a proxy for how well it
    will perform on unseen data (i.e., in the wild or in production), assuming that
    data follows similar distributions. This split is depicted in [Figure 10-5](#trainsolidustest_split).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始特征工程和建模之前，我们将把数据集分成两组：*训练集*和*测试集*。根据您的数据集大小，您的训练/测试比例可能会有所不同，但许多数据科学家使用80/20作为标准的训练/测试分割。您可能会想：“为什么不使用整个数据集来训练模型呢？”问题在于，如果我们在整个数据集上建立模型，可能会导致模型记忆或“过拟合”我们提供的训练数据，并且我们将没有更多数据来评估它在以前未见数据上的泛化能力。模型在测试集上的表现是它在未见数据（即在野外或生产环境中）上表现良好的代理，假设数据遵循相似的分布。这种分割在[图 10-5](#trainsolidustest_split)中有所描绘。
- en: '![Train/test split](assets/lesp_1005.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![Train/test split](assets/lesp_1005.png)'
- en: Figure 10-5\. Train/test split
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 训练/测试分割
- en: Our training set consists of a set of features, X, and a label, y. Here we use
    capital X to denote a matrix with dimensions *n* x *d*, where *n* is the number
    of data points (or examples) and *d* is the number of features (this is what we
    call the fields or columns in our DataFrame). We use lowercase y to denote a vector,
    with dimensions *n* x 1; for every example, there is one label.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练集包含一组特征 X 和一个标签 y。这里我们使用大写 X 表示一个维度为 *n* x *d* 的矩阵，其中 *n* 是数据点（或示例）的数量，*d*
    是特征的数量（这就是我们在 DataFrame 中称为字段或列的东西）。我们使用小写 y 表示一个向量，维度为 *n* x 1；对于每个示例，都有一个标签。
- en: Different metrics are used to measure the performance of the model. For classification
    problems, a standard metric is the *accuracy*, or percentage, of correct predictions.
    Once the model has satisfactory performance on the training set using that metric,
    we will apply the model to our test set. If it performs well on our test set according
    to our evaluation metrics, then we can feel confident that we have built a model
    that will “generalize” to unseen data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的度量标准用于衡量模型的性能。对于分类问题，一个标准度量标准是*准确度*，或者正确预测的百分比。一旦模型在训练集上使用该度量标准表现良好，我们将把模型应用到我们的测试集上。如果它根据我们的评估指标在我们的测试集上表现良好，那么我们可以确信我们已经构建了一个能够“泛化”到未见数据的模型。
- en: 'For our Airbnb data set, we will keep 80% for the training set and set aside
    20% of our data for the test set. Further, we will set a random seed for reproducibility,
    such that if we rerun this code we will get the same data points going to our
    train and test data sets, respectively. The value of the seed itself *shouldn’t*
    matter, but data scientists often like setting it to 42 as that is the answer
    to the [Ultimate Question of Life](https://oreil.ly/sE12h):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的 Airbnb 数据集，我们将保留80%作为训练集，并将20%的数据留作测试集。此外，我们将设置一个随机种子以确保可重现性，这样如果我们重新运行此代码，我们将得到相同的数据点分配到我们的训练和测试数据集中。种子本身的值*不应该*有影响，但是数据科学家经常喜欢将其设置为42，因为这是生命的[终极问题的答案](https://oreil.ly/sE12h)。
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This produces the following output:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: But what happens if we change the number of executors in our Spark cluster?
    The Catalyst optimizer determines the [optimal way to partition your data](https://oreil.ly/Ecd_m)
    as a function of your cluster resources and size of your data set. Given that
    data in a Spark DataFrame is row-partitioned and each worker performs its split
    independently of the other workers, if the data in the partitions changes, then
    the result of the split (by `randomSplit()`) won’t be the same.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们改变 Spark 集群中执行器的数量会发生什么呢？Catalyst 优化器根据集群资源和数据集的大小作为函数确定 [数据分区的最佳方式](https://oreil.ly/Ecd_m)。考虑到
    Spark DataFrame 中的数据是行分区的，每个工作节点独立执行其分区，如果分区中的数据发生变化，则 `randomSplit()` 的结果将不同。
- en: While you could fix your cluster configuration and your seed to ensure that
    you get consistent results, our recommendation is to split your data once, then
    write it out to its own train/test folder so you don’t have these reproducibility
    issues.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以固定您的集群配置和种子以确保获得一致的结果，但我们建议您仅拆分数据一次，然后将其写入到自己的训练/测试文件夹中，以避免这些可再现性问题。
- en: Note
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: During your exploratory analysis, you should cache the training data set because
    you will be accessing it many times throughout the machine learning process. Please
    reference the section on [“Caching and Persistence of Data”](ch07.html#persistence_and_caching_data)
    in [Chapter 7](ch07.html#optimizing_and_tuning_spark_applications).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索性分析期间，您应该缓存训练数据集，因为在整个机器学习过程中您将经常访问它。请参考 [第 7 章](ch07.html#optimizing_and_tuning_spark_applications)
    中关于 [“数据缓存和持久化”](ch07.html#persistence_and_caching_data) 的部分。
- en: Preparing Features with Transformers
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用转换器准备特征
- en: Now that we have split our data into training and test sets, let’s prepare the
    data to build a linear regression model predicting price given the number of bedrooms.
    In a later example, we will include all of the relevant features, but for now
    let’s make sure we have the mechanics in place. Linear regression (like many other
    algorithms in Spark) requires that all the input features are contained within
    a single vector in your DataFrame. Thus, we need to *transform* our data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据分成了训练集和测试集，让我们准备数据来构建一个线性回归模型，预测给定卧室数量的价格。在稍后的示例中，我们将包括所有相关特征，但现在让我们确保已经准备好了机制。线性回归（像
    Spark 中的许多其他算法一样）要求所有输入特征都包含在 DataFrame 中的单个向量中。因此，我们需要 *转换* 我们的数据。
- en: Transformers in Spark accept a DataFrame as input and return a new DataFrame
    with one or more columns appended to it. They do not learn from your data, but
    apply rule-based transformations using the `transform()` method.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中的转换器接受 DataFrame 作为输入，并返回一个新的 DataFrame，其中附加了一个或多个列。它们不会从您的数据中学习，而是使用
    `transform()` 方法应用基于规则的转换。
- en: 'For the task of putting all of our features into a single vector, we will use
    the [`VectorAssembler` transformer](https://oreil.ly/r2MSV). `VectorAssembler`
    takes a list of input columns and creates a new DataFrame with an additional column,
    which we will call `features`. It combines the values of those input columns into
    a single vector:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于将所有特征放入单个向量的任务，我们将使用 [`VectorAssembler` 转换器](https://oreil.ly/r2MSV)。`VectorAssembler`
    接受一列输入列的列表，并创建一个新的 DataFrame，其中包含一个额外的列，我们将其称为 `features`。它将这些输入列的值合并到一个单独的向量中：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You’ll notice that in the Scala code, we had to instantiate the new `VectorAssembler`
    object as well as using setter methods to change the input and output columns.
    In Python, you have the option to pass the parameters directly to the constructor
    of `VectorAssembler` or to use the setter methods, but in Scala you can only use
    the setter methods.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，在 Scala 代码中，我们必须实例化新的 `VectorAssembler` 对象，并使用 setter 方法来更改输入和输出列。在 Python
    中，您可以直接将参数传递给 `VectorAssembler` 的构造函数，或者使用 setter 方法，但在 Scala 中只能使用 setter 方法。
- en: We cover the fundamentals of linear regression next, but if you are already
    familiar with the algorithm, please skip to [“Using Estimators to Build Models”](#using_estimators_to_build_models).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将讲解线性回归的基础知识，但如果您已经熟悉这种算法，请跳转到 [“使用估算器构建模型”](#using_estimators_to_build_models)。
- en: Understanding Linear Regression
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解线性回归
- en: '[Linear regression](https://oreil.ly/dhgZf) models a linear relationship between
    your dependent variable (or label) and one or more independent variables (or features).
    In our case, we want to fit a linear regression model to predict the price of
    an Airbnb rental given the number of bedrooms.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[线性回归](https://oreil.ly/dhgZf)模型描述了依赖变量（或标签）与一个或多个独立变量（或特征）之间的线性关系。在我们的案例中，我们希望拟合一个线性回归模型，以预测Airbnb租金的价格，根据卧室数量。'
- en: In [Figure 10-6](#univariate_linear_regression), we have a single feature *x*
    and an output *y* (this is our dependent variable). Linear regression seeks to
    fit an equation for a line to *x* and *y*, which for scalar variables can be expressed
    as *y = mx + b*, where *m* is the slope and *b* is the offset or intercept.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图10-6](#univariate_linear_regression)中，我们有一个特征*x*和一个输出*y*（这是我们的因变量）。线性回归旨在为*x*和*y*拟合一条直线方程，对于标量变量，可以表示为*y
    = mx + b*，其中*m*是斜率，*b*是偏移或截距。
- en: The dots indicate the true (*x*, *y*) pairs from our data set, and the solid
    line indicates the line of best fit for this data set. The data points do not
    perfectly line up, so we usually think of linear regression as fitting a model
    to y ≈ *mx* + *b* + ε, where ε (epsilon) is an error drawn independently per record
    *x* from some distribution. These are the errors between our model predictions
    and the true values. Often we think of ε as being Gaussian, or normally distributed.
    The vertical lines above the regression line indicate positive ε (or residuals),
    where your true values are above the predicted values, and the vertical lines
    below the regression line indicate negative residuals. The goal of linear regression
    is to find a line that minimizes the square of these residuals. You’ll notice
    that the line can extrapolate predictions for data points it hasn’t seen.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 点表示我们数据集中的真实(*x*, *y*)对，实线表示该数据集的最佳拟合线。数据点并不完全对齐，因此我们通常认为线性回归是将模型拟合到y ≈ *mx*
    + *b* + ε的过程，其中ε（epsilon）是从某个分布独立抽取的每个记录*x*的误差。这些是我们模型预测与真实值之间的误差。通常我们将ε视为高斯或正态分布。回归线上方的垂直线表示正ε（或残差），即您的真实值高于预测值，而回归线下方的垂直线表示负残差。线性回归的目标是找到最小化这些残差平方的线。您会注意到，该线可以对其未见数据点进行预测。
- en: '![Univariate linear regression](assets/lesp_1006.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![单变量线性回归](assets/lesp_1006.png)'
- en: Figure 10-6\. Univariate linear regression
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6\. 单变量线性回归
- en: Linear regression can also be extended to handle multiple independent variables.
    If we had three features as input, *x* = [*x*[1], *x*[2], *x*[3]], then we could
    model *y* as *y* ≈ *w*[0] + *w*[1]*x*[1] + *w*[2]*x*[2] + *w*[3]*x*[3] + ε. In
    this case, there is a separate coefficient (or weight) for each feature and a
    single intercept (*w*[0] instead of *b* here). The process of estimating the coefficients
    and intercept for our model is called *learning* (or *fitting*) the parameters
    for the model. For right now, we’ll focus on the univariate regression example
    of predicting price given the number of bedrooms, and we’ll get back to multivariate
    linear regression in a bit.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归也可以扩展到处理多个独立变量。如果我们有三个输入特征，*x* = [*x*[1], *x*[2], *x*[3]]，那么我们可以将*y*建模为*y*
    ≈ *w*[0] + *w*[1]*x*[1] + *w*[2]*x*[2] + *w*[3]*x*[3] + ε。在这种情况下，每个特征都有一个单独的系数（或权重），并且一个单独的截距（这里是*w*[0]而不是*b*）。估计我们模型的系数和截距的过程称为*学习*（或*拟合*）模型的参数。现在，我们将专注于预测价格与卧室数量的单变量回归示例，并稍后再讨论多变量线性回归。
- en: Using Estimators to Build Models
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用估计器构建模型
- en: After setting up our `vectorAssembler`, we have our data prepared and transformed
    into a format that our linear regression model expects. In Spark, [`LinearRegression`](https://oreil.ly/zxlnL)
    is a type of estimator—it takes in a DataFrame and returns a `Model`. Estimators
    learn parameters from your data, have an `estimator_name.fit()` method, and are
    eagerly evaluated (i.e., kick off Spark jobs), whereas transformers are lazily
    evaluated. Some other examples of estimators include `Imputer`, `DecisionTreeClassifier`,
    and `RandomForestRegressor`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 设置好我们的`vectorAssembler`后，我们准备好了我们的数据，并将其转换为我们的线性回归模型所需的格式。在Spark中，[`LinearRegression`](https://oreil.ly/zxlnL)是一种类型的估计器——它接受一个DataFrame并返回一个`Model`。估计器从您的数据中学习参数，具有`estimator_name.fit()`方法，并且会立即评估（即启动Spark作业），而变换器则是延迟评估的。其他一些估计器的示例包括`Imputer`、`DecisionTreeClassifier`和`RandomForestRegressor`。
- en: 'You’ll notice that our input column for linear regression (`features`) is the
    output from our `vectorAssembler`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，我们线性回归的输入列（`features`）是我们的`vectorAssembler`的输出：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`lr.fit()` returns a [`LinearRegressionModel`](https://oreil.ly/LASya) (`lrModel`),
    which is a transformer. In other words, the output of an estimator’s `fit()` method
    is a transformer. Once the estimator has learned the parameters, the transformer
    can apply these parameters to new data points to generate predictions. Let’s inspect
    the parameters it learned:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`lr.fit()`返回一个[`LinearRegressionModel`](https://oreil.ly/LASya) (`lrModel`)，它是一个转换器。换句话说，估计器的`fit()`方法的输出是一个转换器。一旦估计器学习了参数，转换器可以将这些参数应用于新的数据点以生成预测结果。让我们检查它学到的参数：'
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This prints:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Creating a Pipeline
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个Pipeline
- en: 'If we want to apply our model to our test set, then we need to prepare that
    data in the same way as the training set (i.e., pass it through the vector assembler).
    Oftentimes data preparation pipelines will have multiple steps, and it becomes
    cumbersome to remember not only which steps to apply, but also the ordering of
    the steps. This is the motivation for the [Pipeline API](https://oreil.ly/MG3YM):
    you simply specify the stages you want your data to pass through, in order, and
    Spark takes care of the processing for you. They provide the user with better
    code reusability and organization. In Spark, `Pipeline`s are estimators, whereas
    `PipelineModel`s—fitted `Pipeline`s—are transformers.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将我们的模型应用于测试集，那么我们需要以与训练集相同的方式准备数据（即通过向量组合器）。通常数据准备流水线会有多个步骤，记住应用哪些步骤以及步骤的顺序变得很麻烦。这就是[Pipeline
    API](https://oreil.ly/MG3YM)的动机：您只需指定您希望数据通过的阶段，并按顺序进行处理，Spark会为您处理处理过程。它们提供了更好的代码重用性和组织性。在Spark中，`Pipeline`是估计器，而经过拟合的`PipelineModel`是转换器。
- en: 'Let’s build our pipeline now:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建我们的流水线：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Another advantage of using the Pipeline API is that it determines which stages
    are estimators/transformers for you, so you don’t have to worry about specifying
    `*name*.fit()` versus `*name*.transform()` for each of the stages.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Pipeline API的另一个优点是它会为您确定哪些阶段是估计器/转换器，因此您不必担心为每个阶段指定`*name*.fit()`与`*name*.transform()`。
- en: 'Since `pipelineModel` is a transformer, it is straightforward to apply it to
    our test data set too:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`pipelineModel`是一个转换器，所以将其应用于我们的测试数据集也很简单：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this code we built a model using only a single feature, `bedrooms` (you can
    find the notebook for this chapter in the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2)).
    However, you may want to build a model using all of your features, some of which
    may be categorical, such as `host_is_superhost`. Categorical features take on
    discrete values and have no intrinsic ordering—examples include occupations or
    country names. In the next section we’ll consider a solution for how to treat
    these kinds of variables, known as *one-hot encoding*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们只使用一个特征`bedrooms`来构建模型（您可以在书的[GitHub仓库](https://github.com/databricks/LearningSparkV2)中找到本章的笔记本）。然而，您可能希望使用所有特征构建模型，其中一些可能是分类的，例如`host_is_superhost`。分类特征具有离散值并且没有内在的顺序——例如职业或国家名称。在下一节中，我们将考虑如何处理这些类型的变量的解决方案，称为*独热编码*。
- en: One-hot encoding
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独热编码
- en: In the pipeline we just created, we only had two stages, and our linear regression
    model only used one feature. Let’s take a look at how to build a slightly more
    complex pipeline that incorporates all of our numeric and categorical features.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚创建的流水线中，我们只有两个阶段，我们的线性回归模型只使用了一个特征。让我们看看如何构建一个稍微复杂的流水线，其中包含所有数值和分类特征。
- en: 'Most machine learning models in MLlib expect numerical values as input, represented
    as vectors. To convert categorical values into numeric values, we can use a technique
    called one-hot encoding (OHE). Suppose we have a column called `Animal` and we
    have three types of animals: `Dog`, `Cat`, and `Fish`. We can’t pass the string
    types into our ML model directly, so we need to assign a numeric mapping, such
    as this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib中的大多数机器学习模型都希望输入数值，表示为向量。为了将分类值转换为数值，我们可以使用一种称为独热编码（OHE）的技术。假设我们有一个名为`Animal`的列，有三种动物：`Dog`、`Cat`和`Fish`。我们不能直接将字符串类型传递给我们的ML模型，因此我们需要分配一个数值映射，例如这样：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'However, using this approach we’ve introduced some spurious relationships into
    our data set that weren’t there before. For example, why did we assign `Cat` twice
    the value of `Dog`? The numeric values we use should not introduce any relationships
    into our data set. Instead, we want to create a separate column for every distinct
    value in our `Animal` column:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用这种方法，我们在数据集中引入了一些不存在的虚假关系。例如，为什么我们给`Cat`分配了`Dog`的两倍的值？我们使用的数值不应该在数据集中引入任何关系。相反，我们希望为我们`Animal`列中的每个不同值创建单独的列：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If the animal is a dog, it has a one in the first column and zeros elsewhere.
    If it is a cat, it has a one in the second column and zeros elsewhere. The ordering
    of the columns is irrelevant. If you’ve used pandas before, you’ll note that this
    does the same thing as [`pandas.get_dummies()`](https://oreil.ly/4BsUq).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果动物是狗，则在第一列中有一个`1`，其他位置为`0`。如果是猫，则在第二列中有一个`1`，其他位置为`0`。列的顺序不重要。如果您之前使用过pandas，您会注意到这与[`pandas.get_dummies()`](https://oreil.ly/4BsUq)的功能相同。
- en: 'If we had a zoo of 300 animals, would OHE massively increase consumption of
    memory/compute resources? Not with Spark! Spark internally uses a [`SparseVector`](https://oreil.ly/7rOcC)
    when the majority of the entries are `0`, as is often the case after OHE, so it
    does not waste space storing `0` values. Let’s take a look at an example to better
    understand how `SparseVector`s work:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一只有300只动物的动物园，那么独热编码会大幅增加内存/计算资源的消耗吗？不会，使用Spark！Spark 在大部分条目为`0`（通常在独热编码后如此）时内部使用[`SparseVector`](https://oreil.ly/7rOcC)，因此不会浪费空间存储`0`值。让我们通过一个例子更好地理解`SparseVector`的工作原理：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The [`DenseVector`](https://oreil.ly/F37Ht) in this example contains 10 values,
    all but 2 of which are `0`. To create a `SparseVector`, we need to keep track
    of the size of the vector, the indices of the nonzero elements, and the corresponding
    values at those indices. In this example the size of the vector is 10, there are
    two nonzero values at indices 3 and 5, and the corresponding values at those indices
    are 7 and 2.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，[`DenseVector`](https://oreil.ly/F37Ht) 包含了10个值，除了2个值外其余都是`0`。要创建一个`SparseVector`，我们需要跟踪向量的大小、非零元素的索引以及这些索引处的对应值。在这个例子中，向量的大小是10，索引3和5处有两个非零值，这些索引处的值分别为7和2。
- en: There are a few ways to one-hot encode your data with Spark. A common approach
    is to use the [`StringIndexer`](https://oreil.ly/mqGP6) and [`OneHotEncoder`](https://oreil.ly/D07R0).
    With this approach, the first step is to apply the `StringIndexer` estimator to
    convert categorical values into category indices. These category indices are ordered
    by label frequencies, so the most frequent label gets index 0, which provides
    us with reproducible results across various runs of the same data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以使用Spark对数据进行独热编码。常见的方法是使用[`StringIndexer`](https://oreil.ly/mqGP6)和[`OneHotEncoder`](https://oreil.ly/D07R0)。使用这种方法的第一步是将`StringIndexer`估算器应用于将分类值转换为类别索引。这些类别索引按标签频率排序，因此最常见的标签获得索引0，这使得我们可以在相同数据的多次运行中获得可重复的结果。
- en: Once you have created your category indices, you can pass those as input to
    the `OneHotEncoder` ([`OneHotEncoderEstimator`](https://oreil.ly/SmZTw) if using
    Spark 2.3/2.4). The `OneHotEncoder` maps a column of category indices to a column
    of binary vectors. Take a look at [Table 10-2](#stringindexer_and_onehotencoder_changes)
    to see the differences in the `StringIndexer` and `OneHotEncoder` APIs from Spark
    2.3/2.4 to 3.0.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了类别索引，您可以将其作为输入传递给`OneHotEncoder`（如果使用Spark 2.3/2.4，则为[`OneHotEncoderEstimator`](https://oreil.ly/SmZTw)）。`OneHotEncoder`将类别索引列映射到二进制向量列。查看[表 10-2](#stringindexer_and_onehotencoder_changes)以了解从Spark
    2.3/2.4到3.0中`StringIndexer`和`OneHotEncoder` API的差异。
- en: Table 10-2\. StringIndexer and OneHotEncoder changes in Spark 3.0
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-2\. Spark 3.0中`StringIndexer`和`OneHotEncoder`的更改
- en: '|   | Spark 2.3 and 2.4 | Spark 3.0 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|   | Spark 2.3 和 2.4 | Spark 3.0 |'
- en: '| --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `StringIndexer` | Single column as input/output | Multiple columns as input/output
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| `StringIndexer` | 单列作为输入/输出 | 多列作为输入/输出 |'
- en: '| `OneHotEncoder` | Deprecated | Multiple columns as input/output |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| `OneHotEncoder` | 已弃用 | 多列作为输入/输出 |'
- en: '| `OneHotEncoderEstimator` | Multiple columns as input/output | N/A |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| `OneHotEncoderEstimator` | 多列作为输入/输出 | 不适用 |'
- en: 'The following code demonstrates how to one-hot encode our categorical features.
    In our data set, any column of type `string` is treated as a categorical feature,
    but sometimes you might have numeric features you want treated as categorical
    or vice versa. You’ll need to carefully identify which columns are numeric and
    which are categorical:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码演示了如何对我们的分类特征进行独热编码。在我们的数据集中，任何类型为`string`的列都被视为分类特征，但有时您可能希望将数值特征视为分类特征，反之亦然。您需要仔细识别哪些列是数值型的，哪些是分类的：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now you might be wondering, “How does the `StringIndexer` handle new categories
    that appear in the test data set, but not in the training data set?” There is
    a `handleInvalid` parameter that specifies how you want to handle them. The options
    are `skip` (filter out rows with invalid data), `error` (throw an error), or `keep`
    (put invalid data in a special additional bucket, at index `numLabels`). For this
    example, we just skipped the invalid records.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可能会想，“`StringIndexer`如何处理测试数据集中出现但训练数据集中不存在的新类别？” 这里有一个`handleInvalid`参数，用于指定您想要如何处理它们。选项有`skip`（过滤掉无效数据的行），`error`（抛出错误）或`keep`（将无效数据放入特殊的附加桶中，索引为`numLabels`）。在本例中，我们只跳过了无效记录。
- en: One difficulty with this approach is that you need to tell `StringIndexer` explicitly
    which features should be treated as categorical features. You could use [`VectorIndexer`](https://oreil.ly/tNE1d)
    to automatically detect all the categorical variables, but it is computationally
    expensive as it has to iterate over every single column and detect if it has fewer
    than `maxCategories` distinct values. `maxCategories` is a parameter the user
    specifies, and determining this value can also be difficult.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法的一个困难在于，您需要明确告诉`StringIndexer`哪些特征应被视为分类特征。您可以使用[`VectorIndexer`](https://oreil.ly/tNE1d)自动检测所有分类变量，但这会消耗大量计算资源，因为它必须遍历每一列并检测其是否具有少于`maxCategories`个不同值。`maxCategories`是用户指定的参数，确定该值也可能很困难。
- en: 'Another approach is to use [`RFormula`](https://oreil.ly/Jh7Q9). The syntax
    for this is inspired by the R programming language. With `RFormula`, you provide
    your label and which features you want to include. It supports a limited subset
    of the R operators, including `~`, `.`, `:`, `+`, and `-`. For example, you might
    specify `formula = "y ~ bedrooms + bathrooms"`, which means to predict `y` given
    just `bedrooms` and `bathrooms`, or `formula = "y ~ ."`, which means to use all
    of the available features (and automatically excludes `y` from the features).
    `RFormula` will automatically `StringIndex` and OHE all of your `string` columns,
    convert your numeric columns to `double` type, and combine all of these into a
    single vector using `VectorAssembler` under the hood. Thus, we can replace all
    of the preceding code with a single line, and we will get the same result:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用[`RFormula`](https://oreil.ly/Jh7Q9)。其语法受到R编程语言的启发。使用`RFormula`时，您提供标签和要包含的特征。它支持R语言的有限子集运算符，包括`~`、`.`、`:`、`+`和`-`。例如，您可以指定`formula
    = "y ~ bedrooms + bathrooms"`，这意味着根据`bedrooms`和`bathrooms`预测`y`，或者`formula = "y
    ~ ."`，这意味着使用所有可用特征（并自动排除`y`）。`RFormula`将自动`StringIndex`和OHE所有您的`string`列，将您的数值列转换为`double`类型，并使用`VectorAssembler`将所有这些组合成一个单一向量。因此，我们可以用一行代码替换所有前面的代码，得到相同的结果：
- en: '[PRE21]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The downside of `RFormula` automatically combining the `StringIndexer` and `OneHotEncoder`
    is that one-hot encoding is not required or recommended for all algorithms. For
    example, tree-based algorithms can handle categorical variables directly if you
    just use the `StringIndexer` for the categorical features. You do not need to
    one-hot encode categorical features for tree-based methods, and it will often
    [make your tree-based models worse](https://oreil.ly/xfR-_). Unfortunately, there
    is no one-size-fits-all solution for feature engineering, and the ideal approach
    is closely related to the downstream algorithms you plan to apply to your data
    set.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`RFormula`自动结合`StringIndexer`和`OneHotEncoder`的缺点在于，并非所有算法都需要或建议使用独热编码。例如，基于树的算法可以直接处理分类变量，只需使用`StringIndexer`处理分类特征即可。对于基于树的方法，您不需要对分类特征进行独热编码，通常这样做会使您的基于树的模型表现更差（详情见[这里](https://oreil.ly/xfR-_)）。不幸的是，特征工程并没有一种适合所有情况的解决方案，最佳方法与您计划应用于数据集的下游算法密切相关。'
- en: Note
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If someone else performs the feature engineering for you, make sure they document
    how they generated those features.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人为您执行特征工程，请确保他们记录了如何生成这些特征。
- en: Once you’ve written the code to transform your data set, you can add a linear
    regression model using all of the features as input.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦编写了用于转换数据集的代码，您可以使用所有特征作为输入添加线性回归模型。
- en: 'Here, we put all the feature preparation and model building into the pipeline,
    and apply it to our data set:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将所有特征准备和模型构建放入管道中，并将其应用于我们的数据集：
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As you can see, the features column is represented as a `SparseVector`. There
    are 98 features after one-hot encoding, followed by the nonzero indices and then
    the values themselves. You can see the whole output if you pass in `truncate=False`
    to `show()`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，特征列表示为`SparseVector`。经过独热编码后有98个特征，接着是非零索引，然后是值本身。如果将`truncate=False`传递给`show()`，您可以看到完整的输出。
- en: How is our model performing? You can see that while some of the predictions
    might be considered “close,” others are far off (a negative price for a rental!?).
    Next, we’ll numerically evaluate how well our model performs across our entire
    test set.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型表现如何？您可以看到，尽管某些预测可能被认为“接近”，但其他预测则差距很大（租金为负数！？）。接下来，我们将在整个测试集上数值评估我们模型的表现。
- en: Evaluating Models
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: Now that we have built a model, we need to evaluate how well it performs. In
    `spark.ml` there are classification, regression, clustering, and ranking evaluators
    (introduced in Spark 3.0). Given that this is a regression problem, we will use
    [root-mean-square error (RMSE)](https://oreil.ly/mAQXq) and [*R*²](https://oreil.ly/nE8Cp)
    (pronounced “R-squared”) to evaluate our model’s performance.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了一个模型，我们需要评估其表现。在`spark.ml`中，有分类、回归、聚类和排名评估器（在Spark 3.0中引入）。鉴于这是一个回归问题，我们将使用[均方根误差（RMSE）](https://oreil.ly/mAQXq)和[*R*²](https://oreil.ly/nE8Cp)（发音为“R平方”）来评估我们模型的表现。
- en: RMSE
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RMSE
- en: RMSE is a metric that ranges from zero to infinity. The closer it is to zero,
    the better.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE是一个从零到无穷大的度量。它越接近零，表现越好。
- en: 'Let’s walk through the mathematical formula step by step:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步浏览数学公式：
- en: 'Compute the difference (or error) between the true value *y[i]* and the predicted
    value *ŷ[i]* (pronounced *y*-hat, where the “hat” indicates that it is a predicted
    value of the quantity under the hat):'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算真实值*y[i]*和预测值*ŷ[i]*（发音为*y*-hat，其中“hat”表示它是帽子下的预测值）之间的差异（或误差）：
- en: <math display="block"><mrow><mtext>Error</mtext> <mo>=</mo> <mo>(</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo> <msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow></math>
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>Error</mtext> <mo>=</mo> <mo>(</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo> <msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow></math>
- en: 'Square the difference between *y[i]* and *ŷ[i]* so that our positive and negative
    residuals do not cancel out. This is known as the squared error:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对*y[i]*和*ŷ[i]*之间的差异进行平方，以防止我们的正负残差相互抵消。这称为平方误差：
- en: <math display="block"><mrow><mtext>Square Error (SE)</mtext> <mo>=</mo> <msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>Square Error (SE)</mtext> <mo>=</mo> <msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: 'Then we sum up the squared error for all *n* of our data points, known as the
    sum of squared errors (SSE) or sum of squared residuals:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们对所有*n*个数据点的平方误差求和，称为平方误差和（SSE）或平方残差和：
- en: <math display="block"><mrow><mtext>Sum of Squared Errors (SSE)</mtext> <mo>=</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover>
    <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub> <mo>-</mo><msub><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>Sum of Squared Errors (SSE)</mtext> <mo>=</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover>
    <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub> <mo>-</mo><msub><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>
- en: 'However, the SSE grows with the number of records *n* in the data set, so we
    want to normalize it by the number of records. The gives us the mean-squared error
    (MSE), a very commonly used regression metric:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，SSE随数据集中记录数*n*的增加而增长，因此我们希望通过记录数对其进行归一化。这给出了均方误差（MSE），一个非常常用的回归指标：
- en: <math display="block"><mrow><mtext>Mean Squared Error (MSE)</mtext> <mo>=</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>Mean Squared Error (MSE)</mtext> <mo>=</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: 'If we stop at MSE, then our error term is on the scale of *unit*². We’ll often
    take the square root of the MSE to get the error back on the scale of the original
    unit, which gives us the root-mean-square error (RMSE):'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们停留在MSE，那么我们的误差项在*unit*²的规模上。我们通常会取MSE的平方根，以使误差回到原始单位的尺度上，这就是均方根误差（RMSE）：
- en: <math display="block"><mrow><mtext>Root Mean Squared Error (RMSE)</mtext> <mo>=</mo>
    <msqrt><mrow><mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>Root Mean Squared Error (RMSE)</mtext> <mo>=</mo>
    <msqrt><mrow><mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
- en: 'Let’s evaluate our model using RMSE:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用RMSE来评估我们的模型：
- en: '[PRE25]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This produces the following output:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出：
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Interpreting the value of RMSE
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解释RMSE的值
- en: So how do we know if 220.6 is a good value for the RMSE? There are various ways
    to interpret this value, one of which is to build a simple baseline model and
    compute its RMSE to compare against. A common baseline model for regression tasks
    is to compute the average value of the label on the training set *ȳ* (pronounced
    *y*-bar), then predict *ȳ* for every record in the test data set and compute the
    resulting RMSE (example code is available in the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2)).
    If you try this, you will see that our baseline model has an RMSE of 240.7, so
    we beat our baseline. If you don’t beat the baseline, then something probably
    went wrong in your model building process.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何知道220.6是RMSE的一个好值？有多种方法可以解释这个值，其中一种方法是构建一个简单的基线模型并计算其RMSE以进行比较。回归任务的一个常见基线模型是计算训练集标签的平均值*ȳ*（读作*y*-bar），然后为测试数据集中的每条记录预测*ȳ*并计算结果的RMSE（示例代码可在该书的[GitHub
    repo](https://github.com/databricks/LearningSparkV2)找到）。如果您尝试这样做，您会看到我们的基线模型具有240.7的RMSE，所以我们打败了基线。如果您没有击败基线，则可能是在建模过程中出了问题。
- en: Note
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If this were a classification problem, you might want to predict the most prevalent
    class as your baseline model.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是一个分类问题，您可能希望预测最普遍的类作为您的基线模型。
- en: Keep in mind that the unit of your label directly impacts your RMSE. For example,
    if your label is height, then your RMSE will be higher if you use centimeters
    rather than meters as your unit of measurement. You could arbitrarily decrease
    the RMSE by using a different unit, which is why it is important to compare your
    RMSE against a baseline.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，您标签的单位直接影响您的RMSE。例如，如果您的标签是高度，则如果您使用厘米而不是米作为您的测量单位，您的RMSE将更高。您可以通过使用不同的单位任意减少RMSE，这就是为什么比较RMSE与基线非常重要。
- en: There are also some metrics that naturally give you an intuition of how you
    are performing against a baseline, such as *R*², which we discuss next.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些指标自然地让您直觉地了解您与基准的表现如何，例如*R*²，接下来我们将讨论它。
- en: R²
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: R²
- en: 'Despite the name *R*² containing “squared,” *R*² values range from negative
    infinity to 1\. Let’s take a look at the math behind this metric. *R*² is computed
    as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名为*R*²的名称包含“平方”，*R*²值的范围从负无穷到1。让我们来看看这一度量背后的数学。*R*²的计算如下：
- en: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mn>1</mn>
    <mo>-</mo> <mfrac><mrow><mi>S</mi><msub><mi>S</mi> <mrow><mi>r</mi><mi>e</mi><mi>s</mi></mrow></msub></mrow>
    <mrow><mi>S</mi><msub><mi>S</mi> <mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow></msub></mrow></mfrac></mrow></math>
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mn>1</mn>
    <mo>-</mo> <mfrac><mrow><mi>S</mi><msub><mi>S</mi> <mrow><mi>r</mi><mi>e</mi><mi>s</mi></mrow></msub></mrow>
    <mrow><mi>S</mi><msub><mi>S</mi> <mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow></msub></mrow></mfrac></mrow></math>
- en: 'where *SS[tot]* is the total sum of squares if you always predict *ȳ*:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*SS[tot]*是如果您始终预测*ȳ*的总平方和：
- en: <math display="block"><mrow><mi>S</mi> <msub><mi>S</mi> <mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>S</mi> <msub><mi>S</mi> <mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>
- en: 'and *SS[res]* is the sum of residuals squared from your model predictions (also
    known as the sum of squared errors, which we used to compute the RMSE):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 而*SS[res]*是从您的模型预测中残差平方和（也称为平方误差和，我们用它来计算RMSE）：
- en: <math display="block"><mrow><mi>S</mi> <msub><mi>S</mi> <mrow><mi>r</mi><mi>e</mi><mi>s</mi></mrow></msub>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>S</mi> <msub><mi>S</mi> <mrow><mi>r</mi><mi>e</mi><mi>s</mi></mrow></msub>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: If your model perfectly predicts every data point, then your *SS[res]* = 0,
    making your *R*² = 1\. And if your *SS[res]* = *SS[tot]*, then the fraction is
    1/1, so your *R*² is 0\. This is what happens if your model performs the same
    as always predicting the average value, *ȳ*.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型完美预测每个数据点，那么您的*SS[res]* = 0，使您的*R*² = 1。如果您的*SS[res]* = *SS[tot]*，那么分数是1/1，因此您的*R*²为0。这就是如果您的模型表现与始终预测平均值*ȳ*相同会发生的情况。
- en: But what if your model performs worse than always predicting *ȳ* and your *SS[res]*
    is really large? Then your *R*² can actually be negative! If your *R*² is negative,
    you should reevaluate your modeling process. The nice thing about using *R*² is
    that you don’t necessarily need to define a baseline model to compare against.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果您的模型表现比始终预测*ȳ*还要差，而且您的*SS[res]*非常大怎么办？那么您的*R*²实际上可以是负的！如果您的*R*²为负数，您应该重新评估您的建模过程。使用*R*²的好处是您不一定需要定义一个基线模型进行比较。
- en: 'If we want to change our regression evaluator to use *R*², instead of redefining
    the regression evaluator, we can set the metric name using the setter property:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要更改我们的回归评估器以使用*R*²，而不是重新定义回归评估器，我们可以使用设置器属性来设置指标名称：
- en: '[PRE28]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为：
- en: '[PRE30]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Our *R*² is positive, but it’s very close to 0\. One of the reasons why our
    model is not performing too well is because our label, `price`, appears to be
    [log-normally distributed](https://oreil.ly/0Patq). If a distribution is log-normal,
    it means that if we take the logarithm of the value, the result looks like a normal
    distribution. Price is often log-normally distributed. If you think about rental
    prices in San Francisco, most cost around $200 per night, but there are some that
    rent for thousands of dollars a night! You can see the distribution of our Airbnb
    prices for our training Dataset in [Figure 10-7](#san_francisco_housing_price_distribution).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 *R*² 是正值，但非常接近 0\. 我们的模型表现不佳的一个原因是，我们的标签 `price` 看起来呈现出[对数正态分布](https://oreil.ly/0Patq)。如果一个分布是对数正态的，这意味着如果我们取该值的对数，结果看起来像正态分布。价格通常是对数正态分布的。如果您考虑一下旧金山的租金价格，大多数每晚约为
    200 美元，但有些则每晚租金高达数千美元！您可以在我们的训练数据集中查看 Airbnb 价格的分布，见[图 10-7](#san_francisco_housing_price_distribution)。
- en: '![San Francisco housing price distribution](assets/lesp_1007.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![旧金山房屋价格分布](assets/lesp_1007.png)'
- en: Figure 10-7\. San Francisco housing price distribution
  id: totrans-186
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-7\. 旧金山房屋价格分布
- en: Let’s take a look at the resulting distribution if we instead look at the log
    of the price ([Figure 10-8](#san_francisco_housing_log_price_distribu)).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如果我们查看价格的对数而不是价格本身的分布情况（[图 10-8](#san_francisco_housing_log_price_distribu)）。
- en: '![San Francisco housing log-price distribution](assets/lesp_1008.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![旧金山房屋对数价格分布](assets/lesp_1008.png)'
- en: Figure 10-8\. San Francisco housing log-price distribution
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. 旧金山房屋对数价格分布
- en: You can see here that our log-price distribution looks a bit more like a normal
    distribution. As an exercise, try building a model to predict price on the log
    scale, then exponentiate the prediction to get it out of log scale and evaluate
    your model. The code can also be found in this chapter’s notebook in the book’s
    [GitHub repo](https://github.com/databricks/LearningSparkV2). You should see that
    your RMSE decreases and your *R*² increases for this data set.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们的对数价格分布看起来更像正态分布。作为练习，尝试构建一个模型来预测对数尺度上的价格，然后指数化预测以将其从对数尺度转换出来，并评估您的模型。代码也可以在本章的笔记本以及书籍的
    [GitHub 仓库](https://github.com/databricks/LearningSparkV2) 中找到。您应该会发现，对于这个数据集，您的
    RMSE 减小，*R*² 增加。
- en: Saving and Loading Models
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存和加载模型
- en: 'Now that we have built and evaluated a model, let’s save it to persistent storage
    for reuse later (or in the event that our cluster goes down, we don’t have to
    recompute the model). Saving models is very similar to writing DataFrames—the
    API is `model.write().save(*path*)`. You can optionally provide the `overwrite()`
    command to overwrite any data contained in that path:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建并评估了一个模型，让我们将其保存到持久存储中以便以后重用（或者在我们的集群崩溃时，我们不需要重新计算模型）。保存模型与编写 DataFrame
    非常相似——API 是 `model.write().save(*path*)`。您可以选择使用 `overwrite()` 命令来覆盖该路径中包含的任何数据：
- en: '[PRE31]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'When you load your saved models, you need to specify the type of model you
    are loading back in (e.g., was it a `LinearRegressionModel` or a `LogisticRegressionModel`?).
    For this reason, we recommend you always put your transformers/estimators into
    a `Pipeline`, so that for all your models you load a `PipelineModel` and only
    need to change the file path to the model:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当您加载已保存的模型时，您需要指定要重新加载的模型类型（例如，是 `LinearRegressionModel` 还是 `LogisticRegressionModel`？）。出于这个原因，我们建议您始终将您的转换器/估计器放入
    `Pipeline` 中，这样对于所有加载的模型，您只需加载一个 `PipelineModel` 并且只需更改模型的文件路径：
- en: '[PRE33]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: After loading, you can apply it to new data points. However, you can’t use the
    weights from this model as initialization parameters for training a new model
    (as opposed to starting with random weights), as Spark has no concept of “warm
    starts.” If your data set changes slightly, you’ll have to retrain the entire
    linear regression model from scratch.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 加载完成后，您可以将其应用于新的数据点。但是，您不能使用此模型的权重作为初始化参数来训练新模型（而不是从随机权重开始），因为 Spark 没有“热启动”的概念。如果您的数据集略有变化，您将不得不从头开始重新训练整个线性回归模型。
- en: With our linear regression model built and evaluated, let’s explore how a few
    other models perform on our data set. In the next section, we will explore tree-based
    models and look at some common hyperparameters to tune in order to improve model
    performance.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建并评估了线性回归模型之后，让我们看看我们的数据集上其他几种模型的表现。在下一节中，我们将探讨基于树的模型，并查看一些常见的超参数，以调整模型性能。
- en: Hyperparameter Tuning
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调优
- en: When data scientists talk about tuning their models, they often discuss tuning
    hyperparameters to improve the model’s predictive power. A *hyperparameter* is
    an attribute that you define about the model prior to training, and it is not
    learned during the training process (not to be confused with parameters, which
    *are* learned in the training process). The number of trees in your random forest
    is an example of a hyperparameter.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据科学家讨论调整模型时，他们经常讨论调整超参数以提高模型的预测能力。*超参数*是在训练之前定义的关于模型的属性，并且在训练过程中不会学习（不要与在训练过程中学习的*参数*混淆）。你随机森林中的树的数量就是一个超参数的例子。
- en: In this section, we will focus on using tree-based models as an example for
    hyperparameter tuning procedures, but the same concepts apply to other models
    as well. Once we set up the mechanics to do hyperparameter tuning with `spark.ml`,
    we will discuss ways to optimize the pipeline. Let’s get started with a brief
    introduction to decision trees, followed by how we can use them in `spark.ml`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点讨论使用基于树的模型作为超参数调整程序的示例，但这些概念同样适用于其他模型。一旦我们设置好使用`spark.ml`进行超参数调整的机制，我们将讨论如何优化流程。让我们从决策树的简要介绍开始，然后介绍如何在`spark.ml`中使用它们。
- en: Tree-Based Models
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于树的模型
- en: Tree-based models such as decision trees, gradient boosted trees, and random
    forests are relatively simple yet powerful models that are easy to interpret (meaning,
    it is easy to explain the predictions they make). Hence, they’re quite popular
    for machine learning tasks. We’ll get to random forests shortly, but first we
    need to cover the fundamentals of decision trees.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树等基于树的模型，如决策树、梯度提升树和随机森林，是相对简单而强大的模型，易于解释（也就是说，易于解释它们所做的预测）。因此，它们在机器学习任务中非常流行。我们很快就会讨论随机森林，但首先我们需要掌握决策树的基础知识。
- en: Decision trees
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树
- en: As an off-the-shelf solution, decision trees are well suited to data mining.
    They are relatively fast to build, highly interpretable, and scale-invariant (i.e.,
    standardizing or scaling the numeric features does not change the performance
    of the tree). So what is a decision tree?
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 作为现成的解决方案，决策树非常适合数据挖掘。它们相对快速建立，高度可解释，并且与缩放无关（即，标准化或缩放数值特征不会改变树的性能）。那么，什么是决策树呢？
- en: A decision tree is a series of if-then-else rules learned from your data for
    classification or regression tasks. Suppose we are trying to build a model to
    predict whether or not someone will accept a job offer, and the features comprise
    salary, commute time, free coffee, etc. If we fit a decision tree to this data
    set, we might get a model that looks like [Figure 10-9](#decision_tree_example).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是从数据中学习的一系列if-then-else规则，用于分类或回归任务。假设我们试图建立一个模型来预测某人是否接受工作提议，特征包括工资、通勤时间、免费咖啡等等。如果我们将决策树拟合到这个数据集，我们可能会得到一个看起来像[图
    10-9](#decision_tree_example)的模型。
- en: '![Decision tree example](assets/lesp_1009.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![Decision tree example](assets/lesp_1009.png)'
- en: Figure 10-9\. Decision tree example
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-9\. 决策树示例
- en: The node at the top of the tree is called the “root” of the tree because it’s
    the first feature that we “split” on. This feature should give the most informative
    split—in this case, if the salary is less than $50,000, then the majority of candidates
    will decline the job offer. The “Decline offer” node is known as a “leaf node”
    as there are no other splits coming out of that node; it’s at the end of a branch.
    (Yes, it’s a bit funny that we call it a decision “tree” but draw the root of
    the tree at the top and the leaves at the bottom!)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 树的顶部节点称为树的“根”，因为它是我们“分割”的第一个特征。这个特征应该提供最具信息量的分割——在这种情况下，如果工资低于$50,000，那么大多数候选人会拒绝工作提议。“拒绝提议”节点被称为“叶子节点”，因为在该节点没有其他分割出来；它在一个分支的末端。（是的，我们称之为决策“树”，但是把树的根画在顶部，把叶子画在底部确实有点有趣！）
- en: However, if the salary offered is greater than $50,000, we proceed to the next
    most informative feature in the decision tree, which in this case is the commute
    time. Even if the salary is over $50,000, if the commute is longer than one hour,
    then the majority of people will decline the job offer.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果提供的工资高于$50,000，我们会继续处理决策树中下一个最具信息量的特征，即通勤时间。即使工资超过$50,000，如果通勤时间超过一小时，那么大多数人也会拒绝工作提议。
- en: Note
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We won’t get into the details of how to determine which features will give you
    the highest information gain here, but if you’re interested, check out Chapter
    9 of [*The Elements of Statistical Learning*](https://oreil.ly/VHVOW), by Trevor
    Hastie, Robert Tibshirani, and Jerome Friedman (Springer).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里详细讨论如何确定哪些特征将为您提供最高的信息增益，但如果您感兴趣，可以查阅《[*统计学习的要素*](https://oreil.ly/VHVOW)》第9章，作者是Trevor
    Hastie，Robert Tibshirani和Jerome Friedman（Springer）。
- en: The final feature in our model is free coffee. In this case the decision tree
    shows that if the salary is greater than $50,000, the commute is less than an
    hour, and there is free coffee, then the majority of people will accept our job
    offer (if only it were that simple!). As a follow-up resource, [R2D3](https://oreil.ly/uKD8q)
    has a great visualization of how decision trees work.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型中的最终特征是免费咖啡。在这种情况下，决策树显示，如果薪水超过$50,000，通勤时间少于一小时，并且有免费咖啡，那么大多数人将接受我们的工作提议（如果事情真的那么简单的话！）。作为后续资源，[R2D3](https://oreil.ly/uKD8q)提供了关于决策树工作原理的出色可视化。
- en: Note
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It is possible to split on the same feature multiple times in a single decision
    tree, but each split will occur at a different value.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 单个决策树中可能会在同一特征上多次分裂，但每次分裂将发生在不同的值上。
- en: The *depth* of a decision tree is the longest path from the root node to any
    given leaf node. In [Figure 10-9](#decision_tree_example), the depth is three.
    Trees that are very deep are prone to overfitting, or memorizing noise in your
    training data set, but trees that are too shallow will underfit to your data set
    (i.e., could have picked up more signal from the data).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的*深度*是从根节点到任何给定叶节点的最长路径。在[图 10-9](#decision_tree_example)中，深度为三。非常深的树容易过拟合，即在训练数据集中记住噪音，但是过于浅的树将对数据集欠拟合（即可能没有从数据中获取更多信号）。
- en: With the essence of a decision tree explained, let’s resume the topic of feature
    preparation for decision trees. For decision trees, you don’t have to worry about
    standardizing or scaling your input features, because this has no impact on the
    splits—but you do have to be careful about how you prepare your categorical features.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 解释了决策树的本质后，让我们重新回到为决策树准备特征的话题。对于决策树，您无需担心标准化或缩放输入特征，因为这对分裂没有影响，但是您必须注意如何准备分类特征。
- en: 'Tree-based methods can naturally handle categorical variables. In `spark.ml`,
    you just need to pass the categorical columns to the `StringIndexer`, and the
    decision tree can take care of the rest. Let’s fit a decision tree to our data
    set:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的方法可以自然地处理分类变量。在`spark.ml`中，您只需将分类列传递给`StringIndexer`，决策树就可以处理其余的部分。让我们将一个决策树拟合到我们的数据集中：
- en: '[PRE35]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This produces the following error:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下错误：
- en: '[PRE37]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We can see that there is an issue with the `maxBins` parameter. What does that
    parameter do? `maxBins` determines the number of bins into which your continuous
    features are discretized, or split. This discretization step is crucial for performing
    distributed training. There is no `maxBins` parameter in `scikit-learn` because
    all of the data and the model reside on a single machine. In Spark, however, workers
    have all the columns of the data, but only a subset of the rows. Thus, when communicating
    about which features and values to split on, we need to be sure they’re all talking
    about the same split values, which we get from the common discretization set up
    at training time. Let’s take a look at [Figure 10-10](#planet_implementation_of_distributed_dec),
    which shows the [PLANET](https://oreil.ly/a0teT) implementation of distributed
    decision trees, to get a better understanding of distributed machine learning
    and illustrate the `maxBins` parameter.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`maxBins`参数存在问题。该参数是做什么用的？`maxBins`确定连续特征分成的箱数或分裂数。这个离散化步骤对执行分布式训练至关重要。在`scikit-learn`中没有`maxBins`参数，因为所有数据和模型都驻留在单个机器上。然而，在Spark中，工作节点具有数据的所有列，但只有数据的部分行。因此，在通信关于分裂值的特征和值时，我们需要确保它们都在训练时从相同的离散化设置中获取。让我们看一下[图 10-10](#planet_implementation_of_distributed_dec)，展示了[PLANET](https://oreil.ly/a0teT)分布式决策树的实现，以更好地理解分布式机器学习并说明`maxBins`参数。
- en: '![PLANET implementation of distributed decision trees (source: https://oreil.ly/RAvvP)](assets/lesp_1010.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![PLANET implementation of distributed decision trees (source: https://oreil.ly/RAvvP)](assets/lesp_1010.png)'
- en: 'Figure 10-10\. PLANET implementation of distributed decision trees (source:
    [*https://oreil.ly/RAvvP*](https://oreil.ly/RAvvP))'
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-10\. PLANET分布式决策树实现（来源：[*https://oreil.ly/RAvvP*](https://oreil.ly/RAvvP)）
- en: 'Every worker has to compute summary statistics for every feature and every
    possible split point, and those statistics will be aggregated across the workers.
    MLlib requires `maxBins` to be large enough to handle the discretization of the
    categorical columns. The default value for `maxBins` is `32`, and we had a categorical
    column with 36 distinct values, which is why we got the error earlier. While we
    could increase `maxBins` to `64` to more accurately represent our continuous features,
    that would double the number of possible splits for continuous variables, greatly
    increasing our computation time. Let’s instead set `maxBins` to be `40` and retrain
    the pipeline. You’ll notice here that we are using the setter method `setMaxBins()`
    to modify the decision tree rather than redefining it completely:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 每个工作进程都必须计算每个特征和每个可能分割点的汇总统计信息，并且这些统计信息将在工作进程之间进行聚合。MLlib 要求 `maxBins` 要足够大，以处理分类列的离散化。`maxBins`
    的默认值为 `32`，而我们有一个具有 36 个不同值的分类列，这就是我们之前遇到错误的原因。虽然我们可以将 `maxBins` 增加到 `64`，以更准确地表示我们的连续特征，但这将使连续变量的可能分割数翻倍，大大增加计算时间。让我们将
    `maxBins` 设置为 `40`，然后重新训练管道。在这里，您会注意到我们使用 `setMaxBins()` 设置器方法来修改决策树，而不是完全重新定义它：
- en: '[PRE38]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: Due to differences in implementation, oftentimes you won’t get exactly the same
    results when building a model with `scikit-learn` versus MLlib. However, that’s
    OK. The key is to understand why they are different, and to see what parameters
    are in your control to get them to perform the way you need them to. If you are
    porting workloads over from `scikit-learn` to MLlib, we encourage you to take
    a look at the [`spark.ml`](https://oreil.ly/qFgc5) and [`scikit-learn`](https://scikit-learn.org/stable)
    documentation to see what parameters differ, and to tweak those parameters to
    get comparable results for the same data. Once the values are close enough, you
    can scale up your MLlib model to larger data sizes that `scikit-learn` can’t handle.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 由于实现上的差异，当使用 `scikit-learn` 与 MLlib 构建模型时，结果通常不会完全相同。然而，这没有关系。关键是理解它们之间的差异，并查看哪些参数在您的控制之下，以使它们表现出您需要的方式。如果您正在从
    `scikit-learn` 迁移工作负载到 MLlib，请查看 [`spark.ml`](https://oreil.ly/qFgc5) 和 [`scikit-learn`](https://scikit-learn.org/stable)
    文档，了解不同的参数，并调整这些参数以获得相同数据的可比较结果。一旦数值足够接近，您可以将您的 MLlib 模型扩展到 `scikit-learn` 无法处理的更大数据规模。
- en: 'Now that we have successfully built our model, we can extract the if-then-else
    rules learned by the decision tree:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功构建了我们的模型，我们可以提取决策树学到的 if-then-else 规则：
- en: '[PRE40]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This is just a subset of the printout, but you’ll notice that it’s possible
    to split on the same feature more than once (e.g., feature 12), but at different
    split values. Also notice the difference between how the decision tree splits
    on numeric features versus categorical features: for numeric features it checks
    if the value is less than or equal to the threshold, and for categorical features
    it checks if the value is in that set or not.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是打印输出的一个子集，但您会注意到可以多次在同一特征上分割（例如特征 12），但在不同的分割值上。还请注意决策树在数值特征和分类特征上分割的不同之处：对于数值特征，它检查值是否小于或等于阈值，而对于分类特征，则检查值是否在该集合中。
- en: 'We can also extract the feature importance scores from our model to see the
    most important features:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从模型中提取特征重要性分数，以查看最重要的特征：
- en: '[PRE42]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '| Feature | Importance |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| Feature | Importance |'
- en: '| --- | --- |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| bedrooms | 0.283406 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| bedrooms | 0.283406 |'
- en: '| cancellation_policyIndex | 0.167893 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| cancellation_policyIndex | 0.167893 |'
- en: '| instant_bookableIndex | 0.140081 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| instant_bookableIndex | 0.140081 |'
- en: '| property_typeIndex | 0.128179 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| property_typeIndex | 0.128179 |'
- en: '| number_of_reviews | 0.126233 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| number_of_reviews | 0.126233 |'
- en: '| neighbourhood_cleansedIndex | 0.056200 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| neighbourhood_cleansedIndex | 0.056200 |'
- en: '| longitude | 0.038810 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| longitude | 0.038810 |'
- en: '| minimum_nights | 0.029473 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| minimum_nights | 0.029473 |'
- en: '| beds | 0.015218 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| beds | 0.015218 |'
- en: '| room_typeIndex | 0.010905 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| room_typeIndex | 0.010905 |'
- en: '| accommodates | 0.003603 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| accommodates | 0.003603 |'
- en: While decision trees are very flexible and easy to use, they are not always
    the most accurate model. If we were to compute our *R*² on the test data set,
    we would actually get a negative score! That’s worse than just predicting the
    average. (You can see this in this chapter’s notebook in the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2).)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然决策树非常灵活且易于使用，但并不总是最精确的模型。如果我们在测试数据集上计算我们的 *R*²，实际上会得到一个负分数！这比仅预测平均值更糟糕。（您可以在本章的笔记本中查看本书的
    [GitHub 仓库](https://github.com/databricks/LearningSparkV2) 中的具体情况。）
- en: 'Let’s look at improving this model by using an *ensemble* approach that combines
    different models to achieve a better result: random forests.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用*集成*方法来改进这个模型，这种方法结合了不同的模型以获得更好的结果：随机森林。
- en: Random forests
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: '[Ensembles](https://oreil.ly/DoQPU) work by taking a democratic approach. Imagine
    there are many M&Ms in a jar. You ask one hundred people to guess the number of
    M&Ms, and then take the average of all the guesses. The average is probably closer
    to the true value than most of the individual guesses. That same concept applies
    to machine learning models. If you build many models and combine/average their
    predictions, they will be more robust than those produced by any individual model.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[集成](https://oreil.ly/DoQPU)采用民主的方法。想象一下罐子里有很多颗M&M巧克力。你请一百个人猜猜M&M的数量，然后取所有猜测的平均值。平均值可能比大多数个体猜测更接近真实值。这个概念同样适用于机器学习模型。如果建立多个模型并组合/平均它们的预测结果，它们将比任何单个模型更加健壮。'
- en: '[*Random forests*](https://oreil.ly/kpfTc) are an ensemble of decision trees
    with two key tweaks:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[*随机森林*](https://oreil.ly/kpfTc)是由决策树组成的集成学习方法，具有两个关键调整：'
- en: Bootstrapping samples by rows
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 行的自助抽样
- en: '*Bootstrapping* is a technique for simulating new data by sampling with replacement
    from your original data. Each decision tree is trained on a different bootstrap
    sample of your data set, which produces slightly different decision trees, and
    then you aggregate their predictions. This technique is known as [*bootstrap aggregating*](https://oreil.ly/CfWIe),
    or *bagging*. In a typical random forest implementation, each tree samples the
    same number of data points with replacement from the original data set, and that
    number can be controlled through the `subsamplingRate` parameter.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '*自助法*是通过从原始数据中有放回地抽样来模拟新数据的技术。每棵决策树都是在数据集的不同自助样本上训练的，这会产生略有不同的决策树，然后汇总它们的预测结果。这种技术被称为[*自助聚合*](https://oreil.ly/CfWIe)，或者称为*装袋*。在典型的随机森林实现中，每棵树都从原始数据集中有放回地抽取相同数量的数据点进行样本，而这个数量可以通过参数`subsamplingRate`来控制。'
- en: Random feature selection by columns
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 列的随机特征选择
- en: 'The main drawback with bagging is that the trees are all highly correlated,
    and thus learn similar patterns in your data. To mitigate this problem, each time
    you want to make a split you only consider a random subset of the columns (1/3
    of the features for `RandomForestRegressor` and <math display="inline"><msqrt><mo>#</mo><mtext>features</mtext></msqrt></math>
    for `RandomForestClassifier`). Due to this randomness you introduce, you typically
    want each tree to be quite shallow. You might be thinking: each of these trees
    will perform worse than any single decision tree, so how could this approach possibly
    be better? It turns out that each of the trees learns something different about
    your data set, and combining this collection of “weak” learners into an ensemble
    makes the forest much more robust than a single decision tree.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋的主要缺点在于所有树都高度相关，因此在数据中学习相似的模式。为了减轻这个问题，每次想要进行分裂时只考虑列的随机子集（对于`RandomForestRegressor`为特征数的1/3，对于`RandomForestClassifier`为特征数的<math
    display="inline"><msqrt><mo>#</mo><mtext>features</mtext></msqrt></math>）。由于引入了这种随机性，你通常希望每棵树都相对较浅。你可能会想：每棵树都会比任何单个决策树表现得更差，那么这种方法怎么可能更好呢？事实证明，这些树各自从你的数据集中学到了不同的东西，将这些“弱”学习者组合成一个集成使得随机森林比单个决策树更加健壮。
- en: '[Figure 10-11](#random_forest_training) illustrates a random forest at training
    time. At each split, it considers 3 of the 10 original features to split on; finally,
    it picks the best from among those.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-11](#random_forest_training)说明了随机森林在训练时的情况。在每次分裂时，它考虑原始特征中的3个来进行分裂；最终选择其中最佳的。'
- en: '![Random forest training](assets/lesp_1011.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林训练](assets/lesp_1011.png)'
- en: Figure 10-11\. Random forest training
  id: totrans-263
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-11\. 随机森林训练
- en: 'The APIs for random forests and decision trees are similar, and both can be
    applied to regression or classification tasks:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林和决策树的API类似，都可以应用于回归或分类任务：
- en: '[PRE44]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Once you’ve trained your random forest, you can pass new data points through
    the different trees trained in the ensemble.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练了随机森林，就可以将新数据点通过集成中训练的不同树。
- en: As [Figure 10-12](#random_forest_predictions) shows, if you build a random forest
    for classification, it passes the test point through each of the trees in the
    forest and takes a majority vote among the predictions of the individual trees.
    (By contrast, in regression, the random forest simply averages those predictions.)
    Even though each of these trees is less performant than any individual decision
    tree, the collection (or ensemble) actually provides a more robust model.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 10-12](#random_forest_predictions)所示，如果你构建一个用于分类的随机森林，它会通过森林中的每棵树传递测试点，并对每棵树的预测结果进行多数投票。（相比之下，在回归中，随机森林仅仅是对这些预测进行了平均。）尽管这些树中的每一棵都比任何单独的决策树表现都要差，但集合（或整体）实际上提供了一个更为健壮的模型。
- en: '![Random forest predictions](assets/lesp_1012.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林预测](assets/lesp_1012.png)'
- en: Figure 10-12\. Random forest predictions
  id: totrans-270
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-12\. 随机森林预测
- en: Random forests truly demonstrate the power of distributed machine learning with
    Spark, as each tree can be built independently of the other trees (e.g., you do
    not need to build tree 3 before you build tree 10). Furthermore, within each level
    of the tree, you can parallelize the work to find the optimal splits.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林真正展示了使用Spark进行分布式机器学习的威力，因为每棵树都可以独立构建（例如，在构建第10棵树之前，你不需要先构建第3棵树）。此外，在树的每个层级内，你可以并行处理以找到最优的分割点。
- en: So how do we determine what the optimal number of trees in our random forest
    or the max depth of those trees should be? This process is called [*hyperparameter
    tuning*](https://oreil.ly/SqPGA). In contrast to a parameter, a hyperparameter
    is a value that controls the learning process or structure of your model, and
    it is not learned during training. Both the number of trees and the max depth
    are examples of hyperparameters you can tune for random forests. Let’s now shift
    our focus to how we can discover and evaluate the best random forest model by
    tuning some hyperparameters.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何确定我们随机森林中的树的最佳数量或这些树的最大深度应该是多少呢？这个过程被称为[*超参数调优*](https://oreil.ly/SqPGA)。与参数不同，超参数是控制学习过程或模型结构的值，在训练过程中不会被学习。树的数量和最大深度都是你可以为随机森林调整的超参数的例子。现在让我们把焦点转移到如何通过调整一些超参数来发现和评估最佳的随机森林模型。
- en: k-Fold Cross-Validation
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k折交叉验证
- en: 'Which data set should we use to determine the optimal hyperparameter values?
    If we use the training set, then the model is likely to overfit, or memorize the
    nuances of our training data. This means it will be less likely to generalize
    to unseen data. But if we use the test set, then that will no longer represent
    “unseen” data, so we won’t be able to use it to verify how well our model generalizes.
    Thus, we need another data set to help us determine the optimal hyperparameters:
    the *validation* data set.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该使用哪个数据集来确定最佳超参数值呢？如果我们使用训练集，那么模型可能会过拟合，即记住我们训练数据的细微差别。这意味着它很可能不能很好地推广到未见过的数据。但是如果我们使用测试集，那么它就不再代表“未见过”的数据，因此我们不能用它来验证模型的泛化能力。因此，我们需要另一个数据集来帮助我们确定最佳的超参数：*验证*
    数据集。
- en: For example, instead of splitting our data into an 80/20 train/test split, as
    we did earlier, we can do a 60/20/20 split to generate training, validation, and
    test data sets, respectively. We can then build our model on the training set,
    evaluate performance on the validation set to select the best hyperparameter configuration,
    and apply the model to the test set to see how well it performs on new data. However,
    one of the downsides of this approach is that we lose 25% of our training data
    (80% -> 60%), which could have been used to help improve the model. This motivates
    the use of the *k-fold cross-validation* technique to solve this problem.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将数据划分为60/20/20的训练、验证和测试数据集，而不是之前的80/20划分。然后我们可以在训练集上构建我们的模型，在验证集上评估性能以选择最佳的超参数配置，并在测试集上应用模型以查看它在新数据上的表现。然而，这种方法的一个缺点是我们失去了25%的训练数据（80%
    -> 60%），这些数据本来可以帮助改进模型。这促使我们使用*k折交叉验证*技术来解决这个问题。
- en: With this approach, instead of splitting the data set into separate training,
    validation, and test sets, we split it into training and test sets as before—but
    we use the training data for both training and validation. To accomplish this,
    we split our training data into *k* subsets, or “folds” (e.g., three). Then, for
    a given hyperparameter configuration, we train our model on *k–*1 folds and evaluate
    on the remaining fold, repeating this process *k* times. [Figure 10-13](#k_fold_cross_validation)
    illustrates this approach.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法，我们不是将数据集分割为单独的训练、验证和测试集，而是将其分割为与以前相同的训练和测试集，但是我们使用训练数据进行训练和验证。为此，我们将我们的训练数据分成*k*个子集或“折叠”（例如三个）。然后，对于给定的超参数配置，我们在*k-1*个折叠上训练我们的模型，并在剩余的一个折叠上评估，重复这个过程*k*次。[图10-13](#k_fold_cross_validation)说明了这种方法。
- en: '![k-fold cross validation](assets/lesp_1013.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![k折交叉验证](assets/lesp_1013.png)'
- en: Figure 10-13\. k-fold cross-validation
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-13。k折交叉验证
- en: As this figure shows, if we split our data into three folds, our model is first
    trained on the first and second folds (or splits) of the data, and evaluated on
    the third fold. We then build the same model with the same hyperparameters on
    the first and third folds of the data, and evaluate its performance on the second
    fold. Lastly, we build the model on the second and third folds and evaluate it
    on the first fold. We then average the performance of those three (or *k*) validation
    data sets as a proxy of how well this model will perform on unseen data, as every
    data point had the chance to be part of the validation data set exactly once.
    Next, we repeat this process for all of our different hyperparameter configurations
    to identify the optimal one.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本图所示，如果我们将数据分成三折，我们的模型首先在数据的第一和第二折（或分割）上进行训练，并在第三折上进行评估。然后，我们使用相同的超参数在数据的第一和第三折上构建相同的模型，并在第二折上评估其性能。最后，我们在第二和第三折上构建模型，并在第一折上评估它。然后，我们对这三个（或*k*）验证数据集的性能取平均值，作为这个模型在未见数据上表现如何的代理，因为每个数据点有机会恰好成为验证数据集的一部分一次。接下来，我们针对所有不同的超参数配置重复此过程，以确定最佳的配置。
- en: Determining the search space of your hyperparameters can be difficult, and often
    doing a random search of hyperparameters [outperforms a structured grid search](https://oreil.ly/gI7G-).
    There are specialized libraries, such as [Hyperopt](http://hyperopt.github.io/hyperopt),
    to help you identify the optimal [hyperparameter configurations](https://oreil.ly/7IoxC),
    which we touch upon in [Chapter 11](ch11.html#managingcomma_deployingcomma_and_scaling).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 确定超参数的搜索空间可能很困难，通常进行超参数的随机搜索[优于结构化的网格搜索](https://oreil.ly/gI7G-)。有专门的库，比如[Hyperopt](http://hyperopt.github.io/hyperopt)，可以帮助您确定最佳的[超参数配置](https://oreil.ly/7IoxC)，我们在[第11章](ch11.html#managingcomma_deployingcomma_and_scaling)中有所涉及。
- en: 'To perform a hyperparameter search in Spark, take the following steps :'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Spark中执行超参数搜索，请按以下步骤进行：
- en: Define the `estimator` you want to evaluate.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义您想要评估的`estimator`。
- en: Specify which hyperparameters you want to vary, as well as their respective
    values, using the [`ParamGridBuilder`](https://oreil.ly/qOHrU).
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[`ParamGridBuilder`](https://oreil.ly/qOHrU)指定您要变化的超参数及其相应的值。
- en: Define an `evaluator` to specify which metric to use to compare the various
    models.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个`evaluator`来指定用于比较各种模型的度量标准。
- en: Use the [`CrossValidator`](https://oreil.ly/ygbF8) to perform cross-validation,
    evaluating each of the various models.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[`CrossValidator`](https://oreil.ly/ygbF8)执行交叉验证，评估各种模型。
- en: 'Let’s start by defining our pipeline estimator:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定义我们的管道估计器：
- en: '[PRE46]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'For our `ParamGridBuilder`, we’ll vary our `maxDepth` to be 2, 4, or 6 and
    `numTrees` (the number of trees in our random forest) to be 10 or 100\. This will
    give us a grid of 6 (3 x 2) different hyperparameter configurations in total:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的`ParamGridBuilder`，我们将变化我们的`maxDepth`为2、4或6，以及`numTrees`（随机森林中的树的数量）为10或100。这将给我们总共6个（3
    x 2）不同的超参数配置网格：
- en: '[PRE48]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now that we have set up our hyperparameter grid, we need to define how to evaluate
    each of the models to determine which one performed best. For this task we will
    use the `RegressionEvaluator`, and we’ll use RMSE as our metric of interest:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了超参数网格，我们需要定义如何评估每个模型，以确定哪一个表现最佳。为此，我们将使用`RegressionEvaluator`，并将RMSE作为我们感兴趣的度量标准：
- en: '[PRE51]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We will perform our *k*-fold cross-validation using the `CrossValidator`, which
    accepts an `estimator`, `evaluator`, and `estimatorParamMaps` so that it knows
    which model to use, how to evaluate the model, and which hyperparameters to set
    for the model. We can also set the number of folds we want to split our data into
    (`numFolds=3`), as well as setting a seed so we have reproducible splits across
    the folds (`seed=42`). Let’s then fit this cross-validator to our training data
    set:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`CrossValidator`执行我们的*k*折交叉验证，它接受一个`estimator`、`evaluator`和`estimatorParamMaps`，以便知道要使用哪个模型，如何评估模型以及为模型设置哪些超参数。我们还可以设置我们希望将数据拆分为的折数（`numFolds=3`），以及设置一个种子，以便在折叠之间具有可重现的拆分（`seed=42`）。然后让我们将这个交叉验证器适配到我们的训练数据集上：
- en: '[PRE53]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output tells us how long the operation took:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 输出告诉我们操作花费了多长时间：
- en: '[PRE55]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: So, how many models did we just train? If you answered 18 (6 hyperparameter
    configurations x 3-fold cross-validation), you’re close. Once you’ve identified
    the optimal hyperparameter configuration, how do you combine those three (or *k*)
    models together? While some models might be easy enough to average together, some
    are not. Therefore, Spark retrains your model on the entire training data set
    once it has identified the optimal hyperparameter configuration, so in the end
    we trained 19 models. If you want to retain the intermediate models trained, you
    can set `collectSubModels=True` in the `CrossValidator`.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们刚刚训练了多少个模型？如果你回答18（6个超参数配置 x 3折交叉验证），你就接近了。一旦确定了最佳的超参数配置，如何将这三个（或*k*个）模型组合在一起呢？虽然有些模型可能很容易平均起来，但有些则不是。因此，Spark在确定了最佳的超参数配置后会在整个训练数据集上重新训练您的模型，因此最终我们训练了19个模型。如果您想保留训练的中间模型，可以在`CrossValidator`中设置`collectSubModels=True`。
- en: 'To inspect the results of the cross-validator, you can take a look at the `avgMetrics`:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查交叉验证器的结果，您可以查看`avgMetrics`：
- en: '[PRE56]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Here’s the output:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE58]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: We can see that the best model from our `CrossValidator` (the one with the lowest
    RMSE) had `maxDepth=6` and `numTrees=100`. However, this took a long time to run.
    In the next section, we will look at how we can decrease the time to train our
    model while maintaining the same model performance.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们的`CrossValidator`中最佳模型（具有最低的RMSE）具有`maxDepth=6`和`numTrees=100`。然而，这需要很长时间才能运行。在接下来的部分中，我们将看看如何在保持相同模型性能的同时缩短训练时间。
- en: Optimizing Pipelines
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化管道
- en: 'If your code takes long enough for you to think about improving it, then you
    should optimize it. In the preceding code, even though each of the models in the
    cross-validator is technically independent, `spark.ml` actually trains the collection
    of models sequentially rather than in parallel. In Spark 2.3, a [`parallelism`](https://oreil.ly/7-zyU)
    parameter was introduced to solve this problem. This parameter determines the
    number of models to train in parallel, which themselves are fit in parallel. From
    the [Spark Tuning Guide](https://oreil.ly/FCXV7):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的代码执行时间足够长，需要考虑如何改进它，那么您应该对其进行优化。在前面的代码中，尽管交叉验证器中的每个模型在技术上是独立的，但`spark.ml`实际上是按顺序而不是并行地训练模型集合。在Spark
    2.3中，引入了一个[`parallelism`](https://oreil.ly/7-zyU)参数来解决这个问题。此参数确定并行训练的模型数量，这些模型本身是并行适配的。从[Spark调优指南](https://oreil.ly/FCXV7)中了解更多：
- en: The value of `parallelism` should be chosen carefully to maximize parallelism
    without exceeding cluster resources, and larger values may not always lead to
    improved performance. Generally speaking, a value up to `10` should be sufficient
    for most clusters.
  id: totrans-310
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`parallelism`的值应该谨慎选择，以最大化并行处理能力，而不超过集群资源，并且较大的值并不总是会带来更好的性能。一般来说，大多数集群最多可以使用值为`10`的参数。'
- en: 'Let’s set this value to `4` and see if we can train any faster:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个值设为`4`，看看我们是否可以更快地训练：
- en: '[PRE59]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The answer is yes:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是肯定的：
- en: '[PRE61]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We’ve cut the training time in half (from 1.07 minutes to 31.45 seconds), but
    we can still improve it further! There’s another trick we can use to speed up
    model training: putting the cross-validator inside the pipeline (e.g., `Pipeline(stages=[...,
    cv])` instead of putting the pipeline inside the cross-validator (e.g., `CrossValidator(estimator=pipeline,
    ...)`). Every time the cross-validator evaluates the pipeline, it runs through
    every step of the pipeline for each model, even if some of the steps don’t change,
    such as the `StringIndexer`. By reevaluating every step in the pipeline, we are
    learning the same `StringIndexer` mapping over and over again, even though it’s
    not changing.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练时间减少了一半（从1.07分钟到31.45秒），但我们仍然可以进一步改进！还有另一个技巧可以加快模型训练速度：将交叉验证器放在流水线内部（例如，`Pipeline(stages=[...,
    cv])`），而不是将流水线放在交叉验证器内部（例如，`CrossValidator(estimator=pipeline, ...)`）。每次交叉验证器评估流水线时，它会针对每个模型运行流水线的每个步骤，即使某些步骤不变，比如`StringIndexer`也是如此。通过重新评估流水线中的每个步骤，我们反复学习相同的`StringIndexer`映射，尽管它并未更改。
- en: 'If instead we put our cross-validator inside our pipeline, then we won’t be
    reevaluating the `StringIndexer` (or any other estimator) each time we try a different
    model:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将交叉验证器放在流水线内部，那么我们将不会在每次尝试不同模型时重新评估`StringIndexer`（或任何其他估算器）：
- en: '[PRE62]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'This trims five seconds off our training time:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这减少了我们训练时间的五秒钟：
- en: '[PRE64]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Thanks to the `parallelism` parameter and rearranging the ordering of our pipeline,
    that last run was the fastest—and if you apply it to the test data set you’ll
    see that you get the same results. Although these gains were on the order of seconds,
    the same techniques apply to much larger data sets and models, with correspondingly
    larger time savings. You can try running this code yourself by accessing the notebook
    in the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 借助`parallelism`参数和重排我们的流水线顺序，上次运行速度最快，如果您将其应用于测试数据集，您会发现结果相同。尽管这些收益仅为几秒钟，但同样的技术也适用于更大的数据集和模型，相应地节省了更多时间。您可以通过访问书籍的[GitHub
    repo](https://github.com/databricks/LearningSparkV2)中的笔记本来尝试运行此代码。
- en: Summary
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we covered how to build pipelines using Spark MLlib—in particular,
    its DataFrame-based API package, `spark.ml`. We discussed the differences between
    transformers and estimators, how to compose them using the Pipeline API, and some
    different metrics for evaluating models. We then explored how to use cross-validation
    to perform hyperparameter tuning to deliver the best model, as well as tips for
    optimizing cross-validation and model training in Spark.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何使用Spark MLlib构建流水线，特别是其基于DataFrame的API包`spark.ml`。我们讨论了转换器和估算器之间的区别，如何使用Pipeline
    API组合它们，以及一些不同的评估模型的指标。然后，我们探讨了如何使用交叉验证进行超参数调优以提供最佳模型，以及优化交叉验证和模型训练在Spark中的技巧。
- en: All this sets the context for the next chapter, in which we will discuss deployment
    strategies and ways to manage and scale machine learning pipelines with Spark.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些为我们讨论下一章提供了背景，下一章中我们将讨论使用Spark管理和扩展机器学习流水线的部署策略。
