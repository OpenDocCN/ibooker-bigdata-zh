- en: Chapter 4\. Data Transformation Strategies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 4 章 数据转换策略
- en: A recent [report published by Forbes](https://oreil.ly/q-xZU) describes how
    some stockbrokers and trading firms were able to access and analyze data faster
    than their competitors. This allowed them to “execute trades at the best price,
    microseconds ahead of the crowd. The win was ever so slight in terms of time,
    but massive in terms of the competitive advantage gained by speed to insight.”
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，[Forbes 发布的一份报告](https://oreil.ly/q-xZU)描述了一些股票经纪商和交易公司能够比竞争对手更快地访问和分析数据。这使他们能够“在众人之前微秒级地以最佳价格执行交易。在时间上稍微占优，但在速度洞察方面却取得了巨大的竞争优势。”
- en: When considering an analytics solution, speed to insights is important and the
    quicker an organization can respond to a shift in their data, the more competitive
    they will be. In many cases, to get the insights you need, the data needs to be
    transformed. As briefly discussed in [Chapter 3, “Setting Up Your Data Models
    and Ingesting Data”](ch03.html#AR_TGD_CH3), you can use an ETL approach, which
    reads the source data, processes the transformations in an external application,
    and loads the results, or you can use an ELT approach, which uses the data you
    just loaded and transforms the data in-place using the power of the Amazon Redshift
    compute.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑分析解决方案时，洞察力速度很重要，组织能够快速响应其数据变化，将更具竞争力。在许多情况下，为了获取所需的洞察力，数据需要进行转换。正如在[第 3
    章，“设置数据模型和数据摄入”](ch03.html#AR_TGD_CH3)中简要讨论的，您可以使用 ETL 方法，从源数据中读取数据，通过外部应用程序进行转换处理，并加载结果；或者您可以使用
    ELT 方法，使用刚刚加载的数据，并使用 Amazon Redshift 计算的能力在原地转换数据。
- en: In this chapter, we’ll start by [“Comparing ELT and ETL Strategies”](#comparing-elt-etl)
    to help you decide which data loading strategy to use when building your data
    warehouse. We’ll also dive deep into some of the unique features of Redshift that
    were built for analytics use cases and that empower [“In-Database Transformation”](#indb_transformation)
    as well as how you can leverage in-built [“Scheduling and Orchestration”](#scheduling_orchestration)
    capabilities to run your pipelines. Then we’ll cover how Amazon Redshift takes
    the ELT strategy even further, by allowing you to [“Access All Your Data”](#access-all-your-data)
    even if it was not loaded into Redshift. Finally, we’ll cover when it may make
    sense to use an [“External Transformation”](#external_transformation) strategy
    and how to use AWS Glue Studio to build your ETL pipelines.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从[“比较 ELT 和 ETL 策略”](#comparing-elt-etl)开始，帮助您决定在构建数据仓库时使用哪种数据加载策略。我们还将深入探讨一些为分析使用案例而构建的
    Redshift 的独特功能，并支持[“数据库内转换”](#indb_transformation)，以及如何利用内置的[“调度和编排”](#scheduling_orchestration)功能来运行您的数据管道。然后，我们将介绍亚马逊
    Redshift 如何通过允许您即使未加载到 Redshift 中也可以[“访问所有数据”](#access-all-your-data)来进一步采用 ELT
    策略。最后，我们将讨论在何时使用[“外部转换”](#external_transformation)策略以及如何使用 AWS Glue Studio 构建您的
    ETL 管道。
- en: Comparing ELT and ETL Strategies
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较 ELT 和 ETL 策略
- en: 'Regardless of an ELT or ETL strategy, each can support the common goals of
    your data management platform, which typically involve cleansing, transforming,
    and aggregating the data for loading into your reporting data model. These are
    all resource-intensive operations, and the primary difference between the two
    strategies is where the processing happens: in the compute of your ETL server(s)
    or in the compute of your data warehouse platform. ETL processes involve reading
    data from multiple sources and transforming the data using the functions and capabilities
    of the ETL engine. In contrast, ELT processes also involve extracting data from
    various sources but first loading it into the data warehouse. The transformation
    step is performed after the data has been loaded using familiar SQL semantics.
    Some things to consider when choosing between the two include:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是 ELT 还是 ETL 策略，都可以支持数据管理平台的共同目标，这些目标通常涉及为加载到报告数据模型中的数据进行清洗、转换和聚合。这些都是资源密集型的操作，两种策略之间的主要区别在于处理发生的位置：在您的
    ETL 服务器计算中或在您的数据仓库平台计算中。ETL 过程涉及从多个来源读取数据，并使用 ETL 引擎的函数和功能对数据进行转换。相反，ELT 过程还涉及从各种来源提取数据，但首先将其加载到数据仓库中。在加载数据之后，使用熟悉的
    SQL 语义执行数据转换步骤。在选择两者之间时需要考虑的一些因素包括：
- en: Performance and scalability
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 性能和可扩展性
- en: ETL processes are dependent on the resources of the ETL server(s) and require
    platform owners to correctly manage and size the environment. Compute platforms
    like Spark can be used to parallelize the data transformations and AWS Glue is
    provided as a serverless option for managing ETL pipelines. ELT processing is
    performed using the compute resources of the data warehouse. In the case of Amazon
    Redshift, the power of the MPP architecture is used to perform the transformations.
    Historically, transforming the data externally was preferred because the processing
    is offloaded to independent compute. However, modern data warehouse platforms,
    including Amazon Redshift, scale dynamically and can support mixed workloads,
    making an ELT strategy more attractive. In addition, since data warehouse platforms
    are designed to process and transform massive quantities of data using native
    database functions, ELT jobs tend to perform better. Finally, ELT strategies are
    free from network bottlenecks, which are required with ETL to move data in and
    out for processing.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ETL过程依赖于ETL服务器的资源，并要求平台所有者正确管理和调整环境。像Spark这样的计算平台可以用于并行化数据转换，AWS Glue提供了管理ETL管道的无服务器选项。ELT处理是使用数据仓库的计算资源执行的。在Amazon
    Redshift的情况下，使用MPP架构的强大性能来执行转换。在历史上，首选在外部进行数据转换，因为处理被卸载到独立计算上。然而，现代数据仓库平台，包括Amazon
    Redshift，可以动态扩展并支持混合工作负载，使ELT策略更具吸引力。此外，由于数据仓库平台设计用于使用本地数据库函数处理和转换大量数据，ELT作业往往表现更佳。最后，ELT策略不受网络瓶颈的限制，而ETL需要在处理中移动数据。
- en: Flexibility
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 灵活性
- en: While any transformation code in your data platform should follow a development
    lifecycle, with an ETL strategy, the code is typically managed by a team with
    specialized skill in an external application. In contrast, with an ELT strategy,
    all of your raw data is available to query and transform in the data management
    platform. Analysts can write code using familiar SQL functions leveraging the
    skills they already have. Empowering analysts shortens the development lifecycle
    because they can prototype the code and validate the business logic. The data
    platform owners would be responsible for optimizing and scheduling the code.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据平台中的任何转换代码都应遵循开发生命周期，使用ETL策略时，该代码通常由具有外部应用程序专业技能的团队管理。相比之下，ELT策略中，所有原始数据都可以在数据管理平台中查询和转换。分析师可以使用熟悉的SQL函数编写代码，利用他们已有的技能。赋予分析师权力可以缩短开发生命周期，因为他们可以原型化代码并验证业务逻辑。数据平台的所有者将负责优化和调度代码。
- en: Metadata management and orchestration
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据管理和编排
- en: One important consideration for your data strategy is how to manage job metadata
    and orchestration. Leveraging an ELT strategy means that the data platform owner
    needs to keep track of the jobs, their dependencies, and load schedules. ETL tools
    typically have capabilities that capture and organize metadata about sources,
    targets, and job characteristics as well as data lineage. They also can orchestrate
    jobs and build dependencies across multiple data platforms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据策略的一个重要考虑因素是如何管理作业元数据和编排。利用ELT策略意味着数据平台所有者需要跟踪作业、它们的依赖关系和加载计划。ETL工具通常具有捕获和组织源、目标和作业特性以及数据血统的功能。它们还可以编排作业并在多个数据平台之间构建依赖关系。
- en: Ultimately, the choice between ETL and ELT will depend on the specific needs
    of the analytics workload. Both strategies have strengths and weaknesses, and
    the decision of which you use depends on the characteristics of the data sources,
    the transformation requirements, and the performance and scalability needs of
    the project. To mitigate the challenges with each, many users take a hybrid approach.
    You can take advantage of the metadata management and orchestration capabilities
    of ETL tools as well as the performance and scalability of ELT processing by building
    jobs that translate the ETL code to SQL statements. In [“External Transformation”](#external_transformation),
    we will discuss in more detail how this is possible.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，选择ETL和ELT之间的方式将取决于分析工作负载的具体需求。两种策略各有优势和劣势，使用哪种策略取决于数据源的特性、转换需求以及项目的性能和可扩展性需求。为了减轻每种策略的挑战，许多用户采取混合方法。您可以利用ETL工具的元数据管理和编排功能，以及构建将ETL代码转换为SQL语句的作业来利用ELT处理的性能和可扩展性。在[“外部转换”](#external_transformation)中，我们将更详细地讨论这种可能性。
- en: In-Database Transformation
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在数据库转换中
- en: With the variety and velocity of data present today, the challenge of designing
    a data platform is to make it both scalable and flexible. Amazon Redshift continues
    to innovate and provide functionality to process all your data in one place with
    its in-database transformation (ELT) capabilities. Being an ANSI SQL compatible
    relational database, Amazon Redshift supports [SQL commands](https://oreil.ly/d6v8t),
    making it a familiar development environment for most database developers. Amazon
    Redshift also supports advanced functions present in modern data platforms such
    as [Window Functions](https://oreil.ly/5I6s6), [HyperLogLog Functions](https://oreil.ly/cIqsB),
    and [Recursive CTE (common table expressions)](https://oreil.ly/rRPCc), to name
    a few. In addition to those functions you may be familiar with, Amazon Redshift
    supports unique capabilities for analytical processing. For example, Amazon Redshift
    supports in-place querying for [“Semistructured Data”](#semi_structured), providing
    analysts a way to access this data in a performant way and without waiting for
    it to be loaded into tables and columns. In addition, if you need to extend the
    capabilities of Amazon Redshift, you can leverage [“User-Defined Functions”](#udfs)
    that can run inside the database or call external services. Finally, [“Stored
    Procedures”](#stored_procedures) allow you to package your transformation logic.
    They can return a result set given input parameters or even perform data loading
    and managed operations like loading a fact, dimension, or aggregate table.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 面对当今数据的多样性和速度，设计数据平台的挑战在于使其既可扩展又灵活。Amazon Redshift 持续创新并提供功能，以在一个地方处理所有数据，具有其数据库内转换（ELT）能力。作为符合
    ANSI SQL 的关系数据库，Amazon Redshift 支持 [SQL 命令](https://oreil.ly/d6v8t)，使其成为大多数数据库开发人员熟悉的开发环境。Amazon
    Redshift 还支持现代数据平台中的高级功能，如 [窗口函数](https://oreil.ly/5I6s6)，[HyperLogLog 函数](https://oreil.ly/cIqsB)，以及
    [递归 CTE（通用表达式）](https://oreil.ly/rRPCc) 等。除了您熟悉的这些功能外，Amazon Redshift 还支持用于分析处理的独特能力。例如，Amazon
    Redshift 支持就地查询 [“半结构化数据”](#semi_structured)，为分析人员提供了一种高效访问此类数据的方式，而无需等待其加载到表和列中。此外，如果需要扩展
    Amazon Redshift 的功能，您可以利用可以在数据库内运行或调用外部服务的 [“用户定义函数”](#udfs)。最后，[“存储过程”](#stored_procedures)
    允许您打包转换逻辑，可以根据输入参数返回结果集，甚至执行数据加载和管理操作，如加载事实、维度或聚合表。
- en: Semistructured Data
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半结构化数据
- en: Semistructured data falls under the category of data that doesn’t conform to
    a rigid schema expected in relational databases. Semistructured formats are common
    and often preferred in web logs, sensor data, or API messages because these applications
    often have to send data with nested relationships, and rather than making multiple
    round-trips, it is more efficient to send the data once. Semistructured data contains
    complex values such as arrays and nested structures that are associated with serialization
    formats, such as JSON. While there are third-party tools you can use to transform
    your data outside the database, it would require engineering resources to build
    and maintain that code and may not be as performant. Whether you are accessing
    [“External Amazon S3 Data”](#external_s3) or locally loaded data, Amazon Redshift
    leverages the [PartiQL](https://oreil.ly/9kqbr) syntax for analyzing and transforming
    semistructured data. A special data type, [`SUPER`](https://oreil.ly/oyUbd), was
    launched to store this data in its native form. However, when accessed from Amazon
    S3, it will be cataloged with a data type of `struct` or `array`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 半结构化数据属于不符合关系数据库中预期的严格模式的数据类别。半结构化格式在网络日志、传感器数据或 API 消息中常见且通常受欢迎，因为这些应用程序经常需要发送带有嵌套关系的数据，而不是进行多次往返，一次性发送数据更为高效。半结构化数据包含复杂值，如数组和嵌套结构，这些与序列化格式（例如
    JSON）相关联。虽然可以使用第三方工具在数据库之外转换数据，但这将需要工程资源来构建和维护该代码，并且可能性能不佳。无论您是访问 [“外部亚马逊 S3 数据”](#external_s3)
    还是本地加载的数据，Amazon Redshift 利用 [PartiQL](https://oreil.ly/9kqbr) 语法分析和转换半结构化数据。为了以其原生形式存储此数据，特别推出了一个名为
    [`SUPER`](https://oreil.ly/oyUbd) 的特殊数据类型。然而，当从 Amazon S3 访问时，将使用 `struct` 或 `array`
    数据类型进行分类。
- en: In the following example, we’re referencing a file that has landed in an Amazon
    S3 environment. You can catalog this file and make it accessible in Amazon Redshift
    by creating an external schema and mapping any file that exists in this Amazon
    S3 prefix to this table definition.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们引用了一个已经存储在Amazon S3环境中的文件。您可以通过创建外部模式并将存在于此Amazon S3前缀中的任何文件映射到该表定义，对此文件进行目录化并使其在Amazon
    Redshift中可访问。
- en: The first query ([Example 4-1](#query4-1)) finds the total sales revenue per
    event.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个查询（[示例 4-1](#query4-1)）查找每个事件的总销售收入。
- en: Example 4-1\. Create external table from JSON data
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-1\. 从JSON数据创建外部表
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This data file is located in the us-west-2 region, and this example will work
    only if your Amazon Redshift data warehouse is also in that region. Also, we’ve
    referenced the `default` IAM role. Be sure to modify the role to allow read access
    to this Amazon S3 location as well as to have access to manage the AWS Glue Data
    Catalog.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据文件位于us-west-2地区，此示例仅在您的Amazon Redshift数据仓库也位于该地区时有效。此外，我们引用了 `default` IAM
    角色。确保修改角色以允许读取此Amazon S3位置的权限，并且具有管理AWS Glue数据目录的访问权限。
- en: Now that the table is available, it can be queried and you can access top-level
    attributes without any special processing ([Example 4-2](#query4-2)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在表已经可用，可以对其进行查询，您可以在不进行任何特殊处理的情况下访问顶级属性（[示例 4-2](#query4-2)）。
- en: Example 4-2\. Top-level attributes
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-2\. 顶级属性
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using the PartiQL syntax, you can access the nested `struct` data. In [Example 4-3](#query4-3),
    we are un-nesting the data in the `orders` field and showing the multiple orders
    associated to the customer record.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PartiQL语法，您可以访问嵌套的 `struct` 数据。在 [示例 4-3](#query4-3) 中，我们将 `orders` 字段中的数据进行非嵌套化，并展示与客户记录关联的多个订单。
- en: Example 4-3\. Un-nested attributes (external)
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-3\. 非嵌套属性（外部）
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In addition to accessing data in S3, this semistructured data can be loaded
    into your Amazon Redshift table using the `SUPER` data type. In [Example 4-4](#query4-4),
    this same file is loaded into a physical table. One notable difference when loading
    into Amazon Redshift is that no information about the schema of the `orders` column
    mapped to the `SUPER` data type is required. This simplifies the loading and metadata
    management process as well as provides flexibility in case of metadata changes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 除了访问S3中的数据外，此半结构化数据还可以使用 `SUPER` 数据类型加载到您的Amazon Redshift表中。在 [示例 4-4](#query4-4)
    中，同一文件加载到物理表中。在加载到Amazon Redshift时，与映射到 `SUPER` 数据类型的 `orders` 列的架构相关的信息不需要。这简化了加载和元数据管理过程，并在元数据更改时提供了灵活性。
- en: Example 4-4\. Create local table from JSON data
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-4\. 从JSON数据创建本地表
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We’ve referenced the `default` IAM role. Be sure to modify the role to grant
    access to read from this Amazon S3 location.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引用了 `default` IAM 角色。确保修改角色以授予从此Amazon S3位置读取的权限。
- en: Now that the table is available, it can be queried. Using the same PartiQL syntax,
    you can access the order details ([Example 4-5](#query4-5)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在表已经可用，可以对其进行查询。使用相同的PartiQL语法，您可以访问订单详情（[示例 4-5](#query4-5)）。
- en: Example 4-5\. Unnested attributes (local)
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-5\. 非嵌套属性（本地）
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `enable_case_sensitive_identifier` is an important parameter when querying
    `SUPER` data if your input has mixed case identifiers. For more information, see
    the [online documentation](https://oreil.ly/Ou8HU).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`enable_case_sensitive_identifier` 是在查询 `SUPER` 数据时的一个重要参数，如果您的输入具有混合大小写标识符。有关更多信息，请参阅[在线文档](https://oreil.ly/Ou8HU)。'
- en: For more details and examples on querying semistructured data, see the [online
    documentation](https://oreil.ly/YjliK).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有关查询半结构化数据的更多详细信息和示例，请参阅[在线文档](https://oreil.ly/YjliK)。
- en: User-Defined Functions
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户定义函数
- en: 'If a built-in function is not available for your specific transformation needs,
    Amazon Redshift has a few options for extending the functionality of the platform.
    Amazon Redshift allows you to create *scalar user-defined functions* (UDFs) in
    three flavors: SQL, Python, and Lambda. For detailed documentation on creating
    each of these types of functions, see the [online documentation](https://oreil.ly/VO8gm).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有适合您特定转换需求的内置函数，Amazon Redshift提供了几种选项来扩展平台功能。Amazon Redshift允许您创建三种类型的 *标量用户定义函数*（UDF）：SQL、Python
    和 Lambda。有关创建每种类型函数的详细文档，请参阅[在线文档](https://oreil.ly/VO8gm)。
- en: A scalar function will return exactly one value per invocation. In most cases,
    you can think of this as returning one value per row.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 标量函数每次调用将返回一个确切的值。在大多数情况下，您可以将其视为每行返回一个值。
- en: An SQL UDF leverages existing SQL syntax. It can be used to ensure consistent
    logic is applied and to simplify the amount of code each user would have to write
    individually. In [Example 4-6](#query4-6), from the [Amazon Redshift UDFs GitHub
    Repo](https://oreil.ly/Rd33u), you’ll see an SQL function that takes two input
    parameters; the first `varchar` field is the data to be masked, and the second
    field is the classification of the data. The result is a different masking strategy
    based on the data classification.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: SQL UDF 利用现有的 SQL 语法。它可以用于确保应用一致的逻辑，并简化每个用户需要单独编写的代码量。在 [示例 4-6](#query4-6)
    中，来自 [Amazon Redshift UDFs GitHub 仓库](https://oreil.ly/Rd33u) 的示例中，您将看到一个 SQL
    函数，它接受两个输入参数；第一个 `varchar` 字段是要掩码的数据，第二个字段是数据的分类。结果是基于数据分类的不同掩码策略。
- en: Example 4-6\. SQL UDF definition
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-6\. SQL UDF 定义
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Users can reference an SQL UDF within a `SELECT` statement. In this scenario,
    you might write the `SELECT` statement as shown in [Example 4-7](#query4-7).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以在 `SELECT` 语句中引用 SQL UDF。在这种情况下，您可能会编写 [示例 4-7](#query4-7) 中所示的 `SELECT`
    语句。
- en: Example 4-7\. SQL UDF access
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-7\. SQL UDF 访问
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `SELECT` statement in [Example 4-7](#query4-7) results in the following
    output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 4-7](#query4-7) 中的 `SELECT` 语句的结果如下：'
- en: '| mask_name | name | mask_email | email | mask_ssn | ssn |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| mask_name | name | mask_email | email | mask_ssn | ssn |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Janxxxxx | Jane Doe | [jdoxxxx@org.com](mailto:jdoxxxx@org.com) | [jdoe@org.com](mailto:jdoe@org.com)
    | 123-45-xxxx | 123-45-6789 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Janxxxxx | Jane Doe | [jdoxxxx@org.com](mailto:jdoxxxx@org.com) | [jdoe@org.com](mailto:jdoe@org.com)
    | 123-45-xxxx | 123-45-6789 |'
- en: A Python UDF allows users to leverage Python code to transform their data. In
    addition to core Python libraries, users can import their own libraries to extend
    the functionality available in Amazon Redshift. In [Example 4-8](#query4-8), from
    the [Amazon Redshift UDFs GitHub Repo](https://oreil.ly/d_ief), you’ll see a Python
    function that leverages an external library, `ua_parser`, which can parse a user-agent
    string into a JSON object and return the client OS family.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Python UDF 允许用户利用 Python 代码转换其数据。除了核心 Python 库外，用户还可以导入自己的库，以扩展在 Amazon Redshift
    中可用的功能。在 [示例 4-8](#query4-8) 中，来自 [Amazon Redshift UDFs GitHub 仓库](https://oreil.ly/d_ief)
    的示例中，您将看到一个利用外部库 `ua_parser` 的 Python 函数，该函数可以将用户代理字符串解析为 JSON 对象并返回客户端操作系统系列。
- en: Example 4-8\. Python UDF definition
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-8\. Python UDF 定义
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Similar to SQL UDFs, users can reference a Python UDF within a `SELECT` statement.
    In this example, you might write the `SELECT` statement shown in [Example 4-9](#query4-9).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 SQL UDF，用户可以在 `SELECT` 语句中引用 Python UDF。在本例中，您可能会编写 [示例 4-9](#query4-9)
    中所示的 `SELECT` 语句。
- en: Example 4-9\. Python UDF access
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-9\. Python UDF 访问
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `SELECT` statement in [Example 4-9](#query4-9) results in the following
    output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 4-9](#query4-9) 中的 `SELECT` 语句的结果如下：'
- en: '| family | agent |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| family | agent |'
- en: '| --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Chrome | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36
    (KHTML, like Gecko) Chrome/41.0.2272.104 Safari/537.36 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Chrome | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36
    (KHTML, like Gecko) Chrome/41.0.2272.104 Safari/537.36 |'
- en: Lastly, the Lambda UDF allows users to interact and integrate with external
    components outside of Amazon Redshift. You can write Lambda UDFs in any supported
    programming language, such as Java, Go PowerShell, Node.js, C#, Python, Ruby,
    or a custom runtime. This functionality enables new Amazon Redshift use cases,
    including data enrichment from external data stores (e.g., Amazon DynamoDB, Amazon
    ElastiCache, etc.), data enrichment from external APIs (e.g., Melissa Global Address
    Web API, etc.), data masking and tokenization from external providers (e.g., Protegrity),
    and conversion of legacy UDFs written in other languages such as C, C++, and Java.
    In [Example 4-10](#query4-10), from the [Amazon Redshift UDFs GitHub Repo](https://oreil.ly/pll2W),
    you’ll see a Lambda function that leverages the AWS Key Management Service (KMS)
    and takes an incoming string to return the encrypted value. The first code block
    establishes a Lambda function, `f-kms-encrypt`, which expects a nested array of
    arguments passed to the function. In this example, the user would supply the `kmskeyid`
    and the `columnValue` as input parameters; `argument[0]` and `argument[1]`. The
    function will use the `boto3` library to call the `kms` service to return the
    encrypted `response`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Lambda UDF允许用户与Amazon Redshift之外的外部组件进行交互和集成。您可以使用任何支持的编程语言编写Lambda UDF，例如Java、Go、PowerShell、Node.js、C#、Python、Ruby或自定义运行时。此功能支持新的Amazon
    Redshift用例，包括从外部数据存储（例如Amazon DynamoDB、Amazon ElastiCache等）进行数据丰富、从外部API（例如Melissa
    Global Address Web API等）进行数据丰富、从外部提供者（例如Protegrity）进行数据掩码和令牌化，以及将其他语言（如C、C++和Java）编写的遗留UDF转换为Lambda
    UDF。在 [示例 4-10](#query4-10) 中，从 [Amazon Redshift UDFs GitHub Repo](https://oreil.ly/pll2W)
    中，您将看到一个Lambda函数，利用AWS密钥管理服务（KMS）并将传入的字符串返回为加密值。第一个代码块定义了一个Lambda函数 `f-kms-encrypt`，该函数期望传递给函数的参数的嵌套数组。在此示例中，用户将作为输入参数提供
    `kmskeyid` 和 `columnValue`；`argument[0]` 和 `argument[1]`。该函数将使用 `boto3` 库调用 `kms`
    服务以返回加密的 `response`。
- en: Example 4-10\. Lambda function definition
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-10\. Lambda函数定义
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The next code block establishes the Amazon Redshift UDF, which references the
    Lambda function ([Example 4-11](#query4-11)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块定义了Amazon Redshift UDF，其中引用了Lambda函数（[示例 4-11](#query4-11)）。
- en: Example 4-11\. Lambda UDF definition
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-11\. Lambda UDF定义
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We’ve referenced the `default` IAM role. Be sure to modify the role to grant
    access to execute the Lambda function created previously.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引用了`default` IAM角色。请确保修改角色以授予执行先前创建的Lambda函数的访问权限。
- en: Just like the SQL and Python UDFs, users can reference a Lambda UDF within a
    `SELECT` statement. In this scenario, you might write the `SELECT` statement show
    in [Example 4-12](#query4-12).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 就像SQL和Python UDF一样，用户可以在 `SELECT` 语句中引用Lambda UDF。在这种场景中，您可能会编写在 [示例 4-12](#query4-12)
    中显示的 `SELECT` 语句。
- en: Example 4-12\. Lambda UDF access
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-12\. Lambda UDF访问
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `SELECT` statement in [Example 4-12](#query4-12) results in the following
    output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 4-12](#query4-12) 中的 `SELECT` 语句将产生以下输出：
- en: '| email_encrypt | email |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| email_encrypt | email |'
- en: '| --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| AQICAHiQbIJ478Gbu8DZyl0frUxOrbgDlP+CyfuWCuF0kHJyWg …​ | [jdoe@org.com](mailto:jdoe@org.com)
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| AQICAHiQbIJ478Gbu8DZyl0frUxOrbgDlP+CyfuWCuF0kHJyWg …​ | [jdoe@org.com](mailto:jdoe@org.com)
    |'
- en: For more details on Python UDFs, see the [“Introduction to Python UDFs in Amazon
    Redshift” blog post](https://oreil.ly/aX77w), and for more details on Lambda UDFs,
    see the [“Accessing External Components Using Amazon Redshift Lambda UDFs” blog
    post](https://oreil.ly/cEeQH).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Python UDF的更多详细信息，请参阅 [“Amazon Redshift中Python UDF的介绍”博文](https://oreil.ly/aX77w)，有关Lambda
    UDF的更多详细信息，请参阅 [“使用Amazon Redshift Lambda UDF访问外部组件”博文](https://oreil.ly/cEeQH)。
- en: Stored Procedures
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储过程
- en: An Amazon Redshift stored procedure is a user-created object to perform a set
    of SQL queries and logical operations. The procedure is stored in the database
    and is available to users who have privileges to execute it. Unlike a scalar UDF
    function, which can operate on only one row of data in a table, a *stored procedure*
    can incorporate data definition language (DDL) and data manipulation language
    (DML) in addition to `SELECT` queries. Also, a stored procedure doesn’t have to
    return a value and can contain looping and conditional expressions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift存储过程是用户创建的对象，用于执行一组SQL查询和逻辑操作。该过程存储在数据库中，并可供有执行权限的用户使用。与仅能在表中操作一行数据的标量UDF函数不同，*存储过程*
    可以包含数据定义语言（DDL）和数据操作语言（DML），还可以包含循环和条件表达式。此外，存储过程不必返回值。
- en: Stored procedures are commonly used to encapsulate logic for data transformation,
    data validation, and business-specific operations as an alternative to shell scripting,
    or complex ETL and orchestration tools. Stored procedures allow the ETL/ELT logical
    steps to be fully enclosed in a procedure. You may write the procedure to commit
    data incrementally or so that it either succeeds completely (processes all rows)
    or fails completely (processes no rows). Because all the processing occurs on
    the data warehouse, there is no overhead to move data across the network and you
    can take advantage of Amazon Redshift’s ability to perform bulk operations on
    large quantities of data quickly because of its MPP architecture.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 存储过程通常用于封装数据转换、数据验证和业务特定操作的逻辑，作为替代 Shell 脚本或复杂的 ETL 和编排工具。存储过程允许将 ETL/ELT 的逻辑步骤完全封装在一个过程中。您可以编写过程以便增量提交数据，或者使其完全成功（处理所有行）或完全失败（不处理任何行）。由于所有处理均在数据仓库上进行，因此无需在网络上移动数据，而且您可以利用
    Amazon Redshift 的 MPP 架构快速执行大量数据的批量操作。
- en: In addition, because stored procedures are implemented in the PL/pgSQL programming
    language, you may not need to learn a new programming language to use them. In
    fact, you may have existing stored procedures in your legacy data platform that
    can be migrated to Amazon Redshift with minimal code changes. Re-creating the
    logic of your existing processes using an external programming language or a new
    ETL platform could be a large project. AWS also provides the [AWS Schema Conversion
    Tool (SCT)](https://oreil.ly/h0Tn-), a migration assistant that can help by converting
    existing code in other database programming languages to Amazon Redshift native
    PL/pgSQL code.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，由于存储过程是用 PL/pgSQL 编程语言实现的，您可能不需要学习新的编程语言来使用它们。实际上，您可能在传统数据平台上已有现有的存储过程，可以通过最少的代码更改迁移到
    Amazon Redshift。使用外部编程语言或新的 ETL 平台重新创建现有流程的逻辑可能是一个大项目。AWS 还提供[AWS Schema Conversion
    Tool (SCT)](https://oreil.ly/h0Tn-)，这是一个迁移助手，可以将其他数据库编程语言中的现有代码转换为 Amazon Redshift
    本机的 PL/pgSQL 代码。
- en: In [Example 4-13](#query4-13), you can see a simple procedure that will load
    data into a staging table from Amazon S3 and load new records into the `lineitem`
    table, while ensuring that duplicates are deleted. This procedure takes advantage
    of the `MERGE` operator and can accomplish the task using one statement. In this
    example, there are constant variables for the `l_orderyear` and `l_ordermonth`.
    However, this can be easily made dynamic using the `date_part` function and `current_date`
    variable to determine the current year and month to load or by passing in a `year`
    and `month` parameter to the procedure.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 4-13](#query4-13)中，您可以看到一个简单的过程，该过程将从 Amazon S3 加载数据到暂存表，然后将新记录加载到`lineitem`表中，同时确保删除重复项。该过程利用了`MERGE`运算符，可以使用一个语句完成任务。在此示例中，`l_orderyear`和`l_ordermonth`有常量变量。然而，通过使用`date_part`函数和`current_date`变量确定要加载的当前年份和月份，或者通过向过程传递`year`和`month`参数，可以轻松地使其动态化。
- en: Example 4-13\. Stored procedure definition
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-13\. 存储过程定义
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We’ve referenced the `default` IAM role. Be sure to modify the role to grant
    access to read from this Amazon S3 location.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已引用`default` IAM 角色。请确保修改角色以授予读取此 Amazon S3 位置的访问权限。
- en: You can execute a stored procedure by using the `call` keyword ([Example 4-14](#query4-14)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`call`关键字执行存储过程（参见[示例 4-14](#query4-14)）。
- en: Example 4-14\. Stored procedure access
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-14\. 存储过程访问
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For more details on Amazon Redshift stored procedures, see the [“Bringing Your
    Stored Procedures to Amazon Redshift”](https://oreil.ly/z3RUq) blog post.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Amazon Redshift 存储过程的更多详细信息，请参阅[“将您的存储过程带到 Amazon Redshift”](https://oreil.ly/z3RUq)
    博客文章。
- en: Scheduling and Orchestration
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调度和编排
- en: When you start to think about orchestrating your data pipeline, you want to
    consider the complexity of the workflow and the dependencies on external processes.
    Some users have to manage multiple systems with complex dependencies. You may
    have advanced notification requirements if a job fails or misses an SLA. If so,
    you may consider a third-party scheduling tool. Popular third-party enterprise
    job scheduling tools include [Tivoli](https://oreil.ly/c0QVz), [Control-M](https://oreil.ly/eEY_P),
    and [AutoSys](https://oreil.ly/fVA8y), which each have integrations with Amazon
    Redshift allowing you to initiate a connection and execute one or multiple SQL
    statements. AWS also offers the [Amazon Managed Workflow orchestration for Apache
    Airflow (MWAA)](https://oreil.ly/eRpJA) service, which is based on the open source
    [Apache Airflow](https://airflow.apache.org) project. This can be useful if you
    are already running an Apache Airflow workflow and would like to migrate it to
    the cloud.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当您开始考虑编排数据管道时，您需要考虑工作流程的复杂性以及对外部进程的依赖性。一些用户必须管理具有复杂依赖关系的多个系统。如果作业失败或未达到 SLA，您可能需要高级通知要求。如果是这样，您可能要考虑使用第三方调度工具。流行的第三方企业作业调度工具包括[Tivoli](https://oreil.ly/c0QVz)，[Control-M](https://oreil.ly/eEY_P)和[AutoSys](https://oreil.ly/fVA8y)，每个工具都与
    Amazon Redshift 集成，允许您建立连接并执行一个或多个 SQL 语句。AWS 还提供基于开源[Apache Airflow](https://airflow.apache.org)项目的[Amazon
    管理的工作流编排（MWAA）](https://oreil.ly/eRpJA)服务。如果您已经在运行 Apache Airflow 工作流并希望将其迁移到云端，这将非常有用。
- en: However, if you can trigger your loads based on time-based triggers, you can
    leverage the query scheduler. When you’re using the query scheduler, the UI will
    leverage the foundational services of the Amazon Redshift Data API and EventBridge.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您可以基于时间触发您的加载操作，您可以利用查询调度程序。当您使用查询调度程序时，UI 将利用 Amazon Redshift Data API
    和 EventBridge 的基础服务。
- en: To use the query scheduler to trigger simple time-based queries, navigate to
    the [Query Editor V2](https://oreil.ly/OfaXg), prepare your query, and click the
    Schedule button ([Figure 4-1](#schedule_button)). For this example, we will use
    a `COPY` statement to load the `stage_lineitem` table.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用查询调度程序触发简单的基于时间的查询，请导航至[查询编辑器 V2](https://oreil.ly/OfaXg)，准备您的查询，然后点击计划按钮（[图 4-1](#schedule_button)）。对于此示例，我们将使用`COPY`语句加载`stage_lineitem`表。
- en: Set the connection ([Figure 4-2](#choose_connection)) as well as the IAM role
    the scheduler will assume to execute the query. In the subsequent dialog, select
    the applicable Amazon Redshift data warehouse from the list and the corresponding
    account and region. In our case, we will use “Temporary credentials” to connect.
    See [Chapter 2, “Getting Started with Amazon Redshift”](ch02.html#AR_TGD_CH2)
    for more details on other connection strategies.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 设置连接（[图 4-2](#choose_connection)）以及调度程序将用来执行查询的 IAM 角色。在随后的对话框中，从列表中选择适用的 Amazon
    Redshift 数据仓库以及相应的帐户和区域。在我们的情况下，我们将使用“临时凭证”进行连接。有关其他连接策略的详细信息，请参阅[第 2 章，“Amazon
    Redshift 入门”](ch02.html#AR_TGD_CH2)。
- en: '![Schedule button](assets/ardg_0401.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![计划按钮](assets/ardg_0401.png)'
- en: Figure 4-1\. Schedule button
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 计划按钮
- en: '![Choose connection](assets/ardg_0402.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![选择连接](assets/ardg_0402.png)'
- en: Figure 4-2\. Choose connection
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 选择连接
- en: Next, set the query name that will be executed and the optional description
    ([Figure 4-3](#set_query)). The query will be copied over from the editor page.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，设置将要执行的查询名称以及可选描述（[图 4-3](#set_query)）。查询将从编辑器页面复制过来。
- en: '![Set query](assets/ardg_0403.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![设置查询](assets/ardg_0403.png)'
- en: Figure 4-3\. Set query
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 设置查询
- en: Next, set the time-based schedule either in Cron format or by selecting the
    applicable radio options ([Figure 4-4](#set_schedule)). Optionally, choose if
    you’d like execution events to be delivered to an Amazon Simple Notification Service
    (SNS) topic so you can receive notifications. Click Save Changes to save the schedule.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，按照 Cron 格式设置基于时间的计划，或者通过选择适用的单选选项（[图 4-4](#set_schedule)）。可选地，选择是否希望将执行事件传递到
    Amazon 简单通知服务（SNS）主题，以便接收通知。单击“保存更改”保存计划。
- en: '![Set schedule](assets/ardg_0404.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![设置计划](assets/ardg_0404.png)'
- en: Figure 4-4\. Set schedule
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 设置计划
- en: To see the list of scheduled queries, navigate to the [Scheduled queries page](https://oreil.ly/jvCFh)
    of the Query Editor V2 ([Figure 4-5](#list_scheduled)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看已安排的查询列表，请导航至[已安排查询页面](https://oreil.ly/jvCFh)的查询编辑器 V2（[图 4-5](#list_scheduled)）。
- en: '![List scheduled queries](assets/ardg_0405.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![已安排的查询列表](assets/ardg_0405.png)'
- en: Figure 4-5\. List scheduled queries
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 已安排的查询列表
- en: To manage the scheduled job, click on the scheduled query. In this screen you
    can modify the job, deactivate it, or delete it. You can also inspect the history,
    which contains the start/stop time as well as the job status (see [Figure 4-6](#see_schedule_history)).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要管理调度作业，请点击调度查询。在此屏幕上，您可以修改作业、停用它或删除它。您还可以检查历史记录，其中包含开始/停止时间以及作业状态（参见[图 4-6](#see_schedule_history)）。
- en: '![See schedule history](assets/ardg_0406.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![查看调度历史](assets/ardg_0406.png)'
- en: Figure 4-6\. See schedule history
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 查看调度历史
- en: You can also see the resources created in EventBridge. Navigate to the [EventBridge
    Rules page](https://oreil.ly/Phb4H) and notice a new Scheduled rule was created
    ([Figure 4-7](#scheduled_rule)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以查看在EventBridge中创建的资源。导航到[EventBridge规则页面](https://oreil.ly/Phb4H)，注意已创建一个新的调度规则（[图 4-7](#scheduled_rule)）。
- en: '![Scheduled rule](assets/ardg_0407.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![调度规则](assets/ardg_0407.png)'
- en: Figure 4-7\. Scheduled rule
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 调度规则
- en: Inspecting the rule target ([Figure 4-8](#scheduled_rule_target)), you will
    see `Redshift cluster` target type along with the parameter needed to execute
    the query.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 检查规则目标（[图 4-8](#scheduled_rule_target)），您将看到`Redshift cluster`目标类型以及执行查询所需的参数。
- en: '![Scheduled rule target](assets/ardg_0408.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![调度规则目标](assets/ardg_0408.png)'
- en: Figure 4-8\. Scheduled rule target
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. 调度规则目标
- en: Access All Your Data
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问所有数据
- en: To complete the ELT story, Amazon Redshift supports access to data even if it
    was not loaded. The Amazon Redshift compute will process your data using all the
    transformation capabilities already mentioned without the need of a separate server
    for processing. Whether it is [“External Amazon S3 Data”](#external_s3), [“External
    Operational Data”](#external_operational), or even [“External Amazon Redshift
    Data”](#external_redshift), queries are submitted in your Amazon Redshift data
    warehouse using familiar ANSI SQL syntax; only the applicable data is processed
    by the Amazon Redshift compute. It can be joined to local data and used to populate
    tables local to your Amazon Redshift data warehouse.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成ELT故事，Amazon Redshift支持访问未加载的数据。Amazon Redshift计算将使用所有已提到的转换能力处理您的数据，无需单独的处理服务器。无论是[“外部Amazon
    S3数据”](#external_s3)，[“外部操作数据”](#external_operational)，甚至是[“外部Amazon Redshift数据”](#external_redshift)，在您的Amazon
    Redshift数据仓库中使用熟悉的ANSI SQL语法提交查询；只有适用的数据由Amazon Redshift计算处理。它可以与本地数据进行连接，并用于填充您的Amazon
    Redshift数据仓库本地表。
- en: External Amazon S3 Data
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部Amazon S3数据
- en: 'Amazon Redshift enables you to read and write external data that is stored
    in Amazon S3 using simple SQL queries. Accessing data on Amazon S3 enhances the
    interoperability of your data because you can access the same Amazon S3 data from
    multiple compute platforms beyond Amazon Redshift. Such platforms include Amazon
    Athena, Amazon EMR, Presto, and any other compute platform that can access Amazon
    S3\. Using this feature Amazon Redshift can join external Amazon S3 tables with
    tables that reside on the local disk of your Amazon Redshift data warehouse. When
    using a provisioned cluster, Amazon Redshift will leverage a fleet of nodes called
    Amazon Redshift Spectrum, which further isolates the Amazon S3 processing and
    applies optimizations like predicate pushdown and aggregation to the Amazon Redshift
    Spectrum compute layer, improving query performance. Types of predicate operators
    you can push to Amazon Redshift Spectrum include: `=`, `LIKE`, `IS NULL`, and
    `CASE WHEN`. In addition, you can employ transformation logic where many aggregation
    and string functions are pushed to the Amazon Redshift Spectrum layer. Types of
    aggregate functions include: `COUNT`, `SUM`, `AVG`, `MIN`, and `MAX`.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift使您能够使用简单的SQL查询读取和写入存储在Amazon S3中的外部数据。访问Amazon S3上的数据增强了您的数据的互操作性，因为您可以从Amazon
    Redshift以外的多个计算平台访问相同的Amazon S3数据。这些平台包括Amazon Athena、Amazon EMR、Presto以及任何其他能够访问Amazon
    S3的计算平台。使用此功能，Amazon Redshift可以将外部Amazon S3表与驻留在Amazon Redshift数据仓库本地磁盘上的表进行连接。在使用预置集群时，Amazon
    Redshift将利用称为Amazon Redshift Spectrum的节点群集，进一步隔离Amazon S3处理，并应用优化，如谓词推送和聚合到Amazon
    Redshift Spectrum计算层，提高查询性能。您可以推送到Amazon Redshift Spectrum的谓词运算符类型包括：`=`, `LIKE`,
    `IS NULL`和`CASE WHEN`。此外，您还可以使用转换逻辑，在Amazon Redshift Spectrum层推送许多聚合和字符串函数。聚合函数的类型包括：`COUNT`,
    `SUM`, `AVG`, `MIN`和`MAX`。
- en: Amazon Redshift Spectrum processes Amazon S3 data using compute up to 10 times
    the number of slices in your provisioned cluster. It also comes with a cost of
    $5/TB scanned. In contrast, when you query Amazon S3 data using Amazon Redshift
    serverless, the processing occurs on your Amazon Redshift compute and the cost
    is part of RPU pricing.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift Spectrum 使用计算资源处理 Amazon S3 数据，计算资源是您的预配集群中切片数量的 10 倍。此外，扫描每
    TB 数据的成本为 $5。相比之下，使用 Amazon Redshift 无服务器查询 Amazon S3 数据时，处理发生在您的 Amazon Redshift
    计算资源上，并且成本包含在 RPU 定价中。
- en: Querying external Amazon S3 data works by leveraging an external `metadata catalog`
    that organizes datasets into `databases` and `tables`. You then map a database
    to an Amazon Redshift `schema` and provide credentials via an `IAM ROLE`, which
    determines what level of access you have. In [Example 4-15](#query4-15), your
    metadata catalog is the AWS Glue `data catalog`, which contains a database called
    `externaldb`. If that database doesn’t exist, this command will create it. We’ve
    mapped that database to a new schema, `externalschema`, using the `default` IAM
    role attached to the data warehouse. In addition to the AWS Glue `data catalog`,
    users may map to a `hive metastore` if your data is located in an EMR cluster
    or in a self-managed Apache Hadoop environment. For more details on options when
    creating external schema, see the [online documentation](https://oreil.ly/FtiZ-).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 查询外部 Amazon S3 数据通过利用一个外部 `metadata catalog`，该目录将数据集组织成 `databases` 和 `tables`。然后，您将一个数据库映射到
    Amazon Redshift 的 `schema`，并通过 `IAM ROLE` 提供凭据，确定您的访问级别。在 [示例 4-15](#query4-15)
    中，您的 metadata catalog 是 AWS Glue `data catalog`，其中包含一个名为 `externaldb` 的数据库。如果该数据库不存在，此命令将创建它。我们已将该数据库映射到一个新的架构
    `externalschema`，使用附加到数据仓库的 `default` IAM 角色。除了 AWS Glue `data catalog` 外，用户还可以映射到
    `hive metastore`，如果您的数据位于 EMR 集群或自管理的 Apache Hadoop 环境中。有关创建外部架构选项的更多详细信息，请参阅
    [在线文档](https://oreil.ly/FtiZ-)。
- en: Example 4-15\. Create external S3 schema
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-15\. 创建外部 S3 架构
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We’ve referenced the `default` IAM role. Be sure to modify the role to have
    access to manage the AWS Glue Data Catalog.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经引用了`default` IAM 角色。请确保修改角色以便访问和管理 AWS Glue 数据目录。
- en: Once the external schema is created, you can easily query the data similar to
    a table that was loaded into Amazon Redshift. In [Example 4-16](#query4-16), you
    can query data from your external table joined with data that is stored locally.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦外部架构创建完成，您可以像操作已加载到 Amazon Redshift 中的表一样轻松查询数据。在 [示例 4-16](#query4-16) 中，您可以查询与本地存储的数据进行关联的外部表数据。
- en: Example 4-16\. External S3 table access
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-16\. 外部 S3 表访问
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This query has a filter that restricts the data from the external table to January
    2022 and a simple aggregation. When using a provisioned cluster, this filter and
    partial aggregation will be processed at the Amazon Redshift Spectrum layer, reducing
    the amount of data sent to your compute nodes and improving the query performance.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 此查询具有一个筛选器，将外部表数据限制为 2022 年 1 月和一个简单的聚合操作。在使用预配集群时，此筛选器和部分聚合将在 Amazon Redshift
    Spectrum 层处理，从而减少发送到计算节点的数据量，并提高查询性能。
- en: Because you’ll get the best performance when querying data that is stored locally
    in Amazon Redshift, it is a best practice to have your most recent data loaded
    in Amazon Redshift and query less frequently accessed data from external sources.
    By following this strategy, you can ensure the hottest data is stored closest
    to the compute and in a format that is optimized for analytical processing. In
    [Example 4-17](#query4-17), you may have a load process that populates the transaction
    table with the latest month of data but have all of your data in Amazon S3\. When
    exposed to your users, they will see a consolidated view of the data, but when
    they access the hottest data, Amazon Redshift will retrieve it from local storage.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在 Amazon Redshift 中查询本地存储的数据时可以获得最佳性能，因此将最近的数据加载到 Amazon Redshift 并从外部来源查询不经常访问的数据是最佳实践。通过遵循这一策略，您可以确保最热的数据存储在计算资源附近，并且以优化了的分析处理格式存在。在
    [示例 4-17](#query4-17) 中，您可能会有一个加载过程，用最新一个月的数据填充交易表，但所有数据都存在于 Amazon S3 中。当用户访问时，他们将看到数据的汇总视图，但当他们访问最热的数据时，Amazon
    Redshift 将从本地存储中检索它。
- en: Example 4-17\. Union S3 and local data
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-17\. 合并 S3 和本地数据
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The clause `NO SCHEMA BINDING` must be used for external tables to ensure that
    data can be loaded in Amazon S3 without any impact or dependency on Amazon Redshift.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`NO SCHEMA BINDING` 子句必须用于外部表，以确保数据可以在 Amazon S3 中加载，而不会对 Amazon Redshift 造成任何影响或依赖。'
- en: For more details on Amazon Redshift Spectrum optimization techniques, see the
    [Amazon Redshift Spectrum best practices blog](https://oreil.ly/A1_Xz).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Amazon Redshift Spectrum 优化技术的更多详细信息，请参阅 [Amazon Redshift Spectrum 最佳实践博客](https://oreil.ly/A1_Xz)。
- en: External Operational Data
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部运营数据
- en: Amazon Redshift federated query enables you to directly query data stored in
    transactional databases for real-time data integration and simplified ETL processing.
    Using federated query, you can provide real-time insights to your users. A typical
    use case is when you have a batch ingestion to your data warehouse, but you have
    a requirement for real-time analytics. You can provide a combined view of the
    data loaded in batch from Amazon Redshift, and the current real-time data in transactional
    database. Federated query also exposes the metadata from these source databases
    as external tables, allowing BI tools like Tableau and Amazon QuickSight to query
    federated sources. This enables new data warehouse use cases where you can seamlessly
    query operational data, simplify ETL pipelines, and build data into a late-binding
    view combining operational data with Amazon Redshift local data. As of 2022, the
    transactional databases supported include Amazon Aurora PostgreSQL/MySQL and Amazon
    RDS for PostgreSQL/MySQL.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 联合查询允许您直接查询存储在事务性数据库中的数据，实现实时数据集成和简化的 ETL 处理。使用联合查询，您可以为用户提供实时洞察。一个典型的用例是当您向数据仓库批量注入数据时，但您需要实时分析时。您可以提供
    Amazon Redshift 批量加载数据和事务性数据库当前实时数据的结合视图。联合查询还将这些源数据库的元数据暴露为外部表，允许像 Tableau 和
    Amazon QuickSight 这样的 BI 工具查询联合数据源。这为您提供了无缝查询运营数据、简化 ETL 流水线以及构建数据到绑定视图的新数据仓库用例。截至
    2022 年，支持的事务性数据库包括 Amazon Aurora PostgreSQL/MySQL 和 Amazon RDS for PostgreSQL/MySQL。
- en: Amazon Redshift federated query works by making a TCP/IP connection to your
    operational data store and mapping that to an external schema. You provide the
    database type and connection information as well as the connection credentials
    via an AWS Secrets Manager secret. In [Example 4-18](#query4-18), the database
    type is `POSTGRES` with the connection information specifying the `DATABASE`,
    `SCHEMA`, and `URI` of the DB. For more details on options when creating external
    schema using federated query, see the [online documentation](https://oreil.ly/zhWJR).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 联合查询通过与您的操作数据存储建立 TCP/IP 连接并将其映射到外部模式来工作。您需要通过 AWS Secrets Manager
    密钥提供数据库类型和连接信息以及连接凭据。在 [示例 4-18](#query4-18) 中，数据库类型为 `POSTGRES`，连接信息指定了数据库的 `DATABASE`、`SCHEMA`
    和 `URI`。有关使用联合查询创建外部模式时的选项的详细信息，请参阅 [在线文档](https://oreil.ly/zhWJR)。
- en: Example 4-18\. Create external schema
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-18\. 创建外部模式
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We’ve referenced the `default` IAM role. Be sure to modify the role to grant
    access to use Secrets Manager to retrieve a secret named `pgsecret`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引用了 `default` IAM 角色。确保修改角色以授予使用 Secrets Manager 检索名为 `pgsecret` 的秘密的访问权限。
- en: Once the external schema is created, you can query the tables as you would query
    a local Amazon Redshift table. In [Example 4-19](#query4-19), you can query data
    from your external table joined with data that is stored locally, similar to the
    query executed when querying external Amazon S3 data. The query also has a filter
    that restricts the data from the federated table to January 2022\. Amazon Redshift
    federated query intelligently pushes down predicates to restrict the amount of
    data scanned from the federated source, greatly improving query performance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 创建外部模式后，您可以像查询本地 Amazon Redshift 表一样查询这些表。在 [示例 4-19](#query4-19) 中，您可以查询从外部表获取的数据，与本地存储的数据进行连接，类似于查询外部
    Amazon S3 数据时执行的查询。查询还包含一个筛选器，将联合表中的数据限制为 2022 年 1 月。Amazon Redshift 联合查询智能地将谓词推送到联合源以限制扫描的数据量，极大地提高了查询性能。
- en: Example 4-19\. External table access
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-19\. 外部表访问
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since federated query executes queries on the transactional system, be careful
    to limit the data queried. A good practice is to use data in Amazon Redshift local
    tables for historical data and access only the latest data in the federated database.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于联合查询在事务系统上执行查询，请注意限制查询的数据。一个好的实践是使用 Amazon Redshift 本地表中的历史数据，并仅访问联合数据库中的最新数据。
- en: 'In addition to querying live data, federated query opens up opportunities to
    simplify ETL processes. A common ETL pattern many organizations use when building
    their data warehouse is `upsert`. An `upsert` is when data engineers are tasked
    with scanning the source of your data warehouse table and determining if it should
    have new records inserted or existing records updated/deleted. In the past, that
    was accomplished in multiple steps:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 除了查询实时数据外，联合查询还开辟了简化 ETL 过程的机会。许多组织在构建其数据仓库时使用的常见 ETL 模式是 `upsert`。 `upsert`
    是指数据工程师任务是扫描数据仓库表的源，确定是否应插入新记录或更新/删除现有记录。过去，通常需要多个步骤来完成此操作：
- en: Creating a full extract of your source table, or if your source has change tracking,
    extracting those records since the last time the load was processed.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建源表的完全抽取，或者如果您的源有变更跟踪，则提取自上次加载处理以来的记录。
- en: Moving that extract to a location local to your data warehouse. In the case
    of Amazon Redshift, that would be Amazon S3.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该抽取移动到您数据仓库的本地位置。对于亚马逊 Redshift，这将是 Amazon S3。
- en: Using a bulk loader to load that data into a staging table. In the case of Amazon
    Redshift, that would be the `COPY` command.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用批量加载器将数据加载到暂存表中。对于亚马逊 Redshift，这将是 `COPY` 命令。
- en: Executing `MERGE` (`UPSERT`—`UPDATE` and `INSERT`) commands against your target
    table based on the data that was staged.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据已经暂存的数据执行 `MERGE`（`UPSERT`—`UPDATE` 和 `INSERT`）命令，以更新目标表。
- en: With federated query, you can bypass the need for incremental extracts in Amazon
    S3 and the subsequent load via `COPY` by querying the data in place within its
    source database. In [Example 4-20](#query4-20), we’ve shown how the customer table
    can be synced from the operational source by using a single `MERGE` statement.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用联合查询，您可以绕过在 Amazon S3 中进行增量提取和随后通过 `COPY` 加载数据的需求，直接在其源数据库中的原地查询数据。在 [示例 4-20](#query4-20)
    中，我们展示了如何通过单个 `MERGE` 语句从操作源同步客户表。
- en: Example 4-20\. Incremental update with `MERGE`
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-20\. 使用 `MERGE` 进行增量更新
- en: '[PRE19]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For more details on federated query optimization techniques, see the blog post
    [“Best Practices for Amazon Redshift Federated Query”](https://oreil.ly/fhtqi),
    and for more details on other ways you can simplify your ETL strategy, see the
    blog post [“Build a Simplified ETL and Live Data Query Solution Using Amazon Redshift
    Federated Query”](https://oreil.ly/wYhtm).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解有关联合查询优化技术的更多详细信息，请参阅博文 [“亚马逊 Redshift 联合查询的最佳实践”](https://oreil.ly/fhtqi)，以及有关简化
    ETL 策略的其他方法的更多详细信息，请参阅博文 [“使用亚马逊 Redshift 联合查询构建简化的 ETL 和实时数据查询解决方案”](https://oreil.ly/wYhtm)。
- en: External Amazon Redshift Data
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部亚马逊 Redshift 数据
- en: Amazon Redshift data sharing enables you to directly query live data stored
    in the Amazon RMS of another Amazon Redshift data warehouse, whether it is a provisioned
    cluster using the RA3 node type or a serverless data warehouse. This functionality
    enables data produced in one Amazon Redshift data warehouse to be accessible in
    another Amazon Redshift data warehouse. Similar to other external data sources,
    the data sharing functionality also exposes the metadata from the producer Amazon
    Redshift data warehouse as external tables, allowing the consumer to query that
    data without having to make local copies. This enables new data warehouse use
    cases such as distributing the ownership of the data and isolating the execution
    of different workloads. In [Chapter 7, “Collaboration with Data Sharing”](ch07.html#AR_TGD_CH7),
    we’ll go into more detail on these use cases. In the following example, you’ll
    learn how to configure a datashare using SQL statement and how it can be used
    in your ETL/ELT processes. For more details on how you can enable and configure
    data sharing from the Redshift console, see the [online documentation](https://oreil.ly/3DCQT).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 Redshift 数据共享使您能够直接查询存储在另一个亚马逊 Redshift 数据仓库的 Amazon RMS 中的实时数据，无论是使用 RA3
    节点类型的预配集群还是服务器无数据仓库。此功能使得在另一个亚马逊 Redshift 数据仓库中访问在一个亚马逊 Redshift 数据仓库中生成的数据成为可能。与其他外部数据源类似，数据共享功能还将来自生产者亚马逊
    Redshift 数据仓库的元数据作为外部表暴露出来，允许消费者在不必制作本地副本的情况下查询该数据。这使得新的数据仓库用例成为可能，例如分配数据所有权和隔离不同工作负载的执行。在
    [第 7 章，“与数据共享协作”](ch07.html#AR_TGD_CH7) 中，我们将更详细地介绍这些用例。在接下来的示例中，您将了解如何使用 SQL
    语句配置数据共享以及如何在您的 ETL/ELT 过程中使用它。有关如何从 Redshift 控制台启用和配置数据共享的详细信息，请参阅 [在线文档](https://oreil.ly/3DCQT)。
- en: The first step in data sharing is to understand the `namespace` of your producer
    and consumer data warehouses. Execute the following on each data warehouse to
    retrieve the corresponding values ([Example 4-21](#query4-21)).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 数据共享的第一步是了解生产者和消费者数据仓库的`命名空间`。在每个数据仓库上执行以下操作以检索相应的值（[示例 4-21](#query4-21)）。
- en: Example 4-21\. Current namespace
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-21\. 当前命名空间
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, create a datashare object and add database objects such as a `schema`
    and `table` in the producer data warehouse ([Example 4-22](#query4-21a)).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在生产者数据仓库中创建一个数据共享对象，并添加诸如`架构`和`表`等数据库对象（[示例 4-22](#query4-21a)）。
- en: Example 4-22\. Create datashare
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-22\. 创建数据共享
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You can now grant the datashare access from the producer to the consumer by
    referencing its `namespace` ([Example 4-23](#query4-22)).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以通过引用其`命名空间`（[示例 4-23](#query4-22)）将数据共享的访问权从生产者授予给消费者。
- en: Example 4-23\. Grant datashare usage
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-23\. 授予数据共享使用权
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Lastly, you create a database on the consumer, referencing the datashare name
    as well as the `namespace` of the producer ([Example 4-24](#query4-23)).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您在消费者端创建一个数据库，引用数据共享名称以及生产者的`命名空间`（[示例 4-24](#query4-23)）。
- en: Example 4-24\. Create datashare database
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-24\. 创建数据共享数据库
- en: '[PRE23]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Datashares can also be granted across accounts. In this scenario, an additional
    step is required by the administrator associated with the datashare. See the [online
    documentation](https://oreil.ly/072-D) for more information.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 数据共享还可以跨帐户授予。在这种情况下，与数据共享相关的管理员需要执行额外的步骤。有关更多信息，请参阅[在线文档](https://oreil.ly/072-D)。
- en: Once the external database is created, you can easily query the data as you
    would a table that was local to your Amazon Redshift data warehouse. In [Example 4-25](#query4-24),
    you’re querying data from your external table joined with data that is stored
    locally, similar to the query executed when using external Amazon S3 and operational
    data. Similarly, the query has a filter that restricts the data from the external
    table to January 2022.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦外部数据库创建完成，您可以像本地Amazon Redshift数据仓库中的表一样轻松查询数据。在[示例 4-25](#query4-24)中，您正在查询外部表与本地存储的数据联接，类似于使用外部Amazon
    S3和运营数据时执行的查询。同样，该查询包含一个筛选器，将外部表的数据限制为2022年1月的数据。
- en: Example 4-25\. Datashare access
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-25\. 数据共享访问
- en: '[PRE24]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You can imagine a setup where one department may be responsible for managing
    sales transactions and another department is responsible for customer relations.
    The customer relations department is interested in determining their best and
    worst customers to send targeted marketing to. Instead of needing to maintain
    a single data warehouse and share resources, each department can leverage their
    own Amazon Redshift data warehouse and be responsible for their own data. Instead
    of duplicating the transaction data, the customer relations group can query it
    directly. They can build and maintain an aggregate of that data and join it with
    data they have on previous marketing initiatives as well as customer sentiment
    data to construct their marketing campaign.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以想象一种设置，其中一个部门可能负责管理销售交易，另一个部门负责客户关系。客户关系部门有兴趣确定他们最好和最差的客户，以便发送定向营销。与其需要维护单一的数据仓库并共享资源，每个部门可以利用自己的Amazon
    Redshift数据仓库，并负责自己的数据。客户关系团队可以直接查询而不是复制交易数据。他们可以构建和维护该数据的聚合，并将其与以前的营销活动数据以及客户情感数据联接，以构建他们的营销活动。
- en: Read more about data sharing in [“Sharing Amazon Redshift Data Securely Across
    Amazon Redshift Clusters for Workload Isolation”](https://oreil.ly/UOfaa) and
    [“Amazon Redshift Data Sharing Best Practices and Considerations”](https://oreil.ly/jf1Mu).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多有关数据共享的信息，请阅读[“在Amazon Redshift集群之间安全共享Amazon Redshift数据”](https://oreil.ly/UOfaa)和[“Amazon
    Redshift数据共享的最佳实践和考虑因素”](https://oreil.ly/jf1Mu)。
- en: External Transformation
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部转换
- en: In scenarios where you want to use an external tool for your data transformations,
    Amazon Redshift can connect to an ETL platform of your choice using JDBC and ODBC
    drivers either packaged with those applications or which can be downloaded. Popular
    ETL platforms that integrate with Amazon Redshift include third-party tools like
    [Informatica](https://www.informatica.com), [Matillion](https://www.matillion.com),
    and [dbt](https://www.getdbt.com) as well as AWS-native tools like [“AWS Glue”](#aws_glue).
    ETL tools are a valuable way to manage all the components of your data pipeline.
    They provide a job repository to organize and maintain metadata, making it easier
    for organizations to manage their code instead of storing that logic in SQL scripts
    and stored procedures. They also have scheduling capabilities to facilitate job
    orchestration, which can be useful if you are not using the [“Scheduling and Orchestration”](#scheduling_orchestration)
    available natively in AWS.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在希望使用外部工具进行数据转换的场景中，Amazon Redshift 可以通过 JDBC 和 ODBC 驱动程序连接到您选择的 ETL 平台，这些驱动程序可以是这些应用程序打包的，也可以是可下载的。与
    Amazon Redshift 集成的流行 ETL 平台包括第三方工具如[Informatica](https://www.informatica.com)、[Matillion](https://www.matillion.com)和[dbt](https://www.getdbt.com)，以及
    AWS 本地工具如[“AWS Glue”](#aws_glue)。ETL 工具是管理数据流水线所有组件的宝贵方式。它们提供作业存储库来组织和维护元数据，使组织能够更轻松地管理其代码，而不是将该逻辑存储在
    SQL 脚本和存储过程中。它们还具有调度功能，有助于作业编排，这在您没有使用 AWS 本地提供的[“调度和编排”](#scheduling_orchestration)时非常有用。
- en: Some ETL tools also have the ability to “push down” the transformation logic.
    In the case where you may be reading and writing from your Amazon Redshift data
    warehouse, you can design your job using the visual capabilities of the ETL tool,
    but instead of actually extracting the data to the compute of the ETL server(s),
    the code is converted into SQL statements that run on Amazon Redshift. This strategy
    can be very performant when transforming high quantities of data, but can also
    consume a lot of resources that your end users may need for analyzing data. When
    you are not using the push-down capabilities of your ETL tool, either because
    your job is not reading and writing to Amazon Redshift or because you’ve decided
    you want to offload the transformation logic, it’s important to ensure that your
    ETL tool is reading and writing data from Amazon Redshift in a performant way.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 ETL 工具还具有“下推”转换逻辑的能力。在您可能需要从 Amazon Redshift 数据仓库读取和写入数据的情况下，您可以使用 ETL 工具的可视化能力设计作业，但不是将数据实际提取到
    ETL 服务器的计算中，而是将代码转换为在 Amazon Redshift 上运行的 SQL 语句。当转换大量数据时，这种策略可以非常高效，但也可能消耗您的最终用户可能需要用于分析数据的大量资源。当您不使用
    ETL 工具的下推功能时，无论是因为您的作业不是从 Amazon Redshift 读取和写入数据，还是因为您决定要卸载转换逻辑，都很重要确保您的 ETL
    工具以高效的方式从 Amazon Redshift 读取和写入数据。
- en: 'As discussed in [Chapter 3, “Setting Up Your Data Models and Ingesting Data”](ch03.html#AR_TGD_CH3),
    the most performant way to load data is to use the `COPY` statement. Because of
    the partnership between AWS and ETL vendors like Informatica and Matillion, AWS
    has ensured that vendors have built connectors with this strategy in mind. For
    example, in the Informatica Amazon Redshift architecture in [Figure 4-9](#informatica_redshift),
    you can see that if you have specified an Amazon Redshift target and a staging
    area in Amazon S3, instead of directly loading the target via an insert, the tool
    will instead write to Amazon S3 and then use the Amazon Redshift `COPY` statement
    to load into the target table. This same strategy also works for `update` and
    `delete` statements, except instead of directly loading the target table, Informatica
    will write to a staging table and perform postload `update` and `delete` statements.
    This optimization is possible because AWS partners with multiple software vendors
    to ensure that users easily leverage the tool and ensure their data pipelines
    are performant. See the following guides, which have been published for more details
    on best practices when using some of the popular third-party ETL tools:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第3章，“设置数据模型和数据摄入”](ch03.html#AR_TGD_CH3)中讨论的那样，加载数据的最佳性能方式是使用`COPY`语句。由于AWS与Informatica和Matillion等ETL供应商的合作，AWS确保供应商基于这一策略构建了连接器。例如，在Informatica
    Amazon Redshift架构的[图4-9](#informatica_redshift)中，您可以看到，如果您已指定了Amazon Redshift目标和Amazon
    S3中的临时区域，工具将不会直接通过插入加载目标，而是会先将数据写入Amazon S3，然后使用Amazon Redshift的`COPY`语句加载到目标表中。这种策略对于`update`和`delete`语句同样适用，只是Informatica会将数据写入临时表，并执行后续的`update`和`delete`语句。这种优化得益于AWS与多个软件供应商的合作，确保用户轻松利用工具并确保其数据管道的性能。详细的最佳实践指南，请参阅以下已发布的内容：
- en: '[Informatica—Amazon Redshift Connector Best Practices](https://oreil.ly/CX57A)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Informatica——Amazon Redshift连接器最佳实践](https://oreil.ly/CX57A)'
- en: '[Matillion—Amazon Redshift Best Practices for ETL Processing](https://oreil.ly/0jRs2)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Matillion——Amazon Redshift ETL处理的最佳实践](https://oreil.ly/0jRs2)'
- en: '[dbt—Best Practices for Leveraging Amazon Redshift and dbt](https://oreil.ly/mRYaI)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[dbt——利用Amazon Redshift和dbt的最佳实践](https://oreil.ly/mRYaI)'
- en: '![Informatica Amazon Redshift architecture](assets/ardg_0409.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![Informatica Amazon Redshift架构](assets/ardg_0409.png)'
- en: Figure 4-9\. Informatica Amazon Redshift architecture
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9\. Informatica Amazon Redshift架构
- en: AWS Glue
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Glue
- en: AWS Glue is one of the native serverless data integration services commonly
    used to transform data using Python or Scala language and run on a data processing
    engine. With AWS Glue ([Figure 4-10](#AWS_Glue_Architecture)), you can read Amazon
    S3 data, apply transformations, and ingest data into Amazon Redshift data warehouses
    as well other data platforms. AWS Glue makes it easier to discover, prepare, move,
    and integrate data from multiple sources for analytics, ML, and application development.
    It offers multiple data integration engines, which include AWS Glue for Apache
    Spark, AWS Glue for Ray, and AWS Glue for Python Shell. You can use the appropriate
    engine for your workload, based on the characteristics of your workload and the
    preferences of your developers and analysts.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue是常用的本地无服务器数据集成服务之一，可使用Python或Scala语言进行数据转换，并在数据处理引擎上运行。通过AWS Glue（[图4-10](#AWS_Glue_Architecture)），您可以读取Amazon
    S3数据，应用转换，并将数据摄入Amazon Redshift数据仓库以及其他数据平台。AWS Glue使发现、准备、移动和集成来自多个来源的数据，以进行分析、机器学习和应用程序开发变得更加简单。它提供多个数据集成引擎，包括AWS
    Glue for Apache Spark、AWS Glue for Ray和AWS Glue for Python Shell。您可以根据工作负载的特性和开发人员与分析师的偏好选择合适的引擎。
- en: '![ETL integration using AWS Glue](assets/ardg_0410.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![使用AWS Glue进行ETL集成](assets/ardg_0410.png)'
- en: Figure 4-10\. ETL integration using AWS Glue
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-10\. 使用AWS Glue进行ETL集成
- en: Since AWS Glue V4, a new Amazon Redshift Spark connector with a new JDBC driver
    is featured with AWS Glue ETL jobs. You can use it to build Apache Spark applications
    that read from and write to data in Amazon Redshift as part of your data ingestion
    and transformation pipelines. The new connector and driver supports pushing down
    relational operations such as joins, aggregations, sort, and scalar functions
    from Spark to Amazon Redshift to improve your job performance by reducing the
    amount of data needing to be processed. It also supports IAM-based roles to enable
    single sign-on capabilities and integrates with AWS Secrets Manager for securely
    managing keys.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 自 AWS Glue V4 起，AWS Glue ETL 作业配备了一个新的 Amazon Redshift Spark 连接器和新的 JDBC 驱动程序。您可以使用它构建
    Apache Spark 应用程序，作为数据摄取和转换管道的一部分，读取和写入 Amazon Redshift 中的数据。新的连接器和驱动程序支持将关系操作（如连接、聚合、排序和标量函数）从
    Spark 推送到 Amazon Redshift，以减少需要处理的数据量，从而提高作业性能。它还支持基于 IAM 的角色，以实现单点登录功能，并与 AWS
    Secrets Manager 集成，用于安全管理密钥。
- en: To manage your AWS Glue jobs, AWS provides a visual authoring tool, [AWS Glue
    Studio](https://oreil.ly/vvdyo). This service follows many of the same best practices
    as the third-party ETL tools already mentioned; however, because of the integration,
    it requires fewer steps to build and manage your data pipelines.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 要管理您的 AWS Glue 作业，AWS 提供了一种可视化的创作工具，[AWS Glue Studio](https://oreil.ly/vvdyo)。该服务遵循已经提到的第三方
    ETL 工具的许多最佳实践，但由于集成，构建和管理数据管道所需的步骤更少。
- en: In [Example 4-26](#query4-25), we will build a job that loads incremental transaction
    data from Amazon S3 and merge it into a `lineitem` table using the key (`l_orderkey`,
    `l⁠_⁠l⁠i⁠n⁠e​n⁠u⁠m⁠b⁠e⁠r`) in your Amazon Redshift data warehouse.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 4-26](#query4-25) 中，我们将构建一个作业，从 Amazon S3 加载增量交易数据，并将其合并到您的 Amazon Redshift
    数据仓库中的 `lineitem` 表中，使用键 (`l_orderkey`, `l⁠_⁠l⁠i⁠n⁠e​n⁠u⁠m⁠b⁠e⁠r`)。
- en: Example 4-26\. Create `lineitem` table
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-26\. 创建 `lineitem` 表
- en: '[PRE25]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: To build a Glue job, we will follow the instructions in the next two sections.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个 Glue 作业，请按照接下来的两个部分的说明操作。
- en: Register Amazon Redshift target connection
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注册 Amazon Redshift 目标连接
- en: Navigate to [“Create connection”](https://oreil.ly/Jlypu) to create a new AWS
    Glue connection. Name the connection and choose the connection type of Amazon
    Redshift (see [Figure 4-11](#amazon_redshift_connection_name_ch04_1692201404126)).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 转到 [“创建连接”](https://oreil.ly/Jlypu) 创建一个新的 AWS Glue 连接。命名连接并选择 Amazon Redshift
    作为连接类型（参见 [图 4-11](#amazon_redshift_connection_name_ch04_1692201404126)）。
- en: '![Amazon Redshift connection name](assets/ardg_0411.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![Amazon Redshift 连接名称](assets/ardg_0411.png)'
- en: Figure 4-11\. Amazon Redshift connection name
  id: totrans-193
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-11\. Amazon Redshift 连接名称
- en: Next, select the database instance from the list of autodiscovered Amazon Redshift
    data warehouses found in your AWS account and region. Set the database name and
    access credentials. You have an option of either setting a username and password
    or using AWS Secrets Manager. Finally, click “Create connection” (see [Figure 4-12](#amazon_redshift_connection_instance_ch04_1692201519346)).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从您的 AWS 帐户和区域的自动发现的 Amazon Redshift 数据仓库列表中选择数据库实例。设置数据库名称和访问凭据。您可以选择设置用户名和密码，或使用
    AWS Secrets Manager。最后，点击“创建连接”（参见 [图 4-12](#amazon_redshift_connection_instance_ch04_1692201519346)）。
- en: '![Amazon Redshift connection instance](assets/ardg_0412.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![Amazon Redshift 连接实例](assets/ardg_0412.png)'
- en: Figure 4-12\. Amazon Redshift connection instance
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-12\. Amazon Redshift 连接实例
- en: Build and run your AWS Glue job
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建并运行您的 AWS Glue 作业
- en: To build an AWS Glue job, navigate to the AWS Glue Studio [jobs page](https://oreil.ly/G873X).
    You will see a dialog prompting you with options for your job ([Figure 4-13](#glue_create_job)).
    In this example, we will select “Visual with a source and target.” Modify the
    target to Amazon Redshift and select Create.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个 AWS Glue 作业，请导航至 AWS Glue Studio 的 [作业页面](https://oreil.ly/G873X)。您将看到一个对话框，提示您选择作业的选项（参见
    [图 4-13](#glue_create_job)）。在本例中，我们将选择“可视化，带有源和目标”。将目标修改为 Amazon Redshift，并选择创建。
- en: '![Glue Create job](assets/ardg_0413.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![Glue 创建作业](assets/ardg_0413.png)'
- en: Figure 4-13\. AWS Glue Create job
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-13\. AWS Glue 创建作业
- en: 'Next, you will be presented with a visual representation of your job. The first
    step will be to select the data source node and set the S3 source type ([Figure 4-14](#glue_set_s3_bucket)).
    For our use case we’ll use an S3 location and enter the location of our data:
    s3://redshift-immersionday-labs/data/lineitem-part/. Choose the parsing details
    such as the data format, delimiter, escape character, etc. For our use case, the
    files will have a CSV format, are pipe (|) delimited, and do not have column headers.
    Finally, click the “Infer schema” button.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将看到作业的可视化表示。第一步是选择数据源节点并设置 S3 源类型（[图 4-14](#glue_set_s3_bucket)）。对于我们的用例，我们将使用一个
    S3 位置，并输入我们数据的位置：s3://redshift-immersionday-labs/data/lineitem-part/。选择解析细节，如数据格式、分隔符、转义字符等。对于我们的用例，文件将采用
    CSV 格式，是以管道符（|）分隔的，并且没有列标题。最后，点击“推断模式”按钮。
- en: '![AWS Glue Set Amazon S3 bucket](assets/ardg_0414.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![AWS Glue 设置 Amazon S3 存储桶](assets/ardg_0414.png)'
- en: Figure 4-14\. AWS Glue set Amazon S3 bucket
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-14\. AWS Glue 设置 Amazon S3 存储桶
- en: If you have established a data lake that you are using for querying with other
    AWS services like Amazon Athena, Amazon EMR, or even Amazon Redshift as an external
    table, you can alternatively use the “Data Catalog table” option.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已建立了一个数据湖，并且正在使用它来查询其他 AWS 服务，如 Amazon Athena、Amazon EMR，甚至是作为外部表的 Amazon
    Redshift，您也可以选择使用“数据目录表”选项。
- en: Next, we can transform our data ([Figure 4-15](#glue_apply_mapping)). The job
    is built with a simple ApplyMapping node, but you have many options for transforming
    your data such as joining, splitting, and aggregating data. See [“Editing AWS
    Glue Managed Data Transform Nodes” AWS documentation](https://oreil.ly/JwlWy)
    for additional transform nodes. Select the Transform node and set the target key
    that matches the source key. In our case, the source data did not have column
    headers and were registered with generic columns (col#). Map them to the corresponding
    columns in your `lineitem` table.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以转换我们的数据（[图 4-15](#glue_apply_mapping)）。该作业建立在一个简单的 ApplyMapping 节点上，但您有许多选项来转换您的数据，如连接、拆分和聚合数据。请参阅[“编辑
    AWS Glue 管理的数据转换节点” AWS 文档](https://oreil.ly/JwlWy)了解更多转换节点信息。选择转换节点，并设置与源键匹配的目标键。在我们的情况下，源数据没有列标题，并且以通用列（col#）注册。将它们映射到您
    `lineitem` 表中的相应列。
- en: '![AWS Glue apply mapping](assets/ardg_0415.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![AWS Glue 应用映射](assets/ardg_0415.png)'
- en: Figure 4-15\. AWS Glue apply mapping
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-15\. AWS Glue 应用映射
- en: Now you can set the Amazon Redshift details ([Figure 4-16](#glue_set_redshift)).
    Choose “Direct data connection” and select the applicable schema (public) and
    table (lineitem). You can also set how the job will handle new records; you can
    either just insert every record or set a key so the job can update data that needs
    to be reprocessed. For our use case, we’ll choose `MERGE` and set the key l_orderkey
    and l_linenumber. By doing so, when the job runs, the data will first be loaded
    into a staging table, then a `MERGE` statement will be run based on any data that
    already exists in the target before the new data is loaded with an `INSERT` statement.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以设置 Amazon Redshift 的详细信息（[图 4-16](#glue_set_redshift)）。选择“直接数据连接”，并选择适用的模式（public）和表（lineitem）。您还可以设置作业如何处理新记录；您可以选择仅插入每条记录或设置一个键，以便作业可以更新需要重新处理的数据。对于我们的用例，我们将选择`MERGE`，并设置键
    `l_orderkey` 和 `l_linenumber`。这样做的好处是，当作业运行时，数据将首先加载到暂存表中，然后基于目标中已存在的任何数据运行`MERGE`语句，然后再使用`INSERT`语句加载新数据。
- en: '![Glue set Amazon Redshift target](assets/ardg_0416.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![Glue 设置 Amazon Redshift 目标](assets/ardg_0416.png)'
- en: Figure 4-16\. AWS Glue set Amazon Redshift target
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-16\. AWS Glue 设置 Amazon Redshift 目标
- en: Before you can save and run the job, you must set some additional job details
    like the IAM role, which will be used to run the job, and the script filename
    ([Figure 4-17](#glue_set_details)). The role should have permissions to access
    the files in your Amazon S3 location and should also be able to be assumed by
    the AWS Glue service. Once you have created and set the IAM role, click Save and
    Run to execute your job.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在保存并运行作业之前，您必须设置一些额外的作业详细信息，如 IAM 角色，该角色将用于运行作业，并设置脚本文件名（[图 4-17](#glue_set_details)）。该角色应具有访问
    Amazon S3 位置中文件的权限，并且还应该能够被 AWS Glue 服务所假定。创建和设置 IAM 角色后，单击“保存并运行”来执行您的作业。
- en: '![AWS Glue set job details](assets/ardg_0417.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![AWS Glue 设置作业详细信息](assets/ardg_0417.png)'
- en: Figure 4-17\. AWS Glue set job details
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-17\. AWS Glue 设置作业详细信息
- en: You can inspect the job run by navigating to the Runs tab. You will see details
    about the job ID and the run statistics ([Figure 4-18](#glue_run_details)).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过导航到“运行”选项卡来检查作业运行。您将看到有关作业 ID 和运行统计信息的详细信息（[图 4-18](#glue_run_details)）。
- en: '![AWS Glue job run details](assets/ardg_0418.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![AWS Glue 作业运行详情](assets/ardg_0418.png)'
- en: Figure 4-18\. AWS Glue job run details
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-18\. AWS Glue 作业运行详情
- en: For AWS Glue to access Amazon S3, you will need to create a VPC endpoint if
    you have not created one already. See the [online documentation](https://oreil.ly/440WX)
    for more details.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 要让 AWS Glue 访问 Amazon S3，如果还没有创建 VPC 终端节点，您需要先创建一个。请参阅[在线文档](https://oreil.ly/440WX)获取更多详细信息。
- en: Once the job is completed, you can navigate to the [Amazon Redshift console](https://oreil.ly/aPeIg)
    to inspect the queries and loads ([Figure 4-19](#redshift_query_history)). You’ll
    see the queries required to create the temporary table, load the Amazon S3 files,
    and execute the merge statement that deletes old data and inserts new data.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 作业完成后，您可以导航到[Amazon Redshift 控制台](https://oreil.ly/aPeIg)查看查询和加载情况（[图 4-19](#redshift_query_history)）。您将看到创建临时表、加载
    Amazon S3 文件以及执行合并语句以删除旧数据并插入新数据所需的查询。
- en: '![Amazon Redshift query history](assets/ardg_0419.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![Amazon Redshift 查询历史](assets/ardg_0419.png)'
- en: Figure 4-19\. Amazon Redshift query history
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-19\. Amazon Redshift 查询历史
- en: Summary
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter described the various ways you can transform data using Amazon
    Redshift. With Amazon Redshift’s ability to access all your data whether it has
    been loaded or not, you can transform data in your data lake, operational sources,
    or other Amazon Redshift data warehouses quickly and easily. In addition, we showed
    that you can implement time-based schedules using the Amazon Redshift query scheduler
    to orchestrate these jobs. Lastly, we covered how Amazon Redshift partners with
    third-party ETL and orchestration vendors to provide optimal execution performance
    and integrate with tools you may already have in your organization.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了使用 Amazon Redshift 转换数据的各种方法。借助 Amazon Redshift 能够访问您加载或未加载的所有数据，您可以快速轻松地转换数据湖、运营来源或其他
    Amazon Redshift 数据仓库中的数据。此外，我们展示了如何使用 Amazon Redshift 查询调度程序实现基于时间的调度以编排这些作业。最后，我们介绍了
    Amazon Redshift 如何与第三方 ETL 和编排供应商合作，以提供最佳执行性能，并与您组织中可能已有的工具集成。
- en: In the next chapter, we’ll talk about how Amazon Redshift scales when you make
    changes to your workload. We’ll also cover how an Amazon Redshift serverless data
    warehouse will automatically scale and how you have control over how to scale
    your provisioned data warehouse. We’ll also talk about how you can get the best
    price performance with Amazon Redshift by implementing best practices.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论当您对工作负载进行更改时 Amazon Redshift 如何扩展。我们还将介绍 Amazon Redshift 无服务器数据仓库如何自动扩展，以及您如何控制扩展您的预配数据仓库的方式。此外，我们还将讨论如何通过实施最佳实践在
    Amazon Redshift 中获得最佳性价比。
