- en: Chapter 28\. Real-Time Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第28章 实时机器学习
- en: In this chapter, we explore how to build *online* classification and clustering
    algorithms. By online, we mean that these algorithms learn to produce an optimal
    classification and clustering result on the fly as new data is provided to them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何构建*在线*分类和聚类算法。所谓在线，意味着这些算法能够在接收到新数据时实时学习并生成最优的分类和聚类结果。
- en: Note
  id: totrans-2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This chapter assumes that you have a basic understanding of machine learning
    algorithms, including concepts such as supervised and unsupervised learning, and
    classification versus clustering algorithms. If you want a quick brush-up of fundamental
    machine learning concepts, we recommend reading [[Conway2012]](app04.xhtml#Conway2012).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设您对机器学习算法有基本的理解，包括监督学习和无监督学习的概念，以及分类与聚类算法的区别。如果您希望快速复习基本的机器学习概念，我们建议阅读[[Conway2012]](app04.xhtml#Conway2012)。
- en: 'In the following sections, we explain the training of several machine learning
    algorithms performed in an online fashion, with data coming from a stream. Before
    that, let’s acknowledge that most industry implementations of machine learning
    models already have an online component: they perform training in a batch fashion—with
    data at rest—and link it with online inference, as examples are read over a stream
    of data and the previously trained models are used to score (or predict on) the
    streaming data.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将解释几种以在线方式执行的机器学习算法的训练过程，这些数据来自流式数据。在此之前，让我们承认，大多数行业中机器学习模型的实现已经有一个在线组件：它们以批处理方式进行训练（数据处于静止状态），并将其与在线推理连接起来，例如从数据流中读取示例并使用先前训练过的模型对流式数据进行评分（或预测）。
- en: In this sense, most machine learning algorithms are already deployed in a streaming
    context, and Spark offers features for making this task easier, whether it’s the
    simple compatibility between Spark’s batch and streaming APIs (which we’ve addressed
    in prior chapters) or external projects that aim to make deployment simpler, such
    as [MLeap](http://bit.ly/2PLbGGz).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个意义上讲，大多数机器学习算法已经在流式处理环境中部署，并且Spark提供了使这一任务更加简单的功能，无论是Spark的批处理和流处理API之间的简单兼容性（我们在前几章中已经讨论过），还是旨在简化部署的外部项目，例如[MLeap](http://bit.ly/2PLbGGz)。
- en: 'The challenge in making this architecture work—batch training followed by online
    inference—relies mostly on deploying copies of the model to sufficient machines
    to cope with the influx of requests. General horizontal scaling is an important
    engineering topic, but a broader one than what this book focuses on. We are rather
    interested in the challenge of training machine learning models when the conditions
    do not permit or encourage this pattern of batch training. This can happen for
    several reasons:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让这种架构工作的挑战——批量训练后进行在线推理——主要依赖于在足够多的机器上部署模型的副本来处理请求的涌入。一般的横向扩展是一个重要的工程话题，但比本书关注的范围更广。我们更感兴趣的是在不允许或不鼓励批量训练模式的条件下训练机器学习模型的挑战。这可能出现的原因有几个：
- en: The size of the data is such that batch training can occur only at low frequencies
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据量很大，批量训练只能以较低的频率进行。
- en: The data changes in nature frequently enough that models grow stale very fast
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的变化频繁，导致模型很快过时。
- en: As we’ll see with Hoeffding trees, the training does not require seeing all
    the data to produce accurate results
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如我们将在Hoeffding树中看到的那样，训练不需要看到所有数据就能产生准确的结果。
- en: In this chapter, we explore two online classification algorithms and one clustering
    algorithm. Those algorithms have clear batch alternatives, and as such form a
    good example of how the transformation from batch to streaming carries over to
    machine learning training.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了两种在线分类算法和一种聚类算法。这些算法都有明确的批处理替代方案，因此很好地展示了从批处理到流式处理在机器学习训练中的转换方式。
- en: The first classification algorithm that we approach is Multinomial Naive Bayes.
    It is the classifier most often used by basic spam filters and an appropriate
    classifier when you want to use discrete features (e.g., word counts for text
    classification).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先接触的分类算法是多项式朴素贝叶斯。它是基本垃圾邮件过滤器最常用的分类器，也是当你想使用离散特征（例如文本分类的单词计数）时的适当分类器。
- en: We then study the Hoeffding trees, an online version of decision trees. The
    logic behind decision trees relies on classifying the dataset by repeatedly deciding
    on the most important feature to decide on an example. But in an online context,
    we should be able to learn what the most salient input features are to classify
    our dataset, despite just looking at a fragment of it. In this chapter, we demonstrate
    how, by relying on powerful statistical bounds, the algorithm is able to achieve
    this feat economically and quickly.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们研究了霍夫丁树，这是决策树的在线版本。决策树的逻辑依赖于通过反复决定最重要的特征来对数据集进行分类。但在在线环境中，我们应该能够学习到哪些最显著的输入特征来对数据集进行分类，尽管只是查看其片段。在本章中，我们演示了通过依赖强大的统计界限，算法如何经济快速地实现这一目标。
- en: Finally, we see an online adaptation of the *K*-Means clustering algorithm offered
    in Spark Streaming natively, which buckets the data into a fixed set of groups.
    The online version uses a weight decay to reduce the impact of old examples, along
    with a technique for pruning irrelevant clusters, to ensure it constantly provides
    the most relevant results on fresh data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看到了 *K*-Means 聚类算法的在线适应版，该算法在 Spark Streaming 中本地提供，将数据分桶为一组固定的群组。在线版本使用权重衰减来减少旧示例的影响，并采用修剪无关群组的技术，以确保始终在新数据上提供最相关的结果。
- en: This chapter should equip you with powerful online techniques that should form
    an initial point for applying machine learning techniques in an online learning
    context.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章应该为您提供了强大的在线技术，这些技术应该作为在线学习背景下应用机器学习技术的初始点。
- en: Streaming Classification with Naive Bayes
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯进行流式分类
- en: Naive Bayes methods are a set of supervised learning algorithms based on applying
    Bayes’ theorem with the “naive” assumption of independence between every pair
    of features. In this section, we look in detail at a classifier for natural-language
    documents that use this technique, and we illustrate how an efficient classifier
    without any deep representation of language is realized.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Naive Bayes 方法是一组基于贝叶斯定理并采用“朴素”独立性假设的监督学习算法。在本节中，我们详细研究了一个用于自然语言文档分类的分类器，使用了这种技术，并展示了如何实现一个不需要深入语言表示的高效分类器。
- en: Multinomial Naive Bayes implements the naive Bayes algorithm for a multiple-class
    distribution of data. It is one of the two classic naive Bayes variants used in
    text classification, the other being a Bernoulli model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯实现了多类分布数据的朴素贝叶斯算法。它是文本分类中使用的两种经典朴素贝叶斯变体之一，另一种是伯努利模型。
- en: In our exploration of Multinomial Naive Bayes, we use a simple representation
    in which the data is represented as word count vectors. This means that a document
    is represented as a mixed bag of words, where a bag is a set that allows repeating
    elements and reflects only which words appear in the document and how many times
    they occur, while throwing away the word order.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索多项式朴素贝叶斯时，我们使用了一个简单的表示法，其中数据表示为单词计数向量。这意味着文档被表示为单词的混合袋，其中一个袋子是一个允许重复元素的集合，仅反映了文档中出现的单词以及它们出现的次数，而忽略了单词的顺序。
- en: 'Let’s call the collection of these documents *D*, whose class is given by *C*.
    *C* represents the different classes in the classification. For example, in the
    classic case of email spam filtering there are two classes for *C*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们称这些文档的集合为 *D*，其类别由 *C* 给出。*C* 表示分类中的不同类别。例如，在电子邮件垃圾过滤的经典案例中，*C* 有两个类别：
- en: S (spam) and
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S（垃圾邮件）和
- en: H (ham, or not spam).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: H（正常邮件，非垃圾邮件）。
- en: We classify *D* as the class that has the highest posterior probability *P*(*C*|*D*),
    which we read as *the probability of the class* C *given the document D*. We can
    reexpressed this using Bayes’ Theorem, which you can see in [Equation 28-1](#bayes).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 *D* 分类为具有最高后验概率 *P*(*C*|*D*) 的类别，这被理解为 *给定文档 D，类别 C 的概率*。我们可以使用贝叶斯定理重新表达这一点，你可以在
    [方程 28-1](#bayes) 中看到。
- en: Equation 28-1\. Bayes’ Theorem
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 28-1\. 贝叶斯定理
- en: <math alttext="upper P left-parenthesis upper C vertical-bar upper D right-parenthesis
    equals StartFraction upper P left-parenthesis upper D vertical-bar upper C right-parenthesis
    upper P left-parenthesis upper C right-parenthesis Over upper P left-parenthesis
    upper D right-parenthesis EndFraction" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <mi>C</mi> <mo>|</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>D</mi><mo>|</mo><mi>C</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>C</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>D</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis upper C vertical-bar upper D right-parenthesis
    equals StartFraction upper P left-parenthesis upper D vertical-bar upper C right-parenthesis
    upper P left-parenthesis upper C right-parenthesis Over upper P left-parenthesis
    upper D right-parenthesis EndFraction" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <mi>C</mi> <mo>|</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>D</mi><mo>|</mo><mi>C</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>C</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>D</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: To select the best class *C* for our document *D*, we express *P*(*C*|*D*) and
    try to choose the class that maximizes this expression. Using Bayes’ Theorem,
    we come to [Equation 28-1](#bayes), which expresses this as a fraction with <math
    alttext="upper P left-parenthesis upper D vertical-bar upper C right-parenthesis
    upper P left-parenthesis upper C right-parenthesis"><mrow><mi>P</mi> <mo>(</mo>
    <mi>D</mi> <mo>|</mo> <mi>C</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo> <mi>C</mi> <mo>)</mo></mrow></math>
    as a numerator. This is the quantity we’ll try to seek a maximum for, depending
    on *C*—and because it reaches a maximum when the initial fraction reaches a maximum,
    we can drop the constant denominator *P*(*D*).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择我们文档*D*的最佳类*C*，我们表达*P*(*C*|*D*)并尝试选择最大化这个表达式的类。应用贝叶斯定理，我们得到[公式28-1](#bayes)，它用<math
    alttext="upper P left-parenthesis upper D vertical-bar upper C right-parenthesis
    upper P left-parenthesis upper C right-parenthesis"><mrow><mi>P</mi> <mo>(</mo>
    <mi>D</mi> <mo>|</mo> <mi>C</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo> <mi>C</mi> <mo>)</mo></mrow></math>表示为分数的形式。这是我们尝试寻找最大值的量，取决于*C*——因为当初始分数达到最大值时，它也达到最大值，所以我们可以去掉分母*P*(*D*)的常数。
- en: Our model represents documents using feature vectors whose components correspond
    to word types. If we have a vocabulary *V*, containing |*V*| word types, then
    the feature vector has a dimension d = |V|. In a multinomial document model, a
    document is represented by a feature vector with integer elements whose value
    is the frequency of that word in the document. In that sense, representing word
    counts as <math alttext="x 1 ellipsis x Subscript n"><mrow><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>...</mo> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math> , we want to study
    <math alttext="upper P left-parenthesis x Baseline 1 comma x Baseline 2 comma
    ellipsis comma x n vertical-bar c right-parenthesis"><mrow><mi>P</mi> <mo>(</mo>
    <mi>x</mi> <mn>1</mn> <mo>,</mo> <mi>x</mi> <mn>2</mn> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <mi>x</mi> <mi>n</mi> <mo>|</mo> <mi>c</mi> <mo>)</mo></mrow></math>
    . Assuming those features are independent from one another, this just amounts
    to computing each of the <math alttext="upper P left-parenthesis x Subscript i
    Baseline vertical-bar c right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>|</mo> <mi>c</mi> <mo>)</mo></mrow></math> . In sum, we
    seek to find the class <math alttext="c Subscript j"><msub><mi>c</mi> <mi>j</mi></msub></math>
    , which reaches the maximum of <math alttext="upper P left-parenthesis c Subscript
    j Baseline right-parenthesis product upper P left-parenthesis x vertical-bar c
    Subscript j Baseline right-parenthesis"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mo>∏</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>|</mo> <msub><mi>c</mi> <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math>
    .
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型使用特征向量来表示文档，其组成部分对应于单词类型。如果我们有一个包含|*V*|个单词类型的词汇表*V*，那么特征向量的维度为d = |V|。在多项式文档模型中，一个文档由具有整数元素的特征向量表示，其值为文档中该单词的频率。在这种意义上，将单词计数表示为<math
    alttext="x 1 ellipsis x Subscript n"><mrow><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>...</mo> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>，我们想研究<math alttext="upper
    P left-parenthesis x Baseline 1 comma x Baseline 2 comma ellipsis comma x n vertical-bar
    c right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>x</mi> <mn>1</mn> <mo>，</mo>
    <mi>x</mi> <mn>2</mn> <mo>，</mo> <mo>...</mo> <mo>，</mo> <mi>x</mi> <mi>n</mi>
    <mo>|</mo> <mi>c</mi> <mo>)</mo></mrow></math>。假设这些特征彼此独立，这只是计算每个<math alttext="upper
    P left-parenthesis x Subscript i Baseline vertical-bar c right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>|</mo> <mi>c</mi> <mo>)</mo></mrow></math>。总之，我们试图找到达到<math
    alttext="upper P left-parenthesis c Subscript j Baseline right-parenthesis product
    upper P left-parenthesis x vertical-bar c Subscript j Baseline right-parenthesis"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>c</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <mo>∏</mo>
    <mi>P</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <msub><mi>c</mi> <mi>j</mi></msub>
    <mo>)</mo></mrow></mrow></math>的最大值的类<math alttext="c Subscript j"><msub><mi>c</mi>
    <mi>j</mi></msub></math>。
- en: For each class, the probability of observing a word given that class is estimated
    from the training data by computing the relative frequency of each word in the
    collection of documents for that class. Of course, it seems that we’ll have a
    problem if we have seen no training document for a particular (word, class) pair.
    In that case, we use something called a *Laplace smoothing factor* that represents
    a frequency for that word that is guaranteed to be minimal with respect to the
    size of the vocabulary. The classifier also requires the prior probabilities,^([1](ch28.xhtml#idm46385808883048))
    which are straightforward to estimate from the frequency of classes in the training
    set.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个类别，根据训练数据估算出现该类别时观察到单词的概率，方法是计算该类别文档集合中每个单词的相对频率。当然，如果我们对特定（单词，类别）对没有见过训练文档，就会出现问题。在这种情况下，我们使用称为*Laplace平滑因子*的东西，它表示对于该单词的频率保证相对于词汇量的大小是最小的。分类器还需要先验概率，^[1](ch28.xhtml#idm46385808883048)这些可以通过训练集中类别的频率直接估算出来。
- en: streamDM Introduction
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: streamDM简介
- en: Before we get into how to train a Bayesian model for classification, we want
    to get into the structure of the streamDM library that we will use for the purpose.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论如何为分类训练贝叶斯模型之前，我们想先了解我们将用于此目的的streamDM库的结构。
- en: In streamDM, the model operates on a labeled `Example`. In other words, that
    internally, the streams are represented as Spark Streaming DStream, containing
    our internal instance data structure, the `Example`. An `Example` encompasses
    a tuple of input and output `Instance` and a weight; in turn, the `Instance` can
    contain a data structure depending on the input format of the streams (e.g., dense
    instances in comma-separated values [CSV] text format, sparse instances in LibSVM
    format, and text instances).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在streamDM中，模型操作于标记的`Example`。换句话说，内部流表示为Spark Streaming DStream，包含我们的内部实例数据结构`Example`。一个`Example`包括输入和输出`Instance`的元组以及一个权重；`Instance`可以根据流的输入格式包含不同的数据结构（例如，逗号分隔值[CSV]文本格式中的稠密实例，LibSVM格式中的稀疏实例和文本实例）。
- en: 'The `Example` class consists of the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`Example`类包括以下内容：'
- en: '`input: Instance(data, x_j)`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input: Instance(data, x_j)`'
- en: '`output: Instance(labels, y_j)`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output: Instance(labels, y_j)`'
- en: '`weight: Double`'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight: Double`'
- en: In our example, we create the `Example` by using a convenience function called
    `Example.fromArff()`, which sets the default weight (=1) and reads our ARFF-formatted
    file. All operations are made on the `Example`; this allows for task design without
    the need for the model implementer to know the details of the implementation of
    the instances.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用一个方便的函数`Example.fromArff()`创建`Example`，它设置默认权重（=1）并读取我们的ARFF格式文件。所有操作都在`Example`上进行；这允许进行任务设计，而无需模型实现者了解实例实现的详细信息。
- en: The ARFF File Format
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ARFF文件格式
- en: ARFF is an acronym that stands for attribute-relation file format. It is an
    extension of the CSV file format in which a header is used that provides metadata
    about the data types in the columns, and is a staple of the Weka suite of machine
    learning software. Some of the ways of organizing the presentation of features
    is inspired from it. You can find more information on the [Weka wiki](https://bit.ly/30HHh0A).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ARFF是属性-关系文件格式的缩写。它是CSV文件格式的扩展，使用头部提供关于列中数据类型的元数据，并且是Weka机器学习软件套件的一个支柱。某些组织功能展示的方式受到它的启发。您可以在[Weka
    wiki](https://bit.ly/30HHh0A)上找到更多信息。
- en: 'A task is a combination of blocks that are generally ordered, as in any learning
    process setup:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是一组通常有序的块的组合，就像任何学习过程设置中那样：
- en: A `StreamReader` (reads and parses Examples and creates a stream)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `StreamReader`（读取和解析示例并创建流）
- en: A `Learner` (provides the train method from an input stream)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `Learner`（从输入流提供`train`方法）
- en: A `Model` (data structure and set of methods used for Learner)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `Model`（用于学习者的数据结构和方法集）
- en: An `Evaluator` (evaluation of predictions)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`Evaluator`（评估预测）
- en: A `StreamWriter` (manages the output of streams)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `StreamWriter`（管理流的输出）
- en: streamDM packages this flow into predefined tasks that it links through the
    method of *prequential evaluation*, which is something we’re about to detail.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: streamDM将此流程打包成预定义任务，并通过*预序评估*方法连接，我们即将详细介绍它。
- en: Naive Bayes in Practice
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践中的朴素贝叶斯
- en: Multinomial Naive Bayes classifiers are properly a class of closely related
    algorithms. We’ll focus on the one found in the streamDM library^([2](ch28.xhtml#idm46385808856168)),
    which is an implementation of [[McCallum1998]](app04.xhtml#McCallum1998).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯分类器实际上是一类密切相关的算法。我们将重点放在streamDM库中的一个发现^([2](ch28.xhtml#idm46385808856168))，这是[[McCallum1998]](app04.xhtml#McCallum1998)的一个实现。
- en: It uses the streamDM implementation, which is based on a three-step prequential
    evaluation method. The prequential evaluation method, or interleaved test-then-train
    method, is an alternative to the traditional batch-oriented method that clearly
    separates these phases into training, validation, and scoring.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用基于三步骤预顺序评估方法的streamDM实现。预顺序评估方法，或者交错的测试-训练方法，是传统批处理方法的替代方案，清晰地将训练、验证和评分阶段分开。
- en: The prequential evaluation [Figure 28-1](#evaluate-prequential) is designed
    specifically for stream-oriented settings in the sense that each sample serves
    two purposes, and that samples are analyzed sequentially, in order of arrival,
    and become immediately unavailable. Samples are effectively seen only once.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 预顺序评估 [Figure 28-1](#evaluate-prequential) 是专门为流式设置设计的，每个样本的作用是两个目的，样本按到达顺序依次分析，并立即变得不可用。样本实际上只看一次。
- en: This method consists of using each sample to test the model, which means to
    make a prediction, and then the same sample is used to train the model (partial
    fit). This way the model is always tested on samples that it hasn’t seen yet.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法包括使用每个样本来测试模型，这意味着进行预测，然后使用同一个样本来训练模型（部分拟合）。这样模型总是在它尚未见过的样本上进行测试。
- en: '![spas 2801](Images/spas_2801.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2801](Images/spas_2801.png)'
- en: Figure 28-1\. Model training and evaluation with prequential evaluation
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图28-1\. 使用预顺序评估进行模型训练和评估
- en: streamDM integrates a `FileReader`, which is used to read data from one file
    of full data to simulate a stream of data. It works well with ARFF files.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: streamDM集成了`FileReader`，用于从一个完整数据文件中读取数据，模拟数据流。它与ARFF文件兼容。
- en: Training a Movie Review Classifier
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练电影评论分类器
- en: We can download a labeled version of the IMDB movie reviews dataset classified
    in movie genre categories from [the MEKA project](http://bit.ly/2VJJngU).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从[MEKA项目](http://bit.ly/2VJJngU)下载IMDB电影评论数据集的带标签版本，按电影流派分类。
- en: The beginning of the files lists the included attributes, starting with the
    categories in binary form, followed by a word vector encoding of the review.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的开头列出了包括的属性，以二进制形式开始，接着是评论的词向量编码。
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here the nominal specifications within brackets correspond to the categories
    given to the movie by IMDB and then the word vector model follows. We can see
    the actual representation of one review later in the file:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里方括号内的名义规范对应于IMDB给电影的类别，然后是词向量模型。我们可以在文件的后面看到一个评论的实际表示：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The review gives an idea of the classification of the movie, followed by the
    word vectors indicating keywords in the review.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇评论提供了电影分类的想法，接着是评论中关键词的词向量。
- en: 'streamDM comes with a command-line feature that connects all of the classes
    in a task (which we detailed earlier) It makes calling our `MultinomialNaiveBayes`
    model a simple command, launched from the compiled checkout of the streamDM repository,
    at `streamDM/scripts`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: streamDM带有一个命令行功能，连接任务中所有类别（我们之前详细介绍过）。它使得调用我们的`MultinomialNaiveBayes`模型变得简单，可以从编译检出的streamDM存储库中的`streamDM/scripts`启动：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The model constructor expects three arguments:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构造函数期望三个参数：
- en: The number of features in each example (by default 3), indicated by option `-c`
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个示例中的特征数量（默认为3），选项为`-c`。
- en: The number of classes in each example (by default 2), indicated by option `-c`
    and
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个示例中的类别数量（默认为2），选项为`-c`和
- en: A Laplace smoothing factor to handle the zero frequency issue, indicated by
    option `-l`.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个拉普拉斯平滑因子用于处理零频率问题，选项为`-l`。
- en: 'The model outputs a confusion matrix of the generated data. On a reduced size
    example, we would obtain, for example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出了生成数据的混淆矩阵。在一个缩小的示例中，我们可以得到，例如：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Introducing Decision Trees
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入决策树
- en: In a classification problem, we usually deal with a set of *N* training examples
    of the form (*x*, *y*), where *y* is a class label and *x* is a vector of attributes
    ( <math><mrow><mi>x</mi> <mo>=</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>.</mo> <mo>.</mo> <msub><mi>x</mi>
    <mi>n</mi></msub></mrow></math> ). The goal is to produce a model *y* = *f(x)*
    from these examples, which will predict the classes *y* of future examples *x*
    with high accuracy.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，我们通常处理一组*N*个训练样本，其形式为（*x*, *y*），其中*y*是类别标签，*x*是属性向量（ <math><mrow><mi>x</mi>
    <mo>=</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>.</mo> <mo>.</mo> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>
    ）。我们的目标是从这些样本中产生一个模型*y* = *f(x)*，该模型能够高精度地预测未来样本*x*的类别*y*。
- en: One of the most effective and widely used classification methods is *decision-tree
    learning*. Learners of this type produce models in the form of decision trees,
    where each node contains a test on an attribute (a *split*), each branch from
    a node corresponds to a possible outcome of the test, and each leaf contains a
    class prediction. In doing so, the algorithm produces a series of splits, such
    as the one illustrated in [Figure 28-2](#decision-tree-car-deal).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最有效和广泛使用的分类方法之一是*决策树学习*。这种学习方式生成决策树模型，其中每个节点包含对属性的测试（*分割*），每个节点的分支对应于测试的可能结果，并且每个叶子包含一个类别预测。这样，算法产生一系列的分割，例如在[图28-2](#decision-tree-car-deal)中所示的那种。
- en: '![spas 2802](Images/spas_2802.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2802](Images/spas_2802.png)'
- en: Figure 28-2\. Decision tree for a car deal
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图28-2\. 用于汽车交易的决策树
- en: The label *y* = *DT*(*x*) for an example *x* results from passing the example
    down from the root to a leaf, testing the appropriate attribute at each node and
    following the branch corresponding to the attribute’s value in the example. A
    decision tree is trained by recursively replacing leaves by test nodes, starting
    at the root. The attribute to test at a given node is chosen by comparing all
    of the available attributes and choosing the best one according to some heuristic
    measure, often one called *information gain*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个示例*x*，其标签*y* = *DT*(*x*)的结果来自将示例从根节点传递到叶节点，测试每个节点的适当属性，并跟随与示例中属性值对应的分支。决策树通过递归地将叶节点替换为测试节点来训练，从根节点开始。在给定节点测试的属性是通过比较所有可用属性并根据某些启发式度量（通常称为*信息增益*）选择最佳属性来确定的。
- en: Information gain is, informally, a statistical indicator that measures how,
    while observing our potential decision variable (“does the input email contain
    the word *Viagra*?”), we learn something about the final classification variable
    (“is this email a spam email?”). It captures the best variables on which to base
    our decision, the first of those variables being the ones that give us the most
    information on the outcome of the classification.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益是一个统计指标，非正式地衡量我们在观察潜在决策变量（“输入电子邮件中是否包含*Viagra*？”）时学到的关于最终分类变量（“这封电子邮件是否为垃圾邮件？”）的信息。它捕捉到基于哪些变量做决策是最好的，首先是那些提供关于分类结果最多信息的变量。
- en: The Spark implementation of decision trees relies on the *ID3 algorithm* (iterative
    dichotomizer, [[Quinlan1986]](app04.xhtml#Quinlan1986)) and relies on information
    gain.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Spark实现的决策树依赖于*ID3算法*（迭代分割器，[Quinlan1986](app04.xhtml#Quinlan1986)），并依赖于信息增益。
- en: Tip
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Decision tree classifiers are a well-known decision algorithm in machine learning.
    Hoeffding trees are a streaming extension of decision trees that is well grounded
    in probability theory.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类器是机器学习中众所周知的决策算法。赫夫丁树是决策树的流式扩展，其在概率理论中有良好的基础。
- en: If you are unfamiliar with decision trees, we recommend you have a look at section
    4 of *Advanced Analytics with Spark* ([[Laserson2017]](app04.xhtml#Laserson2017)),
    which provides an extensive treatment.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对决策树不熟悉，我们建议你查看《Spark高级分析》的第4章（[Laserson2017](app04.xhtml#Laserson2017)），该书对此进行了详尽的阐述。
- en: These learners use every training example to select the best attribute at each
    split, a policy that requires all examples to be available during the entire training
    process. This makes those classic decision-tree learning procedures batch algorithms
    inapplicable to the streaming context for which data is available only in small
    increments.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些学习者利用每个训练样本来选择每个分割中最好的属性，这种策略要求在整个训练过程中始终可用所有样本。这使得那些经典的决策树学习过程批处理算法不适用于仅以小增量提供数据的流式环境。
- en: Hoeffding Trees
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 赫夫丁树
- en: Hoeffding trees,^([3](ch28.xhtml#idm46385808771480)) ([[Domingos2000]](app04.xhtml#Domingos2000))
    address this problem and are able to learn from streaming data within tight time
    and memory constraints—without having to hold all previously seen data in memory.
    They accomplish this by remarking that it is mathematically sufficient to use
    only a small sample of the data when choosing the split attribute at any given
    node. Thus, only the first examples to arrive on the data stream need to be used
    to choose the split attribute at the root; subsequent examples are passed through
    the induced portion of the tree until they reach a leaf, are used to choose a
    split attribute there, and so on, recursively.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 霍夫丁树^([3](ch28.xhtml#idm46385808771480))（[[多明戈斯2000]](app04.xhtml#Domingos2000)）解决了这个问题，并能够在严格的时间和内存约束条件下从流数据中学习，而无需在内存中保存所有先前看到的数据。它们通过指出，在选择任何给定节点的分割属性时，仅使用数据流的一个小样本就足够了，从而实现了这一点。因此，在根节点选择分割属性时，只需使用数据流中最先到达的几个示例；随后的示例通过树的感应部分，直至它们达到叶子节点，然后在那里选择分割属性，并依此类推。
- en: To determine the number of examples needed for each decision, this algorithm
    uses a statistical result known as a Hoeffding bound. A very informal description
    of the *Hoeffding bound* is that if you have a function on *n* random variables
    and if it doesn’t change too much when you resample one of the variables, the
    function will be close to its mean with high probability. This result helps us
    understand in precise terms how the behavior of a sample of the values observed
    in a dataset is reflective of the entire distribution.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定每个决策所需的示例数量，此算法使用称为霍夫丁界限的统计结果。关于*霍夫丁界限*的非正式描述是，如果你对*n*个随机变量上的一个函数进行重新取样时，函数的变化不大，那么该函数在接近其均值时将具有高概率。这个结果帮助我们精确地理解，在数据集中观察到的值样本的行为如何反映整个分布。
- en: 'In concrete terms, this means that the values we observe one by one—a sample
    of the stream—will be reflective of the entire stream: we replace the *frequency
    counters* of decision trees by estimators. There is no need for windows of instances
    because sufficient statistics are kept by the estimators separately. In particular,
    the salience of some features is revealed from the data in the same way they would
    come out from examining the whole dataset.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，这意味着我们逐个观察的值——流的样本——将反映整个流的情况：我们用估计器替换决策树的*频率计数器*。无需实例窗口，因为估计器分别保留足够的统计数据。特别是，某些特征的显著性从数据中显现出来，就像检查整个数据集时一样。
- en: This means that we have a clear idea of the number of values that we need to
    see in order to confidently make a decision on how our decision tree should be
    created—it tells us that learning a classification while having seen a few values
    is possible!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们清楚地知道我们需要看到多少值才能自信地决定我们的决策树应该如何创建——它告诉我们，通过看到少量值就可以学习分类！
- en: The Hoeffding Bound
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[霍夫丁界限](https://wiki.example.org/hoeffding_bound)'
- en: 'After *n* independent observations of a real-valued random variable *r* with
    range *R*, the Hoeffding bound ensures that, with confidence 1 – δ, the true mean
    of *r* is at least *V* - ε, where *V* is the observed mean of the samples:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实值随机变量*r*的*n*次独立观察，霍夫丁界限确保，在置信度1 - δ下，*r*的真实均值至少为*V* - ε，其中*V*是样本的观察均值：
- en: <math alttext="epsilon equals StartRoot StartFraction upper R squared ln left-parenthesis
    l slash delta right-parenthesis Over 2 n EndFraction EndRoot"><mrow><mi>ϵ</mi>
    <mo>=</mo> <msqrt><mfrac><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo form="prefix">ln</mo><mrow><mo>(</mo><mi>l</mi><mo>/</mo><mi>δ</mi><mo>)</mo></mrow></mrow>
    <mrow><mn>2</mn><mi>n</mi></mrow></mfrac></msqrt></mrow></math>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="epsilon equals StartRoot StartFraction upper R squared ln left-parenthesis
    l slash delta right-parenthesis Over 2 n EndFraction EndRoot"><mrow><mi>ϵ</mi>
    <mo>=</mo> <msqrt><mfrac><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo form="prefix">ln</mo><mrow><mo>(</mo><mi>l</mi><mo>/</mo><mi>δ</mi><mo>)</mo></mrow></mrow>
    <mrow><mn>2</mn><mi>n</mi></mrow></mfrac></msqrt></mrow></math>
- en: The interesting part of this bound is that this is true irrespective of the
    probability distribution that generated the observations in the first place, which
    lets us apply this to any stream. The price of this generality is that the bound
    is more conservative than bounds that are distribution dependent (i.e., it will
    take more observations to reach the same δ and ε).^([4](ch28.xhtml#idm46385808753032))
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个界限的有趣之处在于，这是真实的，不受生成观察的概率分布的限制，这使我们可以将其应用于任何流。这种广泛适用性的代价是，该界限比依赖于分布的界限更为保守（即，需要更多的观察才能达到相同的δ和ε）。^([4](ch28.xhtml#idm46385808753032))
- en: As with any decision tree, we want to decide whether we should “expand” a leaf,
    making the decision on the subgroup of requests that end at this leaf split.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何决策树一样，我们想决定是否应该“扩展”一个叶子节点，这决定了在这个叶子节点分割的请求子组。
- en: Let’s call *G*(*Xi*) the heuristic measure used to choose test attributes (e.g.,
    information gain). After seeing *n* samples at a leaf, let *Xa* be the attribute
    with the best heuristic measure and *Xb* be the attribute with the second best.
    Let *∆G = G(Xa) - G(Xb)* be a new random variable for the difference between the
    observed heuristic values. Applying the Hoeffding bound to ∆*G*, we see that if
    ∆*G* > ε (that we set with the bound using a δ that we choose), we can confidently
    say that the difference between *G*(*Xa*) and *G*(*Xb*) is larger than zero, and
    select *Xa* as the new split attribute.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们称*G(X_i)*为选择测试属性（例如信息增益）的启发式度量。在叶子看到*n*个样本后，*Xa*是具有最佳启发式度量的属性，*Xb*是具有第二最佳启发式度量的属性。让*∆G
    = G(Xa) - G(Xb)*成为观察到的启发式值之间差异的新随机变量。将Hoeffding界限应用于∆*G*，我们看到如果∆*G* > ε（使用我们选择的δ设置界限），我们可以自信地说*G(Xa)*和*G(Xb)*之间的差异大于零，并选择*Xa*作为新的分割属性。
- en: The sketch of the algorithm then proceeds by sorting every example (*x*, *y*)
    into a leaf l using a provisional decision tree. For each value *x*__i_ in *x*
    such that *Xi* ∈ *Xl*, it then increments a counter of examples seen for this
    feature and class, and tries to label the leaf l with the majority class among
    the examples seen so far at l. If those are not all of the same class, it tries
    to split by computing a split heuristic _G_l(_X_i) for each attribute _X_i ∈ _X_l,
    using the counters. If the difference in heuristic results between the first and
    second attribute is greater than an ε computed with the Hoeffding bound as just
    shown, we replace l by an internal node that splits on this first attribute, and
    update the tree we use then.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 算法草图随后通过将每个示例(*x*, *y*)使用临时决策树分类到叶子节点*l*。对于*x*中的每个值*x_i*，使得*X_i* ∈ *X_l*，然后增加这个特征和类别的示例计数器，并尝试使用到目前为止在*l*中看到的示例中的多数类别来标记叶子*l*。如果这些不全是同一类别，则尝试通过计算每个属性*X_i
    ∈ X_l*的分割启发式*G_l(X_i)*来进行分割。如果第一个和第二个属性之间的启发式结果差异大于使用Hoeffding界限计算的ε，则我们将*l*替换为在此第一个属性上分割的内部节点，并更新我们所使用的树。
- en: The counts are the sufficient statistics needed to compute most heuristic measures;
    this makes the algorithm frugal with memory.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计数是计算大多数启发式度量所需的充分统计量；这使得算法在内存使用上很节俭。
- en: Hoeffding Trees Heuristics
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hoeffding Trees 启发法
- en: 'The algorithm also implements a number of edge heuristics:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法还实现了一些边缘启发法：
- en: Under memory pressure, Hoeffding trees deactivate the least promising leaves
    in order to make room for new ones; when data is readily available, these can
    be reactivated later.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在内存压力下，Hoeffding Trees会停用最不具有前景的叶子节点，以便为新叶子节点腾出空间；当数据容易获取时，可以稍后重新激活这些节点。
- en: It also employs a tie mechanism that precludes it from spending inordinate time
    deciding between attributes whose practical difference is negligible. Indeed,
    it declares a tie and selects *Xa* as the split attribute any time ∆*G* < ε <
    τ (where τ is a user-supplied tie threshold).
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还采用一种绑定机制，避免在选择实际差异微小的属性时花费过多时间。事实上，它在 ∆*G* < ε < τ（其中 τ 是用户提供的绑定阈值）时宣布平局，并选择*Xa*作为分割属性。
- en: Prepruning is carried out by considering at each node a “null” attribute *X_0
    that consists of not splitting the node. Thus, a split will be made only if, with
    confidence 1 - δ, the best split found is better according to _G* than not splitting.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在每个节点考虑一个“null”属性*X_0*来进行预修剪，该属性表示不对节点进行分割。因此，只有在置信度为1 - δ时，找到的最佳分割按*G*的标准优于不分割时，才会进行分割。
- en: Notice that the tests for splits and ties are executed only once for every *m*
    (a user-supplied value) examples that arrive at a leaf. This is justified because
    that procedure is unlikely to make a decision after any given example, so it is
    wasteful to carry out these calculations for each one of them. This makes Hoeffding
    Trees particularly suitable for microbatching, as is done in Spark Streaming.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于到达叶子的每个*m*（用户提供的值）个示例，分割和平局测试仅执行一次。这是因为该过程不太可能在任何给定示例后做出决策，因此为每个示例执行这些计算是浪费的。这使得Hoeffding
    Trees特别适合于微批处理，正如在Spark Streaming中所做的那样。
- en: Hoeffding Trees in Spark, in Practice
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hoeffding Trees 在 Spark 中的实际运用
- en: The implementation of Hoeffding Trees for Spark can be found in the streamDM
    library. As is the case with the Naive Bayes classifier, it answers to a slightly
    different API than the usual Spark Streaming processing API. We could use the
    streamDM default task setup, as in the naive Bayes example. Such a setup for model
    evaluation is available in streamDM and is documented [on GitHub](https://huawei-noah.github.io/streamDM/docs/HDT.html).
    Here, we sketch a manual construction of the training of the algorithm.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中实现 Hoeffding Trees 的实现可以在 streamDM 库中找到。与朴素贝叶斯分类器类似，它对比通常的 Spark Streaming
    处理 API 有稍微不同的回应。我们可以使用 streamDM 默认的任务设置，如朴素贝叶斯示例中所述。这样的模型评估设置在 streamDM 中是可用的，并在
    [GitHub 上进行了文档化](https://huawei-noah.github.io/streamDM/docs/HDT.html)。这里，我们概述了算法训练的手动构建过程。
- en: The Hoeffding Tree model requires a specification for initializing instances,
    named `ExampleSpecification`. An `ExampleSpecification` contains information about
    the input and output features in the data. It contains a reference to an input
    `InstanceSpecification` and an output `InstanceSpecification`. Those `InstanceSpecification`
    elements contain information about the features, differentiating between numerical
    and categorical features.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Hoeffding Tree 模型需要初始化实例的规范，称为 `ExampleSpecification`。`ExampleSpecification`
    包含有关数据中输入和输出特征的信息。它包含一个指向输入 `InstanceSpecification` 和一个输出 `InstanceSpecification`
    的引用。这些 `InstanceSpecification` 元素包含有关特征的信息，区分数值和分类特征。
- en: 'The Hoeffding Tree implementation presented here is made to work well with
    the ARFF file format. We can create features for the well-known Iris flower dataset,
    an Iris species classification dataset containing four features: length and width
    for petal and sepal parts of iris samples:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里呈现的 Hoeffding Tree 实现适用于 ARFF 文件格式。我们可以为著名的鸢尾花数据集创建特征，这是一个包含四个特征的鸢尾花物种分类数据集：鸢尾花样本的花瓣和萼片的长度和宽度：
- en: '[PRE4]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The feature specification is done by separating numeric features to help the
    splitting algorithm organize how it should discriminate on this variable. The
    setup of the algorithm is then simpler:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 特征规范是通过将数值特征分隔来完成的，以帮助分割算法组织它应如何在这个变量上进行歧视。然后，算法的设置就更简单了：
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The training of the model comes first. After the model has been trained with
    the appropriate examples, we can query it with a single record by using the `predict`
    method, which operates on a `String`. Remember from the Naive Bayes example that
    the `Example` is a wrapper on top of the `Instance` class hierarchy. It contains
    a reference to an input `Instance` and an output `Instance`, and provides setters
    and getters for the features and labels.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的训练首先进行。在用适当的示例训练完模型后，我们可以使用 `predict` 方法查询单个记录，该方法操作的是一个 `String`。从朴素贝叶斯的示例中记住，`Example`
    是在 `Instance` 类层次结构之上的包装器。它包含一个输入 `Instance` 和一个输出 `Instance` 的引用，并为特征和标签提供设置器和获取器。
- en: Streaming Clustering with Online K-Means
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线 K-Means 流式聚类
- en: In machine learning, clustering is the practice of grouping the elements of
    a set in an unsupervised manner, given a notion of similarity between those points.
    The most famous algorithm for clustering is *K*-means, and in this section, we
    study its online adaptation. The rest of this section is going to show you how
    to adapt an unsupervised algorithm to a streaming context.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，聚类是在无监督方式下根据这些点之间的相似性概念来对集合元素进行分组的实践。最著名的聚类算法是 *K*-means，在本节中，我们研究其在线适应性。本节的其余部分将展示如何将无监督算法适应流式上下文。
- en: '*K*-Means Clustering'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*K*-Means 聚类'
- en: '*K*-means clustering is a parametric, unsupervised algorithm for grouping data
    points in a metric space into specifically *k* clusters, where *k* is a predefined
    integer. The clusters are each identified by a *centroid* (a point in this metric
    space) with the property that the centroid is the *barycenter*^([5](ch28.xhtml#idm46385808502616))
    of the data points that are assigned to it.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*K*-means 聚类是一种参数化的无监督算法，用于将度量空间中的数据点分组成具有特定 *k* 个聚类，其中 *k* 是预定义的整数。每个聚类由一个*质心*（该度量空间中的一个点）标识，该质心具有质心是分配给它的数据点的*重心*^([5](ch28.xhtml#idm46385808502616))的属性。'
- en: It functions by running through several epochs of alternatively caring about
    the attachment of points to a centroid on the one hand, and repositioning centroids
    on the other hand. In the first phase, each of the data points is associated with
    the centroid that is the closest to it—with the set of data points associated
    with a particular centroid forming *its cluster*. The later phase consists of
    selecting and resetting where the centroid is for each and every cluster by selecting
    the barycenter of all the points in the centroid’s cluster. So for example, if
    we have 100 points and, say, 10 centroids, we can initially have the 10 centroids
    set at random positions in our space. We then assign all of our 100 points each
    to their closest centroid among the possible 10, and then we alternate and start
    for each group to recompute where the centroid should be. That recomputation is
    a barycenter calculation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过连续运行几个时期来运行，交替关注点与质心的附着关系一方面，以及重新定位质心另一方面。在第一阶段，将每个数据点与最接近它的质心关联起来，每个质心关联的数据点集形成*其聚类*。后续阶段包括选择并重新设置每个聚类的质心，通过选择该质心聚类中所有点的重心来进行。例如，如果我们有100个点和10个质心，我们可以最初在空间中随机设置这10个质心的位置。然后，我们将所有100个点分配给它们可能的10个最接近的质心之一，然后我们交替开始为每个组重新计算质心应该在哪里。这种重新计算是一个重心计算。
- en: Now, the minimization of that overall distance is the core of the *k*-means
    algorithm. And if we want a convergence to an ideal clustering, the algorithm
    depends only on how many rounds will allow ourselves to execute before getting
    an error measure and the initial position of the centroids.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最小化总体距离是*k*-均值算法的核心。如果我们希望收敛到理想的聚类，算法仅依赖于允许自己在获取错误度量和质心初始位置之前执行多少轮次。
- en: Online Data and *K*-Means
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线数据和*K*-均值
- en: In the traditional *K*-means algorithm, we are given (and operate on) the total
    dataset before doing this ideal centroid computation. If, on the other hand, we
    need to operate in a sequential manner, considering points one by one and observing
    them only once, we will need to adopt a different approach.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的*K*-均值算法中，我们在进行理想质心计算之前给定（并操作）整个数据集。另一方面，如果我们需要按顺序操作，逐点考虑并仅观察一次，则需要采用不同的方法。
- en: Suppose, for example, that we consider points appearing on each RDD as being
    part of our dataset. We have fixed *k*, so that we know that we really want to
    separate our dataset into, say, 10 clusters. Yet we still don’t want to understand
    precisely just the last RDD as being the complete epitome of our dataset. What
    we do to adapt this is that we let the most recent RDDs—that is the most recent
    batches of data—modify our centroid assignments and position with a stronger importance.
    We’ll quantify this importance numerically using a notion of weight.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们考虑每个RDD上出现的点作为我们数据集的一部分。我们有固定的*k*，以便知道我们真正想要将数据集分为，比如，10个聚类。但是，我们仍然不希望精确理解最后一个RDD仅作为我们数据集的完整典范。我们对此进行调整的方式是，让最近的RDD——即最新的数据批次——通过更强的重要性修改我们的质心分配和位置。我们将使用权重的概念定量化这种重要性数值化。
- en: The online *K*-means algorithm ([[Shindler2011]](app04.xhtml#Shindler2011))
    consists of adapting *K*-means with a notion known as *forgetfulness*. This amounts
    to letting the oldest points that we have seen in the dataset have less impact
    over the centroid’s position than the most recent points, reflecting the notion
    that our clustering should be an accurate snapshot of the best way to cluster
    the *most recent* points coming over the wire.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在线*K*-均值算法（[Shindler2011](app04.xhtml#Shindler2011)）包括使用称为*遗忘性*的概念来调整*K*-均值。这意味着让我们在数据集中看到的最旧的点对质心位置的影响小于最近的点，反映了我们聚类应该是对最近传输的数据点的最佳聚类的准确快照。
- en: In practical terms, it means that the output of assignment of points under centroids
    is a fresh representation of the last few batches of the data that we have seen.
    To do this, we introduce a notion of weight (*w* < 1), or decay, that accompanies
    every single point. This is very relevant because the computation of a barycenter
    that we have seen in the vanilla batch interpretation of *K*-means is something
    that natively extends into a weighted computation. That weight reflects on every
    single batch for which we multiply points by decay factor. And because we do that
    on every batch, the weight of any particular point is exponentially decreased
    based on the number of batches elapsed since that point was read on the wire from
    our data source.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，这意味着在质心下分配点的输出是我们看到的最后几批数据的新鲜表示。为此，我们引入了一个权重（*w* < 1）或衰减的概念，伴随着每一个单独的点。这非常重要，因为我们在*K*-means的基本批量解释中看到的质心的计算自然延伸到加权计算。该权重反映了我们通过衰减因子将每个批次中的点相乘。因为我们在每个批次上都这样做，任何特定点的权重都会根据自从从数据源的线上读取该点以来经过的批次数以指数方式减少。
- en: This factor can be expressed in Spark using the decay factor expressed as a
    float in the parametrization of the online *K*-means algorithm, or it can be expressed
    as a half-life. The half-life says how many batches a point should survive for
    before half of the dataset, in view of the algorithm, should be decreased. In
    fact, we’re considering the number of batches after which the weight of each and
    every single point that was seen at that particular time diminishes by half.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个因素可以在Spark中通过将衰减因子表达为在线*K*-means算法的浮点数来表达，或者可以表达为半衰期。半衰期表示一个点在降低数据集一半在算法视图中的批次之后应该存活多少批次。实际上，我们考虑的是每个点的权重在看到那个特定时间点的批次之后如何减半。
- en: Tip
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Another alternative in the parametrization of the algorithm consists in *counting
    the number of points* that we have seen of the entire stream. Indeed, the batch
    notion that Spark applies for determining the age of a particular point—and therefore
    the decay that should be applied to it in the algorithm—is something that is not
    always the most appropriate, because the simple amount and vastness of data for
    a stream of which we know the throughput varies a lot is a better indication of
    the recency of results than the timestamp at which the event arrived.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的参数化的另一种选择是*计算我们已经看到的点的数量*。事实上，Spark为确定特定点的年龄（因此在算法中应用的衰减）所应用的批量概念，并不总是最合适的，因为我们知道吞吐量变化很大的流数据的简单数量和广泛性更好地指示结果的新近性，而不是事件到达的时间戳。
- en: The Problem of Decaying Clusters
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 衰减簇的问题
- en: In any case, that forgetfulness lets us have centroids that carry the barycenter
    of lesser and lesser points. Now, since we have an assignment for those points,
    we can reposition the centroids based on the most recent points to arrive in that
    cluster. But how exactly are we going to deal with the fact that, after a given
    amount of time, it is often the case that some centroids can be completely forgotten?
    For example, let’s consider a data stream that is operating on a cartesian, bounded
    plane. If we have, at the beginning, mostly points that hit the *upper-left* quadrant
    of our particular bounded plane or a particular zone of our space, we will probably
    assign, very early on, a centroid or several to that region.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，这种遗忘性让我们可以拥有质心，这些质心携带了越来越少点的重心。现在，由于我们对这些点有了分配，我们可以根据最近到达该簇的点重新定位质心。但是我们如何处理这样一个事实：在经过一定时间后，某些质心往往会完全被遗忘？例如，让我们考虑一个在笛卡尔有界平面上运行的数据流。如果在开始时，我们主要是在特定有界平面的*左上*象限或空间的特定区域中有大部分点，那么很早就会分配一个或多个质心给该区域。
- en: After some time, let’s then assume that the content of the point of our data
    stream moves to the *lower-right* quadrant of our space. In that case, the evolution
    of our online *K*-means is meant to play centroid in the new region rather than
    in the upper-left quadrant. As the points of a given cluster will move downward
    and rightward, the centroid will equally move but within the same cluster. What
    if, in any given point, we create such a gap in point location that we cannot
    assign a single point to the old cluster that was still in the upper left quadrant?
    Then, that cluster, while having a very small number of points, will still have
    “old” points associated to it. That is the total weight, being multiplied by a
    very small factor, will be negligible but strictly positive, meaning that there
    will still be a cluster “grouping” those old points. This is sub-optimal because
    it “consumes” a centroid to group points that are long past being relevant to
    our current stream.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 过一段时间后，让我们假设数据流的点内容移动到空间的*右下*象限。在这种情况下，我们在线*K*-means的演化意味着在新区域中放置质心，而不是在左上象限。随着给定聚类的点向下和向右移动，质心也会随之移动，但仍在同一聚类中。如果在任何给定的点上，我们创建了一个使点位置出现间隙的情况，我们无法将单个点分配给仍然位于左上象限的旧聚类。那么，这个聚类，虽然点数非常少，但仍然有“旧”点与之相关联。这是不理想的，因为它“消耗”了一个质心来聚合那些已经不相关于当前数据流的旧点。
- en: The solution to this is to analyze decaying clusters that have such a small
    weight that they are in fact negligible. When using *K*-means, we have *K* possible
    centroids in the general formulation of the problem. Because of that invariant,
    we will need to examine “dying” (low-weight) clusters by comparing their total
    weight to the maximum weight witnessed at the “strongest” cluster. What lets us
    determine that a cluster is “dying,” is to compare the total weight, in terms
    of number of points, multiplied by their decay factor for the lightest centroid
    to the total weight of the heaviest centroid, which is the centroid that has the
    most amount of weighted points. If the relative discrepancy between those centroids
    is higher than 10 million,^([6](ch28.xhtml#idm46385808589032)) Spark will abandon
    the dying centroid and split the “heaviest” centroid in two.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 解决此问题的方法是分析那些权重非常小以至于可以忽略不计的衰减聚类。在使用*K*-means时，我们在问题的一般形式中有*K*个可能的质心。由于这个不变性，我们需要通过比较“死亡”（低权重）聚类的总权重与“最强”聚类中最大权重的总和来检查这些聚类。我们能够确定一个聚类是否“死亡”的依据是，比较点数乘以最轻质心的衰减因子的总权重与最重质心的总权重。如果这些质心之间的相对差异超过1千万^([6](ch28.xhtml#idm46385808589032))，Spark将放弃衰减的质心，并将“最重”的质心分成两个质心。
- en: In [Figure 28-3](#dying-clusters), clusters A and B start at the upper-left
    corner and move progressively, downwards and rightwards, as the more recent points
    take over most of the weight of the cluster. However, when there is a discontinuous
    gap (in red) in the position of new point arrivals, the new points are assigned
    to another, distinct cluster C and begin disrupting the position of its centroid
    (in gray). We would instead like those new points to form a new cluster in the
    lower-left corner, with the red centroid. That is impossible as long as A stays
    in place as a cluster, regardless of how small its aggregated weight is (due to
    the decay factor).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图28-3](#dying-clusters)中，聚类A和B从左上角开始，逐渐向右下角移动，随着最近的点占据了大部分聚类的权重。然而，当新点到达的位置存在不连续的间隙（红色标记）时，新点会被分配到另一个不同的聚类C，并开始扰乱其质心（灰色）。我们希望这些新点能够形成一个新的聚类在左下角，有红色的质心。但由于A仍然作为一个聚类存在，无论其聚合权重多小（由于衰减因子），这是不可能的。
- en: '![spas 2803](Images/spas_2803.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2803](Images/spas_2803.png)'
- en: Figure 28-3\. The effect of “dying” clusters
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图28-3\. “死亡”聚类的影响
- en: This operation occurring in every microbatch will help the algorithm find a
    newer location of centroids that well corresponds better to the exact allocation
    of points in our data stream irrelevant of whether large movements of points have
    occurred in space over the history of our data stream. The relative difference
    is not configurable, and, as a consequence, it would be advisable to make sure
    that we use large decay factors that will let Spark Streaming figure out across
    a stream with relatively long batteries that it should be very reactive in killing
    those decaying centroids and moving them around.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作在每个微批次中发生，将帮助算法找到一个新的质心位置，更好地对应数据流中点的精确分配，而不管在数据流的历史中点是否发生了大的移动。相对差异不可配置，因此建议确保使用大的衰减因子，这将使得
    Spark Streaming 能够在相对较长的数据流中找出它应该在杀死这些衰减的质心并将其移动时非常敏感。
- en: Note
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The practice of doing a test on a particular assignation of clusters separating
    a set of points, deleting a “bad” cluster (for some notion of goodness), and “splitting”
    the largest cluster in two is a powerful and systematic way of enhancing the basic
    *K*-means idea into more powerful algorithms.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定分配集群的测试上进行实践，删除一个“坏”聚类（根据某种优良性的概念），并将最大的聚类“分裂”成两个，这是将基本 *K*-means 想法增强为更强大算法的有效和系统的方法之一。
- en: Among those enhancements, we can cite the *G*-means algorithm, that consists
    of testing a candidate *K*-means cluster with a Gaussianity test, checking that
    the positions of the points in the cluster obey a Gaussian hypothesis, something
    that is valid for various kinds of data. If that Gaussianity assumption is not
    reached in data that should reflect it, *G*-means considers that it is looking
    for a number of clusters that is too low. The algorithm then splits this bad cluster
    and increases *k*, the number of clusters the algorithms seeks to find in the
    data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些改进中，我们可以提到 *G*-means 算法，它包括对一个候选的 *K*-means 聚类进行测试，使用高斯性测试检查聚类中点的位置是否服从高斯假设，这对各种类型的数据都有效。如果在数据中未达到高斯假设，*G*-means
    认为正在寻找的聚类数目过低。然后，该算法分割此不良聚类并增加 *k*，即算法试图在数据中找到的聚类数目。
- en: You can find more references in [[Hammerly2003]](app04.xhtml#Hammerly2003) and
    an implementation in the SMILE library in [[Li2017]](app04.xhtml#Li2017)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [[Hammerly2003]](app04.xhtml#Hammerly2003) 找到更多参考资料，并在 SMILE 库中查看 [[Li2017]](app04.xhtml#Li2017)
    的实现。
- en: Streaming *K*-Means with Spark Streaming
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Spark Streaming 的流式 *K*-Means
- en: 'A streaming *K*-means model is available in Spark Streaming since Spark 1.2\.
    It works with two streams of data: one for training, and another one for prediction.
    The *K*-means model is initialized with a builder pattern:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 自 Spark 1.2 起，Spark Streaming 中提供了流式 *K*-means 模型。它使用两个数据流：一个用于训练，另一个用于预测。*K*-means
    模型使用构建器模式初始化：
- en: '[PRE6]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this example, we set out to determine three centers, with initial weighted
    random centers with a weight of 0, and points that are five-dimensional (a dimension
    we will need to find in the data). The decay factor of 1.0 indicates the forgetfulness
    of the previous centroids, marking the weight to assign to newer data in comparison
    to older ones. By default, this value of 1 does not operate any decay—assuming
    slow-moving values in the stream. We could also have passed the number of batch
    intervals after which the impact of a batch of points drops to half its value
    with `setHalfLife(batches: Double, durationUnit: String)`.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个例子中，我们设定了三个中心点，初始的加权随机中心点的权重为0，并且数据是五维的（这是我们需要在数据中找到的一个维度）。衰减因子1.0 表示了对之前质心的遗忘，标记了相对于旧数据而言分配给新数据的权重。默认情况下，这个值为1不会进行任何衰减——假设数据流中的值变化缓慢。我们也可以通过传递批次间隔的数量来设置，之后一批点的影响降至其值的一半，使用
    `setHalfLife(batches: Double, durationUnit: String)` 方法。'
- en: 'The training data should contain each point formatted as [*x_1, _x_2, …, _xn*],
    and each test data point should be formatted as (*y*, [*x_1, _x_2, …, _xn*]),
    where y is the label of this point. Note that `model.trainOn` does not, per se,
    create an output: indeed, the process mutates the `model` object without further
    ado. This is why we use Spark Streaming’s printing on the test dataset, which
    let us witness new classifications on updates of the test dataset. Note that the
    `latestModel()` function can be called on `StreamingKMeans` to access the latest
    version of the model.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据应该包含每个点的格式化形式为[*x_1, _x_2, …, _xn*]，每个测试数据点应该格式化为(*y*, [*x_1, _x_2, …, _xn*])，其中*y*是这个点的标签。注意，`model.trainOn`并不会直接创建输出：实际上，这个过程会在没有其他说明的情况下改变`model`对象。这就是为什么我们在测试数据集上使用Spark
    Streaming的打印功能，这让我们能够在测试数据集更新时见证新的分类。注意，可以调用`StreamingKMeans`上的`latestModel()`函数来访问模型的最新版本。
- en: ^([1](ch28.xhtml#idm46385808883048-marker)) The prior probabilities, in this
    context, are the frequencies for the classes (e.g., spam, ham) implied by counting
    the supervised labels. They represent the larger notion of probability of an event
    based on established knowledge.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch28.xhtml#idm46385808883048-marker)) 在这个背景下，先验概率是指通过计算监督标签暗示的类别（例如，垃圾邮件，正常邮件）频率来表示的事件概率的更广义概念。
- en: ^([2](ch28.xhtml#idm46385808856168-marker)) streamDM is sadly not published
    as a package, which is understandable given that it impinges on the `org.apache.spark`
    namespace, as we can see in the example.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch28.xhtml#idm46385808856168-marker)) `streamDM`很遗憾并没有作为一个包发布，考虑到它侵占了`org.apache.spark`的命名空间，在例子中我们可以看到这一点。
- en: ^([3](ch28.xhtml#idm46385808771480-marker)) Sometimes also known as Very Fast
    Decision Trees (VFDT), after the name used in the paper that introduced them.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch28.xhtml#idm46385808771480-marker)) 有时也被称为非常快速决策树（VFDT），源自介绍它们的论文中使用的名称。
- en: ^([4](ch28.xhtml#idm46385808753032-marker)) The Hoeffding bound is also often
    called *additive Chernoff bound*. Note that it requires a bounded range of values
    *R*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch28.xhtml#idm46385808753032-marker)) Hoeffding界限也经常被称为*加法Chernoff界限*。注意它需要一定范围内的有界数值*R*。
- en: ^([5](ch28.xhtml#idm46385808502616-marker)) The barycenter is the point that
    minimizes the distance of each and every point existing in the cluster of attention
    at any given moment.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch28.xhtml#idm46385808502616-marker)) 质心是在任何给定时刻注意力聚焦集群中每个点的距离最小化的点。
- en: ^([6](ch28.xhtml#idm46385808589032-marker)) This is a hardcoded constant in
    the current Streaming *K*-means implementation, unfortunately.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch28.xhtml#idm46385808589032-marker)) 这是当前Streaming *K*-means实现中的硬编码常量，不幸的是。
