- en: 'Chapter 12\. Epilogue: Apache Spark 3.0'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 12 章。结语：Apache Spark 3.0
- en: At the time we were writing this book, Apache Spark 3.0 had not yet been officially
    released; it was still under development, and we got to work with Spark 3.0.0-preview2\.
    All the code samples in this book have been tested against Spark 3.0.0-preview2,
    and they should work no differently with the official Spark 3.0 release. Whenever
    possible in the chapters, where relevant, we mentioned when features were new
    additions or behaviors in Spark 3.0\. In this chapter, we survey the changes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们撰写本书时，Apache Spark 3.0 尚未正式发布；它仍在开发中，我们使用的是 Spark 3.0.0-preview2 版本。本书中的所有代码示例均经过
    Spark 3.0.0-preview2 版本测试，且它们在正式发布的 Spark 3.0 版本中应该没有区别。在各章节中，尽可能地提及了 Spark 3.0
    中新增的特性和行为变化。在本章中，我们对这些变化进行了概述。
- en: The bug fixes and feature enhancements are numerous, so for brevity, we highlight
    just a selection of the notable changes and features pertaining to Spark components.
    Some of the new features are, under the hood, advanced and beyond the scope of
    this book, but we mention them here so you can explore them when the release is
    generally available.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 修复的错误和功能增强非常多，为了简洁起见，我们仅突出了与 Spark 组件相关的一些显著变化和功能。一些新功能在幕后非常先进，超出了本书的范围，但我们在此提到它们，以便您在发布普遍可用时进行探索。
- en: Spark Core and Spark SQL
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Core 和 Spark SQL
- en: Let’s first consider what’s new under the covers. A number of changes have been
    introduced in Spark Core and the Spark SQL engine to help speed up queries. One
    way to expedite queries is to read less data using dynamic partition pruning.
    Another is to adapt and optimize query plans during execution.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先考虑一下底层的新变化。Spark Core 和 Spark SQL 引擎都引入了一些变化来加快查询速度。一种加快查询速度的方法是通过动态分区裁剪来减少读取的数据量。另一种方法是在执行期间调整和优化查询计划。
- en: Dynamic Partition Pruning
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态分区裁剪
- en: 'The idea behind [dynamic partition pruning (DPP)](https://oreil.ly/fizdc) is
    to skip over the data you don’t need in a query’s results. The typical scenario
    where DPP is optimal is when you are joining two tables: a fact table (partitioned
    over multiple columns) and a dimension table (nonpartitioned), as shown in [Figure 12-1](#dynamic_filter_is_injected_from_the_dime).
    Normally, the filter is on the nonpartitioned side of the table (`Date`, in our
    case). For example, consider this common query over two tables, `Sales` and `Date`:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[动态分区裁剪（DPP）](https://oreil.ly/fizdc) 的理念是跳过查询结果中不需要的数据。DPP 最适合的典型场景是在联接两个表时：一个是分区的事实表，另一个是非分区的维度表，如图
    [12-1](#dynamic_filter_is_injected_from_the_dime) 所示。通常，过滤器位于表的非分区一侧（在我们的例子中是`Date`）。例如，考虑以下在两个表`Sales`和`Date`上执行的常见查询：'
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Dynamic filter is injected from the dimension table into the fact table](assets/lesp_1201.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![动态过滤器从维度表注入到事实表中](assets/lesp_1201.png)'
- en: Figure 12-1\. Dynamic filter is injected from the dimension table into the fact
    table
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-1。动态过滤器从维度表注入到事实表中
- en: The key optimization technique in DPP is to take the result of the filter from
    the dimension table and inject it into the fact table as part of the scan operation
    to limit the data read, as shown in [Figure 12-1](#dynamic_filter_is_injected_from_the_dime).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: DPP 中的关键优化技术是将来自维度表的过滤结果作为扫描操作的一部分注入到事实表中，以限制读取的数据量，如图 [12-1](#dynamic_filter_is_injected_from_the_dime)
    所示。
- en: 'Consider a case where the dimension table is smaller than the fact table and
    we perform a join, as shown in [Figure 12-2](#spark_injects_a_dimension_table_filter_i).
    In this case, Spark most likely will do a broadcast join (discussed in [Chapter 7](ch07.html#optimizing_and_tuning_spark_applications)).
    During this join, Spark will conduct the following steps to minimize the amount
    of data scanned from the larger fact table:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设维度表比事实表小，并且我们进行了一个联接操作，如图 [12-2](#spark_injects_a_dimension_table_filter_i)
    所示。在这种情况下，Spark 很可能会执行广播联接（在 [第 7 章](ch07.html#optimizing_and_tuning_spark_applications)
    中讨论）。在此联接过程中，Spark 将执行以下步骤来最小化从较大的事实表中扫描的数据量：
- en: On the dimension side of the join, Spark will build a hash table from the dimension
    table, also known as the build relation, as part of this filter query.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在联接的维度表侧，Spark 将从维度表构建一个哈希表，也称为构建关系，作为此过滤查询的一部分。
- en: Spark will plug the result of this query into the hash table and assign it to
    a broadcast variable, which is distributed to all executors involved in this join
    operation.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 将把此查询的结果插入到哈希表中，并将其分配给广播变量，该广播变量分发到所有参与此联接操作的执行器。
- en: On each executor, Spark will probe the broadcasted hash table to determine what
    corresponding rows to read from the fact table.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个执行器上，Spark将探测广播的哈希表，以确定从事实表中读取哪些对应的行。
- en: Finally, Spark will inject this filter dynamically into the file scan operation
    of the fact table and reuse the results from the broadcast variable. This way,
    as part of the file scan operation on the fact table, only the partitions that
    match the filter are scanned and only the data needed is read.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，Spark将动态地将此过滤器注入到事实表的文件扫描操作中，并重用广播变量的结果。这样，在事实表的文件扫描操作中，只会扫描与过滤器匹配的分区，并且只会读取所需的数据。
- en: '![Spark injects a dimension table filter into the fact table during a broadcast
    join](assets/lesp_1202.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Spark将一个维度表过滤器注入事实表中的广播连接](assets/lesp_1202.png)'
- en: Figure 12-2\. Spark injects a dimension table filter into the fact table during
    a broadcast join
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-2\. Spark在广播连接期间将一个维度表过滤器注入到事实表中
- en: Enabled by default so that you don’t have to explicitly configure it, all this
    happens dynamically when you perform joins between two tables. With the DPP optimization,
    Spark 3.0 can work much better with star-schema queries.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下启用，因此您无需显式配置，在执行两个表之间的连接时会动态发生这一切。通过DPP优化，Spark 3.0可以更好地处理星型模式查询。
- en: Adaptive Query Execution
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应查询执行
- en: 'Another way Spark 3.0 optimizes query performance is by adapting its physical
    execution plan at runtime. [*Adaptive Query Execution (AQE)*](https://oreil.ly/mO8Ua)
    reoptimizes and adjusts query plans based on runtime statistics collected in the
    process of query execution. It attempts to to do the following at runtime:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 3.0另一种优化查询性能的方法是在运行时调整其物理执行计划。[*自适应查询执行（AQE）*](https://oreil.ly/mO8Ua)
    根据在查询执行过程中收集的运行时统计信息，重新优化和调整查询计划。它试图在运行时执行以下操作：
- en: Reduce the number of reducers in the shuffle stage by decreasing the number
    of shuffle partitions.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过减少洗牌阶段的Reducer数量来减少洗牌分区的数量。
- en: Optimize the physical execution plan of the query, for example by converting
    a `SortMergeJoin` into a `BroadcastHashJoin` where appropriate.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化查询的物理执行计划，例如在适当的情况下将`SortMergeJoin`转换为`BroadcastHashJoin`。
- en: Handle data skew during a join.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理连接期间的数据倾斜。
- en: All these adaptive measures take place during the execution of the plan at runtime,
    as shown in [Figure 12-3](#aqe_reexamines_and_reoptimizes_the_execu). To use AQE
    in Spark 3.0, set the configuration `spark.sql.adaptive.enabled` to `true`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些自适应措施都发生在运行时计划的执行过程中，如[图12-3](#aqe_reexamines_and_reoptimizes_the_execu)所示。要在Spark
    3.0中使用AQE，请将配置`spark.sql.adaptive.enabled`设置为`true`。
- en: '![AQE reexamines and reoptimizes the execution plan at runtime](assets/lesp_1203.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![AQE在运行时重新检查并重新优化执行计划](assets/lesp_1203.png)'
- en: Figure 12-3\. AQE reexamines and reoptimizes the execution plan at runtime
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3\. AQE在运行时重新检查并重新优化执行计划
- en: The AQE framework
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AQE框架
- en: 'Spark operations in a query are pipelined and executed in parallel processes,
    but a shuffle or broadcast exchange breaks this pipeline, because the output of
    one stage is needed as input to the next stage (see [“Step 3: Understanding Spark
    Application Concepts”](ch02.html#step_3_understanding_spark_application_c) in
    [Chapter 2](ch02.html#downloading_apache_spark_and_getting_sta)). These breaking
    points are called *materialization points* in a query stage, and they present
    an opportunity to reoptimize and reexamine the query, as illustrated in [Figure 12-4](#a_query_plan_reoptimized_in_the_aqe_fram).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 查询中的Spark操作被流水线化并并行执行，但是洗牌或广播交换会打破这个流水线，因为一个阶段的输出需要作为下一个阶段的输入（参见[“第3步：理解Spark应用概念”](ch02.html#step_3_understanding_spark_application_c)
    在[第2章](ch02.html#downloading_apache_spark_and_getting_sta)）。在查询阶段中，这些断点称为*物化点*，它们提供了重新优化和重新检查查询的机会，如[图12-4](#a_query_plan_reoptimized_in_the_aqe_fram)所示。
- en: '![A query plan reoptimized in the AQE framework](assets/lesp_1204.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![在AQE框架中重新优化的查询计划](assets/lesp_1204.png)'
- en: Figure 12-4\. A query plan reoptimized in the AQE framework
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-4\. 在AQE框架中重新优化的查询计划
- en: 'Here are the conceptual steps the AQE framework iterates over, as depicted
    in this figure:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是AQE框架迭代的概念步骤，如图所示：
- en: All the leaf nodes, such as scan operations, of each stage are executed.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行每个阶段的所有叶节点，例如扫描操作。
- en: Once the materialization point finishes executing, it’s marked as complete,
    and all the relevant statistics garnered during execution are updated in its logical
    plan.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦物化点执行完成，它被标记为完成，并且在其逻辑计划中更新所有相关的统计信息。
- en: 'Based on these statistics, such as number of partitions read, bytes of data
    read, etc., the framework runs the Catalyst optimizer again to understand whether
    it can:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于这些统计数据，如读取的分区数、读取的数据字节数等，框架再次运行Catalyst优化器，以了解它是否可以：
- en: Coalesce the number of partitions to reduce the number of reducers to read shuffle
    data.
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并分区数以减少读取洗牌数据的减少器数量。
- en: Replace a sort merge join, based on the size of tables read, with a broadcast
    join.
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据读取的表的大小，用广播连接替换排序合并连接。
- en: Try to remedy a skew join.
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试修复倾斜连接。
- en: Create a new optimized logical plan, followed by a new optimized physical plan.
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的优化逻辑计划，然后创建一个新的优化物理计划。
- en: This process is repeated until all the stages of the query plan are executed.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 直到查询计划的所有阶段都执行完毕为止，该过程将重复进行。
- en: In short, this reoptimization is done dynamically, as shown in [Figure 12-3](#aqe_reexamines_and_reoptimizes_the_execu),
    and the objective is to dynamically coalesce the shuffle partitions, decrease
    the number of reducers needed to read the shuffle output data, switch join strategies
    if appropriate, and remedy any skew joins.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这种重新优化是动态进行的，如[图 12-3](#aqe_reexamines_and_reoptimizes_the_execu)所示，其目标是动态合并洗牌分区、减少需要读取洗牌输出数据的减少器数量、在适当时切换连接策略并修复任何倾斜连接。
- en: 'Two Spark SQL configurations dictate how AQE will reduce the number of reducers:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 两个Spark SQL配置决定了AQE如何减少减少器的数量：
- en: '`spark.sql.adaptive.coalescePartitions.enabled` (set to `true`)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.sql.adaptive.coalescePartitions.enabled`（设置为`true`）'
- en: '`spark.sql.adaptive.skewJoin.enabled` (set to `true`)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.sql.adaptive.skewJoin.enabled`（设置为`true`）'
- en: At the time of writing, the Spark 3.0 community blog, documentation, and examples
    had not been published publicly, but by the time of publication they should have
    been. These resources will enable you to get more detailed information if you
    wish to see how these features work under the hood—including on how you can inject
    SQL join hints, discussed next.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Spark 3.0社区博客、文档和示例尚未公开发布，但在出版时它们应该已经发布。如果您希望了解这些功能在幕后的工作原理——包括如何注入SQL连接提示，我们将在接下来讨论。
- en: SQL Join Hints
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SQL连接提示
- en: Adding to the existing `BROADCAST` hints for joins, Spark 3.0 adds join hints
    for all [Spark join strategies](https://oreil.ly/GqlqH) (see [“A Family of Spark
    Joins”](ch07.html#a_family_of_spark_joins) in [Chapter 7](ch07.html#optimizing_and_tuning_spark_applications)).
    Examples are provided here for each type of join.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 除了连接的现有`BROADCAST`提示外，Spark 3.0还为所有[Spark连接策略](https://oreil.ly/GqlqH)添加了连接提示（参见[“Spark连接家族”](ch07.html#a_family_of_spark_joins)中的[第7章](ch07.html#optimizing_and_tuning_spark_applications)）。下面为每种连接类型提供了示例。
- en: Shuffle sort merge join (SMJ)
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 洗牌排序合并连接（SMJ）
- en: 'With these new hints, you can suggest to Spark that it perform a `SortMergeJoin`
    when joining tables `a` and `b` or `customers` and `orders`, as shown in the following
    examples. You can add one or more hints to a `SELECT` statement inside `/*+ ...
    */` comment blocks:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 借助这些新提示，您可以建议Spark在连接表`a`和`b`或`customers`和`orders`时执行`SortMergeJoin`，如下例所示。您可以在`/*+
    ... */`注释块内的`SELECT`语句中添加一个或多个提示：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Broadcast hash join (BHJ)
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 广播哈希连接（BHJ）
- en: 'Similarly, for a broadcast hash join, you can provide a hint to Spark that
    you prefer a broadcast join. For example, here we broadcast table `a` to join
    with table `b` and table `customers` to join with table `orders`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，对于广播哈希连接，您可以向Spark提供提示，表明您更喜欢广播连接。例如，在这里我们将表`a`广播连接到表`b`和表`customers`广播连接到表`orders`：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Shuffle hash join (SHJ)
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 洗牌哈希连接（SHJ）
- en: 'You can offer hints in a similar way to perform shuffle hash joins, though
    this is less commonly encountered than the previous two supported join strategies:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以以类似的方式提供提示以执行洗牌哈希连接，尽管这比前两种支持的连接策略更少见：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Shuffle-and-replicate nested loop join (SNLJ)
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 洗牌并复制嵌套循环连接（SNLJ）
- en: 'Finally, the shuffle-and-replicate nested loop join adheres to a similar form
    and syntax:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，洗牌并复制嵌套循环连接遵循相似的形式和语法：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Catalog Plugin API and DataSourceV2
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录插件API和DataSourceV2
- en: 'Not to be confined only to the Hive metastore and catalog, Spark 3.0’s experimental
    DataSourceV2 API extends the Spark ecosystem and affords developers three core
    capabilities. Specifically, it:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 3.0的实验性DataSourceV2 API不仅限于Hive元数据存储和目录，还扩展了Spark生态系统，并为开发人员提供了三个核心能力。具体来说，它：
- en: Enables plugging in an external data source for catalog and table management
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许插入外部数据源以进行目录和表管理
- en: Supports predicate pushdown to additional data sources with supported file formats
    like ORC, Parquet, Kafka, Cassandra, Delta Lake, and Apache Iceberg.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持将谓词下推到其他数据源，支持的文件格式如ORC、Parquet、Kafka、Cassandra、Delta Lake和Apache Iceberg。
- en: Provides unified APIs for streaming and batch processing of data sources for
    sinks and sources
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为流数据源和汇提供统一的API，支持数据的批处理和流处理
- en: Aimed at developers who want to extend Spark’s ability to use external sources
    and sinks, the Catalog API provides both SQL and programmatic APIs to create,
    alter, load, and drop tables from the specified pluggable catalog. The catalog
    provides a hierarchical abstraction of functionalities and operations performed
    at different levels, as shown in [Figure 12-5](#catalog_plugin_apiapostrophes_hierarchic).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 针对希望扩展Spark使用外部源和汇的开发人员，Catalog API提供了SQL和编程API，以从指定的可插拔目录创建、修改、加载和删除表。该目录提供了不同层次上执行的功能和操作的分层抽象，如[图 12-5](#catalog_plugin_apiapostrophes_hierarchic)所示。
- en: '![Catalog plugin API’s hierarchical level of functionality](assets/lesp_1205.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![Catalog plugin API’s hierarchical level of functionality](assets/lesp_1205.png)'
- en: Figure 12-5\. Catalog plugin API’s hierarchical level of functionality
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-5\. Catalog plugin API的功能层次结构
- en: The initial interaction between Spark and a specific connector is to resolve
    a relation to its actual `Table` object. `Catalog` defines how to look up tables
    in this connector. Additionally, `Catalog` can define how to modify its own metadata,
    thus [enabling operations](https://oreil.ly/TrscV) like `CREATE TABLE`, `ALTER
    TABLE`, etc.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Spark与特定连接器的初始交互是将关系解析为其实际的`Table`对象。`Catalog`定义了如何在此连接器中查找表。此外，`Catalog`还可以定义如何修改其自身的元数据，从而实现像`CREATE
    TABLE`、`ALTER TABLE`等操作。
- en: 'For example, in SQL you can now issue commands to create namespaces for your
    catalog. To use a pluggable catalog, enable the following configs in your *spark-defaults.conf*
    file:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在SQL中，您现在可以发出命令为您的目录创建命名空间。要使用可插拔目录，在您的*spark-defaults.conf*文件中启用以下配置：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, the connector to the data source catalog has two options: `option1->value1`
    and `option2->value2`. Once they’ve been defined, application users in Spark or
    SQL can use the `DataFrameReader` and `DataFrameWriter` API methods or Spark SQL
    commands with these defined options as methods for data source manipulation. For
    example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，与数据源目录的连接器有两个选项：`option1->value1`和`option2->value2`。一旦它们被定义，Spark或SQL中的应用程序用户可以使用`DataFrameReader`和`DataFrameWriter`
    API方法或具有这些定义选项的Spark SQL命令作为数据源操作的方法。例如：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: While these catalog plugin APIs extend Spark’s ability to utilize external data
    sources as sinks and sources, they are still experimental and should not be used
    in production. A detailed guide to their use is beyond the scope of this book,
    but we encourage you to check the release documentation for additional information
    if you want to write a custom connector to an external data source as a catalog
    to manage your external tables and their associated metadata.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些目录插件API扩展了Spark利用外部数据源作为汇和源的能力，但它们仍处于试验阶段，不应在生产环境中使用。本书不涵盖其详细使用指南，但如果您希望编写自定义连接器以将外部数据源作为目录来管理您的外部表及其相关元数据，则建议您查阅发布文档以获取更多信息。
- en: Note
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding code snippets are examples of what your code may look like after
    you have defined and implemented your catalog connectors and populated them with
    data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在您定义和实现目录连接器并用数据填充后，您的代码可能看起来像的示例代码片段。
- en: Accelerator-Aware Scheduler
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速器感知调度器
- en: '[Project Hydrogen](https://oreil.ly/Jk4rA), a community initiative to bring
    AI and big data together, has three major goals: implementing barrier execution
    mode, accelerator-aware scheduling, and optimized data exchange. A basic implementation
    of [barrier execution mode](https://oreil.ly/RDyb1) was introduced in Apache Spark
    2.4.0\. In Spark 3.0, a basic [scheduler](https://oreil.ly/9TOyT) has been implemented
    to take advantage of hardware accelerators such as GPUs on target platforms where
    Spark is deployed in standalone mode, YARN, or Kubernetes.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[Project Hydrogen](https://oreil.ly/Jk4rA)，一个将人工智能和大数据结合起来的社区倡议，有三个主要目标：实现障碍执行模式、加速器感知调度和优化数据交换。Apache
    Spark 2.4.0引入了[障碍执行模式](https://oreil.ly/RDyb1)的基本实现。在Spark 3.0中，已实现了基本的[调度器](https://oreil.ly/9TOyT)，以利用目标平台上Spark独立模式、YARN或Kubernetes上的GPU等硬件加速器。'
- en: 'For Spark to take advantage of these GPUs in an organized way for specialized
    workloads that use them, you have to specify the hardware resources available
    via configs. Your application can then discover them with the help of a discovery
    script. Enabling GPU use is a three-step process in your Spark application:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要使 Spark 以组织良好的方式利用这些 GPU 用于使用它们的专门工作负载，您必须通过配置指定可用的硬件资源。然后，您的应用程序可以通过发现脚本帮助发现它们。在
    Spark 应用程序中启用 GPU 使用是一个三步骤过程：
- en: 'Write a discovery script that discovers the addresses of the underlying GPUs
    available on each Spark executor. This script is set in the following Spark configuration:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个发现脚本，以发现每个 Spark 执行器上可用的底层 GPU 的地址。此脚本设置在以下 Spark 配置中：
- en: '[PRE8]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Set up configuration for your Spark executors to use these discovered GPUs:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置您的 Spark 执行器以使用这些发现的 GPU：
- en: '[PRE9]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Write RDD code to leverage these GPUs for your task:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写 RDD 代码以利用这些 GPU 完成您的任务：
- en: '[PRE10]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: These steps are still experimental, and further development will continue in
    future Spark 3.x releases to support seamless discovery of GPU resources, both
    at the command line (with `spark-submit`) and at the Spark task level.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤仍处于实验阶段，未来 Spark 3.x 的进一步开发将继续支持无缝发现 GPU 资源，无论是在命令行（使用 `spark-submit`）还是在
    Spark 任务级别。
- en: Structured Streaming
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流
- en: 'To inspect how your Structured Streaming jobs fare with the ebb and flow of
    data during the course of execution, the Spark 3.0 UI has a new Structured Streaming
    tab alongside the other tabs we explored in [Chapter 7](ch07.html#optimizing_and_tuning_spark_applications).
    This tab offers two sets of statistics: aggregate information about completed
    streaming query jobs ([Figure 12-6](#structured_streaming_tab_showing_aggrega))
    and detailed statistics about the streaming queries, including the input rate,
    process rate, number of input rows, batch duration, and operation duration ([Figure 12-7](#showing_detailed_statistics_of_a_complet)).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查您的结构化流作业在执行过程中随数据的起伏如何变化，Spark 3.0 UI 有一个新的结构化流选项卡，与我们在 [第 7 章](ch07.html#optimizing_and_tuning_spark_applications)
    探索的其他选项卡并列。此选项卡提供两组统计信息：关于已完成的流查询作业的聚合信息（[图 12-6](#structured_streaming_tab_showing_aggrega)）和关于流查询的详细统计信息，包括输入速率、处理速率、输入行数、批处理持续时间和操作持续时间（[图 12-7](#showing_detailed_statistics_of_a_complet)）。
- en: '![Structured Streaming tab showing aggregate statistics of a completed streaming
    job](assets/lesp_1206.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![显示已完成流作业的聚合统计信息的结构化流选项卡](assets/lesp_1206.png)'
- en: Figure 12-6\. Structured Streaming tab showing aggregate statistics of a completed
    streaming job
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-6\. 结构化流选项卡显示已完成流作业的聚合统计信息
- en: Note
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The [Figure 12-7](#showing_detailed_statistics_of_a_complet) screenshot was
    taken with Spark 3.0.0-preview2; with the final release, you should see the query
    name and ID in the name identifier on the UI page.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark 3.0.0-preview2 截取的 [图 12-7](#showing_detailed_statistics_of_a_complet)
    屏幕截图；在最终版本中，您应该在 UI 页面的名称标识符中看到查询名称和 ID。
- en: '![Showing detailed statistics of a completed streaming job](assets/lesp_1207.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![显示已完成流作业的详细统计信息](assets/lesp_1207.png)'
- en: Figure 12-7\. Showing detailed statistics of a completed streaming job
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-7\. 显示已完成流作业的详细统计信息
- en: '[No configuration is required](https://oreil.ly/wP1QB); all configurations
    works straight out of the Spark 3.0 installation, with the following defaults:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[不需要任何配置](https://oreil.ly/wP1QB)；所有配置都在 Spark 3.0 安装后直接工作，默认情况如下：'
- en: '`spark.sql.streaming.ui.enabled=true`'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.sql.streaming.ui.enabled=true`'
- en: '`spark.sql.streaming.ui.retainedProgressUpdates=100`'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.sql.streaming.ui.retainedProgressUpdates=100`'
- en: '`spark.sql.streaming.ui.retainedQueries=100`'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.sql.streaming.ui.retainedQueries=100`'
- en: PySpark, Pandas UDFs, and Pandas Function APIs
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark、Pandas UDF 和 Pandas 函数 API
- en: Spark 3.0 requires pandas version 0.23.2 or higher to employ any pandas-related
    methods, such as `DataFrame.toPandas()` or `SparkSession.createDataFrame(pandas.DataFrame)`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 3.0 需要 pandas 版本 0.23.2 或更高版本来使用任何与 pandas 相关的方法，例如 `DataFrame.toPandas()`
    或 `SparkSession.createDataFrame(pandas.DataFrame)`。
- en: Furthermore, it requires PyArrow version 0.12.1 or later to use PyArrow functionality
    such as `pandas_udf()`, `DataFrame.toPandas()`, and `SparkSession.createDataFrame(pandas.DataFrame)`
    with the `spark.sql.execution.arrow.enabled` configuration set to `true`. The
    next section will introduce new features in Pandas UDFs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，要使用 PyArrow 功能（如 `pandas_udf()`、`DataFrame.toPandas()` 和 `SparkSession.createDataFrame(pandas.DataFrame)`），需要
    PyArrow 版本 0.12.1 或更高版本，并将 `spark.sql.execution.arrow.enabled` 配置设置为 `true`。接下来的部分将介绍
    Pandas UDF 中的新功能。
- en: Redesigned Pandas UDFs with Python Type Hints
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 类型提示重新设计的 Pandas UDFs
- en: The Pandas UDFs in Spark 3.0 were redesigned by leveraging [Python type hints](https://oreil.ly/tAEA9).
    This enables you to naturally express UDFs without requiring the evaluation type.
    Pandas UDFs are now more “Pythonic” and can themselves define what the UDF is
    supposed to input and output, rather than you specifying it via, for example,
    `@pandas_udf("long", PandasUDFType.SCALAR)` as you did in Spark 2.4.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 3.0 中的 Pandas UDFs 经过重新设计，利用了 [Python 类型提示](https://oreil.ly/tAEA9)。这使您可以自然地表达
    UDFs，而无需评估类型。现在，Pandas UDFs 更具“Pythonic”风格，并且可以自行定义 UDF 应输入和输出的内容，而不需要您像在 Spark
    2.4 中那样通过 `@pandas_udf("long", PandasUDFType.SCALAR)` 指定。
- en: 'Here’s an example:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This new format provides several benefits, such as easier static analysis.
    You can apply the new UDFs in the same way as before:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这种新格式带来了多个好处，例如更容易的静态分析。您可以像以前一样应用新的 UDFs：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Iterator Support in Pandas UDFs
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pandas UDFs 中的迭代器支持
- en: Pandas UDFs are very commonly used to load a model and perform distributed inference
    for single-node machine learning and deep learning models. However, if a model
    is very large, then there is high overhead for the Pandas UDF to repeatedly load
    the same model for every batch in the same Python worker process.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas UDFs 通常用于加载模型并对单节点机器学习和深度学习模型执行分布式推断。然而，如果模型非常大，则 Pandas UDF 为了在同一 Python
    工作进程中的每个批次中重复加载相同的模型会产生高额开销。
- en: 'In Spark 3.0, Pandas UDFs can [accept an iterator](https://oreil.ly/FboVn)
    of `pandas.Series` or `pandas.DataFrame`, as shown here:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 3.0 中，Pandas UDFs 可以接受 `pandas.Series` 或 `pandas.DataFrame` 的迭代器，如下所示：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With this support, you can load the model only once instead of loading it for
    every series in the iterator. The following pseudocode illustrates how to do this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 借助此支持，您可以仅在加载模型一次而不是在迭代器中的每个系列中加载它。以下伪代码说明了如何做到这一点：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: New Pandas Function APIs
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新的 Pandas 函数 APIs
- en: 'Spark 3.0 introduces a few new types of Pandas UDFs that are useful when you
    want to apply a function against an entire DataFrame instead of column-wise, such
    as `mapInPandas()`, introduced in [Chapter 11](ch11.html#managingcomma_deployingcomma_and_scaling).
    These take an iterator of `pandas.DataFrame` as input and output another iterator
    of `pandas.DataFrame`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 3.0 引入了几种新类型的 Pandas UDFs，当您希望对整个 DataFrame 而不是列进行函数应用时非常有用，例如 `mapInPandas()`，在
    [第 11 章](ch11.html#managingcomma_deployingcomma_and_scaling) 中引入。它们接受 `pandas.DataFrame`
    的迭代器作为输入，并输出另一个 `pandas.DataFrame` 的迭代器：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can control the size of the `pandas.DataFrame` by specifying it in the `spark.sql.execution.arrow.maxRecordsPerBatch`
    configuration. Note that the input size and output size do not have to match,
    unlike with most Pandas UDFs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在 `spark.sql.execution.arrow.maxRecordsPerBatch` 配置中指定来控制 `pandas.DataFrame`
    的大小。请注意，输入大小和输出大小不必匹配，这与大多数 Pandas UDFs 不同。
- en: Note
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: All the data of a cogroup will be loaded into memory, which means if there is
    data skew or certain groups are too big to fit in memory you could run into OOM
    issues.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 cogroup 的数据将加载到内存中，这意味着如果存在数据倾斜或某些组过大而无法适应内存，可能会遇到 OOM 问题。
- en: 'Spark 3.0 also introduces cogrouped map Pandas UDFs. The `applyInPandas()`
    function takes two `pandas.DataFrame`s that share a common key and applies a function
    to each cogroup. The returned `pandas.DataFrame`s are then combined as a single
    DataFrame. As with `mapInPandas()`, there is no restriction on the length of the
    returned `pandas.DataFrame`. Here’s an example:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 3.0 还引入了 cogrouped map Pandas UDFs。`applyInPandas()` 函数接受两个共享公共键的 `pandas.DataFrame`，并对每个
    cogroup 应用函数。然后将返回的 `pandas.DataFrame` 合并为单个 DataFrame。与 `mapInPandas()` 一样，返回的
    `pandas.DataFrame` 的长度没有限制。以下是一个示例：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Changed Functionality
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 功能变更
- en: Listing all the functionality changes in Spark 3.0 would transform this book
    into a brick several inches thick. So, in the interest of brevity, we will mention
    a few notable ones here, and leave you to consult the release notes for Spark
    3.0 for full details and all the nuances as soon as they are available.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果列出 Spark 3.0 中所有功能的更改，会使这本书变成几英寸厚的砖头。因此，为了简洁起见，我们在此提及几个显著的更改，并建议您查阅 Spark
    3.0 的发行说明，以获取完整的详细信息和所有细微差别。
- en: Languages Supported and Deprecated
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持的语言和已弃用的语言
- en: Spark 3.0 supports Python 3 and JDK 11, and Scala version 2.12 is required.
    All Python versions earlier than 3.6 and Java 8 are deprecated. If you use these
    deprecated versions you will get warning messages.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 3.0 支持 Python 3 和 JDK 11，并且需要 Scala 版本 2.12。所有早于 3.6 版本的 Python 和 Java
    8 都已不推荐使用。如果您使用这些已弃用版本，将收到警告消息。
- en: Changes to the DataFrame and Dataset APIs
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame 和 Dataset APIs 的变更
- en: In previous versions of Spark, the Dataset and DataFrame APs had deprecated
    the `unionAll()` method. In Spark 3.0 this has been reversed, and `unionAll()`
    is now an alias to the `union()` method.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的 Spark 版本中，Dataset 和 DataFrame 的 API 已弃用了 `unionAll()` 方法。在 Spark 3.0 中，这一点已被颠倒过来，`unionAll()`
    现在是 `union()` 方法的别名。
- en: 'Also, earlier versions of Spark’s `Dataset.groupByKey()` resulted in a grouped
    Dataset with the key spuriously named as `value` when the key was a non-struct
    type (`int`, `string`, `array`, etc.). As such, aggregation results from `ds.groupByKey().count()`
    in the query when displayed looked, counterintuitively, like `(value, count)`.
    This has been rectified to result in `(key, count)`, which is more intuitive.
    For example:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在较早版本的 Spark 中，`Dataset.groupByKey()` 结果会使得非结构化类型（`int`、`string`、`array`
    等）的键在显示时具有误导性地命名为 `value`。因此，查询中的聚合结果看起来反直觉，例如 `(value, count)`。这已被更正为更直观的 `(key,
    count)`。例如：
- en: '[PRE17]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: However, you can preserve the old format if you prefer by setting `spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue`
    to `true`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您愿意，可以通过将 `spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue` 设置为
    `true` 来保留旧格式。
- en: DataFrame and SQL Explain Commands
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame 和 SQL Explain 命令
- en: 'For better readability and formatting, Spark 3.0 introduces the `DataFrame.explain(*FORMAT_MODE*)`
    capability to display different views of the plans the Catalyst optimizer generates.
    The `*FORMAT_MODE*` options include `"simple"` (the default), `"extended"`, `"cost"`,
    `"codegen"`, and `"formatted"`. Here’s a simple illustration:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高可读性和格式化，Spark 3.0 引入了 `DataFrame.explain(*FORMAT_MODE*)` 功能，以显示 Catalyst
    优化器生成的计划的不同视图。`*FORMAT_MODE*` 选项包括 `"simple"`（默认）、`"extended"`、`"cost"`、`"codegen"`
    和 `"formatted"`。这里是一个简单的示例：
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: To see the rest of the format modes in action, you can try the notebook in the
    book’s [GitHub repo](https://github.com/databricks/LearningSparkV2). Also check
    out the [migration guides](https://spark.apache.org/docs/latest/migration-guide.html)
    from Spark 2.x to Spark 3.0.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看其它格式模式的示例，您可以尝试书中的笔记本 [GitHub 仓库](https://github.com/databricks/LearningSparkV2)。同时查看从
    Spark 2.x 迁移到 Spark 3.0 的 [迁移指南](https://spark.apache.org/docs/latest/migration-guide.html)。
- en: Summary
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter provided a cursory highlight of new features in Spark 3.0\. We
    took the liberty of mentioning a few advanced features that are worthy of note.
    They operate under the hood and not at the API level. In particular, we took a
    look at dynamic partition pruning (DPP) and adaptive query execution (AQE), two
    optimizations that enhance Spark’s performance at execution time. We also explored
    how the experimental Catalog API extends the Spark ecosystem to custom data stores
    for sources and sinks for both batch and streaming data, and looked at the new
    scheduler in Spark 3.0 that enables it to take advantage of GPUs in executors.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要介绍了 Spark 3.0 的新功能亮点。我们随意提到了一些值得注意的高级功能。这些功能在 API 层面之下运作。特别是，我们研究了动态分区剪裁（DPP）和自适应查询执行（AQE）这两种优化，它们在执行时提升了
    Spark 的性能。我们还探讨了实验性的 Catalog API 如何将 Spark 生态系统扩展到自定义数据存储，供批处理和流处理数据的源和汇。我们还看了
    Spark 3.0 中的新调度器，使其能够利用执行器中的 GPU。
- en: Complementing our discussion of the Spark UI in [Chapter 7](ch07.html#optimizing_and_tuning_spark_applications),
    we also showed you the new Structured Streaming tab, providing accumulated statistics
    on streaming jobs, additional visualizations, and detailed metrics on each query.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论 Spark UI 的补充中，[第 7 章](ch07.html#optimizing_and_tuning_spark_applications)
    我们还向您展示了新的结构化流选项卡，提供了关于流作业的累积统计信息、额外的可视化效果以及每个查询的详细指标。
- en: Python versions below 3.6 are deprecated in Spark 3.0, and Pandas UDFs have
    been redesigned to support Python type hints and iterators as arguments. There
    are Pandas UDFs that enable transforming an entire DataFrame, as well as combining
    two cogrouped DataFrames into a new DataFrame.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 3.0 中，Python 版本低于 3.6 已被弃用，并且 Pandas UDF 已被重新设计以支持 Python 类型提示和迭代器作为参数。有
    Pandas UDF 可以用于转换整个 DataFrame，以及将两个共组合的 DataFrame 合并为一个新的 DataFrame。
- en: For better readability of query plans, `DataFrame.explain(*FORMAT_MODE*)` and
    `EXPLAIN *FORMAT_MODE*` in SQL display different levels and details of logical
    and physical plans. Additionally, SQL commands can now take join hints for Spark’s
    entire supported family of joins.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地阅读查询计划，`DataFrame.explain(*FORMAT_MODE*)` 和 SQL 中的 `EXPLAIN *FORMAT_MODE*`
    显示逻辑和物理计划的不同级别和细节。此外，SQL 命令现在可以为 Spark 支持的整个连接家族提供连接提示。
- en: While we were unable to enumerate all the changes in the latest version of Spark
    in this short chapter, we urge that you explore the release notes when Spark 3.0
    is released to find out more. Also, for a quick summary of the user-facing changes
    and details on how to migrate to Spark 3.0, we encourage you to check out the
    migration guides.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们无法在本短章节中详细列出最新版本 Spark 中的所有更改，但我们建议您在 Spark 3.0 发布时查阅发布说明以获取更多信息。此外，为了快速了解用户界面的变化以及如何迁移到
    Spark 3.0，请参阅迁移指南。
- en: As a reminder, all the code in this book has been tested on Spark 3.0.0-preview2
    and should work with Spark 3.0 when it is officially released. We hope you’ve
    enjoyed reading this book and learned from this journey with us. We thank you
    for your attention!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，本书中的所有代码都已在 Spark 3.0.0-preview2 上进行了测试，并应在 Spark 3.0 正式发布时正常工作。希望您享受阅读本书，并从这段旅程中收获良多。感谢您的关注！
