- en: Chapter 2\. Stream-Processing Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。流处理模型
- en: In this chapter, we bridge the notion of a data stream—a source of data “on
    the move”—with the programming language primitives and constructs that allow us
    to express stream processing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将数据流的概念——即“移动中的数据”源——与允许我们表达流处理的编程语言原语和构造进行了桥接。
- en: 'We want to describe simple, fundamental concepts first before moving on to
    how Apache Spark represents them. Specifically, we want to cover the following
    as components of stream processing:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望首先描述简单而基本的概念，然后再讨论Apache Spark如何表示它们。具体来说，我们想涵盖以下流处理的组件：
- en: Data sources
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源
- en: Stream-processing pipelines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流处理管道
- en: Data sinks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据汇
- en: We then show how those concepts map to the specific stream-processing model
    implemented by Apache Spark.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来展示这些概念如何映射到由Apache Spark实现的特定流处理模型。
- en: Next, we characterize stateful stream processing, a type of stream processing
    that requires bookkeeping of past computations in the form of some intermediate
    state needed to process new data. Finally, we consider streams of timestamped
    events and basic notions involved in addressing concerns such as “what do I do
    if the order and timeliness of the arrival of those events do not match expectations?”
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们描述具有状态的流处理，这是一种需要通过一些中间状态进行过去计算记录的流处理类型。最后，我们考虑了时间戳事件流以及解决诸如“如果事件的顺序和及时性不符合期望，我该怎么办？”等问题的基本概念。
- en: Sources and Sinks
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 源和汇
- en: As we mentioned earlier, Apache Spark, in each of its two streaming systems—Structured
    Streaming and Spark Streaming—is a programming framework with APIs in the Scala,
    Java, Python, and R programming languages. It can only operate on data that enters
    the runtime of programs using this framework, and it ceases to operate on the
    data as soon as it is being sent to another system.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前文提到的，Apache Spark在其两个流系统——结构化流和Spark Streaming中，是一个具有Scala、Java、Python和R编程语言API的编程框架。它只能操作进入使用此框架程序的运行时的数据，并且一旦数据被发送到另一个系统，它就停止操作该数据。
- en: 'This is a concept that you are probably already familiar with in the context
    of data at rest: to operate on data stored as a file of records, we need to read
    that file into memory where we can manipulate it, and as soon as we have produced
    an output by computing on this data, we have the ability to write that result
    to another file. The same principle applies to databases—another example of data
    at rest.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据静止的情况下，您可能已经熟悉这个概念：要在文件记录中存储的数据上操作，我们需要将该文件读入内存以便操作，一旦我们通过对这些数据进行计算生成了输出，我们就可以将该结果写入另一个文件。同样的原理适用于数据库——这是数据静止的另一个例子。
- en: 'Similarly, data streams can be made accessible as such, in the streaming framework
    of Apache Spark using the concept of streaming *data sources*. In the context
    of stream processing, accessing data from a stream is often referred to as *consuming
    the stream*. This abstraction is presented as an interface that allows the implementation
    of instances aimed to connect to specific systems: Apache Kafka, Flume, Twitter,
    a TCP socket, and so on.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，数据流可以在Apache Spark的流处理框架中作为流式*数据源*访问。在流处理的上下文中，从流中访问数据通常被称为*消费流*。这种抽象被呈现为一个接口，允许实现针对特定系统连接的实例：如Apache
    Kafka、Flume、Twitter、TCP套接字等。
- en: Likewise, we call the abstraction used to write a data stream outside of Apache
    Spark’s control a *streaming sink*. Many connectors to various specific systems
    are provided by the Spark project itself as well as by a rich ecosystem of open
    source and commercial third-party integrations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们将用于在Apache Spark控制之外写入数据流的抽象称为*流式汇*。Spark项目本身提供了许多连接器，以及丰富的开源和商业第三方集成生态系统。
- en: In [Figure 2-1](#simplified-streaming-model), we illustrate this concept of
    sources and sinks in a stream-processing system. Data is consumed from a source
    by a processing component and the eventual results are produced to a sink.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2-1](#simplified-streaming-model)中，我们以流处理系统中源和汇的概念作图说明。数据由处理组件从源消费，最终结果由汇生成。
- en: '![spas 0201](Images/spas_0201.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0201](Images/spas_0201.png)'
- en: Figure 2-1\. Simplified streaming model
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1。简化的流模型
- en: The notion of sources and sinks represents the system’s boundary. This labeling
    of system boundaries makes sense given that a distributed framework can have a
    highly complex footprint among our computing resources. For example, it is possible
    to connect an Apache Spark cluster to another Apache Spark cluster, or to another
    distributed system, of which Apache Kafka is a frequent example. In that context,
    one framework’s sink is the downstream framework’s source. This chaining is commonly
    known as a *pipeline*. The name of sources and sinks is useful to both describe
    data passing from one system to the next and which point of view we are adopting
    when speaking about each system independently.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 源和汇点的概念代表了系统的边界。在分布式框架中，对系统边界的标记是有意义的，因为这种分布式框架在我们的计算资源中可能有着高度复杂的足迹。例如，可以将一个Apache
    Spark集群连接到另一个Apache Spark集群，或者连接到另一个分布式系统，其中Apache Kafka就是一个常见的例子。在这种情况下，一个框架的汇点就是下游框架的源。这种链式连接通常被称为*管道*。源和汇点的名称对于描述数据从一个系统传递到下一个系统，以及我们在独立讨论每个系统时采用的视角，都是有用的。
- en: Immutable Streams Defined from One Another
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从彼此定义的不可变流
- en: Between sources and sinks lie the programmable constructs of a stream-processing
    framework. We do not want to get into the details of this subject yet—you will
    see them appear later in [Part II](part02.xhtml#str-str) and [Part III](part03.xhtml#spark-streaming)
    for Structured Streaming and Spark Streaming, respectively. But we can introduce
    a few notions that will be useful to understand how we express stream processing.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在源和汇点之间，是流处理框架的可编程构造。我们暂不深入讨论这个主题的细节，你将在第二部分（[Part II](part02.xhtml#str-str)）和第三部分（[Part III](part03.xhtml#spark-streaming)）中看到它们在结构化流和Spark流处理中的应用。但我们可以介绍一些概念，这些概念对于理解我们如何表达流处理是有用的。
- en: 'Both stream APIs in Apache Spark take the approach of functional programming:
    they declare the transformations and aggregations they operate on data streams,
    assuming that those streams are immutable. As such, for one given stream, it is
    impossible to mutate one or several of its elements. Instead, we use transformations
    to express how to process the contents of one stream to obtain a derived data
    stream. This makes sure that at any given point in a program, any data stream
    can be traced to its inputs by a sequence of transformations and operations that
    are explicitly declared in the program. As a consequence, any particular process
    in a Spark cluster can reconstitute the content of the data stream using only
    the program and the input data, making computation unambiguous and reproducible.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark中的两个流API都采用了函数式编程的方法：它们声明对数据流进行的转换和聚合操作，假设这些流是不可变的。因此，对于给定的流，不可能改变其一个或多个元素。相反，我们使用转换来表达如何处理一个流的内容以获取派生数据流。这确保在程序的任何给定点，可以通过一系列明确声明的转换和操作来追溯任何数据流到其输入。因此，在Spark集群中的任何特定过程都可以仅通过程序和输入数据重建数据流的内容，使计算变得明确和可重现。
- en: Transformations and Aggregations
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换和聚合
- en: Spark makes extensive use of *transformations* and *aggregations*. Transformations
    are computations that express themselves in the same way for every element in
    the stream. For example, creating a derived stream that doubles every element
    of its input stream corresponds to a transformation. Aggregations, on the other
    hand, produce results that depend on many elements and potentially every element
    of the stream observed until now. For example, collecting the top five largest
    numbers of an input stream corresponds to an aggregation. Computing the average
    value of some reading every 10 minutes is also an example of an aggregation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Spark广泛使用*转换*和*聚合*。转换是表达对流中每个元素的处理方式的计算。例如，创建一个派生流，使其每个输入流元素都加倍，就对应一个转换操作。而聚合则产生依赖于许多元素，可能是流到目前为止的每个元素的结果。例如，收集输入流的前五个最大数值就是一个聚合操作。每10分钟计算某个读数的平均值也是聚合的一个例子。
- en: Another way to designate those notions is to say that transformations have *narrow
    dependencies* (to produce one element of the output, you need only one of the
    elements of the input), whereas aggregations have *wide dependencies* (to produce
    one element of the output you would need to observe many elements of the input
    stream encountered so far). This distinction is useful. It lets us envision a
    way to express basic functions that produces results using higher-order functions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种指代这些概念的方式是说，转换具有 *窄依赖*（为了生成输出的一个元素，你只需要输入的一个元素），而聚合具有 *宽依赖*（为了生成输出的一个元素，你需要观察到迄今为止遇到的许多输入流元素）。这种区别很有用。它让我们能够设想一种使用高阶函数产生结果的基本函数。
- en: Although Spark Streaming and Structured Streaming have distinct ways of representing
    a data stream, the APIs they operate on are similar in nature. They both present
    themselves under the form of a series of transformations applied to immutable
    input streams and produce an output stream, either as a bona fide data stream
    or as an output operation (data sink).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Spark Streaming 和 Structured Streaming 在表示数据流的方式上有所不同，但它们操作的 API 性质相似。它们都以一系列应用于不可变输入流的转换形式呈现自己，并产生一个输出流，可以是真正的数据流，也可以是输出操作（数据汇）。
- en: Window Aggregations
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口聚合
- en: 'Stream-processing systems often feed themselves on actions that occur in real
    time: social media messages, clicks on web pages, ecommerce transactions, financial
    events, or sensor readings are also frequently encountered examples of such events.
    Our streaming application often has a centralized view of the logs of several
    places, whether those are retail locations or simply web servers for a common
    application. Even though seeing every transaction individually might not be useful
    or even practical, we might be interested in seeing the properties of events seen
    over a recent period of time; for example, the last 15 minutes or the last hour,
    or maybe even both.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理系统通常依靠发生在实时的动作进行数据喂养：社交媒体消息、网页点击、电子商务交易、金融事件或传感器读数也是这类事件的常见例子。我们的流处理应用通常具有多个地方日志的集中视图，无论是零售位置还是共同应用的网页服务器。即使单独查看每个事务可能并不实用，甚至可能不切实际，我们可能对近期一段时间内事件的属性感兴趣；例如，过去的
    15 分钟或过去的一小时，甚至两者都可能。
- en: Moreover, the very idea of stream processing is that the system is supposed
    to be long-running, dealing with a continuous stream of data. As these events
    keep coming in, the older ones usually become less and less relevant to whichever
    processing you are trying to accomplish.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，流处理的核心思想是系统应该长时间运行，处理连续的数据流。随着这些事件不断发生，较旧的事件通常对你尝试完成的任何处理变得越来越不相关。
- en: We find many applications of regular and recurrent time-based aggregations that
    we call *windows*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现许多应用程序需要定期和周期性的基于时间的聚合，我们称之为 *窗口*。
- en: Tumbling Windows
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tumbling 窗口
- en: The most natural notion of a window aggregation is that of “a grouping function
    each x period of time.” For instance, “the maximum and minimum ambient temperature
    each hour” or “the total energy consumption (kW) each 15 minutes” are examples
    of window aggregations. Notice how the time periods are inherently consecutive
    and nonoverlapping. We call this grouping of a fixed time period, in which each
    group follows the previous and does not overlap, *tumbling windows*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口聚合的最自然概念是“每隔一段时间执行一次分组函数”。例如，“每小时的最高和最低环境温度”或“每 15 分钟的总能量消耗（千瓦）”是窗口聚合的示例。注意这些时间段天然是连续且不重叠的。我们称这种固定时间段的分组，其中每个组跟随前一个组并且不重叠，为
    *tumbling 窗口*。
- en: Tumbling windows are the norm when we need to produce aggregates of our data
    over regular periods of time, with each period independent from previous periods.
    [Figure 2-2](#tumbling_windows) shows a tumbling window of 10 seconds over a stream
    of elements. This illustration demonstrates the tumbling nature of tumbling windows.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要在固定时间段内对数据进行聚合，并且每个时间段独立于前一段时，tumbling 窗口是一种常见的选择。[图 2-2](#tumbling_windows)
    展示了在元素流上的一个 10 秒 tumbling 窗口。这幅图说明了 tumbling 窗口的 tumbling 特性。
- en: '![spas 0202](Images/spas_0202.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0202](Images/spas_0202.png)'
- en: Figure 2-2\. Tumbling windows
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. Tumbling 窗口
- en: Sliding Windows
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滑动窗口
- en: '*Sliding windows* are aggregates over a period of time that are reported at
    a higher frequency than the aggregation period itself. As such, sliding windows
    refer to an aggregation with two time specifications: the window length and the
    reporting frequency. It usually reads like “a grouping function over a time interval
    *x* reported each *y* frequency.” For example, “the average share price over the
    last day reported hourly.” As you might have noticed already, this combination
    of a sliding window with the average function is the most widely known form of
    a sliding window, commonly known as a *moving average*.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*滑动窗口*是在一段时间内的聚合，其报告频率高于聚合周期本身。因此，滑动窗口是指带有两个时间规范的聚合：窗口长度和报告频率。通常读起来像“每隔 *y*
    频率报告一个时间间隔 *x* 的分组函数”。例如，“过去一天的平均股票价格，每小时报告一次”。正如您可能已经注意到的那样，滑动窗口与平均函数的这种组合是滑动窗口的最广为人知的形式，通常被称为*移动平均*。'
- en: '[Figure 2-3](#sliding_window) shows a sliding window with a window size of
    30 seconds and a reporting frequency of 10 seconds. In the illustration, we can
    observe an important characteristic of *sliding windows*: they are not defined
    for periods of time smaller than the size of the window. We can see that there
    are no windows reported for time `00:10` and `00:20`.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-3](#sliding_window) 展示了一个窗口大小为30秒，报告频率为10秒的滑动窗口。在插图中，我们可以观察到*滑动窗口*的一个重要特征：它们不适用于小于窗口大小的时间段。我们可以看到在时间
    `00:10` 和 `00:20` 没有报告的窗口。'
- en: '![spas 0203](Images/spas_0203.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0203](Images/spas_0203.png)'
- en: Figure 2-3\. Sliding windows
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. 滑动窗口
- en: 'Although you cannot see it in the final illustration, the process of drawing
    the chart reveals an interesting feature: we can construct and maintain a sliding
    window by adding the most recent data and removing the expired elements, while
    keeping all other elements in place.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您在最终插图中看不到它，但绘制图表的过程揭示了一个有趣的特性：我们可以通过添加最新数据和删除过期元素来构建和维护一个滑动窗口，同时保持所有其他元素不变。
- en: It’s worth noting that tumbling windows are a particular case of sliding windows
    in which the frequency of reporting is equal to the window size.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，滚动窗口是滑动窗口的一个特例，在这种情况下，报告频率等于窗口大小。
- en: Stateless and Stateful Processing
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无状态和有状态处理
- en: Now that we have a better notion of the programming model of the streaming systems
    in Apache Spark, we can look at the nature of the computations that we want to
    apply on data streams. In our context, data streams are fundamentally long collections
    of elements, observed over time. In fact, Structured Streaming pushes this logic
    by considering a data stream as a virtual table of records in which each row corresponds
    to an element.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对Apache Spark中流处理系统的编程模型有了更好的了解，我们可以看看我们想要在数据流上应用的计算的性质。在我们的上下文中，数据流基本上是随时间观察到的长集合元素。实际上，结构化流推动了这一逻辑，认为数据流是记录的虚拟表，其中每一行对应一个元素。
- en: Stateful Streams
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有状态流
- en: 'Whether streams are viewed as a continuously extended collection or as a table,
    this approach gives us some insight into the kind of computation that we might
    find interesting. In some cases, the emphasis is put on the continuous and independent
    processing of elements or groups of elements: those are the cases for which we
    want to operate on some elements based on a well-known heuristic, such as alert
    messages coming from a log of events.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 无论流是被视为连续扩展的集合还是表，这种方法为我们提供了对我们可能发现有趣的计算类型的一些见解。在某些情况下，重点放在对元素或元素组的连续和独立处理上：这些是我们希望基于众所周知的启发式处理某些元素的情况，例如来自事件日志的警报消息。
- en: This focus is perfectly valid but hardly requires an advanced analytics system
    such as Apache Spark. More often, we are interested in a real-time reaction to
    new elements based on an analysis that depends on the whole stream, such as detecting
    outliers in a collection or computing recent aggregate statistics from event data.
    For example, it might be interesting to find higher than usual vibration patterns
    in a stream of airplane engine readings, which requires understanding the regular
    vibration measurements for the kind of engine we are interested in.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这种关注是完全合理的，但几乎不需要像Apache Spark这样的高级分析系统。更常见的是，我们对基于整个流的分析的新元素的实时反应感兴趣，例如在集合中检测异常值或从事件数据计算最近的聚合统计信息。例如，可能有趣的是在飞机发动机读数流中找到异常振动模式，这需要理解我们感兴趣的引擎类型的常规振动测量。
- en: This approach, in which we are simultaneously trying to understand new data
    in the context of data already seen, often leads us to *stateful stream processing*.
    Stateful stream processing is the discipline by which we compute something out
    of the new elements of data observed in our input data stream and refresh internal
    data that helps us perform this computation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法同时试图理解新数据和已经看到的数据的上下文，通常会导致*有状态流处理*。有状态流处理是一种学科，我们在输入数据流中观察到的新元素上计算出一些内容，并刷新帮助我们执行此计算的内部数据。
- en: For example, if we are trying to do anomaly detection, the internal state that
    we want to update with every new stream element would be a machine learning model,
    whereas the computation we want to perform is to say whether an input element
    should be classified as an anomaly or not.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们正在尝试进行异常检测，我们希望用每个新的流元素更新的内部状态将是一个机器学习模型，而我们要执行的计算是判断输入元素是否应该分类为异常。
- en: This pattern of computation is supported by a distributed streaming system such
    as Apache Spark because it can take advantage of a large amount of computing power
    and is an exciting new way of reacting to real-time data. For example, we could
    compute the running mean and standard deviation of the elements seen as input
    numbers and output a message if a new element is further away than five standard
    deviations from this mean. This is a simple, but useful, way of marking particular
    extreme outliers of the distribution of our input elements.^([1](ch02.xhtml#idm46385832409864))
    In this case, the internal state of the stream processor only stores the running
    mean and standard deviation of our stream—that is, a couple of numbers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种计算模式受到分布式流处理系统（如 Apache Spark）的支持，因为它可以利用大量的计算能力，并且是对实时数据做出反应的一种新的令人兴奋的方式。例如，我们可以计算输入数字的运行均值和标准差，并且如果一个新元素与这个均值的五个标准差之外，就输出一条消息。这是标记我们输入元素分布中特定极端异常值的一种简单但有用的方式。^([1](ch02.xhtml#idm46385832409864))
    在这种情况下，流处理器的内部状态仅存储我们流的运行均值和标准差——即一对数字。
- en: Bounding the Size of the State
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制状态的大小
- en: One of the common pitfalls of practitioners new to the field of stream processing
    is the temptation to store an amount of internal data that is proportional to
    the size of the input data stream. For example, if you would like to remove duplicate
    records of a stream, a naive way of approaching that problem would be to store
    every message already seen and compare new messages to them. That not only increases
    computing time with each incoming record, but also has an unbounded memory requirement
    that will eventually outgrow any cluster.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新手流处理领域的从业者来说，一个常见的陷阱是试图存储与输入数据流的大小成比例的大量内部数据。例如，如果您想要移除流中的重复记录，一种天真的方法是存储已经看到的每条消息，并将新消息与它们进行比较。这不仅会增加每个传入记录的计算时间，还会有无界的内存需求，最终会超出任何集群的限制。
- en: This is a common mistake because the premise of stream processing is that there
    is no limit to the number of input events and, while your available memory in
    a distributed Spark cluster might be large, it is always limited. As such, intermediary
    state representations can be very useful to express computation that operates
    on elements relative to the global stream of data on which they are observed,
    but it is a somewhat unsafe approach. If you choose to have intermediate data,
    you need to make absolutely sure that the amount of data you might be storing
    at any given time is strictly bounded to a certain upper limit that is less than
    your available memory, independent of the amount of data that you might encounter
    as input.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个常见的错误，因为流处理的前提是输入事件的数量没有限制，虽然在分布式 Spark 集群中可用的内存可能很大，但它始终是有限的。因此，中间状态表示可以非常有用，用来表达相对于全局数据流上观察到的元素进行操作的计算，但这是一种有风险的方法。如果选择有中间数据，您需要确保在任何给定时间可能存储的数据量严格限制在少于可用内存的上限内，而不考虑可能遇到的数据量。
- en: 'An Example: Local Stateful Computation in Scala'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：Scala 中的本地有状态计算
- en: To gain intuition into the concept of statefulness without having to go into
    the complexity of distributed stream processing, we begin with a simple nondistributed
    stream example in Scala.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解状态的概念，而不必深入复杂的分布式流处理，我们从 Scala 中的一个简单的非分布式流示例开始。
- en: 'The Fibonacci Sequence is classically defined as a stateful stream: it’s the
    sequence starting with 0 and 1, and thereafter composed of the sum of its two
    previous elements, as shown in [Example 2-1](#stateful-fib).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 斐波那契序列经典上被定义为有状态流：它是以0和1开始的序列，然后由其前两个元素的和组成，如[示例 2-1](#stateful-fib)所示。
- en: Example 2-1\. A stateful computation of the Fibonacci elements
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-1. 有状态计算的斐波那契元素
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Stateful stream processing refers to any stream processing that looks to past
    information to obtain its result. It’s necessary to maintain some *state* information
    in the process of computing the next element of the stream.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 状态型流处理指的是任何需要查看过去信息以获取结果的流处理。在计算流的下一个元素时，保持一些*状态*信息是必要的。
- en: 'Here, this is held in the recursive argument of the `scanLeft` function, in
    which we can see `fibs` having a tuple of two elements for each element: the sought
    result, and the next value. We can apply a simple transformation to the list of
    tuples `fibs` to retain only the leftmost element and thus obtain the classical
    Fibonacci Sequence.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，这是在`scanLeft`函数的递归参数中实现的，在这里我们可以看到`fibs`每个元素都有两个元素的元组：所寻求的结果和下一个值。我们可以对元组列表`fibs`应用简单的转换，仅保留最左边的元素，从而获得经典的斐波那契数列。
- en: The important point to highlight is that to get the value at the *nth* place,
    we must process all *n–1* elements, keeping the intermediate `(i-1, i)` elements
    as we move along the stream.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 强调的重要点是，为了获取第*n*个位置的值，我们必须处理所有*n–1*个元素，并在沿着流移动时保持中间的`(i-1, i)`元素。
- en: Would it be possible to define it without referring to its prior values, though,
    purely statelessly?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，是否可能在纯粹无状态的情况下定义它，而不是参考其先前的值？
- en: A Stateless Definition of the Fibonacci Sequence as a Stream Transformation
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作为流变换的无状态斐波那契序列的定义
- en: To express this computation as a stream, taking as input the integers and outputting
    the Fibonacci Sequence, we express this as a stream transformation that uses a
    stateless `map` function to transform each number to its Fibonacci value. We can
    see the implementation of this approach in [Example 2-2](#stateless-fib).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要将此计算表达为流，以整数作为输入并输出斐波那契序列，我们将其表达为一种流变换，使用无状态的`map`函数将每个数字转换为其斐波那契值。我们可以在[示例 2-2](#stateless-fib)中看到此方法的实现。
- en: Example 2-2\. A stateless computation of the Fibonacci elements
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-2. 无状态计算的斐波那契元素
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This rather counterintuitive definition uses a stream of integers, starting
    from the single integer (0), to then define the Fibonacci Sequence as a computation
    that takes as input an integer *n* received over the stream and returns the *n-th*
    element of the Fibonacci Sequence as a result. This uses a floating-point number
    formula known as the *Binet formula* to compute the *n-th* element of the sequence
    directly, without requiring the previous elements; that is, without requiring
    knowledge of the state of the stream.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个相当反直觉的定义使用一个整数流，从单个整数(0)开始，然后定义斐波那契序列作为一个计算，它接收作为流的输入的整数*n*，并返回斐波那契序列的第*n*个元素作为结果。这使用了一个称为*Binet公式*的浮点数公式，可以直接计算序列的第*n*个元素，而不需要先前的元素；也就是说，不需要知道流的状态。
- en: Notice how we take a limited number of elements of this sequence and print them
    in Scala, as an explicit operation. This is because the computation of elements
    in our stream is executed lazily, which calls the evaluation of our stream only
    when required, considering the elements needed to produce them from the last materialization
    point to the original source.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何在Scala中获取这个序列的有限数量的元素并打印它们，作为显式操作。这是因为我们的流元素的计算是惰性执行的，它仅在需要时调用我们的流评估，考虑到从最后的实体化点到原始源产生它们所需的元素。
- en: Stateless or Stateful Streaming
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无状态或有状态流处理
- en: 'We illustrated the difference between stateful and stateless stream processing
    with a rather simple case that has a solution using the two approaches. Although
    the stateful version closely resembles the definition, it requires more computing
    resources to produce a result: it needs to traverse a stream and keep intermediate
    values at each step.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用一个相当简单的案例说明了状态型和无状态型流处理之间的区别，这个案例使用了两种方法来解决。虽然有状态的版本与定义非常相似，但它需要更多的计算资源来产生结果：它需要遍历流并在每一步保持中间值。
- en: 'The stateless version, although contrived, uses a simpler approach: we use
    a stateless function to obtain a result. It doesn’t matter whether we need the
    Fibonacci number for 9 or 999999, in both cases the computational cost is roughly
    of the same order.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态版本虽然有点牵强，但采用了更简单的方法：我们使用无状态函数来获得结果。无论我们需要第9个还是第999999个斐波那契数，计算成本大致相同。
- en: 'We can generalize this idea to stream processing. Stateful processing is more
    costly in terms of the resources it uses and also introduces concerns in face
    of failure: what happens if our computation fails halfway through the stream?
    Although a safe rule of thumb is to choose for the stateless option, if available,
    many of the interesting questions we can ask over a stream of data are often stateful
    in nature. For example: how long was the user session on our site? What was the
    path the taxi used across the city? What is the moving average of the pressure
    sensor on an industrial machine?'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个想法推广到流处理。有状态的处理在资源使用上更为昂贵，并且在面对失败时也引入了一些问题：如果我们的计算在流处理过程中半途而废会发生什么？尽管一个安全的经验法则是在可用时选择无状态选项，但许多我们可以在数据流上提出的有趣问题通常是有状态的性质。例如：用户在我们网站上的会话持续多久？出租车在城市中使用的路径是什么？工业机器上的压力传感器的移动平均值是多少？
- en: Throughout the book, we will see that stateful computations are more general
    but they carry their own constraints. It’s an important aspect of the stream-processing
    framework to provide facilities to deal with these constraints and free the user
    to create the solutions that the business needs dictate.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们将看到，有状态的计算更为通用，但它们也带来了自身的约束。流处理框架的重要方面是提供处理这些约束的功能，并使用户能够创建业务需要的解决方案。
- en: The Effect of Time
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间的影响
- en: So far, we have considered how there is an advantage in keeping track of intermediary
    data as we produce results on each element of the data stream because it allows
    us to analyze each of those elements relative to the data stream that they are
    part of as long as we keep this intermediary data of a bounded and reasonable
    size. Now, we want to consider another issue unique to stream processing, which
    is the operation on time-stamped messages.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们考虑了在产生数据流每个元素的结果时跟踪中间数据的优势，因为这使我们能够分析每个元素相对于它们所属的数据流的情况，只要我们保持这些中间数据在有界和合理的大小。现在，我们想考虑另一个流处理独特的问题，即对时间戳消息进行操作。
- en: Computing on Timestamped Events
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于时间戳事件的计算
- en: Elements in a data stream always have a *processing time*. That is, by definition,
    the time at which the stream-processing system observes a new event from a data
    source. That time is entirely determined by the processing runtime and completely
    independent of the content of the stream’s element.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流中的元素始终具有*处理时间*。也就是说，根据定义，流处理系统观察来自数据源的新事件的时间。这个时间完全由处理运行时确定，并且完全独立于数据流元素的内容。
- en: However, for most data streams, we also speak of a notion of *event time*, which
    is the time when the event actually happened. When the capabilities of the system
    sensing the event allow for it, this time is usually added as part of the message
    payload in the stream.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于大多数数据流，我们也谈论到*事件时间*的概念，即事件实际发生的时间。当系统感知事件的能力允许时，这个时间通常作为消息负载的一部分添加到数据流中。
- en: Timestamping is an operation that consists of adding a register of time at the
    moment of the generation of the message, which will become a part of the data
    stream. It is a ubiquitous practice that is present in both the most humble embedded
    devices (provided they have a clock) as well as the most complex logs in financial
    transaction systems.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 时间戳是一个操作，它包括在生成消息的时刻添加时间记录，这将成为数据流的一部分。这是一种无处不在的实践，存在于最简单的嵌入式设备（只要它们有时钟）以及金融交易系统中最复杂的日志中。
- en: Timestamps as the Provider of the Notion of Time
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间戳作为时间概念的提供者
- en: The importance of time stamping is that it allows users to reason on their data
    considering the moment at which it was generated.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 时间戳的重要性在于它允许用户考虑数据生成的时刻来推理其数据。
- en: For example, if I register my morning jog using a wearable device and I synchronize
    the device to my phone when I get back home, I would like to see the details of
    my heart rate and speed as I ran through the forest moments ago, and not see the
    data as a timeless sequence of values as they are being uploaded to some cloud
    server. As we can see, timestamps provide the context of time to data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我使用可穿戴设备记录了早晨的慢跑，并在回到家后将设备与手机同步，我希望看到我刚刚穿过森林时的心率和速度的详细信息，而不是把数据看作上传到某个云服务器的无时间序列值。正如我们所见，时间戳为数据提供了时间的上下文。
- en: So, because event logs form a large proportion of the data streams being analyzed
    today, those timestamps help make sense of what happened to a particular system
    at a given time. This complete picture is something that is often made more elusive
    by the fact that transporting the data from the various systems or devices that
    have created it to the cluster that processes it is an operation prone to different
    forms of failure in which some events could be delayed, reordered, or lost.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由于事件日志在今天被分析的数据流中占据很大一部分，这些时间戳有助于理解特定系统在特定时间发生了什么。由于从创建数据的各种系统或设备传输数据到处理它的群集是一个易于发生不同形式故障的操作，某些事件可能会延迟、重新排序或丢失，所以这个完整的图像通常因此变得更难以捉摸。
- en: 'Often, users of a framework such as Apache Spark want to compensate for those
    hazards without having to compromise on the reactivity of their system. Out of
    this desire was born a discipline for producing the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 常常，像Apache Spark这样的框架的用户希望在不妥协其系统的反应能力的情况下弥补这些风险。出于这种愿望，产生了以下的一个学科：
- en: Clearly marked correct and reordered results
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确标记的正确和重新排序的结果
- en: Intermediary prospective results
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中间前景结果
- en: With that classification reflecting the best knowledge that a stream-processing
    system has of the timestamped events delivered by the data stream and under the
    proviso that this view could be completed by the late arrival of delayed stream
    elements. This process constitutes the basis for *event-time processing*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 根据流处理系统对数据流中的时间戳事件的最佳知识分类，可以反映出此分类是基于数据流提供的事件，并且有待于视图能够通过延迟的流元素的迟到来完成此视图。这个过程构成了*事件时间处理*的基础。
- en: In Spark, this feature is offered natively only by Structured Streaming. Even
    though Spark Streaming lacks built-in support for event-time processing, it is
    a question of development effort and some data consolidation processes to manually
    implement the same sort of primitives, as you will see in [Chapter 22](ch22.xhtml#spark-streaming-stateful).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，这个特性仅由结构化流处理原生支持。尽管Spark Streaming缺乏对事件时间处理的内置支持，但通过开发工作和一些数据整合过程，可以手动实现相同类型的基本功能，正如您将在[第22章](ch22.xhtml#spark-streaming-stateful)中看到的。
- en: Event Time Versus Processing Time
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事件时间与处理时间
- en: 'We recognize that there is a timeline in which the events are created and a
    different one when they are processed:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认识到有一个时间轴，在这个轴上创建了事件，并且在另一个时间轴上处理这些事件：
- en: '*Event time* refers to the timeline when the events were originally generated.
    Typically, a clock available at the generating device places a timestamp in the
    event itself, meaning that all events from the same source could be chronologically
    ordered even in the case of transmission delays.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*事件时间*指的是事件最初生成时的时间线。通常，生成设备上的时钟会在事件本身中放置一个时间戳，这意味着即使在传输延迟的情况下，所有来自同一来源的事件也可以按时间顺序排序。'
- en: '*Processing time* is the time when the event is handled by the stream-processing
    system. This time is relevant only at the technical or implementation level. For
    example, it could be used to add a processing timestamp to the results and in
    that way, differentiate duplicates, as being the same output values with different
    processing times.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*处理时间*是事件被流处理系统处理的时间。这个时间仅在技术或实施层面上是相关的。例如，它可以用来为结果添加处理时间戳，从而区分重复值，即使这些输出值具有不同的处理时间。'
- en: Imagine that we have a series of events produced and processed over time, as
    illustrated in [Figure 2-4](#event-processing-time).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们有一系列随时间产生和处理的事件，如图[2-4](#event-processing-time)所示。
- en: '![spas 0204](Images/spas_0204.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0204](Images/spas_0204.png)'
- en: Figure 2-4\. Event versus processing time
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. 事件与处理时间
- en: 'Let’s look at this more closely:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看一下这个问题：
- en: The x-axis represents the event timeline and the dots on that axis denote the
    time at which each event was generated.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x轴代表事件时间线，该轴上的点表示每个事件生成的时间。
- en: The y-axis is the processing time. Each dot on the chart area corresponds to
    when the corresponding event in the x-axis is processed. For example, the event
    created at `00:08` (first on the x-axis) is processed at approximately `00:12`,
    the time that corresponds to its mark on the y-axis.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: y 轴是处理时间。图表区域上的每个点对应 x 轴中相应事件的处理时间。例如，创建于 `00:08` 的事件（x 轴上的第一个）在大约 `00:12` 左右被处理，这时其在
    y 轴上的标记。
- en: The diagonal line represents the perfect processing time. In an ideal world,
    using a network with zero delay, events are processed immediately as they are
    created. Note that there can be no processing events below that line, because
    it would mean that events are processed before they are created.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对角线代表理想的处理时间。在一个理想的世界中，使用零延迟的网络，事件被创建后立即被处理。请注意，在那条线以下不可能有处理事件，因为这意味着事件在创建之前就已经被处理。
- en: 'The vertical distance between the diagonal and the processing time is the *delivery
    delay*: the time elapsed between the production of the event and its eventual
    consumption.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对角线和处理时间之间的垂直距离是 *交付延迟*：事件生产和最终消费之间经过的时间。
- en: With this framework in mind, let’s now consider a 10-second window aggregation,
    as demonstrated in [Figure 2-5](#processing-time-window).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个框架的指导下，现在让我们考虑一个 10 秒窗口聚合，如 [图 2-5](#processing-time-window) 所示。
- en: '![spas 0205](Images/spas_0205.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0205](Images/spas_0205.png)'
- en: Figure 2-5\. Processing-time windows
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 处理时间窗口
- en: 'We start by considering windows defined on processing time:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先考虑在处理时间上定义的窗口：
- en: The stream processor uses its internal clock to measure 10-second intervals.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流处理器使用其内部时钟来测量 10 秒间隔。
- en: All events that fall in that time interval belong to the window.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有落在该时间间隔内的事件都属于窗口。
- en: In [Figure 2-5](#processing-time-window), the horizontal lines define the 10-second
    windows.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [图 2-5](#processing-time-window) 中，水平线定义了这些 10 秒窗口。
- en: We have also highlighted the window corresponding to the time interval `00:30-00:40`.
    It contains two events with event time `00:33` and `00:39`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还突出显示了时间间隔 `00:30-00:40` 对应的窗口。它包含两个事件，事件时间为 `00:33` 和 `00:39`。
- en: 'In this window, we can appreciate two important characteristics:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个窗口中，我们可以欣赏到两个重要特征：
- en: The window boundaries are well defined, as we can see in the highlighted area.
    This means that the window has a defined start and end. We know what’s in and
    what’s out by the time the window closes.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口边界非常清晰，正如我们在高亮区域中看到的。这意味着窗口有明确定义的开始和结束。窗口关闭时，我们知道什么在里面，什么在外面。
- en: Its contents are arbitrary. They are unrelated to when the events were generated.
    For example, although we would assume that a `00:30-00:40` window would contain
    the event `00:36`, we can see that it has fallen out of the resulting set because
    it was late.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的内容是任意的。它们与事件生成的时间无关。例如，尽管我们会认为 `00:30-00:40` 窗口会包含事件 `00:36`，但我们可以看到它已经不在结果集中，因为它迟到了。
- en: Let’s now consider the same 10-second window defined on event time. For this
    case, we use the *event creation time* as the window aggregation criteria. [Figure 2-6](#event-time-window)
    illustrates how these windows look radically different from the processing-time
    windows we saw earlier. In this case, the window `00:30-00:40` contains all the
    events that were *created* in that period of time. We can also see that this window
    has no natural upper boundary that defines when the window ends. The event created
    at `00:36` was late for more than 20 seconds. As a consequence, to report the
    results of the window `00:30-00:40`, we need to wait at least until `01:00`. What
    if an event is dropped by the network and never arrives? How long do we wait?
    To resolve this problem, we introduce an arbitrary deadline called a *watermark*
    to deal with the consequences of this open boundary, like lateness, ordering,
    and deduplication.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑在事件时间上定义的相同 10 秒窗口。在这种情况下，我们使用 *事件创建时间* 作为窗口聚合标准。[图 2-6](#event-time-window)
    展示了这些窗口如何与我们之前看到的处理时间窗口大不相同。在这种情况下，窗口 `00:30-00:40` 包含在那段时间内*创建*的所有事件。我们还可以看到，这个窗口没有自然的上界来定义窗口何时结束。在事件
    `00:36` 创建晚了超过 20 秒。因此，要报告窗口 `00:30-00:40` 的结果，我们至少需要等到 `01:00`。如果一个事件被网络丢弃并永远不会到达，我们要等多久？为了解决这个问题，我们引入一个称为
    *水印* 的任意截止时间，以处理这种开放边界的后果，如延迟、排序和去重。
- en: '![spas 0206](Images/spas_0206.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0206](Images/spas_0206.png)'
- en: Figure 2-6\. Event-time windows
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 事件时间窗口
- en: Computing with a Watermark
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用水印进行计算
- en: As we have noticed, stream processing produces periodic results out of the analysis
    of the events observed in its input. When equipped with the ability to use the
    timestamp contained in the event messages, the stream processor is able to bucket
    those messages into two categories based on a notion of a watermark.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经注意到的，流处理生成周期性结果，通过分析其输入中观察到的事件。当具备使用事件消息中的时间戳的能力时，流处理器能够根据水印的概念将这些消息分成两类。
- en: The watermark is, at any given moment, *the oldest timestamp that we will accept
    on the data stream*. Any events that are older than this expectation are not taken
    into the results of the stream processing. The streaming engine can choose to
    process them in an alternative way, like report them in a *late arrivals* channel,
    for example.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 水印在任何给定时刻是*我们在数据流上接受的最旧时间戳*。任何比这个期望更旧的事件都不会被包括在流处理的结果中。流引擎可以选择以另一种方式处理它们，例如报告它们在*延迟到达*通道中。
- en: However, to account for possibly delayed events, this watermark is usually much
    larger than the average delay we expect in the delivery of the events. Note also
    that this watermark is a fluid value that monotonically increases over time,^([2](ch02.xhtml#idm46385833311032))
    sliding a window of delay-tolerance as the time observed in the data-stream progresses.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了考虑可能的延迟事件，这个水印通常比我们预期事件传递的平均延迟要大得多。还要注意，这个水印是一个随时间单调增加的流动值，^([2](ch02.xhtml#idm46385833311032))
    它随着数据流的观察时间而滑动一个延迟容忍窗口。
- en: When we apply this concept of watermark to our event-time diagram, as illustrated
    in [Figure 2-7](#event-time-watermark), we can appreciate that the watermark closes
    the open boundary left by the definition of event-time window, providing criteria
    to decide what events belong to the window, and what events are too late to be
    considered for processing.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将水印的概念应用于我们的事件时间图表时，如[图2-7](#event-time-watermark)所示，我们可以欣赏到水印关闭了事件时间窗口定义留下的开放边界，提供了决定哪些事件属于窗口，哪些事件太迟以至于不应被考虑处理的标准。
- en: '![spas 0207](Images/spas_0207.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0207](Images/spas_0207.png)'
- en: Figure 2-7\. Watermark in event time
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7\. 事件时间中的水印
- en: 'After this notion of watermark is defined for a stream, the stream processor
    can operate in one of two modes with relation to that specific stream: either
    it is producing output relative to events that are all older than the watermark,
    in which case the output is final because all of those elements have been observed
    so far, and no further event older than that will ever be considered, or it is
    producing an output relative to the data that is before the watermark and a new,
    delayed element newer than the watermark could arrive on the stream at any moment
    and can change the outcome. In this latter case, we can consider the output as
    provisional because newer data can still change the final outcome, whereas in
    the former case, the result is final and no new data will be able to change it.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当流的水印概念被定义后，流处理器可以在以下两种模式之一下运行：要么生成相对于水印之前所有事件的输出，此时输出是最终的，因为到目前为止已经观察到所有这些元素，而且再也不会考虑比这更旧的事件；要么生成相对于水印之前的数据的输出，同时可能随时在流中到达比水印更新的新的延迟元素，并且这些新数据可以改变输出结果。在后一种情况下，我们可以将输出视为临时的，因为新数据仍然可以改变最终结果，而在前一种情况下，结果是最终的，没有新数据能够改变它。
- en: We examine in detail how to concretely express and operate this sort of computation
    in [Chapter 12](ch12.xhtml#ss-event-time).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们详细研究了如何在[第12章](ch12.xhtml#ss-event-time)中具体表达和操作这种计算方式。
- en: 'Note finally that with provisionary results, we are storing intermediate values
    and in one way or another, we require a method to revise their computation upon
    the arrival of delayed events. This process requires some amount of memory space.
    As such, event-time processing is another form of stateful computation and is
    subject to the same limitation: to handle watermarks, the stream processor needs
    to store a lot of intermediate data and, as such, consume a significant amount
    of memory that roughly corresponds to *the length of the watermark × the rate
    of arrival × message size*.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后请注意，对于临时结果，我们正在存储中间值，并且以某种方式要求在延迟事件到达时修改它们的计算。这个过程需要一定的内存空间。因此，事件时间处理是另一种有状态计算的形式，并且受到相同的限制：为了处理水印，流处理器需要存储大量的中间数据，并因此消耗与*水印长度
    × 到达速率 × 消息大小*大致相对应的大量内存。
- en: Also, since we need to wait for the watermark to expire to be sure that we have
    all elements that comprise an interval, stream processes that use a watermark
    and want to have a unique, final result for each interval, must delay their output
    for at least the length of the watermark.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，由于我们需要等待水印过期以确保我们拥有构成间隔的所有元素，因此使用水印并希望每个间隔有唯一最终结果的流处理过程必须将其输出延迟至水印的长度至少。
- en: Caution
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We want to outline event-time processing because it is an exception to the general
    rule we have given in [Chapter 1](ch01.xhtml#intro-stream-processing) of making
    no assumptions on the throughput of events observed in its input stream.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要概述事件时间处理，因为它是我们在[第一章](ch01.xhtml#intro-stream-processing)中提到的不对事件输入流的吞吐量做任何假设的特例。
- en: With event-time processing, we make the assumption that setting our watermark
    to a certain value is appropriate. That is, we can expect the results of a streaming
    computation based on event-time processing to be meaningful only if the watermark
    allows for the delays that messages of our stream will actually encounter between
    their creation time and their order of arrival on the input data stream.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在事件时间处理中，我们假设将水印设置为某个值是合适的。也就是说，我们只有在水印允许消息在创建时间和到达输入数据流时的顺序之间遇到的延迟时，基于事件时间处理的流计算的结果才是有意义的。
- en: A too small watermark will lead to dropping too many events and produce severely
    incomplete results. A too large watermark will delay the output of results deemed
    complete for too long and increase the resource needs of the stream processing
    system to preserve all intermediate events.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 过小的水印会导致丢弃过多事件并且产生严重不完整的结果。过大的水印会延迟被认为是完整结果的输出时间过长，并增加流处理系统为保留所有中间事件而增加的资源需求。
- en: It is left to the users to ensure they choose a watermark suitable for the event-time
    processing they require and appropriate for the computing resources they have
    available, as well.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 用户需要确保选择一个适合他们所需的事件时间处理并且适合他们可用的计算资源的水印，这一点由用户自己负责。
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: 'In this chapter, we explored the main notions unique to the stream-processing
    programming model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了流处理编程模型中独特的主要概念：
- en: Data sources and sinks
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源和接收器
- en: Stateful processing
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有状态处理
- en: Event-time processing
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件时间处理
- en: We explore the implementation of these concepts in the streaming APIs of Apache
    Spark as we progress through the book.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 随着书籍的进展，我们探索了Apache Spark流式API中这些概念的实现。
- en: ^([1](ch02.xhtml#idm46385832409864-marker)) Thanks to the Chebycheff inequality,
    we know that alerts on this data stream should occur with less than 5% probability.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.xhtml#idm46385832409864-marker)) 由于切比雪夫不等式的帮助，我们知道在这个数据流上的警报应该以不到5%的概率发生。
- en: ^([2](ch02.xhtml#idm46385833311032-marker)) Watermarks are nondecreasing by
    nature.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.xhtml#idm46385833311032-marker)) 水印天生是非递减的。
