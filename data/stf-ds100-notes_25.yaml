- en: 24  PCA I
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 24  PCA I
- en: 原文：[https://ds100.org/course-notes/pca_1/pca_1.html](https://ds100.org/course-notes/pca_1/pca_1.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://ds100.org/course-notes/pca_1/pca_1.html](https://ds100.org/course-notes/pca_1/pca_1.html)
- en: '*Learning Outcomes* **   Discuss the dimensionality of a dataset and strategies
    for dimensionality reduction'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习成果* **讨论数据集的维度和降维的策略'
- en: Define and carry out the procedure of PCA
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义和执行PCA的过程
- en: Understand the connection between PCA and SVD*  *## 24.1 Dimensionality
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解PCA和SVD之间的联系*  *## 24.1 维度
- en: Previously, we have been working with data tables with rows and columns. These
    rows and columns correspond to observations and attributes about said observations.
    Now, we have to be a bit more clear with our wording to follow the language of
    linear algebra.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们一直在处理具有行和列的数据表。这些行和列对应于关于观察的属性。现在，我们必须更清楚地表达我们的措辞，以遵循线性代数的语言。
- en: 'Suppose we have a dataset of:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个数据集：
- en: N observations (data points/rows)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N个观察（数据点/行）
- en: d attributes (features/columns)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: d个属性（特征/列）
- en: 'In Linear Algebra, we think of data being a collection of vectors. Vectors
    have a *dimension*, meaning they have some number of unique elements. Row and
    column now denote the direction a vector is written (horizontally, like a row,
    or vertically, like a column):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数中，我们认为数据是向量的集合。向量有一个*维度*，意味着它们有一些唯一的元素。现在，行和列表示向量写的方向（水平，像一行，或垂直，像一列）：
- en: 'Linear Algebra views our data as a matrix:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数将我们的数据视为一个矩阵：
- en: N row vectors in a d-Dimensions, OR
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: d维度中的N行向量，或
- en: d column vectors in an N-Dimensions
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: d列向量在N维度
- en: '**Dimensionality** of data is a complex topic. Sometimes, it is clear from
    looking at the number of rows/columns, but other times it is not.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据的维度**是一个复杂的话题。有时，从行/列的数量就可以看出来，但有时却不行。'
- en: '![](../Images/1104ea48300ea514eb602eb559595164.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1104ea48300ea514eb602eb559595164.png)'
- en: So far, we’ve used visualizations like rugplots or scatterplots to help us identify
    clusters in our dataset. Since we humans are 3D beings, we can’t visualize beyond
    three dimensions, but many datasets come with more than three features. In order
    to visualize multidimensional data, we can reduce the dataset to lower dimensions
    through **dimensionality reduction**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用了像rugplots或scatterplots这样的可视化来帮助我们识别数据集中的聚类。由于我们人类是3D存在，我们无法可视化超过三个维度，但许多数据集具有超过三个特征。为了可视化多维数据，我们可以通过**降维**来将数据集减少到较低的维度。
- en: For example, the dataset below has 4 columns, but the `Weight (lbs)` column
    is actually just a linear transformation of the `Weight (kg)` column. Thus, no
    new information is captured, and the matrix of our dataset has a (column) rank
    of 3! Despite having 4 columns, we still say that this data is 3-dimensional.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面的数据集有4列，但`重量（磅）`列实际上只是`重量（千克）`列的线性变换。因此，没有捕获到新信息，我们的数据集的矩阵具有3的（列）秩！尽管有4列，我们仍然说这个数据是3维的。
- en: '![](../Images/40d0eeda252f0259c38803ed6f74223e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40d0eeda252f0259c38803ed6f74223e.png)'
- en: Plotting the weight columns together reveals the key visual intuition. While
    the two columns visually span a 2D space as a line, the data does not deviate
    at all from that singular line. This means that one of the weight columns is redundant!
    Even given the option to cover the whole 2D space, the data below does not. It
    might as well not have this dimension, which is why we still do not consider the
    data below to span more than 1 dimension.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 将重量列绘制在一起揭示了关键的视觉直觉。虽然两列在视觉上构成了一条线的2D空间，但数据并没有偏离那条单一的线。这意味着其中一个重量列是多余的！即使有了覆盖整个2D空间的选项，下面的数据也没有。它可能也没有这个维度，这就是为什么我们仍然认为下面的数据不跨越1个维度。
- en: '![](../Images/431517b3e607521448dc0bb00e2b0571.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/431517b3e607521448dc0bb00e2b0571.png)'
- en: What happens when there are outliers? Below, we’ve added one outlier point to
    the dataset above, and just that is enough to change the rank of the matrix from
    1 to 2 dimensions. However, the data is still very much 1-dimensional.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当出现异常值时会发生什么？下面，我们在上面的数据集中添加了一个异常值点，这就足以将矩阵的秩从1维改变为2维。然而，数据仍然是非常明显的1维。
- en: '![](../Images/2465c289fedcbbc39615a4cd85a36923.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2465c289fedcbbc39615a4cd85a36923.png)'
- en: As such, dimensionality reduction is generally an *approximation* of the original
    data that’s achieved by projecting the data onto a desired dimension. However,
    there are many ways to project a dataset onto a lower dimension. How do we choose
    the best one?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，降维通常是通过将数据投影到所需维度来实现对原始数据的*近似*。然而，有许多方法可以将数据集投影到较低的维度。我们如何选择最佳的方法？
- en: '![](../Images/01d12f09fdf0eec020f53fad3a20d999.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01d12f09fdf0eec020f53fad3a20d999.png)'
- en: In general, we want the projection that is the best approximation for the original
    data (the graph on the right). In other words, we want the projection that captures
    the most variance of the original data. In the next section, we’ll see how this
    is calculated.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们希望找到最好的近似原始数据的投影（右侧的图）。换句话说，我们希望捕获原始数据的最大方差。在下一节中，我们将看到如何计算这一点。
- en: 24.2 Matrix as Transformation
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.2 矩阵作为变换
- en: 'Consider the matrix multiplication example below:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑下面的矩阵乘法示例：
- en: '![](../Images/510192fd899e856abf6b3990ab2b1877.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/510192fd899e856abf6b3990ab2b1877.png)'
- en: For table 1, each row of the fruits matrix represents one bowl of fruit; for
    example, the first bowl/row has 2 apples, 2 lemons, and 2 melons.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于表1，水果矩阵的每一行代表一碗水果；例如，第一碗/行有2个苹果，2个柠檬和2个甜瓜。
- en: For table 2, each column of the dollars matrix represents the cost of fruit
    at a store; for example, the first store/column charges 2 dollars for an apple,
    1 dollar for a lemon, and 4 dollars for a melon.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于表2，美元矩阵的每一列代表商店水果的成本；例如，第一个商店/列收取2美元一个苹果，1美元一个柠檬，和4美元一个甜瓜。
- en: The output is the cost of each bowl at each store.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出是每个商店每碗水果的成本。
- en: 'In general, there are two ways to interpret matrix multiplication:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，有两种解释矩阵乘法的方式：
- en: Row dot column to get each datapoint. In this view, we perform multiple linear
    operations on the data![](../Images/54563df5fadde9c2a9a026d33d506692.png)
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 行点乘列以获得每个数据点。从这个角度来看，我们对数据执行多个线性操作！[](../Images/54563df5fadde9c2a9a026d33d506692.png)
- en: Linear transformation of columns![](../Images/504114f21100531d7baac9f2b3595f11.png)
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列的线性变换！[](../Images/504114f21100531d7baac9f2b3595f11.png)
- en: 24.3 Principal Component Analysis (PCA)
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.3 主成分分析（PCA）
- en: In PCA, our goal is to transform observations from high-dimensional data down
    to low dimensions (often 2, as most visualizations are 2D) through linear transformations.
    In other words, we want to find a linear transformation that creates a low-dimension
    representation that captures as much of the original data’s *total variance* as
    possible.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在PCA中，我们的目标是通过线性变换将观察从高维数据转换为低维（通常是2维，因为大多数可视化是2D），换句话说，我们希望找到一个线性变换，创建一个低维表示，尽可能多地捕获原始数据的*总方差*。
- en: '![](../Images/70037bd3d629a55d869ec29583b4f9f8.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70037bd3d629a55d869ec29583b4f9f8.png)'
- en: 'We often perform PCA during the Exploratory Data Analysis (EDA) stage of our
    data science lifecycle when we don’t know what model to use. It helps us with:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常在数据科学生命周期的探索性数据分析（EDA）阶段进行PCA，当我们不知道要使用什么模型时。它帮助我们：
- en: Visually identifying clusters of similar observations in high dimensions.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高维度中直观地识别相似观察的聚类。
- en: Removing irrelevant dimensions if we suspect that the dataset is inherently
    low rank. For example, if the columns are collinear, there are many attributes
    but only a few mostly determine the rest through linear associations.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们怀疑数据集本质上是低秩的，可以去除不相关的维度。例如，如果列是共线的，有许多属性，但只有少数属性通过线性关联大部分决定其他属性。
- en: Finding a small basis for representing variations in complex things, e.g., images,
    genes.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到表示复杂事物变化的小基础，例如图像、基因。
- en: Reducing the number of dimensions to make some computations cheaper.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少维度以使一些计算更便宜。
- en: '![](../Images/5fea13f41c2db0f62adfdfe3c744fd81.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fea13f41c2db0f62adfdfe3c744fd81.png)'
- en: 24.3.1 PCA Procedure (Overview)
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 24.3.1 PCA过程（概述）
- en: 'To perform PCA on a matrix:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对矩阵执行PCA：
- en: Center the data matrix by subtracting the mean of each attribute column.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过减去每个属性列的平均值来使数据矩阵居中。
- en: 'To find the \(i\)-th principal component \(v_i\):'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要找到第\(i\)个主成分\(v_i\)：
- en: \(v\) is a unit vector that linearly combines the attributes.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(v\)是一个单位向量，线性组合属性。
- en: \(v\) gives a one-dimensional projection of the data.
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(v\)给出了数据的一维投影。
- en: \(v\) is chosen to minimize the sum of squared distances between each point
    and its projection onto \(v\).
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择\(v\)以最小化每个点与其在\(v\)上的投影之间的平方距离。
- en: Choose \(v\) such that it is orthogonal to all previous principal components.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择\(v\)，使其与所有先前的主成分正交。
- en: The \(k\) principal components capture the most variance of any \(k\)-dimensional
    reduction of the data matrix.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第\(k\)个主成分捕获了数据矩阵的任何\(k\)维降维的最大方差。
- en: In practice, however, we don’t carry out the procedures in step 2 because they
    take too long to compute. Instead, we use singular value decomposition (SVD) to
    efficiently find all principal components.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，我们不执行步骤2中的程序，因为计算时间太长。相反，我们使用奇异值分解（SVD）来高效地找到所有主成分。
- en: 24.4 Singular Value Decomposition (SVD)
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.4 奇异值分解（SVD）
- en: 'Singular value decomposition (SVD) is an important concept in linear algebra.
    Since this class requires a linear algebra course (MATH 54, 56 or EECS 16A) as
    a pre/co-requisite, we assume you have taken or are taking a linear algebra course,
    so we won’t explain SVD in its entirety. In particular, we will go over:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异值分解（SVD）是线性代数中的一个重要概念。由于这门课程要求先修/同步学习线性代数课程（MATH 54, 56或EECS 16A），我们假设您已经学过或正在学习线性代数课程，因此我们不会完整地解释SVD。特别是，我们将讨论：
- en: Why SVD is a valid decomposition of rectangular matrices
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么SVD是矩形矩阵的有效分解
- en: Why PCA is an application of SVD.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么PCA是SVD的一个应用。
- en: We will not go much into the theory and details of SVD. Instead, we will only
    cover what is needed for a data science interpretation. If you’d like more information,
    check out [EECS 16B Note 14](https://eecs16b.org/notes/sp23/note14.pdf) or [EECS
    16B Note 15](https://eecs16b.org/notes/sp23/note15.pdf).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入研究SVD的理论和细节。相反，我们只会涵盖数据科学解释所需的内容。如果您想了解更多信息，请查看[EECS 16B Note 14](https://eecs16b.org/notes/sp23/note14.pdf)或[EECS
    16B Note 15](https://eecs16b.org/notes/sp23/note15.pdf)。
- en: '*[Linear Algebra] Orthonormality* *Orthonormal is a combination of two words:
    orthogonal and normal.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*[线性代数] 正交性* *正交是两个词的组合：正交和正规。'
- en: When we say the columns of a matrix are orthonormal, we say that 1\. The columns
    are all orthogonal to each other (all pairs of columns have a dot product of zero)
    2\. All columns are unit vectors (the length of each column vector is 1)![](../Images/fa359bda358778c4127394ff8bd822c0.png)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说一个矩阵的列是正交的时，我们说1\. 列之间都是正交的（所有列对的点积为零）2\. 所有列都是单位向量（每个列向量的长度为1）！[](../Images/fa359bda358778c4127394ff8bd822c0.png)
- en: Orthonormal matrices have a few important properties
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正交矩阵有一些重要的性质
- en: '**Orthonormal inverse**: If an \(m \times n\) matrix \(Q\) has orthonormal
    columns, \(QQ^T= Iₘ\) and \(Q^TQ=Iₙ\).'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正交逆**：如果一个\(m \times n\)矩阵\(Q\)具有正交列，\(QQ^T= Iₘ\)和\(Q^TQ=Iₙ\)。'
- en: '**Rotation of coordinates**: the linear transformation represented by an orthonormal
    matrix is often a rotation (and less often a reflection). We can imagine columns
    of the matrix as where the unit vectors of the original space will land.* **[Linear
    Algebra] Diagnomal Matrices* ***Diagonal matrices** are square matrices with non-zero
    values on the diagonal axis and zero everywhere else.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**坐标旋转**：由正交矩阵表示的线性变换通常是旋转（较少是反射）。我们可以想象矩阵的列是原始空间的单位向量将会落在哪里。**[线性代数] 对角矩阵**
    ***对角矩阵**是具有对角轴上非零值的方阵，其他地方都是零。'
- en: Right-multiplied diagonal matrices scale each column up or down by a constant
    factor. Geometrically, this transformation can be viewed as scaling the coordinate
    system.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 右乘对角矩阵通过一个常数因子使每列缩放。从几何上看，这种变换可以被视为缩放坐标系。
- en: '![](../Images/2494e8ead57d638a08d22405a735613d.png)* *Singular value decomposition
    (SVD) describes a matrix \(X\)’s decomposition into three matrices: \[ X = U \Sigma
    V^T \]'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/2494e8ead57d638a08d22405a735613d.png)* *奇异值分解（SVD）描述了矩阵\(X\)的分解成三个矩阵：\[
    X = U \Sigma V^T \]'
- en: Let’s break down each of these terms one by one.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个分解这些术语。
- en: 24.4.1 \(U\)
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 24.4.1 \(U\)
- en: '\(U\) is an \(n \times d\) matrix: \(U \in \mathbb{R}^{n \times d}\).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(U\)是一个\(n \times d\)的矩阵：\(U \in \mathbb{R}^{n \times d}\)。
- en: Its columns are **orthonormal**
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的列是**正交的**
- en: \(\bar{u_i}^T\bar{u_j} = 0\) for all pairs \(i, j\)
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有的\(i, j\)对，\(\bar{u_i}^T\bar{u_j} = 0\)
- en: all vectors \(\bar{u_i}\) are unit vectors with length = 1.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有向量\(\bar{u_i}\)都是长度为1的单位向量。
- en: Columns of U are called the **left singular vectors**.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: U的列被称为**左奇异向量**。
- en: \(UU^T = I_n\) and \(U^TU = I_d\).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(UU^T = I_n\)和\(U^TU = I_d\)。
- en: We can think of \(U\) as a rotation.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以把\(U\)看作是一个旋转。
- en: '![](../Images/c9f40b2ab5987294523e752ca431f965.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9f40b2ab5987294523e752ca431f965.png)'
- en: 24.4.2 \(\Sigma\)
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 24.4.2 \(\Sigma\)
- en: '\(\Sigma\) is a \(d \times d\) matrix: \(\Sigma \in \mathbb{R}^{d \times d}\).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\Sigma\)是一个\(d \times d\)的矩阵：\(\Sigma \in \mathbb{R}^{d \times d}\)。
- en: The majority of the matrix is zero
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵的大部分是零
- en: It has \(r\) **non-zero** **singular values**, and \(r\) is the rank of \(X\)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有\(r\)个**非零奇异值**，\(r\)是\(X\)的秩
- en: Diagonal values (**singular values** \(\sigma_1, \sigma_2, ... \sigma_r\)),
    are ordered from greatest to least \(\sigma_1 > \sigma_2 > ... > \sigma_r\)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对角值（**奇异值**\(\sigma_1, \sigma_2, ... \sigma_r\)）按从大到小的顺序排列\(\sigma_1 > \sigma_2
    > ... > \sigma_r\)
- en: We can think of \(\Sigma\) as scaling.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以把\(\Sigma\)看作是缩放。
- en: '![](../Images/e5cfdbc67e6906eaec10a43468349227.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5cfdbc67e6906eaec10a43468349227.png)'
- en: 24.4.3 \(V^T\)
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 24.4.3 \(V^T\)
- en: '\(V^T\) is an \(d \times d\) matrix: \(V \in \mathbb{R}^{d \times d}\).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(V^T\)是一个\(d \times d\)的矩阵：\(V \in \mathbb{R}^{d \times d}\)。
- en: Columns of \(V\) are orthonormal, so the rows of \(V^T\) are orthonormal
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(V\)的列是正交的，所以\(V^T\)的行也是正交的
- en: Columns of \(V\) are called the **right singular vectors**.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(V\)的列被称为**右奇异向量**。
- en: \(VV^T = V^TV = I_d\)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(VV^T = V^TV = I_d\)
- en: We can think of \(V\) as a rotation.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以把\(V\)看作是一个旋转。
- en: '![](../Images/b15157cacf776f776000b1e42387b196.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b15157cacf776f776000b1e42387b196.png)'
- en: '24.4.4 SVD: Geometric Perspective'
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 24.4.4 SVD：几何视角
- en: '![](../Images/b6c8639af885cff619169b566351db06.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6c8639af885cff619169b566351db06.png)'
- en: We’ve seen that \(U\) and \(V\) represent rotations, and \(\Sigma\) represents
    scaling. Therefore, SVD says that any matrix can be decomposed into a rotation,
    then a scaling, and another rotation.![](../Images/40d113145039c6c266c022a4899cac0b.png)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到\(U\)和\(V\)代表旋转，\(\Sigma\)代表缩放。因此，SVD表示任何矩阵都可以分解为一个旋转，然后一个缩放，再一个旋转。
- en: 24.4.5 SVD in `NumPy`
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 24.4.5 `NumPy`中的SVD
- en: For this demo, we’ll continue working with our rectangular dataset from before
    with \(n=100\) rows and \(d=4\) columns.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个演示，我们将继续使用之前的矩形数据集，其中\(n=100\)行，\(d=4\)列。
- en: <details><summary>Code</summary>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>代码</summary>
- en: '[PRE0]</details>'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE0]</details>'
- en: '|  | width | height | area | perimeter |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 宽度 | 高度 | 面积 | 周长 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 8 | 6 | 48 | 28 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 8 | 6 | 48 | 28 |'
- en: '| 1 | 2 | 4 | 8 | 12 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 4 | 8 | 12 |'
- en: '| 2 | 1 | 3 | 3 | 8 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 3 | 3 | 8 |'
- en: '| 3 | 9 | 3 | 27 | 24 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 9 | 3 | 27 | 24 |'
- en: '| 4 | 9 | 8 | 72 | 34 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 9 | 8 | 72 | 34 |'
- en: In `NumPy`, the SVD algorithm is already written and can be called with `np.linalg.svd`
    ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)).
    There are multiple versions of SVD; to get the version that we will follow, we
    need to set the `full_matrices` parameter to `False`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在`NumPy`中，SVD算法已经编写好，可以用`np.linalg.svd`来调用（[文档](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)）。有多个版本的SVD；要获得我们将遵循的版本，需要将`full_matrices`参数设置为`False`。
- en: '[PRE1]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: First, let’s examine `U`. As we can see, it’s dimensions are \(n \times d\).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们检查`U`。正如我们所看到的，它的维度是\(n \times d\)。
- en: '[PRE2]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The first 5 rows of `U` are shown below.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示了`U`的前5行。
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '|  | 0 | 1 | 2 | 3 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | -0.155151 | 0.064830 | -0.029935 | 0.638430 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 0 | -0.155151 | 0.064830 | -0.029935 | 0.638430 |'
- en: '| 1 | -0.038370 | -0.089155 | 0.062019 | -0.351010 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 1 | -0.038370 | -0.089155 | 0.062019 | -0.351010 |'
- en: '| 2 | -0.020357 | -0.081138 | 0.058997 | 0.018831 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 2 | -0.020357 | -0.081138 | 0.058997 | 0.018831 |'
- en: '| 3 | -0.101519 | -0.076203 | -0.148160 | -0.199067 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 3 | -0.101519 | -0.076203 | -0.148160 | -0.199067 |'
- en: '| 4 | -0.218973 | 0.206423 | 0.007274 | -0.079833 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 4 | -0.218973 | 0.206423 | 0.007274 | -0.079833 |'
- en: \(\Sigma\) is a little different in `NumPy`. Since the only useful values in
    the diagonal matrix \(\Sigma\) are the singular values on the diagonal axis, only
    those values are returned and they are stored in an array.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在`NumPy`中，\(\Sigma\)有点不同。因为对角矩阵\(\Sigma\)中唯一有用的值是对角轴上的奇异值，所以只返回这些值，并将它们存储在一个数组中。
- en: Our `rectangle_data` has a rank of \(3\), so we should have 3 non-zero singular
    values, **sorted from largest to smallest**.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`rectangle_data`的秩是\(3\)，所以我们应该有3个非零奇异值，**按从大到小的顺序排列**。
- en: '[PRE5]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It seems like we have 4 non-zero values instead of 3, but notice that the last
    value is so small (\(10^{-15}\)) that it’s practically \(0\). Hence, we can round
    the values to get 3 singular values.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们有4个非零值而不是3个，但请注意最后一个值非常小（\(10^{-15}\)），实际上几乎等于\(0\)。因此，我们可以四舍五入这些值，得到3个奇异值。
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To get `S` in matrix format, we use `np.diag`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要以矩阵格式获得`S`，我们使用`np.diag`。
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Finally, we can see that `Vt` is indeed a \(d \times d\) matrix.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以看到`Vt`确实是一个\(d \times d\)的矩阵。
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|  | 0 | 1 | 2 | 3 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | -0.146436 | -0.129942 | -8.100201e-01 | -0.552756 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 0 | -0.146436 | -0.129942 | -8.100201e-01 | -0.552756 |'
- en: '| 1 | -0.192736 | -0.189128 | 5.863482e-01 | -0.763727 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 1 | -0.192736 | -0.189128 | 5.863482e-01 | -0.763727 |'
- en: '| 2 | -0.704957 | 0.709155 | 7.951614e-03 | 0.008396 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 2 | -0.704957 | 0.709155 | 7.951614e-03 | 0.008396 |'
- en: '| 3 | -0.666667 | -0.666667 | -5.257886e-17 | 0.333333 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 3 | -0.666667 | -0.666667 | -5.257886e-17 | 0.333333 |'
- en: To check that this SVD is a valid decomposition, we can reverse it and see if
    it matches our original table (it does, yay!).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查这个SVD是否是一个有效的分解，我们可以反向进行，并看它是否与我们原来的表匹配（它确实匹配，耶！）。
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|  | 0 | 1 | 2 | 3 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 8.0 | 6.0 | 48.0 | 28.0 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 8.0 | 6.0 | 48.0 | 28.0 |'
- en: '| 1 | 2.0 | 4.0 | 8.0 | 12.0 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2.0 | 4.0 | 8.0 | 12.0 |'
- en: '| 2 | 1.0 | 3.0 | 3.0 | 8.0 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.0 | 3.0 | 3.0 | 8.0 |'
- en: '| 3 | 9.0 | 3.0 | 27.0 | 24.0 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 9.0 | 3.0 | 27.0 | 24.0 |'
- en: '| 4 | 9.0 | 8.0 | 72.0 | 34.0 |**  **## 24.5 PCA with SVD'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '| 4 | 9.0 | 8.0 | 72.0 | 34.0 |**  **## 24.5 PCA with SVD'
- en: 'Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) can
    be easily mixed up, especially when you have to keep track of so many acronyms.
    Here is a quick summary:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）和奇异值分解（SVD）很容易混淆，尤其是当你不得不记住这么多首字母缩略词时。这里是一个快速总结：
- en: 'SVD: a linear algebra algorithm that splits a matrix into 3 component parts.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'SVD: 一个线性代数算法，将矩阵分成3个组成部分。'
- en: 'PCA: a data science procedure used for dimensionality reduction that *uses*
    SVD as one of the steps.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PCA: 一种用于降维的数据科学程序，*使用* SVD 作为其中一步。'
- en: 'In order to get the first \(k\) principal components from an \(n \times d\)
    matrix \(X\), we:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从一个\(n \times d\)矩阵\(X\)中获得前\(k\)个主成分，我们需要：
- en: Center \(X\) by subtracting the mean from each column. Notice how we specify
    `axis=0` so that the mean is computed per column.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从每一列中减去均值来对\(X\)进行中心化。请注意，我们指定`axis=0`，以便按列计算均值。
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '|  | width | height | area | perimeter |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | 宽度 | 高度 | 面积 | 周长 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 2.97 | 1.35 | 24.78 | 8.64 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2.97 | 1.35 | 24.78 | 8.64 |'
- en: '| 1 | -3.03 | -0.65 | -15.22 | -7.36 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 1 | -3.03 | -0.65 | -15.22 | -7.36 |'
- en: '| 2 | -4.03 | -1.65 | -20.22 | -11.36 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 2 | -4.03 | -1.65 | -20.22 | -11.36 |'
- en: '| 3 | 3.97 | -1.65 | 3.78 | 4.64 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 3.97 | -1.65 | 3.78 | 4.64 |'
- en: '| 4 | 3.97 | 3.35 | 48.78 | 14.64 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3.97 | 3.35 | 48.78 | 14.64 |'
- en: 'Get the Singular Value Decomposition of centered \(X\): \(U\), \(Σ\) and \(V^T\)'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获得中心化\(X\)的奇异值分解：\(U\)，\(Σ\) 和 \(V^T\)
- en: '[PRE16]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Multiply either \(UΣ\) or \(XV\). Mathematically, these give the same result,
    but computationally, floating point approximation results in slightly different
    numbers for very small values (check out the right-most column in the cells below).
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将\(UΣ\)或\(XV\)相乘。从数学上讲，这些给出了相同的结果，但在计算上，浮点近似导致非常小的值的略有不同的数字（查看下面单元格中最右边的列）。
- en: '[PRE17]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|  | 0 | 1 | 2 | 3 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | -26.432217 | 0.162686 | 0.807998 | -1.447811e-15 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 0 | -26.432217 | 0.162686 | 0.807998 | -1.447811e-15 |'
- en: '| 1 | 17.045285 | -2.181451 | 0.347732 | 3.893239e-15 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 17.045285 | -2.181451 | 0.347732 | 3.893239e-15 |'
- en: '| 2 | 23.245695 | -3.538040 | 1.995334 | -4.901321e-16 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 23.245695 | -3.538040 | 1.995334 | -4.901321e-16 |'
- en: '| 3 | -5.383546 | 5.025395 | 0.253448 | -3.544636e-16 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 3 | -5.383546 | 5.025395 | 0.253448 | -3.544636e-16 |'
- en: '| 4 | -51.085217 | -2.586948 | 2.099919 | -4.133102e-16 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 4 | -51.085217 | -2.586948 | 2.099919 | -4.133102e-16 |'
- en: '[PRE18]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '|  | 0 | 1 | 2 | 3 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | -26.432217 | 0.162686 | 0.807998 | -1.539509e-15 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 0 | -26.432217 | 0.162686 | 0.807998 | -1.539509e-15 |'
- en: '| 1 | 17.045285 | -2.181451 | 0.347732 | -2.072416e-16 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 17.045285 | -2.181451 | 0.347732 | -2.072416e-16 |'
- en: '| 2 | 23.245695 | -3.538040 | 1.995334 | 4.588922e-16 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 23.245695 | -3.538040 | 1.995334 | 4.588922e-16 |'
- en: '| 3 | -5.383546 | 5.025395 | 0.253448 | -4.292862e-16 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 3 | -5.383546 | 5.025395 | 0.253448 | -4.292862e-16 |'
- en: '| 4 | -51.085217 | -2.586948 | 2.099919 | -1.650532e-15 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 4 | -51.085217 | -2.586948 | 2.099919 | -1.650532e-15 |'
- en: Take the first \(k\) columns of \(UΣ\) (or \(XV\)). These are the first \(k\)
    principal components of \(X\).
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取\(UΣ\)（或\(XV\)）的前\(k\)列。这些是\(X\)的前\(k\)个主成分。
- en: '[PRE19]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|  | 0 | 1 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 |'
- en: '| --- | --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | -26.432217 | 0.162686 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 0 | -26.432217 | 0.162686 |'
- en: '| 1 | 17.045285 | -2.181451 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 17.045285 | -2.181451 |'
- en: '| 2 | 23.245695 | -3.538040 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 23.245695 | -3.538040 |'
- en: '| 3 | -5.383546 | 5.025395 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 3 | -5.383546 | 5.025395 |'
- en: '| 4 | -51.085217 | -2.586948 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 4 | -51.085217 | -2.586948 |'
- en: 24.6 (Bonus) PCA vs. Regression
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.6 (奖金) PCA vs. 回归
- en: '24.6.1 Regression: Minimizing Horizontal/Verticle Error'
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 24.6.1 回归：最小化水平/垂直误差
- en: Suppose we know the child mortality rate of a given country. Linear regression
    tries to predict the fertility rate from the mortality rate; for example, if the
    mortality is 6, we might guess the fertility is near 4\. The regression line tells
    us the “best” prediction of fertility given all possible mortality values by minimizing
    the root mean squared error [see vertical red lines, only some shown].
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们知道某个国家的儿童死亡率。线性回归试图从死亡率预测生育率；例如，如果死亡率为6，我们可能猜测生育率接近4。回归线告诉我们，通过最小化均方根误差[见垂直红线，仅显示部分]，给出了所有可能死亡率值的生育率的“最佳”预测。
- en: '![](../Images/b3e38d57c0b7d5967f46d9c330c06b3e.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3e38d57c0b7d5967f46d9c330c06b3e.png)'
- en: We can also perform a regression in the reverse direction, that is, given the
    fertility, we try to predict the mortality. In this case, we get a different regression
    line which minimizes the root mean squared length of the horizontal lines.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在相反的方向进行回归，也就是说，给定生育率，我们试图预测死亡率。在这种情况下，我们得到了一条不同的回归线，它最小化了水平线的均方根长度。
- en: '![](../Images/d8ec76bb8f74bb2affbd7a4b967b9778.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8ec76bb8f74bb2affbd7a4b967b9778.png)'
- en: '24.6.2 SVD: Minimizing Perpendicular Error'
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 24.6.2 SVD：最小化垂直误差
- en: The rank 1 approximation is close but not the same as the mortality regression
    line. Instead of minimizing *horizontal* or *vertical* error, our rank 1 approximation
    minimizes the error *perpendicular* to the subspace onto which we’re projecting.
    That is, SVD finds the line such that if we project our data onto that line, the
    error between the projection and our original data is minimized. The similarity
    of the rank 1 approximation and the fertility was just a coincidence. Looking
    at adiposity and bicep size from our body measurements dataset, we see the 1D
    subspace onto which we are projecting is between the two regression lines.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 秩1近似接近但不同于死亡率回归线。我们的秩1近似不是最小化*水平*或*垂直*误差，而是最小化*垂直*到我们投影的子空间上的误差。也就是说，SVD找到了这样一条线，如果我们将我们的数据投影到该线上，投影与我们的原始数据之间的误差将被最小化。秩1近似与生育率的相似性只是一个巧合。从我们的身体测量数据集中查看脂肪和二头肌大小，我们看到我们正在投影的一维子空间位于两条回归线之间。
- en: '![](../Images/adc99a9796e11bd453a37747604563e5.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adc99a9796e11bd453a37747604563e5.png)'
- en: 24.6.3 Beyond 1D and 2D
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 24.6.3 超过1D和2D
- en: In higher dimensions, the idea behind principal components is just the same!
    Suppose we have 30-dimensional data and decide to use the first 5 principal components.
    Our procedure minimizes the error between the original 30-dimensional data and
    the projection of that 30-dimensional data on to the “best” 5-dimensional subspace.
    See [CS189](https://www.eecs189.org/static/notes/n10.pdf) for more details.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在更高的维度中，主成分背后的思想是一样的！假设我们有30维数据，并决定使用前5个主成分。我们的过程最小化了原始30维数据与该30维数据投影到“最佳”5维子空间之间的误差。有关更多详细信息，请参见[CS189](https://www.eecs189.org/static/notes/n10.pdf)。
- en: 24.7 (Bonus) Automatic Factorization
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.7（奖金）自动因子分解
- en: One key fact to remember is that the decomposition are not arbitrary. The *rank*
    of a matrix limits how small our inner dimensions can be if we want to perfectly
    recreate our matrix. The proof for this is out of scope.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 要记住的一个关键事实是，分解并不是任意的。矩阵的*秩*限制了如果我们想要完美重现我们的矩阵，我们的内部维度可以有多小。这个证明超出了范围。
- en: Even if we know we have to factorize our matrix using an inner dimension of
    R, that still leaves a large space of solutions to traverse. What if we have a
    procedure to automatically factorize a rank R matrix into an R dimensional representation
    with some transformation matrix?
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们知道我们必须使用R的内部维度来分解我们的矩阵，仍然留下了一个要遍历的大空间解决方案。如果我们有一个自动将秩为R的矩阵因子分解为具有一些变换矩阵的R维表示的过程呢？
- en: Lower dimensional representation avoids redundant features.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低维表示避免了冗余特征。
- en: 'Imagine a 1000-dimensional dataset: If the rank is only 5, it’s much easier
    to do EDA after this mystery procedure.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象一个1000维的数据集：如果秩只有5，那么在这个神秘过程之后进行EDA会容易得多。
- en: What if we wanted a 2D representation? It’s valuable to compress all of the
    data that is relevant onto as few dimensions as possible in order to plot it efficiently.
    Some 2D matrices yield better approximations than others. How well can we do?
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要一个2D表示呢？将所有相关数据压缩到尽可能少的维度上以便有效地绘制它是有价值的。一些2D矩阵比其他矩阵产生更好的近似。我们能做得有多好呢？
- en: 24.8 (Bonus) Proof of Component Score
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.8（奖金）组分得分的证明
- en: The proof defining component score is out of scope for this class, but it is
    included below for your convenience.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 定义组分得分的证明超出了本课程的范围，但为了您的方便，下面包括了它。
- en: '*Setup*: Consider the design matrix \(X \in \mathbb{R}^{n \times d}\), where
    the \(j\)-th column (corresponding to the \(j\)-th feature) is \(x_j \in \mathbb{R}^n\)
    and the element in row \(i\), column \(j\) is \(x_{ij}\). Further, define \(\tilde{X}\)
    as the **centered** design matrix. The \(j\)-th column is \(\tilde{x}_j \in \mathbb{R}^n\)
    and the element in row \(i\), column \(j\) is \(\tilde{x}_{ij} = x_{ij} - \bar{x_j}\),
    where \(\bar{x_j}\) is the mean of the \(x_j\) column vector from the original
    \(X\).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*设置*：考虑设计矩阵\(X \in \mathbb{R}^{n \times d}\)，其中第\(j\)列（对应于第\(j\)个特征）是\(x_j
    \in \mathbb{R}^n\)，第\(i\)行，第\(j\)列的元素是\(x_{ij}\)。此外，将\(\tilde{X}\)定义为**居中**设计矩阵。第\(j\)列是\(\tilde{x}_j
    \in \mathbb{R}^n\)，第\(i\)行，第\(j\)列的元素是\(\tilde{x}_{ij} = x_{ij} - \bar{x_j}\)，其中\(\bar{x_j}\)是原始\(X\)的\(x_j\)列向量的均值。'
- en: '*Variance*: Construct the **covariance matrix**: \(\frac{1}{n} \tilde{X}^T
    \tilde{X} \in \mathbb{R}^{d \times d}\). The \(j\)-th element along the diagonal
    is the **variance** of the \(j\)-th column of the original design matrix \(X\):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '*方差*：构造**协方差矩阵**：\(\frac{1}{n} \tilde{X}^T \tilde{X} \in \mathbb{R}^{d \times
    d}\)。对角线上的第\(j\)个元素是原始设计矩阵\(X\)的第\(j\)列的**方差**：'
- en: \[\left( \frac{1}{n} \tilde{X}^T \tilde{X} \right)_{jj} = \frac{1}{n} \tilde{x}_j
    ^T \tilde{x}_j = \frac{1}{n} \sum_{i=i}^n (\tilde{x}_{ij} )^2 = \frac{1}{n} \sum_{i=i}^n
    (x_{ij} - \bar{x_j})^2\]
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: \[\left( \frac{1}{n} \tilde{X}^T \tilde{X} \right)_{jj} = \frac{1}{n} \tilde{x}_j
    ^T \tilde{x}_j = \frac{1}{n} \sum_{i=i}^n (\tilde{x}_{ij} )^2 = \frac{1}{n} \sum_{i=i}^n
    (x_{ij} - \bar{x_j})^2\]
- en: '*SVD*: Suppose singular value decomposition of the *centered* design matrix
    \(\tilde{X}\) yields \(\tilde{X} = U \Sigma V^T\), where \(U \in \mathbb{R}^{n
    \times d}\) and \(V \in \mathbb{R}^{d \times d}\) are matrices with orthonormal
    columns, and \(\Sigma \in \mathbb{R}^{d \times d}\) is a diagonal matrix with
    singular values of \(\tilde{X}\).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*SVD*：假设*居中*设计矩阵\(\tilde{X}\)的奇异值分解为\(\tilde{X} = U \Sigma V^T\)，其中\(U \in
    \mathbb{R}^{n \times d}\)和\(V \in \mathbb{R}^{d \times d}\)是具有正交列的矩阵，\(\Sigma
    \in \mathbb{R}^{d \times d}\)是具有\(\tilde{X}\)的奇异值的对角线矩阵。'
- en: \[\begin{aligned} \tilde{X}^T \tilde{X} &= (U \Sigma V^T )^T (U \Sigma V^T)
    \\ &= V \Sigma U^T U \Sigma V^T & (\Sigma^T = \Sigma) \\ &= V \Sigma^2 V^T & (U^T
    U = I) \\ \frac{1}{n} \tilde{X}^T \tilde{X} &= \frac{1}{n} V \Sigma V^T =V \left(
    \frac{1}{n} \Sigma \right) V^T \\ \frac{1}{n} \tilde{X}^T \tilde{X} V &= V \left(
    \frac{1}{n} \Sigma \right) V^T V = V \left( \frac{1}{n} \Sigma \right) & \text{(right
    multiply by }V \rightarrow V^T V = I \text{)} \\ V^T \frac{1}{n} \tilde{X}^T \tilde{X}
    V &= V^T V \left( \frac{1}{n} \Sigma \right) = \frac{1}{n} \Sigma & \text{(left
    multiply by }V^T \rightarrow V^T V = I \text{)} \\ \left( \frac{1}{n} \tilde{X}^T
    \tilde{X} \right)_{jj} &= \frac{1}{n}\sigma_j^2 & \text{(Define }\sigma_j\text{
    as the} j\text{-th singular value)} \\ \frac{1}{n} \sigma_j^2 &= \frac{1}{n} \sum_{i=i}^n
    (x_{ij} - \bar{x_j})^2 \end{aligned}\]
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \[...（中文省略）...
- en: The last line defines the \(j\)-th component score.***
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行定义了第\(j\)个组分得分。***
