- en: Chapter 8\. The Structured Streaming Programming Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章。结构化流处理编程模型
- en: 'Structured Streaming builds on the foundations laid on top of the Spark SQL
    `DataFrame`s and `Dataset`s APIs of Spark SQL. By extending these APIs to support
    streaming workloads, Structured Streaming inherits the traits of the high-level
    language introduced by Spark SQL as well as the underlying optimizations, including
    the use of the Catalyst query optimizer and the low overhead memory management
    and code generation delivered by Project Tungsten. At the same time, Structured
    Streaming becomes available in all the supported language bindings for Spark SQL.
    These are: Scala, Java, Python, and R, although some of the advanced state management
    features are currently available only in Scala. Thanks to the intermediate query
    representation used in Spark SQL, the performance of the programs is identical
    regardless of the *language binding* used.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理建立在构建在 Spark SQL 的 `DataFrame` 和 `Dataset` API 之上的基础上。通过扩展这些 API 以支持流式工作负载，结构化流处理继承了
    Spark SQL 引入的高级语言特性以及底层优化，包括使用 Catalyst 查询优化器和 Project Tungsten 提供的低开销内存管理和代码生成。同时，结构化流处理在
    Spark SQL 的所有支持语言绑定中可用，包括 Scala、Java、Python 和 R，尽管一些高级状态管理功能目前仅在 Scala 中可用。由于
    Spark SQL 使用的中间查询表示形式，程序的性能在使用的*语言绑定*无关时是相同的。
- en: Structured Streaming introduces support for event time across all windowing
    and aggregation operations, making it easy to program logic that uses the time
    when events were generated, as opposed to the time when they enter the processing
    engine, also known as *processing time*. You learned these concepts in [“The Effect
    of Time”](ch02.xhtml#event-time-intro).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理引入了对事件时间的支持，覆盖了所有窗口和聚合操作，可以轻松编写使用事件生成时间而非进入处理引擎的时间（也称为*处理时间*）的逻辑。你在[“时间的影响”](ch02.xhtml#event-time-intro)中学习了这些概念。
- en: With the availability of Structured Streaming in the Spark ecosystem, Spark
    manages to unify the development experience between *classic* batch and stream-based
    data processing.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理在 Spark 生态系统中的可用性使得 Spark 能够统一经典批处理和基于流的数据处理的开发体验。
- en: 'In this chapter, we examine the programming model of Structured Streaming by
    following the sequence of steps that are usually required to create a streaming
    job with Structured Streaming:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过按照通常需要创建结构化流作业的步骤顺序来审视结构化流处理的编程模型：
- en: Initializing Spark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化 Spark
- en: 'Sources: acquiring streaming data'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源：获取流数据
- en: Declaring the operations we want to apply to the streaming data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声明我们希望应用于流数据的操作
- en: 'Sinks: output the resulting data'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据汇：输出生成的数据
- en: Initializing Spark
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化 Spark
- en: Part of the visible unification of APIs in Spark is that `SparkSession` becomes
    the single entry point for *batch* and *streaming* applications that use Structured
    Streaming.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Spark API 的可见统一部分是，`SparkSession` 成为了使用结构化流处理的*批处理*和*流处理*应用程序的单一入口点。
- en: 'Therefore, our entry point to create a Spark job is the same as when using
    the Spark batch API: we instantiate a `SparkSession` as demonstrated in [Example 8-1](#create-spark-session).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们创建 Spark 作业的入口点与使用 Spark 批处理 API 相同：我们实例化一个 `SparkSession`，如[示例 8-1](#create-spark-session)所示。
- en: Example 8-1\. Creating a local Spark Session
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-1\. 创建本地 Spark 会话
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using the Spark Shell
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark Shell
- en: When using the Spark shell to explore Structured Streaming, the `SparkSession`
    is already provided as `spark`. We don’t need to create any additional context
    to use Structured Streaming.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Spark shell 探索结构化流处理时，`SparkSession` 已经提供为 `spark`。我们不需要创建任何额外的上下文来使用结构化流处理。
- en: 'Sources: Acquiring Streaming Data'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据源：获取流数据
- en: In Structured Streaming, a *source* is an abstraction that lets us consume data
    from a streaming data producer. Sources are not directly created. Instead, the
    `sparkSession` provides a builder method, `readStream`, that exposes the API to
    specify a streaming source, called a *format*, and provide its configuration.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化流处理中，*源*是一个抽象概念，允许我们从流数据生产者消费数据。源并非直接创建，而是`sparkSession` 提供了一个构建方法 `readStream`，它公开了用于指定流源（称为*格式*）及其配置的
    API。
- en: For example, the code in [Example 8-2](#source_creation_snippet) creates a `File`
    streaming source. We specify the type of source using the `format` method. The
    method `schema` lets us provide a schema for the data stream, which is mandatory
    for certain source types, such as this `File` source.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[示例 8-2](#source_creation_snippet)中的代码创建一个`File`流式数据源。我们使用`format`方法指定源的类型。`schema`方法允许我们为数据流提供模式，对于某些源类型（如`File`源）来说，这是必需的。
- en: Example 8-2\. File streaming source
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-2\. 文件流式数据源
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Each source implementation has different options, and some have tunable parameters.
    In [Example 8-2](#source_creation_snippet), we are setting the option `mode` to
    `DROPMALFORMED`. This option instructs the JSON stream processor to drop any line
    that neither complies with the JSON format nor matches the provided schema.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每个源实现都有不同的选项，有些有可调参数。在[示例 8-2](#source_creation_snippet)中，我们将选项`mode`设置为`DROPMALFORMED`。此选项指示JSON流处理器放弃任何不符合JSON格式或不匹配提供的模式的行。
- en: Behind the scenes, the call to `spark.readStream` creates a `DataStreamBuilder`
    instance. This instance is in charge of managing the different options provided
    through the builder method calls. Calling `load(...)` on this `DataStreamBuilder`
    instance validates the options provided to the builder and, if everything checks,
    it returns a streaming `DataFrame`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，对`spark.readStream`的调用创建一个`DataStreamBuilder`实例。此实例负责通过构建器方法调用管理提供的不同选项。在此`DataStreamBuilder`实例上调用`load(...)`验证构建器提供的选项，如果一切正常，则返回流`DataFrame`。
- en: Note
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'We can appreciate the symmetry in the Spark API: while `readStream` provides
    the options to declare the source of the stream, `writeStream` lets us specify
    the output sink and the output mode required by our process. They are the counterparts
    of `read` and `write` in the `DataFrame` APIs. As such, they provide an easy way
    to remember the execution mode used in a Spark program:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以欣赏到Spark API中的对称性：`readStream`提供了声明流源的选项，而`writeStream`让我们指定输出接收器和我们的进程所需的输出模式。它们是`DataFrame`API中`read`和`write`的对应物。因此，它们提供了一种记忆Spark程序中使用的执行模式的简便方法：
- en: '`read`/`write`: Batch operation'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read`/`write`：批处理操作'
- en: '`readStream`/`writeStream`: Streaming operation'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readStream`/`writeStream`：流式操作'
- en: In our example, this streaming `DataFrame` represents the stream of data that
    will result from monitoring the provided *path* and processing each new file in
    that path as JSON-encoded data, parsed using the schema provided. All malformed
    code will be dropped from this data stream.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，这个流`DataFrame`表示的是从监视提供的*路径*并处理该路径中的每个新文件作为JSON编码数据而产生的数据流。所有格式不正确的代码都将从此数据流中丢弃。
- en: Loading a streaming source is lazy. What we get is a representation of the stream,
    embodied in the streaming `DataFrame` instance, that we can use to express the
    series of transformations that we want to apply to it in order to implement our
    specific business logic. Creating a streaming `DataFrame` does not result in any
    data actually being consumed or processed until the stream is materialized. This
    requires a *query*, as you will see further on.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 加载流式数据源是延迟执行的。我们得到的是流的表示，体现在流`DataFrame`实例中，我们可以用它来表达我们想要应用的一系列转换，以实现我们特定的业务逻辑。创建流`DataFrame`不会导致任何数据被消耗或处理，直到流被实现为止。这需要一个*查询*，如后面所示。
- en: Available Sources
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用的数据源
- en: 'As of Spark v2.4.0, the following streaming sources are supported:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 自Spark v2.4.0起，支持以下流式数据源：
- en: '`json`, `orc`, `parquet`, `csv`, `text`, `textFile`'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`json`、`orc`、`parquet`、`csv`、`text`、`textFile`'
- en: These are all file-based streaming sources. The base functionality is to monitor
    a path (folder) in a filesystem and consume files atomically placed in it. The
    files found will then be parsed by the formatter specified. For example, if `json`
    is provided, the Spark `json` reader will be used to process the files, using
    the schema information provided.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是基于文件的流式数据源。基本功能是监视文件系统中的路径（文件夹）并原子地消费放置在其中的文件。然后将找到的文件由指定的格式化程序解析。例如，如果提供了`json`，则使用Spark的`json`读取器处理文件，使用提供的模式信息。
- en: '`socket`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`socket`'
- en: Establishes a client connection to a TCP server that is assumed to provide text
    data through a socket connection.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 建立与假定通过套接字连接提供文本数据的TCP服务器的客户端连接。
- en: '`kafka`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`kafka`'
- en: Creates Kafka consumer able to retrieve data from Kafka.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 创建能够从Kafka检索数据的Kafka消费者。
- en: '`rate`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`rate`'
- en: Generates a stream of rows at the rate given by the `rowsPerSecond` option.
    It’s mainly intended as a testing source.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `rowsPerSecond` 选项以给定速率生成行流。这主要是作为测试源使用的。
- en: We look at sources in detail in [Chapter 10](ch10.xhtml#ss-sources).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [第10章](ch10.xhtml#ss-sources) 中详细介绍了源。
- en: Transforming Streaming Data
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换流数据
- en: As we saw in the previous section, the result of calling `load` is a streaming
    `DataFrame`. After we have created our streaming `DataFrame` using a `source`,
    we can use the `Dataset` or `DataFrame` API to express the logic that we want
    to apply to the data in the stream in order to implement our specific use case.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中看到的，调用 `load` 的结果是一个流式 `DataFrame`。在使用 `source` 创建了我们的流式 `DataFrame`
    后，我们可以使用 `Dataset` 或 `DataFrame` API 来表达我们想要应用于流数据的逻辑，以实现我们的特定用例。
- en: Warning
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Remember that `DataFrame` is an alias for `Dataset[Row]`. Although this might
    seem like a small technical distinction, when used from a typed language such
    as Scala, the `Dataset` API presents a typed interface, whereas the `DataFrame`
    usage is untyped. When the structured API is used from a dynamic language such
    as Python, the `DataFrame` API is the only available API.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`DataFrame` 是 `Dataset[Row]` 的别名。虽然这看似是一个小的技术区别，但在从 Scala 等类型化语言使用时，`Dataset`
    API 提供了一个类型化的接口，而 `DataFrame` 的使用是无类型的。当从 Python 等动态语言使用结构化 API 时，`DataFrame`
    API 是唯一可用的 API。
- en: There’s also a performance impact when using operations on a typed `Dataset`.
    Although the SQL expressions used by the `DataFrame` API can be understood and
    further optimized by the query planner, closures provided in `Dataset` operations
    are opaque to the query planner and therefore might run slower than the exact
    same `DataFrame` counterpart.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用类型化 `Dataset` 上的操作时，性能会有影响。虽然 `DataFrame` API 中使用的 SQL 表达式可以被查询计划器理解并进一步优化，但是
    `Dataset` 操作中提供的闭包对查询计划器来说是不透明的，因此可能比完全相同的 `DataFrame` 对应物运行得更慢。
- en: Assuming that we are using data from a sensor network, in [Example 8-3](#data_transformation_snippet)
    we are selecting the fields `deviceId`, `timestamp`, `sensorType`, and `value`
    from a `sensorStream` and filtering to only those records where the sensor is
    of type `temperature` and its `value` is higher than the given `threshold`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在使用传感器网络的数据，在 [示例8-3](#data_transformation_snippet) 中，我们从 `sensorStream`
    中选择 `deviceId`、`timestamp`、`sensorType` 和 `value` 字段，并仅筛选出传感器类型为 `temperature`
    且其 `value` 高于给定 `threshold` 的记录。
- en: Example 8-3\. Filter and projection
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-3\. 过滤和投影
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Likewise, we can aggregate our data and apply operations to the groups over
    time. [Example 8-4](#data_grouping_over_time_snippet) shows that we can use `timestamp`
    information from the event itself to define a time window of five minutes that
    will slide every minute. We cover event time in detail in [Chapter 12](ch12.xhtml#ss-event-time).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以对数据进行聚合，并对组进行时间操作。[示例8-4](#data_grouping_over_time_snippet) 表明我们可以使用事件本身的
    `timestamp` 信息来定义一个五分钟的时间窗口，每分钟滑动一次。我们在 [第12章](ch12.xhtml#ss-event-time) 中详细讨论了事件时间。
- en: What is important to grasp here is that the Structured Streaming API is practically
    the same as the `Dataset` API for batch analytics, with some additional provisions
    specific to stream processing.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要理解的重点是，结构化流 API 在批处理分析中实际上与 `Dataset` API 几乎相同，但具有一些特定于流处理的额外规定。
- en: Example 8-4\. Average by sensor type over time
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-4\. 按传感器类型的时间平均值
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you are not familiar with the structured APIs of Spark, we suggest that
    you familiarize yourself with it. Covering this API in detail is beyond the scope
    of this book. We recommend *Spark: The Definitive Guide* (O’Reilly, 2018) by Bill
    Chambers and Matei Zaharia as a comprehensive reference.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您对 Spark 的结构化 API 不熟悉，我们建议您先熟悉一下。详细介绍这个 API 超出了本书的范围。我们推荐参考 *Spark: The Definitive
    Guide*（O’Reilly，2018），由 Bill Chambers 和 Matei Zaharia 编写，作为一个全面的参考资料。'
- en: Streaming API Restrictions on the DataFrame API
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame API 上的流API限制
- en: As we hinted in the previous chapter, some operations that are offered by the
    standard `DataFrame` and `Dataset` API do not make sense on a streaming context.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章中提到的，一些标准 `DataFrame` 和 `Dataset` API 提供的操作在流上是没有意义的。
- en: We gave the example of `stream.count`, which does not make sense to use on a
    stream. In general, operations that require immediate materialization of the underlying
    dataset are not allowed.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们举了 `stream.count` 的例子，这在流上没有意义。一般来说，需要立即材料化底层数据集的操作是不允许的。
- en: 'These are the API operations not directly supported on streams:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是流上不直接支持的 API 操作：
- en: '`count`'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count`'
- en: '`show`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`show`'
- en: '`describe`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`describe`'
- en: '`limit`'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`limit`'
- en: '`take(n)`'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`take(n)`'
- en: '`distinct`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distinct`'
- en: '`foreach`'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`foreach`'
- en: '`sort`'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sort`'
- en: multiple stacked aggregations
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多重堆叠聚合
- en: Next to these operations, stream-stream and static-stream `join`s are partially
    supported.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些操作之外，部分支持流-流和静态流的`join`。
- en: Understanding the limitations
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解限制
- en: Although some operations, like `count` or `limit`, do not make sense on a stream,
    some other stream operations are computationally difficult. For example, `distinct`
    is one of them. To filter duplicates in an arbitrary stream, it would require
    that you remember all of the data seen so far and compare each new record with
    all records already seen. The first condition would require infinite memory and
    the second has a computational complexity of *O*(*n*²), which becomes prohibitive
    as the number of elements (*n*) increases.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管某些操作，如`count`或`limit`，在流上没有意义，但其他一些流操作在计算上是困难的。例如，`distinct`就是其中之一。要在任意流中过滤重复项，需要记住到目前为止看到的所有数据，并将每个新记录与已看到的所有记录进行比较。第一个条件需要无限的内存，第二个具有*O*(*n*²)的计算复杂度，随着元素数量(*n*)的增加，这变得难以接受。
- en: Operations on aggregated streams
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚合流上的操作
- en: Some of the unsupported operations become defined after we apply an aggregation
    function to the stream. Although we can’t `count` the stream, we could `count`
    messages received per minute or `count` the number of devices of a certain type.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一些不支持的操作在我们对流应用聚合函数后变得定义明确。虽然我们不能对流进行`count`，但我们可以计算每分钟接收的消息数量或某种类型设备的数量。
- en: In [Example 8-5](#count_grouping_over_time_snippet), we define a `count` of
    events per `sensorType` per minute.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Example 8-5](#count_grouping_over_time_snippet)中，我们定义了每分钟每个`sensorType`事件的`count`。
- en: Example 8-5\. Count of sensor types over time
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 8-5\. 按时间计数传感器类型
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Likewise, it’s also possible to define a `sort` on aggregated data, although
    it’s further restricted to queries with output mode `complete`. We examine about
    output modes in greater detail in [“outputMode”](#output-modes).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，也可以在聚合数据上定义`sort`，尽管进一步限制为输出模式为`complete`的查询。我们将在[“outputMode”](#output-modes)中更详细地讨论输出模式。
- en: Stream deduplication
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流去重
- en: 'We discussed that `distinct` on an arbitrary stream is computationally difficult
    to implement. But if we can define a key that informs us when an element in the
    stream has already been seen, we can use it to remove duplicates:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论过，在任意流上执行`distinct`在计算上是困难的。但如果我们能定义一个键，告诉我们何时已经看到流中的元素，我们可以使用它来去除重复项：
- en: '`stream.dropDuplicates("<key-column>")` …'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`stream.dropDuplicates("<key-column>")`…'
- en: Workarounds
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作解决方案
- en: 'Although some operations are not supported in the exact same way as in the
    *batch* model, there are alternative ways to achieve the same functionality:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管某些操作不能像*批量*模型中那样直接支持，但有替代方法可以实现相同的功能：
- en: '`foreach`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`foreach`'
- en: Although `foreach` cannot be directly used on a stream, there’s a *foreach sink*
    that provides the same functionality.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`foreach`不能直接在流上使用，但有一个*foreach sink*提供相同的功能。
- en: Sinks are specified in the output definition of a stream.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 指定在流输出定义中的汇聚。
- en: '`show`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`show`'
- en: Although `show` requires an immediate materialization of the query, and hence
    it’s not possible on a streaming `Dataset`, we can use the `console` sink to output
    data to the screen.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`show`需要立即实现查询，因此无法在流式`Dataset`上使用，但我们可以使用`console` sink将数据输出到屏幕上。
- en: 'Sinks: Output the Resulting Data'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 汇聚：输出生成的数据
- en: All operations that we have done so far—such as creating a stream and applying
    transformations on it—have been declarative. They define from where to consume
    the data and what operations we want to apply to it. But up to this point, there
    is still no data flowing through the system.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所做的所有操作——比如创建流并对其应用转换——都是声明性的。它们定义了从哪里消费数据以及我们想对其应用哪些操作。但到目前为止，系统中仍然没有数据流动。
- en: 'Before we can initiate our stream, we need to first define *where* and *how*
    we want the output data to go:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够启动流之前，我们首先需要定义*输出数据*的*何处*和*如何*去：
- en: '*Where* relates to the streaming sink: the receiving side of our streaming
    data.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Where* 关联流式接收端：我们流式数据的接收端。'
- en: '*How* refers to the output mode: how to treat the resulting records in our
    stream.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*How* 指的是输出模式：如何处理我们流中的结果记录。'
- en: From the API perspective, we materialize a stream by calling `writeStream` on
    a streaming `DataFrame` or `Dataset`, as shown in [Example 8-6](#stream_output_snippet).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从 API 角度来看，我们通过在流式`DataFrame`或`Dataset`上调用`writeStream`来实现流的材料化，如[Example 8-6](#stream_output_snippet)所示。
- en: Calling `writeStream` on a streaming `Dataset` creates a `DataStreamWriter`.
    This is a builder instance that provides methods to configure the output behavior
    of our streaming process.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在流式 `Dataset` 上调用 `writeStream` 创建一个 `DataStreamWriter`。这是一个生成器实例，提供配置流处理过程输出行为的方法。
- en: Example 8-6\. File streaming sink
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-6\. 文件流式处理输出目的地
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We cover sinks in detail in [Chapter 11](ch11.xhtml#ss-sinks).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [第 11 章](ch11.xhtml#ss-sinks) 中详细讨论了输出目的地。
- en: format
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: format
- en: The `format` method lets us specify the output sink by providing the name of
    a built-in sink or the fully qualified name of a custom sink.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`format` 方法允许我们通过提供内置输出目的地的名称或自定义输出目的地的完全限定名称来指定输出目的地。'
- en: 'As of Spark v2.4.0, the following streaming sinks are available:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Spark v2.4.0 开始，以下流式处理输出目的地可用：
- en: '`console` sink'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`console` sink'
- en: A sink that prints to the standard output. It shows a number of rows configurable
    with the option `numRows`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 打印到标准输出的输出目的地。它显示的行数可以通过选项 `numRows` 进行配置。
- en: file sink
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: file sink
- en: 'File-based and format-specific sink that writes the results to a filesystem.
    The format is specified by providing the format name: `csv`, `hive`, `json`, `orc`,
    `parquet`, `avro`, or `text`.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文件和特定格式的输出目的地，将结果写入文件系统。格式由提供的格式名称指定：`csv`、`hive`、`json`、`orc`、`parquet`、`avro`
    或 `text`。
- en: '`kafka` sink'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`kafka` sink'
- en: A Kafka-specific producer sink that is able to write to one or more Kafka topics.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Kafka 特定的生产者 sink，能够写入一个或多个 Kafka 主题。
- en: '`memory` sink'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`memory` sink'
- en: Creates an in-memory table using the provided query name as table name. This
    table receives continuous updates with the results of the stream.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提供的查询名称作为表名创建内存表。此表接收流的结果的持续更新。
- en: '`foreach` sink'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`foreach` sink'
- en: Provides a programmatic interface to access the stream contents, one element
    at the time.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 提供编程接口，逐个访问流内容的元素。
- en: '`foreachBatch` sink'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`foreachBatch` sink'
- en: '`foreachBatch` is a programmatic sink interface that provides access to the
    complete `DataFrame` that corresponds to each underlying microbatch of the Structured
    Streaming execution.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`foreachBatch` 是一个编程接口，提供对每个结构化流执行的每个微批次对应的完整 `DataFrame` 的访问。'
- en: outputMode
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: outputMode
- en: 'The `outputMode` specifies the semantics of how records are added to the output
    of the streaming query. The supported modes are `append`, `update`, and `complete`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`outputMode` 指定了记录如何添加到流查询输出的语义。支持的模式有 `append`、`update` 和 `complete`：'
- en: '`append`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`append`'
- en: (default mode) Adds only *final* records to the output stream. A record is considered
    *final* when no new records of the incoming stream can modify its value. This
    is always the case with linear transformations like those resulting from applying
    projection, filtering, and mapping. This mode guarantees that each resulting record
    will be output only once.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: （默认模式）仅将最终记录添加到输出流。当来自输入流的新记录无法修改其值时，记录被视为*最终*。这种情况始终适用于线性转换，例如应用投影、过滤和映射后的转换。此模式确保每个生成的记录仅输出一次。
- en: '`update`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`update`'
- en: Adds new and updated records since the last trigger to the output stream. `update`
    is meaningful only in the context of an aggregation, where aggregated values change
    as new records arrive. If more than one incoming record changes a single result,
    all changes between trigger intervals are collated into one output record.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 自上次触发以来添加了新的和更新的记录到输出流。`update` 仅在聚合的上下文中有意义，其中随着新记录的到达，聚合值会发生变化。如果多个输入记录更改单个结果，则在触发间隔之间的所有更改将被整理成一个输出记录。
- en: '`complete`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`complete`'
- en: '`complete` mode outputs the complete internal representation of the stream.
    This mode also relates to aggregations, because for nonaggregated streams, we
    would need to remember all records seen so far, which is unrealistic. From a practical
    perspective, `complete` mode is recommended only when you are aggregating values
    over low-cardinality criteria, like *count of visitors by country*, for which
    we know that the number of countries is bounded.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`complete` 模式输出流的完整内部表示。此模式还涉及到聚合，因为对于非聚合流，我们需要记住到目前为止所看到的所有记录，这是不切实际的。从实际角度来看，只有在对低基数条件下聚合值时（如*按国家计数的访问者*），才建议使用
    `complete` 模式，因为我们知道国家的数量是有限的。'
- en: Understanding the append semantic
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 append 语义
- en: When the streaming query contains aggregations, the definition of final becomes
    nontrivial. In an aggregated computation, new incoming records might change an
    existing aggregated value when they comply with the aggregation criteria used.
    Following our definition, we cannot output a record using `append` until we know
    that its value is final. Therefore, the use of the `append` output mode in combination
    with aggregate queries is restricted to queries for which the aggregation is expressed
    using event-time and it defines a `watermark`. In that case, `append` will output
    an event as soon as the `watermark` has expired and hence it’s considered that
    no new records can alter the aggregated value. As a consequence, output events
    in `append` mode will be delayed by the aggregation time window plus the watermark
    offset.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当流查询包含聚合时，最终定义变得不那么显而易见。在聚合计算中，新的传入记录可能在符合使用的聚合标准时改变现有的聚合值。根据我们的定义，我们不能在不知道其值最终的情况下使用`append`输出记录。因此，将`append`输出模式与聚合查询结合使用限制为仅在使用事件时间表达聚合且定义了`watermark`时有效。在这种情况下，当`watermark`过期后，`append`将输出一个事件，因此认为没有新的记录能够改变聚合值。因此，`append`模式下的输出事件将延迟聚合时间窗口加上水印偏移量。
- en: queryName
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: queryName
- en: With `queryName`, we can provide a name for the query that is used by some sinks
    and also presented in the job description in the Spark Console, as depicted in
    [Figure 8-1](#ss-job-name).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`queryName`，我们可以为查询指定一个名称，一些接收器使用它，同时也显示在Spark控制台中的作业描述中，如[图8-1](#ss-job-name)所示。
- en: '![spas 0801](Images/spas_0801.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![spas 0801](Images/spas_0801.png)'
- en: Figure 8-1\. Completed Jobs in the Spark UI showing the query name in the job
    description
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. Spark UI中显示的已完成作业，展示了作业描述中的查询名称。
- en: option
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: option
- en: With the `option` method, we can provide specific key–value pairs of configuration
    to the stream, akin to the configuration of the source. Each sink can have specific
    configuration we can customize using this method.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`option`方法，我们可以为流提供特定的键值对配置，类似于源的配置。每个接收器可以具有特定的配置，我们可以使用此方法进行定制。
- en: We can add as many `.option(...)` calls as necessary to configure the sink.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以添加尽可能多的`.option(...)`调用来配置接收器。
- en: options
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: options
- en: '`options` is an alternative to `option` that takes a `Map[String, String]`
    containing all the key–value configuration parameters that we want to set. This
    alternative is more friendly to an externalized configuration model, where we
    don’t know *a priori* the settings to be passed to the sink’s configuration.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`options`是`option`的替代方案，它接受包含我们要设置的所有键值配置参数的`Map[String, String]`。这种替代方案更适合外部化配置模型，其中我们不预先知道要传递给接收器配置的设置。'
- en: trigger
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: trigger
- en: The optional `trigger` option lets us specify the frequency at which we want
    the results to be produced. By default, Structured Streaming will process the
    input and produce a result as soon as possible. When a trigger is specified, output
    will be produced at each trigger interval.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 可选的`trigger`选项允许我们指定生成结果的频率。默认情况下，结构化流处理将尽快处理输入并生成结果。指定触发器时，将在每个触发间隔产生输出。
- en: '`org.apache.spark.sql.streaming.Trigger` provides the following supported triggers:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`org.apache.spark.sql.streaming.Trigger`提供以下支持的触发器：'
- en: '`ProcessingTime(<interval>)`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`ProcessingTime(<interval>)`'
- en: Lets us specify a time interval that will dictate the frequency of the query
    results.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 允许我们指定决定查询结果频率的时间间隔。
- en: '`Once()`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`Once()`'
- en: A particular `Trigger` that lets us execute a streaming job once. It is useful
    for testing and also to apply a defined streaming job as a single-shot batch operation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 特定的`Trigger`允许我们执行一次流作业。它对于测试和将定义的流作业应用为单次批处理操作非常有用。
- en: '`Continuous(<checkpoint-interval>)`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`Continuous(<checkpoint-interval>)`'
- en: This trigger switches the execution engine to the experimental `continuous`
    engine for low-latency processing. The `checkpoint-interval` parameter indicates
    the frequency of the asynchronous checkpointing for data resilience. It should
    not be confused with the `batch interval` of the `ProcessingTime` trigger. We
    explore this new execution option in [Chapter 15](ch15.xhtml#ss-experimental).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个触发器将执行引擎切换到实验性的`continuous`引擎，用于低延迟处理。`checkpoint-interval`参数指示数据弹性异步检查点的频率。不应与`ProcessingTime`触发器的`batch
    interval`混淆。我们在[第15章](ch15.xhtml#ss-experimental)中探讨了这个新的执行选项。
- en: start()
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: start()
- en: To materialize the streaming computation, we need to start the streaming process.
    Finally, `start()` materializes the complete job description into a streaming
    computation and initiates the internal scheduling process that results in data
    being consumed from the source, processed, and produced to the sink. `start()`
    returns a `StreamingQuery` object, which is a handle to manage the individual
    life cycle of each query. This means that we can simultaneously start and stop
    multiple queries independently of one other within the same `sparkSession`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现流式计算，我们需要启动流处理过程。最后，`start()`将完整的作业描述实现为流式计算，并启动内部调度过程，从源消费数据、处理数据，并将结果生成到接收器。`start()`返回一个`StreamingQuery`对象，这是一个用于管理每个查询个体生命周期的控制句柄。这意味着我们可以在同一个`sparkSession`内独立地同时启动和停止多个查询。
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'After reading this chapter, you should have a good understanding of the Structured
    Streaming programming model and API. In this chapter, you learned the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，您应该对结构化流式编程模型和API有一个很好的理解。在本章中，您学到了以下内容：
- en: Each streaming program starts by defining a source and what sources are currently
    available.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个流处理程序都以定义源和当前可用源为开始。
- en: We can reuse most of the familiar `Dataset` and `DataFrame` APIs for transforming
    the streaming data.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以重复使用大部分熟悉的`Dataset`和`DataFrame`API来转换流数据。
- en: Some common operations from the `batch` API do not make sense in streaming mode.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在流式模式下，一些常见的`batch`API操作是没有意义的。
- en: Sinks are the configurable definition of the stream output.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接收器是流输出的可配置定义。
- en: The relation between output modes and aggregation operations in the stream.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流中输出模式和聚合操作之间的关系。
- en: All transformations are lazy and we need to `start` our stream to get data flowing
    through the system.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有转换都是惰性执行的，我们需要`start`我们的流程以使数据通过系统流动。
- en: In the next chapter, you apply your newly acquired knowledge to create a comprehensive
    stream-processing program. After that, we will zoom into specific areas of the
    Structured Streaming API, such as event-time handing, window definitions, the
    concept of watermarks, and arbitrary state handling.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将应用你新获得的知识来创建一个全面的流处理程序。之后，我们将深入探讨结构化流式处理API的特定领域，如事件时间处理、窗口定义、水印概念和任意状态处理。
