- en: 'Chapter 4\. Spark SQL and DataFrames: Introduction to Built-in Data Sources'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 4 章\. Spark SQL 和 DataFrame：内置数据源简介
- en: In the previous chapter, we explained the evolution of and justification for
    structure in Spark. In particular, we discussed how the Spark SQL engine provides
    a unified foundation for the high-level DataFrame and Dataset APIs. Now, we’ll
    continue our discussion of the DataFrame and explore its interoperability with
    Spark SQL.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们解释了 Spark 结构化的演变和其结构的合理性。特别是，我们讨论了 Spark SQL 引擎如何为高级 DataFrame 和 Dataset
    API 提供统一的基础。现在，我们将继续讨论 DataFrame，并探索其与 Spark SQL 的互操作性。
- en: This chapter and the next also explore how Spark SQL interfaces with some of
    the external components shown in [Figure 4-1](#spark_sql_usage_and_interface).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和下一章还将探讨 Spark SQL 如何与图中显示的一些外部组件接口交互（参见[图 4-1](#spark_sql_usage_and_interface)）。
- en: 'In particular, Spark SQL:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，Spark SQL：
- en: Provides the engine upon which the high-level Structured APIs we explored in
    [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis) are built.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供了高级结构化 API 的引擎，我们在[第 3 章](ch03.html#apache_sparkapostrophes_structured_apis)中探索过。
- en: Can read and write data in a variety of structured formats (e.g., JSON, Hive
    tables, Parquet, Avro, ORC, CSV).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以读取和写入多种结构化格式的数据（例如 JSON、Hive 表、Parquet、Avro、ORC、CSV）。
- en: Lets you query data using JDBC/ODBC connectors from external business intelligence
    (BI) data sources such as Tableau, Power BI, Talend, or from RDBMSs such as MySQL
    and PostgreSQL.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许您使用来自外部商业智能（BI）数据源的 JDBC/ODBC 连接器（如 Tableau、Power BI、Talend）或来自 RDBMS（如 MySQL
    和 PostgreSQL）查询数据。
- en: Provides a programmatic interface to interact with structured data stored as
    tables or views in a database from a Spark application
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 Spark 应用程序中的表或视图存储的结构化数据提供了编程接口。
- en: Offers an interactive shell to issue SQL queries on your structured data.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供了一个交互式 shell，用于在您的结构化数据上发出 SQL 查询。
- en: Supports [ANSI SQL:2003](https://oreil.ly/83QYa)-compliant commands and [HiveQL](https://oreil.ly/QFza4).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持[ANSI SQL:2003](https://oreil.ly/83QYa)兼容命令和[HiveQL](https://oreil.ly/QFza4)。
- en: '![Spark SQL connectors and data sources](assets/lesp_0401.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![Spark SQL 连接器和数据源](assets/lesp_0401.png)'
- en: Figure 4-1\. Spark SQL connectors and data sources
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. Spark SQL 连接器和数据源
- en: Let’s begin with how you can use Spark SQL in a Spark application.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从如何在 Spark 应用程序中使用 Spark SQL 开始。
- en: Using Spark SQL in Spark Applications
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Spark 应用程序中使用 Spark SQL
- en: 'The `SparkSession`, introduced in Spark 2.0, provides a [unified entry point](https://oreil.ly/B7FZh)
    for programming Spark with the Structured APIs. You can use a `SparkSession` to
    access Spark functionality: just import the class and create an instance in your
    code.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession`，在 Spark 2.0 中引入，为使用结构化 API 编程 Spark 提供了一个[统一入口点](https://oreil.ly/B7FZh)。您可以使用
    `SparkSession` 访问 Spark 功能：只需导入该类并在代码中创建一个实例。'
- en: To issue any SQL query, use the `sql()` method on the `SparkSession` instance,
    `spark`, such as `spark.sql("SELECT * FROM myTableName")`. All `spark.sql` queries
    executed in this manner return a DataFrame on which you may perform further Spark
    operations if you desire—the kind we explored in [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis)
    and the ones you will learn about in this chapter and the next.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要发出任何 SQL 查询，请在 `SparkSession` 实例 `spark` 上使用 `sql()` 方法，例如 `spark.sql("SELECT
    * FROM myTableName")`。以这种方式执行的所有 `spark.sql` 查询都会返回一个 DataFrame，您可以在其上执行进一步的 Spark
    操作，如果您愿意——就像我们在[第 3 章](ch03.html#apache_sparkapostrophes_structured_apis)中探索的那些操作，以及您将在本章和下一章中学到的操作。
- en: Basic Query Examples
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本查询示例
- en: In this section we’ll walk through a few examples of queries on the [Airline
    On-Time Performance and Causes of Flight Delays data set](https://oreil.ly/gfzLZ),
    which contains data on US flights including date, delay, distance, origin, and
    destination. It’s available as a CSV file with over a million records. Using a
    schema, we’ll read the data into a DataFrame and register the DataFrame as a temporary
    view (more on temporary views shortly) so we can query it with SQL.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过对[航空准时表现和航班延误原因数据集](https://oreil.ly/gfzLZ)的几个查询示例来进行讲解，该数据集包含有关美国航班的日期、延误、距离、起点和终点的数据。它以
    CSV 文件形式提供，包含超过一百万条记录。使用一个模式，我们将数据读入一个 DataFrame 并将该 DataFrame 注册为临时视图（稍后将详细讨论临时视图），以便我们可以使用
    SQL 进行查询。
- en: Query examples are provided in code snippets, and Python and Scala notebooks
    containing all of the code presented here are available in the book’s [GitHub
    repo](https://github.com/databricks/LearningSparkV2). These examples will offer
    you a taste of how to use SQL in your Spark applications via the [`spark.sql`
    programmatic interface](https://spark.apache.org/sql). Similar to the DataFrame
    API in its declarative flavor, this interface allows you to query structured data
    in your Spark applications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 查询示例以代码片段提供，并且包含所有这里展示的代码的 Python 和 Scala 笔记本可以在本书的 [GitHub 仓库](https://github.com/databricks/LearningSparkV2)
    中找到。这些示例将为您展示如何通过 [`spark.sql` 编程接口](https://spark.apache.org/sql) 在 Spark 应用程序中使用
    SQL。与 DataFrame API 一样，这个接口以其声明性的风格允许您查询结构化数据在您的 Spark 应用程序中。
- en: Normally, in a standalone Spark application, you will create a `SparkSession`
    instance manually, as shown in the following example. However, in a Spark shell
    (or Databricks notebook), the `SparkSession` is created for you and accessible
    via the appropriately named variable `spark`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在独立的 Spark 应用程序中，您会手动创建一个 `SparkSession` 实例，就像下面的例子所示。然而，在 Spark shell（或
    Databricks 笔记本）中，`SparkSession` 会自动为您创建，并且可以通过适当命名的变量 `spark` 访问。
- en: 'Let’s get started by reading the data set into a temporary view:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先将数据集读取到临时视图中：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you want to specify a schema, you can use a DDL-formatted string. For example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想指定一个模式（schema），你可以使用一个 DDL 格式的字符串。例如：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have a temporary view, we can issue SQL queries using Spark SQL.
    These queries are no different from those you might issue against a SQL table
    in, say, a MySQL or PostgreSQL database. The point here is to show that Spark
    SQL offers an ANSI:2003–compliant SQL interface, and to demonstrate the interoperability
    between SQL and DataFrames.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个临时视图，我们可以使用 Spark SQL 发出 SQL 查询。这些查询与您可能在 MySQL 或 PostgreSQL 数据库中针对
    SQL 表发出的查询没有什么不同。这里的重点是显示 Spark SQL 提供了符合 ANSI:2003 标准的 SQL 接口，并演示 SQL 和 DataFrames
    之间的互操作性。
- en: 'The US flight delays data set has five columns:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 美国航班延误数据集有五列：
- en: The `date` column contains a string like `02190925`. When converted, this maps
    to `02-19 09:25 am`.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`date` 列包含类似 `02190925` 的字符串。当转换时，这映射为 `02-19 09:25 am`。'
- en: The `delay` column gives the delay in minutes between the scheduled and actual
    departure times. Early departures show negative numbers.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delay` 列给出计划和实际起飞时间之间的延迟时间（分钟）。提前起飞显示为负数。'
- en: The `distance` column gives the distance in miles from the origin airport to
    the destination airport.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distance` 列给出起飞机场到目的地机场的距离（英里）。'
- en: The `origin` column contains the origin IATA airport code.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`origin` 列包含起飞机场的 IATA 机场代码。'
- en: The `destination` column contains the destination IATA airport code.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`destination` 列包含目的地的 IATA 机场代码。'
- en: With that in mind, let’s try some example queries against this data set.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，让我们尝试一些针对这个数据集的示例查询。
- en: 'First, we’ll find all flights whose distance is greater than 1,000 miles:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将找到所有距离超过 1,000 英里的航班：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As the results show, all of the longest flights were between Honolulu (HNL)
    and New York (JFK). Next, we’ll find all flights between San Francisco (SFO) and
    Chicago (ORD) with at least a two-hour delay:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正如结果所示，所有最长的航班都是在檀香山（HNL）和纽约（JFK）之间。接下来，我们将找到所有从旧金山（SFO）到芝加哥（ORD）的航班，至少延误两小时：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It seems there were many significantly delayed flights between these two cities,
    on different dates. (As an exercise, convert the `date` column into a readable
    format and find the days or months when these delays were most common. Were the
    delays related to winter months or holidays?)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来在这两个城市之间有许多显著延误的航班，不同的日期。（作为练习，将 `date` 列转换为可读格式，并找出这些延误最常见的日期或月份。这些延误是否与冬季月份或节假日有关？）
- en: 'Let’s try a more complicated query where we use the `CASE` clause in SQL. In
    the following example, we want to label all US flights, regardless of origin and
    destination, with an indication of the delays they experienced: Very Long Delays
    (> 6 hours), Long Delays (2–6 hours), etc. We’ll add these human-readable labels
    in a new column called `Flight_Delays`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个更复杂的查询，其中我们在 SQL 中使用 `CASE` 子句。在下面的示例中，我们希望标记所有美国航班，无论起飞地和目的地如何，都显示它们经历的延误情况：非常长的延误（>
    6 小时），长延误（2–6 小时），等等。我们将在一个名为 `Flight_Delays` 的新列中添加这些人类可读的标签：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As with the DataFrame and Dataset APIs, with the `spark.sql` interface you can
    conduct common data analysis operations like those we explored in the previous
    chapter. The computations undergo an identical journey in the Spark SQL engine
    (see [“The Catalyst Optimizer”](ch03.html#the_catalyst_optimizer) in [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis)
    for details), giving you the same results.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与 DataFrame 和 Dataset API 一样，通过 `spark.sql` 接口，您可以进行像前一章中探索的常见数据分析操作。这些计算经历了相同的
    Spark SQL 引擎过程（详见 [“Catalyst 优化器”](ch03.html#the_catalyst_optimizer) 在 [第 3 章](ch03.html#apache_sparkapostrophes_structured_apis)
    中的详细说明），从而给您带来相同的结果。
- en: 'All three of the preceding SQL queries can be expressed with an equivalent
    DataFrame API query. For example, the first query can be expressed in the Python
    DataFrame API as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 前面三个 SQL 查询可以用等效的 DataFrame API 查询来表示。例如，第一个查询可以在 Python 的 DataFrame API 中表示为：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This produces the same results as the SQL query:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生与 SQL 查询相同的结果：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As an exercise, try converting the other two SQL queries to use the DataFrame
    API.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，请尝试将另外两个 SQL 查询转换为使用 DataFrame API 的形式。
- en: As these examples show, using the Spark SQL interface to query data is similar
    to writing a regular SQL query to a relational database table. Although the queries
    are in SQL, you can feel the similarity in readability and semantics to DataFrame
    API operations, which you encountered in [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis)
    and will explore further in the next chapter.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这些示例所示，使用 Spark SQL 接口查询数据类似于向关系数据库表写入常规 SQL 查询。尽管查询使用 SQL 编写，但您可以感受到与 DataFrame
    API 操作的可读性和语义的相似性，您在 [第 3 章](ch03.html#apache_sparkapostrophes_structured_apis)
    中已经遇到并将在下一章进一步探索。
- en: 'To enable you to query structured data as shown in the preceding examples,
    Spark manages all the complexities of creating and managing views and tables,
    both in memory and on disk. That leads us to our next topic: how tables and views
    are created and managed.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使您能够像前面的示例中展示的那样查询结构化数据，Spark 管理创建和管理视图和表的所有复杂性，无论是在内存中还是磁盘上。这将引导我们进入下一个主题：如何创建和管理表和视图。
- en: SQL Tables and Views
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SQL 表和视图
- en: 'Tables hold data. Associated with each table in Spark is its relevant metadata,
    which is information about the table and its data: the schema, description, table
    name, database name, column names, partitions, physical location where the actual
    data resides, etc. All of this is stored in a central metastore.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表存储数据。在 Spark 中，每个表都有相关的元数据，包括表和数据的信息：模式、描述、表名、数据库名、列名、分区、实际数据存储的物理位置等等。所有这些信息都存储在中央元数据存储中。
- en: Instead of having a separate metastore for Spark tables, Spark by default uses
    the Apache Hive metastore, located at /user/hive/warehouse, to persist all the
    metadata about your tables. However, you may change the default location by setting
    the Spark config variable `spark.sql.warehouse.dir` to another location, which
    can be set to a local or external distributed storage.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与为 Spark 表单独设置元数据存储不同，默认情况下 Spark 使用 Apache Hive 元数据存储，位置在 /user/hive/warehouse，用于持久化表的所有元数据。然而，您可以通过设置
    Spark 配置变量 `spark.sql.warehouse.dir` 来更改默认位置，可以设置为本地或外部分布式存储。
- en: Managed Versus UnmanagedTables
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 托管与非托管表
- en: 'Spark allows you to create two types of tables: managed and unmanaged. For
    a *managed* table, Spark manages both the metadata and the data in the file store.
    This could be a local filesystem, HDFS, or an object store such as Amazon S3 or
    Azure Blob. For an *unmanaged* table, Spark only manages the metadata, while you
    manage the data yourself in an external [data source](https://oreil.ly/Scvor)
    such as Cassandra.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 允许您创建两种类型的表：托管表和非托管表。对于 *托管* 表，Spark 管理文件存储中的元数据和数据。这可以是本地文件系统、HDFS 或对象存储（如
    Amazon S3 或 Azure Blob）。对于 *非托管* 表，Spark 只管理元数据，而您需要自己在外部 [数据源](https://oreil.ly/Scvor)（例如
    Cassandra）中管理数据。
- en: With a managed table, because Spark manages everything, a SQL command such as
    `DROP TABLE table_name` deletes both the metadata and the data. With an unmanaged
    table, the same command will delete only the metadata, not the actual data. We
    will look at some examples of how to create managed and unmanaged tables in the
    next section.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于托管表，因为 Spark 管理一切，例如 SQL 命令 `DROP TABLE table_name` 会同时删除元数据和数据。对于非托管表，同样的命令只会删除元数据，而不会删除实际数据。我们将在下一节看一些创建托管和非托管表的示例。
- en: Creating SQL Databases and Tables
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 SQL 数据库和表
- en: 'Tables reside within a database. By default, Spark creates tables under the
    `default` database. To create your own database name, you can issue a SQL command
    from your Spark application or notebook. Using the US flight delays data set,
    let’s create both a managed and an unmanaged table. To begin, we’ll create a database
    called `learn_spark_db` and tell Spark we want to use that database:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表位于数据库中。默认情况下，Spark 在 `default` 数据库下创建表。要创建自己的数据库名称，您可以从您的 Spark 应用程序或笔记本中发出
    SQL 命令。使用美国航班延误数据集，让我们创建一个托管表和一个非托管表。首先，我们将创建一个名为 `learn_spark_db` 的数据库，并告诉 Spark
    我们要使用该数据库：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: From this point, any commands we issue in our application to create tables will
    result in the tables being created in this database and residing under the database
    name `learn_spark_db`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从此处开始，我们在应用程序中发出的任何命令来创建表都将导致这些表被创建在此数据库中，并位于数据库名称 `learn_spark_db` 下。
- en: Creating a managed table
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个托管表
- en: 'To create a managed table within the database `learn_spark_db`, you can issue
    a SQL query like the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 `learn_spark_db` 数据库中创建一个托管表，可以执行如下的 SQL 查询：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can do the same thing using the DataFrame API like this:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 DataFrame API 来执行相同的操作，如下所示：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Both of these statements will create the managed table `us_delay_flights_tbl`
    in the `learn_spark_db` database.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个语句将在 `learn_spark_db` 数据库中创建托管表 `us_delay_flights_tbl`。
- en: Creating an unmanaged table
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个非托管表
- en: By contrast, you can create unmanaged tables from your own data sources—say,
    Parquet, CSV, or JSON files stored in a file store accessible to your Spark application.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，您可以从您自己的数据源（例如存储在文件存储中的 Parquet、CSV 或 JSON 文件）创建非托管表，这些表对您的 Spark 应用程序可访问。
- en: 'To create an unmanaged table from a data source such as a CSV file, in SQL
    use:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要从诸如 CSV 文件之类的数据源创建一个非托管表，在 SQL 中使用：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And within the DataFrame API use:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DataFrame API 内使用：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: To enable you to explore these examples, we have created Python and Scala example
    notebooks that you can find in the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您能够探索这些示例，我们已经创建了 Python 和 Scala 示例笔记本，您可以在本书的 [GitHub 仓库](https://github.com/databricks/LearningSparkV2)
    中找到。
- en: Creating Views
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建视图
- en: 'In addition to creating tables, Spark can create views on top of existing tables.
    Views can be global (visible across all `SparkSession`s on a given cluster) or
    session-scoped (visible only to a single `SparkSession`), and they are temporary:
    they disappear after your Spark application terminates.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除了创建表外，Spark 还可以在现有表的基础上创建视图。视图可以是全局的（在给定集群上的所有 `SparkSession` 中可见）或会话作用域的（仅对单个
    `SparkSession` 可见），它们是临时的：在 Spark 应用程序终止后会消失。
- en: '[Creating views](https://oreil.ly/8OqlM) has a similar syntax to creating tables
    within a database. Once you create a view, you can query it as you would a table.
    The difference between a view and a table is that views don’t actually hold the
    data; tables persist after your Spark application terminates, but views disappear.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[创建视图](https://oreil.ly/8OqlM) 与在数据库内创建表具有类似的语法。创建视图后，您可以像查询表一样查询它。视图和表的区别在于视图实际上不保存数据；表在
    Spark 应用程序终止后保持存在，但视图会消失。'
- en: 'You can create a view from an existing table using SQL. For example, if you
    wish to work on only the subset of the US flight delays data set with origin airports
    of New York (JFK) and San Francisco (SFO), the following queries will create global
    temporary and temporary views consisting of just that slice of the table:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 SQL 从现有表创建视图。例如，如果您希望仅处理具有纽约（JFK）和旧金山（SFO）起飞机场的美国航班延误数据集的子集，则以下查询将创建仅由该表切片组成的全局临时视图和临时视图：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can accomplish the same thing with the DataFrame API as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 DataFrame API 来完成相同的事情：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once you’ve created these views, you can issue queries against them just as
    you would against a table. Keep in mind that when accessing a global temporary
    view you must use the prefix `global_temp*.<view_name>*`, because Spark creates
    global temporary views in a global temporary database called `global_temp`. For
    example:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了这些视图，您可以像对待表一样发出查询。请注意，当访问全局临时视图时，您必须使用前缀 `global_temp*.<view_name>*`，因为
    Spark 在全局临时数据库 `global_temp` 中创建全局临时视图。例如：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'By contrast, you can access the normal temporary view without the `global_temp`
    prefix:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，您可以访问普通的临时视图，而无需使用 `global_temp` 前缀：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You can also drop a view just like you would a table:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以像删除表一样删除视图：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Temporary views versus global temporary views
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 临时视图与全局临时视图
- en: The difference between *temporary* and *global temporary* views being subtle,
    it can be a source of mild confusion among developers new to Spark. A temporary
    view is tied to a single `SparkSession` within a Spark application. In contrast,
    a global temporary view is visible across multiple `SparkSession`s within a Spark
    application. Yes, you can create [multiple `SparkSession`s](https://oreil.ly/YbTFa)
    within a single Spark application—this can be handy, for example, in cases where
    you want to access (and combine) data from two different `SparkSession`s that
    don’t share the same Hive metastore configurations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*临时*视图和*全局临时*视图之间的差异微妙，对于刚接触Spark的开发人员来说可能会造成轻微混淆。临时视图绑定在Spark应用程序中的单个`SparkSession`中。相比之下，全局临时视图在Spark应用程序中的多个`SparkSession`中可见。是的，您可以在单个Spark应用程序中创建[多个`SparkSession`s](https://oreil.ly/YbTFa)——例如，在您想要访问（和合并）不共享相同Hive元存储配置的两个不同`SparkSession`s的数据时，这可能会很方便。'
- en: Viewing the Metadata
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看元数据
- en: As mentioned previously, Spark manages the metadata associated with each managed
    or unmanaged table. This is captured in the [`Catalog`](https://oreil.ly/56HYV),
    a high-level abstraction in Spark SQL for storing metadata. The `Catalog`’s functionality
    was expanded in Spark 2.x with new public methods enabling you to examine the
    metadata associated with your databases, tables, and views. Spark 3.0 extends
    it to use external `catalog` (which we briefly discuss in [Chapter 12](ch12.html#epilogue_apache_spark_3dot0)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，Spark管理每个托管或非托管表相关的元数据。这些数据被捕获在[`Catalog`](https://oreil.ly/56HYV)中，这是Spark
    SQL中用于存储元数据的高级抽象。在Spark 2.x中，`Catalog`的功能通过新的公共方法进行了扩展，使您能够查看与您的数据库、表和视图相关的元数据。Spark
    3.0扩展了其使用外部`catalog`的功能（我们在[第12章](ch12.html#epilogue_apache_spark_3dot0)中简要讨论）。
- en: 'For example, within a Spark application, after creating the `SparkSession`
    variable `spark`, you can access all the stored metadata through methods like
    these:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在Spark应用程序中创建`SparkSession`变量`spark`后，您可以通过类似以下方法访问所有存储的元数据：
- en: '[PRE21]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Import the notebook from the book’s [GitHub repo](https://github.com/databricks/LearningSparkV2)
    and give it a try.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从书的[GitHub repo](https://github.com/databricks/LearningSparkV2)导入笔记本并尝试一下。
- en: Caching SQL Tables
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存SQL表
- en: 'Although we will discuss table caching strategies in the next chapter, it’s
    worth mentioning here that, like DataFrames, you can cache and uncache SQL tables
    and views. In [Spark 3.0](https://oreil.ly/2ptwu), in addition to other options,
    you can specify a table as `LAZY`, meaning that it should only be cached when
    it is first used instead of immediately:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将在下一章讨论表缓存策略，但在这里值得一提的是，与DataFrame一样，您可以缓存和取消缓存SQL表和视图。在[Spark 3.0](https://oreil.ly/2ptwu)中，除了其他选项外，您还可以指定表为`LAZY`，这意味着它只有在首次使用时才会被缓存，而不是立即缓存：
- en: '[PRE22]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Reading Tables into DataFrames
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将表读取到DataFrames中
- en: Often, data engineers build data pipelines as part of their regular data ingestion
    and ETL processes. They populate Spark SQL databases and tables with cleansed
    data for consumption by applications downstream.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师经常会在他们的常规数据摄取和ETL过程中构建数据管道。他们会用经过清洗的数据填充Spark SQL数据库和表，以便应用程序在下游消费。
- en: 'Let’s assume you have an existing database, `learn_spark_db`, and table, `us_delay_flights_tbl`,
    ready for use. Instead of reading from an external JSON file, you can simply use
    SQL to query the table and assign the returned result to a DataFrame:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个现有的数据库，`learn_spark_db`，以及一个准备就绪的表，`us_delay_flights_tbl`。你可以直接使用SQL查询该表，并将返回的结果赋给一个DataFrame：
- en: '[PRE23]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now you have a cleansed DataFrame read from an existing Spark SQL table. You
    can also read data in other formats using Spark’s built-in data sources, giving
    you the flexibility to interact with various common file formats.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经从现有的Spark SQL表中读取了一个经过清洗的DataFrame。您还可以使用Spark的内置数据源读取其他格式的数据，从而灵活地与各种常见文件格式交互。
- en: Data Sources for DataFrames and SQL Tables
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrames和SQL表的数据源
- en: As shown in [Figure 4-1](#spark_sql_usage_and_interface), Spark SQL provides
    an interface to a variety of data sources. It also provides a set of common methods
    for reading and writing data to and from these data sources using the [Data Sources
    API](https://oreil.ly/_8-6A).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图4-1](#spark_sql_usage_and_interface)所示，Spark SQL提供了与各种数据源交互的接口。它还提供了一套通用方法，用于通过[数据源API](https://oreil.ly/_8-6A)从这些数据源读取和写入数据。
- en: 'In this section we will cover some of the [built-in data sources](https://oreil.ly/Hj9pd),
    available file formats, and ways to load and write data, along with specific options
    pertaining to these data sources. But first, let’s take a closer look at two high-level
    Data Source API constructs that dictate the manner in which you interact with
    different data sources: `DataFrameReader` and `DataFrameWriter`.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些[内置数据源](https://oreil.ly/Hj9pd)、可用文件格式以及加载和写入数据的方式，以及与这些数据源相关的特定选项。但首先，让我们更详细地了解两个高级别的数据源API构造，它们决定了您与不同数据源交互的方式：`DataFrameReader`和`DataFrameWriter`。
- en: DataFrameReader
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrameReader
- en: '[`DataFrameReader`](https://oreil.ly/UZXdx) is the core construct for reading
    data from a data source into a DataFrame. It has a defined format and a recommended
    pattern for usage:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[`DataFrameReader`](https://oreil.ly/UZXdx)是从数据源读取数据到DataFrame的核心构造。它具有定义的格式和推荐的使用模式：'
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This pattern of stringing methods together is common in Spark, and easy to read.
    We saw it in [Chapter 3](ch03.html#apache_sparkapostrophes_structured_apis) when
    exploring common data analysis patterns.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，将方法串联在一起的模式很常见且易于阅读。在我们探索常见数据分析模式时（见[第3章](ch03.html#apache_sparkapostrophes_structured_apis)），我们已经看到过这种模式。
- en: 'Note that you can only access a `DataFrameReader` through a `SparkSession`
    instance. That is, you cannot create an instance of `DataFrameReader`. To get
    an instance handle to it, use:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只能通过`SparkSession`实例访问`DataFrameReader`。也就是说，不能创建`DataFrameReader`的实例。要获取对它的实例句柄，请使用：
- en: '[PRE26]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: While `read` returns a handle to `DataFrameReader` to read into a DataFrame
    from a static data source, `readStream` returns an instance to read from a streaming
    source. (We will cover Structured Streaming later in the book.)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`read`方法时，返回一个`DataFrameReader`的句柄，用于从静态数据源读取DataFrame；而使用`readStream`方法时，则返回一个实例，用于从流式数据源读取。（我们将在本书后面讨论结构化流处理。）
- en: Arguments to each of the public methods to `DataFrameReader` take different
    values. [Table 4-1](#dataframereader_methodscomma_argumentsco) enumerates these,
    with a subset of the supported arguments.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrameReader`的每个公共方法的参数取不同的值。[表4-1](#dataframereader_methodscomma_argumentsco)列举了这些参数及其支持的子集。'
- en: Table 4-1\. DataFrameReader methods, arguments, and options
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-1\. DataFrameReader方法、参数和选项
- en: '| Method | Arguments | Description |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 参数 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `format()` | `"parquet"`, `"csv"`, `"txt"`, `"json"`, `"jdbc"`, `"orc"`,
    `"avro"`, etc. | If you don’t specify this method, then the default is Parquet
    or whatever is set in `spark.sql.sources.default`. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| `format()` | `"parquet"`, `"csv"`, `"txt"`, `"json"`, `"jdbc"`, `"orc"`,
    `"avro"`等 | 如果不指定此方法，则默认为Parquet格式或者根据`spark.sql.sources.default`设置的格式。 |'
- en: '| `option()` | `("mode", {PERMISSIVE &#124; FAILFAST &#124; DROPMALFORMED }
    )` `("inferSchema", {true &#124; false})`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '| `option()` | `("mode", {PERMISSIVE &#124; FAILFAST &#124; DROPMALFORMED})`
    `("inferSchema", {true &#124; false})`'
- en: '`("path", "path_file_data_source")` | A series of key/value pairs and options.
    The [Spark documentation](https://oreil.ly/XujEK) shows some examples and explains
    the different modes and their actions. The default mode is `PERMISSIVE`. The `"inferSchema"`
    and `"mode"` options are specific to the JSON and CSV file formats. |'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`("path", "path_file_data_source")` | 一系列键/值对和选项。[Spark文档](https://oreil.ly/XujEK)展示了一些示例，并解释了不同模式及其作用。默认模式是`PERMISSIVE`。`"inferSchema"`和`"mode"`选项特定于JSON和CSV文件格式。
    |'
- en: '| `schema()` | DDL `String` or `StructType`, e.g., `''A INT, B STRING''` or
    `StructType(...)` | For JSON or CSV format, you can specify to infer the schema
    in the `option()` method. Generally, providing a schema for any format makes loading
    faster and ensures your data conforms to the expected schema. |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| `schema()` | DDL `String`或`StructType`，例如，`''A INT, B STRING''`或`StructType(...)`
    | 对于JSON或CSV格式，可以在`option()`方法中指定推断模式。通常，为任何格式提供模式可以加快加载速度，并确保数据符合预期的模式。 |'
- en: '| `load()` | `"/path/to/data/source"` | The path to the data source. This can
    be empty if specified in `option("path", "...")`. |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| `load()` | `"/path/to/data/source"` | 数据源的路径。如果在`option("path", "...")`中指定，则可以为空。
    |'
- en: 'While we won’t comprehensively enumerate all the different combinations of
    arguments and options, the [documentation for Python, Scala, R, and Java](https://oreil.ly/RsfRg)
    offers suggestions and guidance. It’s worthwhile to show a couple of examples,
    though:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不会详尽列举所有不同参数和选项的组合，但[Python、Scala、R和Java的文档](https://oreil.ly/RsfRg)提供了建议和指导。尽管如此，展示一些示例仍然是值得的：
- en: '[PRE27]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In general, no schema is needed when reading from a static Parquet data source—the
    Parquet metadata usually contains the schema, so it’s inferred. However, for streaming
    data sources you will have to provide a schema. (We will cover reading from streaming
    data sources in [Chapter 8](ch08.html#structured_streaming).)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，从静态 Parquet 数据源读取时不需要模式——Parquet 元数据通常包含模式，因此可以推断出   通常，当从静态的 Parquet 数据源读取时，不需要
    schema —— Parquet 的元数据通常包含 schema，因此可以推断出来。然而，对于流式数据源，你需要提供 schema。（我们将在[第 8 章](ch08.html#structured_streaming)介绍如何从流式数据源读取。）
- en: Parquet is the default and preferred data source for Spark because it’s efficient,
    uses columnar storage, and employs a fast compression algorithm. You will see
    additional benefits later (such as columnar pushdown), when we cover the Catalyst
    optimizer in greater depth.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 是 Spark 的默认和首选数据源，因为它高效，使用列式存储，并采用快速压缩算法。你将在后面看到额外的好处（例如列式下推），当我们更深入介绍
    Catalyst 优化器时。
- en: DataFrameWriter
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrameWriter
- en: '[`DataFrameWriter`](https://oreil.ly/SM1LR) does the reverse of its counterpart:
    it saves or writes data to a specified built-in data source. Unlike with `DataFrameReader`,
    you access its instance not from a `SparkSession` but from the DataFrame you wish
    to save. It has a few recommended usage patterns:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[`DataFrameWriter`](https://oreil.ly/SM1LR) 的功能与其对应方法相反：它将数据保存或写入到指定的内置数据源。与
    `DataFrameReader` 不同，你的实例不是从 `SparkSession` 获取，而是从你希望保存的 DataFrame 获取。它有一些推荐的使用模式：'
- en: '[PRE28]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To get an instance handle, use:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取实例句柄，使用：
- en: '[PRE29]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Arguments to each of the methods to `DataFrameWriter` also take different values.
    We list these in [Table 4-2](#dataframewriter_methodscomma_argumentsco), with
    a subset of the supported arguments.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrameWriter` 的每个方法的参数也会取不同的值。我们在[表 4-2](#dataframewriter_methodscomma_argumentsco)中列出这些参数，并列出了一部分支持的参数。'
- en: Table 4-2\. DataFrameWriter methods, arguments, and options
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-2\. DataFrameWriter 方法、参数和选项
- en: '| Method | Arguments | Description |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 参数 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `format()` | `"parquet"`, `"csv"`, `"txt"`, `"json"`, `"jdbc"`, `"orc"`,
    `"avro"`, etc. | If you don’t specify this method, then the default is Parquet
    or whatever is set in `spark.sql.sources.default`. |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| `format()` | `"parquet"`，`"csv"`，`"txt"`，`"json"`，`"jdbc"`，`"orc"`，`"avro"`，等等。|
    如果不指定此方法，则默认是 Parquet 或 `spark.sql.sources.default` 中设置的格式。 |'
- en: '| `option()` | `("mode", {append &#124; overwrite &#124; ignore &#124; error
    or errorifexists} )` `("mode", {SaveMode.Overwrite &#124; SaveMode.Append, SaveMode.Ignore,
    SaveMode.ErrorIfExists})`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '| `option()` | `("mode", {append &#124; overwrite &#124; ignore &#124; error
    or errorifexists} )` `("mode", {SaveMode.Overwrite &#124; SaveMode.Append, SaveMode.Ignore,
    SaveMode.ErrorIfExists})`'
- en: '`("path", "path_to_write_to")` | A series of key/value pairs and options. The
    [Spark documentation](https://oreil.ly/w7J0I) shows some examples. This is an
    overloaded method. The default mode options are `error or errorifexists` and `SaveMode.ErrorIfExists`;
    they throw an exception at runtime if the data already exists. |'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`("path", "path_to_write_to")` | 一系列键/值对和选项。[Spark 文档](https://oreil.ly/w7J0I)展示了一些示例。这是一个重载的方法。默认模式选项是
    `error or errorifexists` 和 `SaveMode.ErrorIfExists`；如果数据已存在，在运行时会抛出异常。 |'
- en: '| `bucketBy()` | `(numBuckets, col, col..., coln)` | The number of buckets
    and names of columns to bucket by. Uses Hive’s bucketing scheme on a filesystem.
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| `bucketBy()` | `(numBuckets, col, col..., coln)` | 桶的数量和要按其分桶的列名。使用 Hive
    的 bucketing 方案在文件系统上。|'
- en: '| `save()` | `"/path/to/data/source"` | The path to save to. This can be empty
    if specified in `option("path", "...")`. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| `save()` | `"/path/to/data/source"` | 要保存的路径。如果在 `option("path", "...")`
    中指定，这里可以为空。|'
- en: '| `saveAsTable()` | `"table_name"` | The table to save to. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| `saveAsTable()` | `"table_name"` | 要保存到的表名。 |'
- en: 'Here’s a short example snippet to illustrate the use of methods and arguments:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个简短的示例片段，说明方法和参数的使用：
- en: '[PRE30]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parquet
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquet
- en: We’ll start our exploration of data sources with [Parquet](https://oreil.ly/-wptz),
    because it’s the default data source in Spark. Supported and widely used by many
    big data processing frameworks and platforms, Parquet is an open source columnar
    file format that offers many I/O optimizations (such as compression, which saves
    storage space and allows for quick access to data columns).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 [Parquet](https://oreil.ly/-wptz) 开始探索数据源，因为它是 Spark 的默认数据源。Parquet 被许多大数据处理框架和平台支持和广泛使用，是一种开源的列式文件格式，提供许多
    I/O 优化（例如压缩，节省存储空间并快速访问数据列）。
- en: Because of its efficiency and these optimizations, we recommend that after you
    have transformed and cleansed your data, you save your DataFrames in the Parquet
    format for downstream consumption. (Parquet is also the default table open format
    for Delta Lake, which we will cover in [Chapter 9](ch09.html#building_reliable_data_lakes_with_apache).)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其高效性和这些优化，建议在转换和清洗数据后，将DataFrame保存为Parquet格式以供下游消费。（Parquet也是Delta Lake的默认表开放格式，我们将在[第9章](ch09.html#building_reliable_data_lakes_with_apache)中介绍。）
- en: Reading Parquet files into a DataFrame
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将Parquet文件读入DataFrame
- en: '[Parquet files](https://oreil.ly/CTVzK) are stored in a directory structure
    that contains the data files, metadata, a number of compressed files, and some
    status files. Metadata in the footer contains the version of the file format,
    the schema, and column data such as the path, etc.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[Parquet文件](https://oreil.ly/CTVzK)存储在包含数据文件、元数据、若干压缩文件和一些状态文件的目录结构中。页脚中的元数据包含文件格式的版本、模式、列数据（如路径等）等信息。'
- en: 'For example, a directory in a Parquet file might contain a set of files like
    this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Parquet文件目录可能包含一组文件如下所示：
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: There may be a number of *part-XXXX* compressed files in a directory (the names
    shown here have been shortened to fit on the page).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在目录中可能有一些压缩文件，文件名为*part-XXXX*（此处显示的名称已缩短以适应页面）。
- en: 'To read Parquet files into a DataFrame, you simply specify the format and path:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要将Parquet文件读入DataFrame，只需指定格式和路径：
- en: '[PRE32]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Unless you are reading from a streaming data source there’s no need to supply
    the schema, because Parquet saves it as part of its metadata.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 除非您从流数据源读取，否则无需提供模式，因为Parquet将其保存为其元数据的一部分。
- en: Reading Parquet files into a Spark SQL table
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将Parquet文件读入Spark SQL表
- en: 'As well as reading Parquet files into a Spark DataFrame, you can also create
    a Spark SQL unmanaged table or view directly using SQL:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将Parquet文件读入Spark DataFrame之外，您还可以直接使用SQL创建Spark SQL非托管表或视图：
- en: '[PRE34]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Once you’ve created the table or view, you can read data into a DataFrame using
    SQL, as we saw in some earlier examples:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 创建表或视图后，您可以使用SQL将数据读入DataFrame，就像我们在一些早期示例中看到的那样：
- en: '[PRE35]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Both of these operations return the same results:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个操作返回相同的结果：
- en: '[PRE37]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Writing DataFrames to Parquet files
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将DataFrame写入Parquet文件
- en: 'Writing or saving a DataFrame as a table or file is a common operation in Spark.
    To write a DataFrame you simply use the methods and arguments to the `DataFrameWriter`
    outlined earlier in this chapter, supplying the location to save the Parquet files
    to. For example:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，将DataFrame写入表或文件是常见操作。要写入DataFrame，只需使用本章早些时候介绍的`DataFrameWriter`方法和参数，并提供保存Parquet文件的位置。例如：
- en: '[PRE38]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Recall that Parquet is the default file format. If you don’t include the `format()`
    method, the DataFrame will still be saved as a Parquet file.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，Parquet是默认文件格式。如果不包含`format()`方法，DataFrame仍将保存为Parquet文件。
- en: 'This will create a set of compact and compressed Parquet files at the specified
    path. Since we used snappy as our compression choice here, we’ll have snappy compressed
    files. For brevity, this example generated only one file; normally, there may
    be a dozen or so files created:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在指定路径创建一组紧凑且压缩的Parquet文件。由于我们在这里选择了snappy作为压缩方式，因此我们将获得snappy压缩的文件。为简洁起见，本示例仅生成了一个文件；通常会创建十几个文件：
- en: '[PRE40]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Writing DataFrames to Spark SQL tables
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将DataFrame写入Spark SQL表
- en: 'Writing a DataFrame to a SQL table is as easy as writing to a file—just use
    `saveAsTable()` instead of `save()`. This will create a managed table called `us_delay_flights_tbl`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 将DataFrame写入SQL表与写入文件一样简单，只需使用`saveAsTable()`而不是`save()`。这将创建一个名为`us_delay_flights_tbl`的托管表：
- en: '[PRE41]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: To sum up, Parquet is the preferred and default built-in data source file format
    in Spark, and it has been adopted by many other frameworks. We recommend that
    you use this format in your ETL and data ingestion processes.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，Parquet是Spark中首选和默认的内置数据源文件格式，并已被许多其他框架采纳。我们建议您在ETL和数据摄入过程中使用这种格式。
- en: JSON
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JSON
- en: 'JavaScript Object Notation (JSON) is also a popular data format. It came to
    prominence as an easy-to-read and easy-to-parse format compared to XML. It has
    two representational formats: [single-line mode and multiline mode](https://oreil.ly/bBdLc).
    Both modes are supported in Spark.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[JavaScript对象表示法（JSON）](https://oreil.ly/bBdLc)也是一种流行的数据格式。与XML相比，它因易于阅读和解析而广受欢迎。它有两种表示格式：单行模式和多行模式。在Spark中，两种模式都受支持。'
- en: In single-line mode [each line denotes a single JSON object](http://jsonlines.org/),
    whereas in multiline mode the entire multiline object constitutes a single JSON
    object. To read in this mode, set `multiLine` to true in the `option()` method.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在单行模式下 [每行表示一个单独的 JSON 对象](http://jsonlines.org/)，而在多行模式下整个多行对象构成一个单独的 JSON
    对象。要在此模式下读取，请在 `option()` 方法中将 `multiLine` 设置为 true。
- en: Reading a JSON file into a DataFrame
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 JSON 文件读取到 DataFrame 中
- en: 'You can read a JSON file into a DataFrame the same way you did with Parquet—just
    specify `"json"` in the `format()` method:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像使用 Parquet 文件一样将 JSON 文件读入 DataFrame 中——只需在 `format()` 方法中指定 `"json"` 即可：
- en: '[PRE43]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Reading a JSON file into a Spark SQL table
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 JSON 文件读取到 Spark SQL 表中
- en: 'You can also create a SQL table from a JSON file just like you did with Parquet:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以像使用 Parquet 一样从 JSON 文件创建 SQL 表：
- en: '[PRE45]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Once the table is created, you can read data into a DataFrame using SQL:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 创建表格后，您可以像以前一样使用 SQL 将数据读入 DataFrame 中：
- en: '[PRE46]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Writing DataFrames to JSON files
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 DataFrame 写入 JSON 文件
- en: 'Saving a DataFrame as a JSON file is simple. Specify the appropriate `DataFrameWriter`
    methods and arguments, and supply the location to save the JSON files to:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 将 DataFrame 保存为 JSON 文件很简单。指定适当的 `DataFrameWriter` 方法和参数，并提供保存 JSON 文件的位置：
- en: '[PRE47]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This creates a directory at the specified path populated with a set of compact
    JSON files:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作会在指定路径创建一个目录，并填充一组紧凑的 JSON 文件：
- en: '[PRE49]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: JSON data source options
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JSON 数据源选项
- en: '[Table 4-3](#json_options_for_dataframereader_and_dat) describes common JSON
    options for [`DataFrameReader`](https://oreil.ly/iDZ2T) and [`DataFrameWriter`](https://oreil.ly/MunK1).
    For a comprehensive list, we refer you to the documentation.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 4-3](#json_options_for_dataframereader_and_dat) 描述了 [`DataFrameReader`](https://oreil.ly/iDZ2T)
    和 [`DataFrameWriter`](https://oreil.ly/MunK1) 的常见 JSON 选项。详细清单请参阅文档。'
- en: Table 4-3\. JSON options for DataFrameReader and DataFrameWriter
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4-3\. DataFrameReader 和 DataFrameWriter 的 JSON 选项
- en: '| Property name | Values | Meaning | Scope |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 属性名称 | 值 | 含义 | 范围 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `compression` | `none`, `uncompressed`, `bzip2`, `deflate`, `gzip`, `lz4`,
    or `snappy` | Use this compression codec for writing. Note that read will only
    detect the compression or codec from the file extension. | Write |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| `compression` | `none`, `uncompressed`, `bzip2`, `deflate`, `gzip`, `lz4`,
    或 `snappy` | 使用此压缩编解码器进行写入。请注意，读取将仅从文件扩展名检测压缩或编解码器。 | 写入 |'
- en: '| `dateFormat` | `yyyy-MM-dd` or `DateTimeFormatter` | Use this format or any
    format from Java’s `DateTimeFormatter`. | Read/write |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| `dateFormat` | `yyyy-MM-dd` 或 `DateTimeFormatter` | 使用此格式或 Java 的任何 `DateTimeFormatter`
    格式。 | 读/写 |'
- en: '| `multiLine` | `true`, `false` | Use multiline mode. Default is `false` (single-line
    mode). | Read |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| `multiLine` | `true`, `false` | 使用多行模式。默认为 `false`（单行模式）。 | 读取 |'
- en: '| `allowUnquotedFieldNames` | `true`, `false` | Allow unquoted JSON field names.
    Default is `false`. | Read |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| `allowUnquotedFieldNames` | `true`, `false` | 允许未引用的 JSON 字段名称。默认为 `false`。
    | 读取 |'
- en: CSV
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CSV
- en: As widely used as plain text files, this common text file format captures each
    datum or field delimited by a comma; each line with comma-separated fields represents
    a record. Even though a comma is the default separator, you may use other delimiters
    to separate fields in cases where commas are part of your data. Popular spreadsheets
    can generate CSV files, so it’s a popular format among data and business analysts.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通文本文件一样广泛使用的是这种通用文本文件格式，每个数据或字段由逗号分隔；每行逗号分隔的字段表示一个记录。即使逗号是默认分隔符，您也可以在数据中使用其他分隔符来分隔字段，以避免逗号作为数据的一部分。流行的电子表格可以生成
    CSV 文件，因此它在数据和业务分析师中很受欢迎的格式。
- en: Reading a CSV file into a DataFrame
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 CSV 文件读取到 DataFrame 中
- en: 'As with the other built-in data sources, you can use the `DataFrameReader`
    methods and arguments to read a CSV file into a DataFrame:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他内置数据源一样，您可以使用 `DataFrameReader` 方法和参数将 CSV 文件读取到 DataFrame 中：
- en: '[PRE50]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Reading a CSV file into a Spark SQL table
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 CSV 文件读取到 Spark SQL 表中
- en: 'Creating a SQL table from a CSV data source is no different from using Parquet
    or JSON:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 从 CSV 数据源创建 SQL 表与使用 Parquet 或 JSON 没有区别：
- en: '[PRE52]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Once you’ve created the table, you can read data into a DataFrame using SQL
    as before:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 创建表格后，您可以像以前一样使用 SQL 将数据读入 DataFrame 中：
- en: '[PRE53]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Writing DataFrames to CSV files
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 DataFrame 写入 CSV 文件
- en: 'Saving a DataFrame as a CSV file is simple. Specify the appropriate `DataFrameWriter`
    methods and arguments, and supply the location to save the CSV files to:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 将 DataFrame 保存为 CSV 文件很简单。指定适当的 `DataFrameWriter` 方法和参数，并提供保存 CSV 文件的位置：
- en: '[PRE54]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This generates a folder at the specified location, populated with a bunch of
    compressed and compact files:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在指定位置生成一个文件夹，并填充一堆压缩和紧凑的文件：
- en: '[PRE56]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: CSV data source options
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CSV 数据源选项
- en: '[Table 4-4](#csv_options_for_dataframereader_and_data) describes some of the
    common CSV options for [`DataFrameReader`](https://oreil.ly/Au6Kd) and [`DataFrameWriter`](https://oreil.ly/4g-vz).
    Because CSV files can be complex, many options are available; for a comprehensive
    list we refer you to the documentation.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 4-4](#csv_options_for_dataframereader_and_data) 描述了 [`DataFrameReader`](https://oreil.ly/Au6Kd)
    和 [`DataFrameWriter`](https://oreil.ly/4g-vz) 的一些常见 CSV 选项。因为 CSV 文件可能很复杂，提供了许多选项；详细列表请参阅文档。'
- en: Table 4-4\. CSV options for DataFrameReader and DataFrameWriter
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-4\. DataFrameReader 和 DataFrameWriter 的 CSV 选项
- en: '| Property name | Values | Meaning | Scope |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 属性名称 | 值 | 含义 | 范围 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `compression` | `none`, `bzip2`, `deflate`, `gzip`, `lz4`, or `snappy` |
    Use this compression codec for writing. | Write |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| `compression` | `none`, `bzip2`, `deflate`, `gzip`, `lz4` 或 `snappy` | 用于写入的压缩编解码器。
    | 写入 |'
- en: '| `dateFormat` | `yyyy-MM-dd` or `DateTimeFormatter` | Use this format or any
    format from Java’s `DateTimeFormatter`. | Read/write |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| `dateFormat` | `yyyy-MM-dd` 或 `DateTimeFormatter` | 使用此格式或 Java 的 `DateTimeFormatter`
    中的任何格式。 | 读取/写入 |'
- en: '| `multiLine` | `true`, `false` | Use multiline mode. Default is `false` (single-line
    mode). | Read |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| `multiLine` | `true`, `false` | 使用多行模式。默认为`false`（单行模式）。 | 读取 |'
- en: '| `inferSchema` | `true`, `false` | If `true`, Spark will determine the column
    data types. Default is `false`. | Read |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| `inferSchema` | `true`, `false` | 如果为 `true`，Spark 将确定列数据类型。默认为 `false`。
    | 读取 |'
- en: '| `sep` | Any character | Use this character to separate column values in a
    row. Default delimiter is a comma (`,`). | Read/write |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| `sep` | 任意字符 | 用于分隔行中列值的字符。默认分隔符为逗号（`,`）。 | 读取/写入 |'
- en: '| `escape` | Any character | Use this character to escape quotes. Default is
    `\`. | Read/write |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| `escape` | 任意字符 | 用于转义引号的字符。默认为 `\`。 | 读取/写入 |'
- en: '| `header` | `true`, `false` | Indicates whether the first line is a header
    denoting each column name. Default is `false`. | Read/write |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| `header` | `true`, `false` | 指示第一行是否为标头，表示每个列名。默认为 `false`。 | 读取/写入 |'
- en: Avro
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Avro
- en: Introduced in [Spark 2.4](https://oreil.ly/gqZl0) as a built-in data source,
    the [Avro format](https://oreil.ly/UaJoR) is used, for example, by [Apache Kafka](https://oreil.ly/jhdTI)
    for message serializing and deserializing. It offers many benefits, including
    direct mapping to JSON, speed and efficiency, and bindings available for many
    programming languages.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [Spark 2.4](https://oreil.ly/gqZl0) 中作为内置数据源引入，[Avro 格式](https://oreil.ly/UaJoR)
    被使用，例如由 [Apache Kafka](https://oreil.ly/jhdTI) 用于消息序列化和反序列化。它提供许多好处，包括直接映射到 JSON、速度和效率以及许多编程语言的绑定。
- en: Reading an Avro file into a DataFrame
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 Avro 文件读取到 DataFrame 中
- en: 'Reading an Avro file into a DataFrame using `DataFrameReader` is consistent
    in usage with the other data sources we have discussed in this section:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `DataFrameReader` 将 Avro 文件读取到 DataFrame 中，在本节中，它的使用方式与我们讨论过的其他数据源一致：
- en: '[PRE57]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Reading an Avro file into a Spark SQL table
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 Avro 文件读取到 Spark SQL 表中
- en: 'Again, creating SQL tables using an Avro data source is no different from using
    Parquet, JSON, or CSV:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，使用 Avro 数据源创建 SQL 表与使用 Parquet、JSON 或 CSV 没有什么不同：
- en: '[PRE59]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Once you’ve created a table, you can read data into a DataFrame using SQL:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 创建表后，可以使用 SQL 将数据读取到 DataFrame 中：
- en: '[PRE60]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Writing DataFrames to Avro files
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 DataFrame 写入 Avro 文件
- en: 'Writing a DataFrame as an Avro file is simple. As usual, specify the appropriate
    `DataFrameWriter` methods and arguments, and supply the location to save the Avro
    files to:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 将 DataFrame 写入 Avro 文件很简单。像往常一样，指定适当的 `DataFrameWriter` 方法和参数，并提供保存 Avro 文件的位置：
- en: '[PRE62]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'This generates a folder at the specified location, populated with a bunch of
    compressed and compact files:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在指定位置生成一个文件夹，其中包含一堆压缩和紧凑的文件：
- en: '[PRE64]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Avro data source options
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Avro 数据源选项
- en: '[Table 4-5](#avro_options_for_dataframereader_and_dat) describes common options
    for `DataFrameReader` and `DataFrameWriter`. A comprehensive list of options is
    in the [documentation](https://oreil.ly/Jvrd_).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 4-5](#avro_options_for_dataframereader_and_dat) 描述了 `DataFrameReader` 和
    `DataFrameWriter` 的常见选项。详细列表请参阅[文档](https://oreil.ly/Jvrd_)。'
- en: Table 4-5\. Avro options for DataFrameReader and DataFrameWriter
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-5\. DataFrameReader 和 DataFrameWriter 的 Avro 选项
- en: '| Property name | Default value | Meaning | Scope |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 属性名称 | 默认值 | 含义 | 范围 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `avroSchema` | None | Optional Avro schema provided by a user in JSON format.
    The data type and naming of record fields should match the input Avro data or
    Catalyst data (Spark internal data type), otherwise the read/write action will
    fail. | Read/write |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| `avroSchema` | None | 用户以 JSON 格式提供的可选 Avro 模式。记录字段的数据类型和命名应与输入的 Avro 数据或
    Catalyst 数据（Spark 内部数据类型）匹配，否则读写操作将失败。 | 读/写 |'
- en: '| `recordName` | `topLevelRecord` | Top-level record name in write result,
    which is required in the Avro spec. | Write |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| `recordName` | `topLevelRecord` | 写结果中的顶级记录名称，Avro 规范要求的。 | 写 |'
- en: '| `recordNamespace` | `""` | Record namespace in write result. | Write |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| `recordNamespace` | `""` | 写结果中的记录命名空间。 | 写 |'
- en: '| `ignoreExtension` | `true` | If this option is enabled, all files (with and
    without the *.avro* extension) are loaded. Otherwise, files without the *.avro*
    extension are ignored. | Read |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| `ignoreExtension` | `true` | 如果启用此选项，则加载所有文件（带有和不带有 *.avro* 扩展名的文件）。否则，将忽略没有
    *.avro* 扩展名的文件。 | 读 |'
- en: '| `compression` | `snappy` | Allows you to specify the compression codec to
    use in writing. Currently supported codecs are `uncompressed`, `snappy`, `deflate`,
    `bzip2`, and `xz`. If this option is not set, the value in `spark.sql.avro.compression.codec`
    is taken into account. | Write |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| `compression` | `snappy` | 允许您指定写入时要使用的压缩编解码器。当前支持的编解码器包括 `uncompressed`、`snappy`、`deflate`、`bzip2`
    和 `xz`。如果未设置此选项，则将考虑 `spark.sql.avro.compression.codec` 中的值。 | 写 |'
- en: ORC
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ORC
- en: As an additional optimized columnar file format, Spark 2.x supports a [vectorized
    ORC reader](https://oreil.ly/N_Brd). Two Spark configurations dictate which ORC
    implementation to use. When `spark.sql.orc.impl` is set to `native` and `spark.sql.orc.enableVectorizedReader`
    is set to `true`, Spark uses the vectorized ORC reader. A [vectorized reader](https://oreil.ly/E2xiZ)
    reads blocks of rows (often 1,024 per block) instead of one row at a time, streamlining
    operations and reducing CPU usage for intensive operations like scans, filters,
    aggregations, and joins.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一种优化的列式文件格式，Spark 2.x 支持[矢量化 ORC 读取器](https://oreil.ly/N_Brd)。两个 Spark 配置决定了使用哪种
    ORC 实现。当 `spark.sql.orc.impl` 设置为 `native` 且 `spark.sql.orc.enableVectorizedReader`
    设置为 `true` 时，Spark 使用矢量化 ORC 读取器。[矢量化读取器](https://oreil.ly/E2xiZ)一次读取数据块（通常每块
    1,024 行），而不是逐行读取，从而优化了扫描、过滤、聚合和连接等密集操作，减少 CPU 使用。
- en: For Hive ORC SerDe (serialization and deserialization) tables created with the
    SQL command `USING HIVE OPTIONS (fileFormat 'ORC')`, the vectorized reader is
    used when the Spark configuration parameter `spark.sql.hive.convertMetastoreOrc`
    is set to `true`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用 SQL 命令 `USING HIVE OPTIONS (fileFormat 'ORC')` 创建的 Hive ORC SerDe（序列化和反序列化）表，当
    Spark 配置参数 `spark.sql.hive.convertMetastoreOrc` 设置为 `true` 时，使用矢量化读取器。
- en: Reading an ORC file into a DataFrame
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 ORC 文件读取为 DataFrame
- en: 'To read in a DataFrame using the ORC vectorized reader, you can just use the
    normal `DataFrameReader` methods and options:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 ORC 矢量化读取器读取 DataFrame，您只需使用常规的 `DataFrameReader` 方法和选项：
- en: '[PRE65]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Reading an ORC file into a Spark SQL table
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 ORC 文件读入 Spark SQL 表中
- en: 'There is no difference from Parquet, JSON, CSV, or Avro when creating a SQL
    view using an ORC data source:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ORC 数据源创建 SQL 视图时与 Parquet、JSON、CSV 或 Avro 没有区别：
- en: '[PRE67]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Once a table is created, you can read data into a DataFrame using SQL as usual:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 创建表后，您可以像往常一样使用 SQL 将数据读入 DataFrame：
- en: '[PRE68]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Writing DataFrames to ORC files
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 DataFrame 写入 ORC 文件
- en: 'Writing back a transformed DataFrame after reading is equally simple using
    the `DataFrameWriter` methods:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取后写回转换后的 DataFrame 同样简单，使用 `DataFrameWriter` 方法：
- en: '[PRE69]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The result will be a folder at the specified location containing some compressed
    ORC files:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是一个指定位置包含一些压缩 ORC 文件的文件夹：
- en: '[PRE71]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Images
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像
- en: In Spark 2.4 the community introduced a new data source, [image files](https://oreil.ly/JfKBD),
    to support deep learning and machine learning frameworks such as TensorFlow and
    PyTorch. For computer vision–based machine learning applications, loading and
    processing image data sets is important.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 2.4 中，社区引入了一个新的数据源，[图像文件](https://oreil.ly/JfKBD)，以支持深度学习和机器学习框架，如 TensorFlow
    和 PyTorch。对于基于计算机视觉的机器学习应用，加载和处理图像数据集至关重要。
- en: Reading an image file into a DataFrame
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将图像文件读入 DataFrame
- en: 'As with all of the previous file formats, you can use the `DataFrameReader`
    methods and options to read in an image file as shown here:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前所有文件格式一样，您可以使用 `DataFrameReader` 方法和选项来读取图像文件，如下所示：
- en: '[PRE72]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Binary Files
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二进制文件
- en: 'Spark 3.0 adds support for [binary files as a data source](https://oreil.ly/UXHZl).
    The `DataFrameReader` converts each binary file into a single DataFrame row (record)
    that contains the raw content and metadata of the file. The binary file data source
    produces a DataFrame with the following columns:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 3.0增加了对[二进制文件作为数据源的支持](https://oreil.ly/UXHZl)。`DataFrameReader`将每个二进制文件转换为包含文件的原始内容和元数据的单个DataFrame行（记录）。二进制文件数据源生成一个包含以下列的DataFrame：
- en: 'path: StringType'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'path: StringType'
- en: 'modificationTime: TimestampType'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'modificationTime: TimestampType'
- en: 'length: LongType'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'length: LongType'
- en: 'content: BinaryType'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'content: BinaryType'
- en: Reading a binary file into a DataFrame
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将二进制文件读取为DataFrame
- en: 'To read binary files, specify the data source format as a `binaryFile`. You
    can load files with paths matching a given global pattern while preserving the
    behavior of partition discovery with the data source option `pathGlobFilter`.
    For example, the following code reads all JPG files from the input directory with
    any partitioned directories:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取二进制文件，请将数据源格式指定为`binaryFile`。您可以使用数据源选项`pathGlobFilter`加载符合给定全局模式的路径的文件，同时保留分区发现的行为。例如，以下代码从输入目录中读取所有JPG文件，包括任何分区目录：
- en: '[PRE74]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'To ignore partitioning data discovery in a directory, you can set `recursiveFileLookup`
    to `"true"`:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 要忽略目录中的分区数据发现，您可以将`recursiveFileLookup`设置为`"true"`：
- en: '[PRE76]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Note that the `label` column is absent when the `recursiveFileLookup` option
    is set to `"true"`.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 当将`recursiveFileLookup`选项设置为`"true"`时，注意`label`列将不存在。
- en: Currently, the binary file data source does not support writing a DataFrame
    back to the original file format.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，二进制文件数据源不支持将DataFrame写回原始文件格式。
- en: In this section, you got a tour of how to read data into a DataFrame from a
    range of supported file formats. We also showed you how to create temporary views
    and tables from the existing built-in data sources. Whether you’re using the DataFrame
    API or SQL, the queries produce identical outcomes. You can examine some of these
    queries in the notebook available in the [GitHub repo](https://github.com/databricks/LearningSparkV2)
    for this book.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解如何从一系列支持的文件格式中将数据读取到DataFrame中。我们还展示了如何从现有的内置数据源创建临时视图和表。无论您使用DataFrame
    API还是SQL，查询都会产生相同的结果。您可以在本书的[GitHub存储库](https://github.com/databricks/LearningSparkV2)中的笔记本中查看其中一些查询。
- en: Summary
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'To recap, this chapter explored the interoperability between the DataFrame
    API and Spark SQL. In particular, you got a flavor of how to use Spark SQL to:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，本章探讨了DataFrame API与Spark SQL之间的互操作性。特别是，您了解了如何使用Spark SQL来：
- en: Create managed and unmanaged tables using Spark SQL and the DataFrame API.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark SQL和DataFrame API创建托管和非托管表。
- en: Read from and write to various built-in data sources and file formats.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从各种内置数据源和文件格式读取和写入。
- en: Employ the `spark.sql` programmatic interface to issue SQL queries on structured
    data stored as Spark SQL tables or views.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`spark.sql`编程接口来对存储为Spark SQL表或视图的结构化数据发出SQL查询。
- en: Peruse the Spark `Catalog` to inspect metadata associated with tables and views.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浏览Spark `Catalog`以检查与表和视图相关联的元数据。
- en: Use the `DataFrameWriter` and `DataFrameReader` APIs.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`DataFrameWriter`和`DataFrameReader` API。
- en: Through the code snippets in the chapter and the notebooks available in the
    book’s [GitHub repo](https://github.com/databricks/LearningSparkV2), you got a
    feel for how to use DataFrames and Spark SQL. Continuing in this vein, the next
    chapter further explores how Spark interacts with the external data sources shown
    in [Figure 4-1](#spark_sql_usage_and_interface). You’ll see some more in-depth
    examples of transformations and the interoperability between the DataFrame API
    and Spark SQL.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章的代码片段和本书的[GitHub存储库](https://github.com/databricks/LearningSparkV2)中的笔记本，您可以了解如何使用DataFrame和Spark
    SQL。在这个过程中，下一章进一步探讨了Spark如何与[图4-1](#spark_sql_usage_and_interface)中显示的外部数据源进行交互。您将看到更多关于转换和DataFrame
    API与Spark SQL互操作性的深入示例。
