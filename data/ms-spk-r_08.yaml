- en: Chapter 7\. Connections
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章。连接
- en: They don’t get to choose.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 他们没有选择权。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Daenerys Targaryen
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —丹妮莉丝·坦格利安
- en: '[Chapter 6](ch06.html#clusters) presented the major cluster computing trends,
    cluster managers, distributions, and cloud service providers to help you choose
    the Spark cluster that best suits your needs. In contrast, this chapter presents
    the internal components of a Spark cluster and how to connect to a particular
    Spark cluster.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 6 章](ch06.html#clusters) 展示了主要的集群计算趋势、集群管理器、分发和云服务提供商，帮助您选择最适合您需求的 Spark
    集群。相比之下，本章介绍了 Spark 集群的内部组件以及如何连接到特定的 Spark 集群。'
- en: When reading this chapter, don’t try to execute every line of code; this would
    be quite hard since you would need to prepare different Spark environments. Instead,
    if you already have a Spark cluster or if the previous chapter gets you motivated
    enough to sign up for an on-demand cluster, now is the time to learn how to connect
    to it. This chapter helps you connect to your cluster, which you should have already
    chosen by now. Without a cluster, we recommend that you learn the concepts and
    come back to execute code later on.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章时，请不要尝试执行每一行代码；这将非常困难，因为您需要准备不同的 Spark 环境。相反，如果您已经有了 Spark 集群，或者前一章激发了您注册按需集群的兴趣，现在是学习如何连接的时候了。本章帮助您连接到您已经选择的集群。如果没有集群，我们建议您先学习这些概念，然后再回来执行代码。
- en: In addition, this chapter provides various troubleshooting connection techniques.
    While we hope you won’t need to use them, this chapter prepares you to use them
    as effective techniques to resolve connectivity issues.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本章提供了各种故障排除连接技术。虽然希望您不需要使用它们，但本章将您准备好使用它们作为解决连接问题的有效技术。
- en: While this chapter might feel a bit dry—connecting and troubleshooting connections
    is definitely not the most exciting part of large-scale computing—it introduces
    the components of a Spark cluster and how they interact, often known as the *architecture*
    of Apache Spark. This chapter, along with Chapters [8](ch08.html#data) and [9](ch09.html#tuning),
    will provide a detailed view of how Spark works, which will help you move toward
    becoming an intermediate Spark user who can truly understand the exciting world
    of distributed computing using Apache Spark.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章可能会让人感到有些枯燥——连接和故障排除连接显然不是大规模计算中最令人兴奋的部分——它介绍了 Apache Spark 的组件以及它们如何交互，通常被称为
    Apache Spark 的*架构*。本章与第 [8](ch08.html#data) 章和第 [9](ch09.html#tuning) 章一起，将详细介绍
    Spark 的工作原理，帮助您成为能够真正理解使用 Apache Spark 进行分布式计算的中级用户。
- en: Overview
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概览
- en: 'The overall connection architecture for a Spark cluster is composed of three
    types of compute instances: the *driver node*, the *worker nodes*, and the *cluster
    manager*. A cluster manager is a service that allows Spark to be executed in the
    cluster; this was detailed in [“Managers”](ch06.html#clusters-manager). The worker
    nodes (also referred to as *executors*) execute compute tasks over partitioned
    data and communicate intermediate results to other workers or back to the driver
    node. The driver node is tasked with delegating work to the worker nodes, but
    also with aggregating their results and controlling computation flow. For the
    most part, aggregation happens in the worker nodes; however, even after the nodes
    aggregate data, it is often the case that the driver node would need to collect
    the worker’s results. Therefore, the driver node usually has at least, but often
    much more, compute resources (memory, CPUs, local storage, etc.) as the worker
    node.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 集群的整体连接架构由三种类型的计算实例组成：*驱动节点*、*工作节点*和*集群管理器*。集群管理器是一个允许 Spark 在集群中执行的服务；这在
    [“管理器”](ch06.html#clusters-manager) 中有详细说明。工作节点（也称为*执行者*）执行分区数据上的计算任务，并与其他工作节点或驱动节点交换中间结果。驱动节点负责将工作委派给工作节点，同时汇总它们的结果并控制计算流程。在大多数情况下，聚合发生在工作节点中；然而，即使在节点聚合数据之后，驱动节点通常也需要收集工作节点的结果。因此，驱动节点通常至少具有，但通常比工作节点拥有更多的计算资源（内存、CPU、本地存储等）。
- en: Strictly speaking, the driver node and worker nodes are just names assigned
    to machines with particular roles, while the actual computation in the driver
    node is performed by the *Spark context*. The Spark context is the main entry
    point for Spark functionality since it’s tasked with scheduling tasks, managing
    storage, tracking execution status, specifying access configuration settings,
    canceling jobs, and so on. In the worker nodes, the actual computation is performed
    under a *spark executor*, which is a Spark component tasked with executing subtasks
    against a specific data partition.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，驱动节点和工作节点只是赋予特定角色机器的名称，而驱动节点中的实际计算由*Spark 上下文*执行。Spark 上下文是 Spark 功能的主要入口点，负责调度任务、管理存储、跟踪执行状态、指定访问配置设置、取消作业等。在工作节点中，实际计算是由*Spark
    executor*执行的，它是负责针对特定数据分区执行子任务的 Spark 组件。
- en: '[Figure 7-1](#connections-architecture) illustrates this concept, where the
    driver node orchestrates a worker’s work through the cluster manager.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-1](#connections-architecture)说明了这一概念，驱动节点通过集群管理器协调工作节点的工作。'
- en: '![Apache Spark architecture](assets/mswr_0701.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark 架构](assets/mswr_0701.png)'
- en: Figure 7-1\. Apache Spark connection architecture
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. Apache Spark 连接架构
- en: If you already have a Spark cluster in your organization, you should request
    the connection information to this cluster from your cluster administrator, read
    their usage policies carefully, and follow their advice. Since a cluster can be
    shared among many users, you want to ensure that you request only the compute
    resources you need. We cover how to request resources in [Chapter 9](ch09.html#tuning).
    Your system administrator will specify whether it’s an *on-premises* or *cloud*
    cluster, the cluster manager being used, supported *connections*, and supported
    *tools*. You can use this information to jump directly to [Local](#connections-local),
    [Standalone](#connections-standalone), [YARN](#connections-yarn), [Mesos](#connections-mesos),
    [Livy](#connections-livy), or [Kubernetes](#connections-kubernetes) based on which
    is appropriate for your situation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的组织中已经有了 Spark 集群，您应该向集群管理员请求连接到该集群的连接信息，仔细阅读其使用政策，并遵循其建议。由于一个集群可以被多个用户共享，您希望确保仅请求您所需的计算资源。我们在[第 9 章](ch09.html#tuning)中介绍了如何请求资源。系统管理员将指定集群是*本地*还是*云*集群，使用的集群管理器，支持的*连接*和支持的*工具*。您可以使用这些信息直接跳转到适合您情况的[Local](#connections-local)、[Standalone](#connections-standalone)、[YARN](#connections-yarn)、[Mesos](#connections-mesos)、[Livy](#connections-livy)或[Kubernetes](#connections-kubernetes)。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: After you’ve used `spark_connect()` to connect, you can use all the techniques
    described in previous chapters using the `sc` connection; for instance, you can
    do data analysis or modeling with the same code previous chapters presented.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `spark_connect()` 连接后，您可以使用 `sc` 连接使用前面章节中介绍的所有技术；例如，您可以使用相同代码进行数据分析或建模。
- en: Edge Nodes
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 边缘节点
- en: Computing clusters are configured to enable high bandwidth and fast network
    connectivity between nodes. To optimize network connectivity, the nodes in the
    cluster are configured to trust one another and to disable security features.
    This improves performance but requires you to close all external network communication,
    making the entire cluster secure as a whole except for a few cluster machines
    that are carefully configured to accept connections from outside the cluster;
    conceptually, these machines are located in the “edge” of the cluster and are
    known as *edge nodes*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 计算集群配置为在节点之间实现高带宽和快速网络连接。为了优化网络连接，集群中的节点被配置为相互信任并禁用安全功能。这样可以提高性能，但需要关闭所有外部网络通信，使整个集群在整体上变得安全，除了一些仔细配置为接受外部连接的集群机器之外；从概念上讲，这些机器位于集群的“边缘”，称为*边缘节点*。
- en: 'Therefore, before connecting to Apache Spark, it is likely that you will first
    need to connect to an edge node in your cluster. There are two methods to connect:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在连接到 Apache Spark 之前，您可能需要先连接到集群中的一个边缘节点。有两种方法可以连接：
- en: Terminal
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 终端
- en: Using a computer terminal application, you can use a [Secure Shell](http://bit.ly/2TE8cY9)
    to establish a remote connection into the cluster; after you connect into the
    cluster, you can launch R and then use `sparklyr`. However, a terminal can be
    cumbersome for some tasks, like exploratory data analysis, so it’s often used
    only while configuring the cluster or troubleshooting issues.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用计算机终端应用程序，您可以使用[安全外壳](http://bit.ly/2TE8cY9)建立到集群的远程连接；连接到集群后，您可以启动R然后使用`sparklyr`。然而，对于一些任务，比如探索性数据分析，终端可能不太方便，因此通常仅在配置集群或解决问题时使用。
- en: Web browser
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Web浏览器
- en: While using `sparklyr` from a terminal is possible, it is usually more productive
    to install a *web server* in an edge node that provides access to run R with `sparklyr`
    from a web browser. Most likely, you will want to consider using RStudio or Jupyter
    rather than connecting from the terminal.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以从终端使用`sparklyr`，但通常在边缘节点上安装*Web服务器*更具生产力，该服务器提供通过Web浏览器运行带有`sparklyr`的R的访问。大多数情况下，您可能希望考虑使用RStudio或Jupyter而不是从终端连接。
- en: '[Figure 7-2](#connections-spark-edge) explains these concepts visually. The
    left block is usually your web browser, and the right block is the edge node.
    Client and edge nodes communicate over HTTP when using a web browser or Secure
    Shell (SSH) when using the terminal.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-2](#connections-spark-edge)通过可视化方式解释了这些概念。左边的块通常是您的Web浏览器，右边的块是边缘节点。在使用Web浏览器时，客户端和边缘节点通过HTTP进行通信；在使用终端时，通过安全外壳（SSH）进行通信。'
- en: '![Connecting to Spark’s edge node](assets/mswr_0702.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![连接到Spark的边缘节点](assets/mswr_0702.png)'
- en: Figure 7-2\. Connecting to Spark’s edge node
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. 连接到Spark的边缘节点
- en: Spark Home
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark主页
- en: After you connect to an edge node, the next step is to determine where Spark
    is installed, a location known as the `SPARK_HOME`. In most cases, your cluster
    administrator will have already set the `SPARK_HOME` environment variable to the
    correct installation path. If not, you will need to get the correct *SPARK_HOME*
    path. You must specify the `SPARK_HOME` path as an environment variable or explicitly
    when running `spark_connect()` using the `spark_home` parameter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到边缘节点后的下一步是确定Spark安装的位置，这个位置称为`SPARK_HOME`。在大多数情况下，您的集群管理员将已经设置了`SPARK_HOME`环境变量以指向正确的安装路径。如果没有，则必须获取正确的*SPARK_HOME*路径，并在运行`spark_connect()`时使用`spark_home`参数显式指定。
- en: 'If your cluster provider or cluster administrator already provided `SPARK_HOME`
    for you, the following code should return a path instead of an empty string:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的集群提供程序或集群管理员已为您提供了`SPARK_HOME`，则以下代码应返回路径而不是空字符串：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If this code returns an empty string, this would mean that the `SPARK_HOME`
    environment variable is not set in your cluster, so you will need to specify `SPARK_HOME`
    while using `spark_connect()`, as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果此代码返回空字符串，则意味着您的集群中未设置`SPARK_HOME`环境变量，因此您需要在使用`spark_connect()`时指定`SPARK_HOME`，如下所示：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this example, `master` is set to the correct cluster manager master for [Spark
    Standalone](#connections-standalone), [YARN](#connections-yarn), [Mesos](#connections-mesos),
    [Kubernetes](#connections-kubernetes), or [Livy](#connections-livy).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，`master`被设置为[Spark Standalone](#connections-standalone)，[YARN](#connections-yarn)，[Mesos](#connections-mesos)，[Kubernetes](#connections-kubernetes)，或[Livy](#connections-livy)的正确集群管理器主节点。
- en: Local
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地
- en: When you connect to Spark in local mode, Spark starts a single process that
    runs most of the cluster components like the Spark context and a single executor.
    This is ideal to learn Spark, work offline, troubleshoot issues, or test code
    before you run it over a large compute cluster. [Figure 7-3](#connections-local-diagram)
    depicts a local connection to Spark.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当您连接到本地模式的Spark时，Spark会启动一个单进程，该进程运行大部分集群组件，如Spark上下文和单个执行器。这非常适合学习Spark、离线工作、故障排除问题或在运行大型计算集群之前测试代码。[图 7-3](#connections-local-diagram)展示了连接到Spark的本地连接。
- en: '![Local connection diagram](assets/mswr_0703.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![本地连接图表](assets/mswr_0703.png)'
- en: Figure 7-3\. Local connection diagram
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 本地连接图表
- en: Notice that there is neither a cluster manager nor worker process since, in
    local mode, everything runs inside the driver application. It’s also worth noting
    that `sparklyr` starts the Spark context through `spark-submit`, a script available
    in every Spark installation to enable users to submit custom applications to Spark.
    If you’re curious, [Chapter 13](ch13.html#contributing) explains the internal
    processes that take place in `sparklyr` to submit this application and connect
    properly from R.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在本地模式下，既没有集群管理器也没有工作进程，因为一切都在驱动应用程序内部运行。值得注意的是，`sparklyr`通过`spark-submit`启动Spark上下文，这是每个Spark安装中都有的脚本，允许用户提交自定义应用程序到Spark。如果你感兴趣，[第13章](ch13.html#contributing)解释了在`sparklyr`中提交此应用程序并从R正确连接时发生的内部流程。
- en: 'To perform this local connection, we can use the following familiar code from
    previous chapters:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此本地连接，我们可以使用前几章节中熟悉的代码：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Standalone
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独立模式
- en: Connecting to a Spark Standalone cluster requires the location of the cluster
    manager’s master instance, which you can find in the cluster manager web interface
    as described in the [“Standalone”](ch06.html#clusters-standalone) section. You
    can find this location by looking for a URL starting with `spark://`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到Spark独立集群需要集群管理器主实例的位置，您可以在集群管理器Web界面上找到它，如[“独立”](ch06.html#clusters-standalone)章节所述。您可以通过查找以`spark://`开头的URL来找到此位置。
- en: A connection in standalone mode starts from `sparklyr`, which launches `spark-submit`,
    which then submits the `sparklyr` application and creates the Spark Context, which
    requests executors from the Spark Standalone instance running under the given
    `master` address.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在独立模式下的连接始于`sparklyr`，它启动`spark-submit`，然后提交`sparklyr`应用程序并创建Spark上下文，该上下文请求来自指定`master`地址下运行的Spark独立实例的执行器。
- en: '[Figure 7-4](#connections-standalone-diagram) illustrates this process, which
    is quite similar to the overall connection architecture from [Figure 7-1](#connections-architecture)
    but with additional details that are particular to standalone clusters and `sparklyr`.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-4](#connections-standalone-diagram)说明了这个过程，它与[图7-1](#connections-architecture)中的整体连接架构非常相似，但包含了针对独立集群和`sparklyr`的特定细节。'
- en: '![Spark Standalone connection diagram](assets/mswr_0704.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![Spark独立连接图解](assets/mswr_0704.png)'
- en: Figure 7-4\. Spark Standalone connection diagram
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4. Spark独立连接图解
- en: 'To connect, use `master = "spark://hostname:port"` in `spark_connect()` as
    follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接，请在`spark_connect()`中使用`master = "spark://hostname:port"`，如下所示：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: YARN
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YARN
- en: 'Hadoop YARN is the cluster manager from the Hadoop project. It’s the most common
    cluster manager that you are likely to find in clusters, which started out as
    Hadoop clusters; with Cloudera, Hortonworks, and MapR distributions as when using
    Amazon EMR. YARN supports two connection modes: YARN client and YARN cluster.
    However, YARN client mode is much more common than YARN cluster since it’s more
    efficient and easier to set up.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop YARN是来自Hadoop项目的集群管理器。它是最常见的集群管理器，您可能会在以Hadoop集群为基础的集群中找到，包括Cloudera、Hortonworks和MapR发行版，以及在使用Amazon
    EMR时。YARN支持两种连接模式：YARN客户端和YARN集群。然而，与YARN集群相比，YARN客户端模式更为常见，因为它更高效且更容易设置。
- en: YARN Client
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YARN客户端
- en: When you connect in YARN client mode, the driver instance runs R, `sparklyr`,
    and the Spark context, which requests worker nodes from YARN to run Spark executors,
    as shown in [Figure 7-5](#connections-yarn-client-diagram).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当你以YARN客户端模式连接时，驱动实例运行R、`sparklyr`和Spark上下文，它请求YARN从YARN获取工作节点以运行Spark执行器，如[图7-5](#connections-yarn-client-diagram)所示。
- en: '![YARN client connection diagram](assets/mswr_0705.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![YARN客户端连接图解](assets/mswr_0705.png)'
- en: Figure 7-5\. YARN client connection diagram
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5. YARN客户端连接图解
- en: 'To connect, you simply run with `master = "yarn"`, as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接，只需运行`master = "yarn"`，如下所示：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Behind the scenes, when you’re running YARN in client mode, the cluster manager
    will do what you would expect a cluster manager would do: it allocates resources
    from the cluster and assigns them to your Spark application, which the Spark context
    will manage for you. The important piece to notice in [“YARN”](#connections-yarn)
    is that the Spark context resides in the same machine where you run R code; this
    is different when you’re running YARN in cluster mode.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，当你以YARN客户端模式运行时，集群管理器会执行你期望集群管理器执行的操作：从集群分配资源并将它们分配给你的Spark应用程序，由Spark上下文为您管理。在[“YARN”](#connections-yarn)中需要注意的重要部分是，Spark上下文驻留在您运行R代码的同一台机器上；而在集群模式下运行YARN时则不同。
- en: YARN Cluster
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YARN集群
- en: The main difference between running YARN in cluster mode and running YARN in
    client mode is that, in cluster mode, the driver node is not required to be the
    node where R and `sparklyr` were launched; instead, the driver node remains the
    designated driver node, which is usually a different node than the edge node where
    R is running. It can be helpful to consider using cluster mode when the edge node
    has too many concurrent users, when it is lacking computing resources, or when
    tools (like RStudio or Jupyter) need to be managed independently of other cluster
    resources.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群模式和客户端模式下运行 YARN 的主要区别在于，在集群模式下，驱动节点不需要是运行 R 和 `sparklyr` 的节点；相反，驱动节点仍然是指定的驱动节点，通常是运行
    R 的边缘节点的不同节点。当边缘节点具有过多并发用户、缺乏计算资源或需要独立管理工具（如 RStudio 或 Jupyter）时，考虑使用集群模式可能会有所帮助。
- en: '[Figure 7-6](#connections-yarn-cluster-diagram) shows how the different components
    become decoupled when running in cluster mode. Notice there is still a line connecting
    the client with the cluster manager since, first of all, resources still need
    to be allocated from the cluster manager; however, after they’re allocated, the
    client communicates directly with the driver node, which communicates with the
    worker nodes. From [Figure 7-6](#connections-yarn-cluster-diagram), you might
    think that cluster mode looks much more complicated than client mode—this would
    be a correct assessment; therefore, if possible, it’s best to avoid cluster mode
    due to its additional configuration overhead.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-6](#connections-yarn-cluster-diagram) 显示了在集群模式下运行时不同组件如何解耦。请注意，仍然存在一条连接线将客户端与集群管理器连接在一起；但是，在分配资源后，客户端直接与驱动节点通信，驱动节点再与工作节点通信。从
    [图 7-6](#connections-yarn-cluster-diagram) 可以看出，集群模式看起来比客户端模式复杂得多——这种评估是正确的；因此，如果可能的话，最好避免使用集群模式，因为它会增加额外的配置开销。'
- en: '![YARN cluster connection diagram](assets/mswr_0706.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![YARN 集群连接图示](assets/mswr_0706.png)'
- en: Figure 7-6\. YARN cluster connection diagram
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. YARN 集群连接图示
- en: 'To connect in YARN cluster mode, simply run the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 YARN 集群模式下连接，只需运行以下命令：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Cluster mode assumes that the node running `spark_connect()` is properly configured,
    meaning that `yarn-site.xml` exists and the `YARN_CONF_DIR` environment variable
    is properly set. When using Hadoop as a file system, you will also need the `HADOOP_CONF_DIR`
    environment variable properly configured. In addition, you would need to ensure
    proper network connectivity between the client and the driver node—not just by
    having both machines reachable, but also by making sure that they have sufficient
    bandwidth between them. This configuration is usually provided by your system
    administrator and is not something that you would need to manually configure.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 集群模式假设运行 `spark_connect()` 的节点已经正确配置，即 `yarn-site.xml` 存在并且 `YARN_CONF_DIR`
    环境变量已经正确设置。当使用 Hadoop 作为文件系统时，您还需要正确配置 `HADOOP_CONF_DIR` 环境变量。此外，您需要确保客户端和驱动节点之间的网络连接良好——不仅仅是两台机器可以相互访问，还要确保它们之间有足够的带宽。通常情况下，这些配置由系统管理员提供，不是您需要手动配置的内容。
- en: Livy
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Livy
- en: As opposed to other connection methods that require using an edge node in the
    cluster, [Livy](ch06.html#clusters-livy) provides a *web API* that makes the Spark
    cluster accessible from outside the cluster and does not require a Spark installation
    in the client. After it’s connected through the web API, the *Livy Service* starts
    the Spark context by requesting resources from the cluster manager and distributing
    work as usual. [Figure 7-7](#connections-livy-diagram) illustrates a Livy connection;
    notice that the client connects remotely to the driver through a web API.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与需要在集群中使用边缘节点的其他连接方法相反，[Livy](ch06.html#clusters-livy) 提供了一个 *web API*，可以从集群外访问
    Spark 集群，并且不需要在客户端安装 Spark。通过 web API 连接后，*Livy 服务* 通过向集群管理器请求资源并像往常一样分发工作来启动
    Spark 上下文。[图 7-7](#connections-livy-diagram) 展示了一个 Livy 连接示例；请注意，客户端通过 web API
    远程连接到驱动节点。
- en: '![Livy connection diagram](assets/mswr_0707.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![Livy 连接图示](assets/mswr_0707.png)'
- en: Figure 7-7\. Livy connection diagram
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-7\. Livy 连接图示
- en: 'Connecting through Livy requires the URL to the Livy service, which should
    be similar to `https://hostname:port/livy`. Since remote connections are allowed,
    connections usually require, at the very least, basic authentication:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Livy 连接需要 Livy 服务的 URL，类似于 `https://hostname:port/livy`。由于允许远程连接，连接通常至少需要基本认证：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To try out Livy on your local machine, you can install and run a Livy service
    as described under the [“Livy”](ch06.html#clusters-livy) section and then connect
    as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要在本地机器上尝试Livy，您可以安装和运行Livy服务，如“Livy”章节所述，然后按以下方式连接：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: After you’re connected through Livy, you can make use of any `sparklyr` feature;
    however, Livy is not suitable for exploratory data analysis, since executing commands
    has a significant performance cost. That said, while running long-running computations,
    this overhead could be considered irrelevant. In general, you should prefer to
    avoid using Livy and work directly within an edge node in the cluster; when this
    is not feasible, using Livy could be a reasonable approach.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 连接通过Livy后，您可以使用任何`sparklyr`功能；但是，Livy不适合探索性数据分析，因为执行命令会有显著的性能成本。尽管如此，在运行长时间计算时，这种开销可能被认为是不重要的。总的来说，您应该尽量避免使用Livy，并直接在集群的边缘节点上工作；当不可行时，使用Livy可能是一个合理的方法。
- en: Note
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Specifying the Spark version through the `spark_version` parameter is optional;
    however, when the version is specified, performance is significantly improved
    by deploying precompiled Java binaries compatible with the given version. Therefore,
    it is a best practice to specify the Spark version when connecting to Spark using
    Livy.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`spark_version`参数指定Spark版本是可选的；但是，当指定版本时，通过部署与给定版本兼容的预编译Java二进制文件，可以显著提高性能。因此，连接到Spark使用Livy时最佳实践是指定Spark版本。
- en: Mesos
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mesos
- en: Similar to YARN, Mesos supports client mode and a cluster mode; however, `sparklyr`
    currently supports only client mode under Mesos. Therefore, the diagram shown
    in [Figure 7-8](#connections-mesos-diagram) is equivalent to YARN client’s diagram
    with only the cluster manager changed from YARN to Mesos.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与YARN类似，Mesos支持客户端模式和集群模式；但是，`sparklyr`目前仅支持Mesos下的客户端模式。因此，图7-8所示的图表与仅将集群管理器从YARN更改为Mesos的YARN客户端图表相当。
- en: '![Mesos connection diagram](assets/mswr_0708.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![Mesos连接图](assets/mswr_0708.png)'
- en: Figure 7-8\. Mesos connection diagram
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8\. Mesos连接图
- en: 'Connecting requires the address to the Mesos master node, usually in the form
    of `mesos://host:port` or `mesos://zk://host1:2181,host2:2181,host3:2181/mesos`
    for Mesos using ZooKeeper:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 连接需要Mesos主节点的地址，通常形式为`mesos://host:port`或者对于使用ZooKeeper的Mesos为`mesos://zk://host1:2181,host2:2181,host3:2181/mesos`：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `MESOS_NATIVE_JAVA_LIBRARY` environment variable needs to be set by your
    system administrator or manually set when you are running Mesos on your local
    machine. For instance, in macOS, you can install and initialize Mesos from a terminal,
    followed by manually setting the `mesos` library and connecting with `spark_connect()`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在本地机器上运行Mesos时，需要由系统管理员或手动设置`MESOS_NATIVE_JAVA_LIBRARY`环境变量。例如，在macOS上，您可以从终端安装和初始化Mesos，然后手动设置`mesos`库并使用`spark_connect()`连接：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Kubernetes
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Kubernetes clusters do not support client modes like Mesos or YARN; instead,
    the connection model is similar to YARN cluster, where the driver node is assigned
    by Kubernetes, as illustrated in [Figure 7-9](#connections-kubernetes-diagram).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群不支持像Mesos或YARN那样的客户端模式；相反，连接模型类似于YARN集群，其中由Kubernetes分配驱动节点，如图7-9所示。
- en: '![Kubernetes connection diagram](assets/mswr_0709.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes连接图](assets/mswr_0709.png)'
- en: Figure 7-9\. Kubernetes connection diagram
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9\. Kubernetes连接图
- en: 'To use Kubernetes, you need to prepare a virtual machine with Spark installed
    and properly configured; however, it is beyond the scope of this book to present
    how to create one. Once created, connecting to Kubernetes works as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Kubernetes，您需要准备一个安装了Spark并正确配置的虚拟机；但是，本书不涵盖如何创建虚拟机的范围。一旦创建，连接到Kubernetes的工作方式如下：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If your computer is already configured to use a Kubernetes cluster, you can
    use the following command to find the `apiserver-host` and `apiserver-port`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的计算机已配置为使用Kubernetes集群，您可以使用以下命令查找`apiserver-host`和`apiserver-port`：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Cloud
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云端
- en: 'When you are working with cloud providers, there are a few connection differences.
    For instance, connecting from Databricks requires the following connection method:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用云服务提供商时，有几点连接差异。例如，从Databricks连接需要以下连接方法：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Since Amazon EMR makes use of YARN, you can connect using `master = "yarn"`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Amazon EMR使用YARN，您可以使用`master = "yarn"`连接：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Connecting to Spark when using IBM’s Watson Studio requires you to retrieve
    a configuration object through a `load_spark_kernels()` function that IBM provides:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 IBM 的 Watson Studio 连接到 Spark 时，需要通过 IBM 提供的 `load_spark_kernels()` 函数检索配置对象：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In Microsoft Azure HDInsights and when using ML Services (R Server), a Spark
    connection is initialized as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Microsoft Azure HDInsights 和使用 ML 服务（R Server）时，Spark 连接初始化如下：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Connecting from Qubole requires using the `qubole` connection method:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Qubole 连接需要使用 `qubole` 连接方法：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Refer to your cloud provider’s documentation and support channels if you need
    help.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要帮助，请参考您的云服务提供商的文档和支持渠道。
- en: Batches
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批处理
- en: Most of the time, you use `sparklyr` interactively; that is, you explicitly
    connect with `spark_connect()` and then execute commands to analyze and model
    large-scale data. However, you can also automate processes by scheduling Spark
    jobs that use `sparklyr`. Spark does not provide tools to schedule data-processing
    tasks; instead, you would use other workflow management tools. This can be useful
    to transform data, prepare a model and score data overnight, or to make use of
    Spark by other systems.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分时间，您会交互式地使用 `sparklyr`；也就是说，您会明确使用 `spark_connect()` 进行连接，然后执行命令来分析和建模大规模数据。然而，您也可以通过调度使用
    `sparklyr` 的 Spark 作业来自动化流程。Spark 并没有提供调度数据处理任务的工具；相反，您可以使用其他工作流管理工具。这对于在夜间转换数据、准备模型和得分数据或通过其他系统利用
    Spark 非常有用。
- en: 'As an example, you can create a file named `batch.R` with the following contents:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以创建一个名为 `batch.R` 的文件，其内容如下：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You then can submit this application to Spark in batch mode using `spark_submit()`;
    the `master` parameter should be set appropriately:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您随后可以使用 `spark_submit()` 将此应用程序以批处理模式提交到 Spark；`master` 参数应适当设置：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can also invoke `spark-submit` from the shell directly through the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以直接从 shell 中通过以下方式调用 `spark-submit`：
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The last parameters represent the port number `8880` and the session number
    `12345`, which you can set to any unique numeric identifier. You can use the following
    R code to get the correct paths:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的参数表示端口号 `8880` 和会话号 `12345`，您可以将其设置为任何唯一的数值标识符。您可以使用以下 R 代码获取正确的路径：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You can customize your script by passing additional command-line arguments to
    `spark-submit` and then read them back in R using `commandArgs()`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过向 `spark-submit` 传递额外的命令行参数来自定义脚本，然后在 R 中使用 `commandArgs()` 读取这些参数。
- en: Tools
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具
- en: When connecting to a Spark cluster using tools like Jupyter and RStudio, you
    can run the same connection parameters presented in this chapter. However, since
    many cloud providers make use of a web proxy to secure Spark’s web interface,
    to use `spark_web()` or the RStudio Connections pane extension, you need to properly
    configure the `sparklyr.web.spark` setting, which you would then pass to `spark_config()`
    through the `config` parameter.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用像 Jupyter 和 RStudio 这样的工具连接到 Spark 集群时，您可以运行本章中提供的相同连接参数。然而，由于许多云提供商使用 Web
    代理来保护 Spark 的 Web 界面，要使用 `spark_web()` 或 RStudio Connections 窗格扩展，您需要正确配置 `sparklyr.web.spark`
    设置，然后通过 `config` 参数传递给 `spark_config()`。
- en: 'For instance, when using Amazon EMR, you can configure `sparklyr.web.spark`
    and `sparklyr.web.yarn` by dynamically retrieving the YARN application and building
    the EMR proxy URL:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在使用 Amazon EMR 时，您可以通过动态检索 YARN 应用程序和构建 EMR 代理 URL 来配置 `sparklyr.web.spark`
    和 `sparklyr.web.yarn`：
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Multiple Connections
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多连接
- en: 'It is common to connect once, and only once, to Spark. However, you can also
    open multiple connections to Spark by connecting to different clusters or by specifying
    the `app_name` parameter. This can be helpful to compare Spark versions or validate
    your analysis before submitting to the cluster. The following example opens connections
    to Spark 1.6.3, 2.3.0 and Spark Standalone:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通常只需一次连接到 Spark。但是，您也可以通过连接到不同的集群或指定 `app_name` 参数来打开多个连接到 Spark。这对于比较 Spark
    版本或在提交到集群之前验证分析结果非常有帮助。以下示例打开到 Spark 1.6.3、2.3.0 和 Spark Standalone 的连接：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, you can disconnect from each connection:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以断开每个连接：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Alternatively, you can disconnect from all connections at once:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您也可以一次断开所有连接：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Troubleshooting
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除
- en: 'Last but not least, we introduce the following troubleshooting techniques:
    *Logging*, *Spark Submit*, and *Windows*. When in doubt about where to begin,
    start with the Windows section when using Windows systems, followed by Logging
    and finally Spark Submit. These techniques are useful when running `spark_connect()`
    fails with an error message.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们介绍以下排查技术：*日志记录*、*Spark提交*和*Windows*。当对如何开始感到犹豫不决时，请从使用Windows系统的Windows部分开始，然后是日志记录，最后是Spark提交。在使用`spark_connect()`时失败并显示错误消息时，这些技术非常有用。
- en: Logging
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志记录
- en: 'The first technique to troubleshoot connections is to print Spark logs directly
    to the console to help you spot additional error messages:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 排查连接问题的第一技术是直接将Spark日志打印到控制台，以帮助您发现额外的错误消息：
- en: '[PRE26]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In addition, you can enable verbose logging by setting the `sparklyr.verbose`
    option to `TRUE` when connecting:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当连接时，您可以通过将`sparklyr.verbose`选项设置为`TRUE`来启用详细日志记录：
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Spark Submit
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提交Spark作业
- en: 'You can diagnose whether a connection issue is specific to R or Spark in general
    by running an example job through `spark-submit` and validating that no errors
    are thrown:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行示例作业通过`spark-submit`来诊断连接问题是否特定于R或Spark：
- en: '[PRE28]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, execute the sample compute Pi example by replacing `"local"` with the
    correct master parameter that you are troubleshooting:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过将`"local"`替换为您正在排查的正确主节点参数，执行样本计算Pi示例：
- en: '[PRE29]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If the preceding message is not displayed, you will need to investigate why
    your Spark cluster is not properly configured, which is beyond the scope of this
    book. As a start, rerun the Pi example but remove `stderr = FALSE`; this prints
    errors to the console, which you then can use to investigate what the problem
    might be. When using a cloud provider or a Spark distribution, you can contact
    their support team to help you troubleshoot this further; otherwise, Stack Overflow
    is a good place to start.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未显示前述消息，则需要调查您的Spark集群为何未正确配置，这超出了本书的范围。作为开始，重新运行Pi示例但删除`stderr = FALSE`；这会将错误打印到控制台，您可以使用这些错误来调查可能出现的问题。在使用云提供商或Spark分发时，您可以联系其支持团队以帮助您进一步排查；否则，Stack
    Overflow是一个很好的起点。
- en: If you do see the message, this means that your Spark cluster is properly configured
    but somehow R is not able to use Spark, so you need to troubleshoot in detail,
    as we will explain next.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看到该消息，则表示您的Spark集群已正确配置，但某种方式R无法使用Spark，因此您需要详细排查问题，如我们将在接下来解释的那样。
- en: Detailed troubleshooting
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 详细排查
- en: To troubleshoot the connection process in detail, you can manually replicate
    the two-step connection process, which is often very helpful to diagnose connection
    issues. First, `spark-submit` is triggered from R, which submits the application
    to Spark; second, R connects to the running Spark application.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要详细排查连接过程，您可以手动复制两步连接过程，这通常非常有助于诊断连接问题。首先，从R触发`spark-submit`，将应用程序提交到Spark；其次，R连接到运行中的Spark应用程序。
- en: 'First, [identify the Spark installation directory](#troubleshoot-spark-submit)
    and the path to the correct `sparklyr*.jar` file by running the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，[确定Spark安装目录](#troubleshoot-spark-submit)和运行以下命令来查找正确的`sparklyr*.jar`文件的路径：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Ensure that you identify the correct version that matches your Spark cluster—for
    instance, `sparklyr-2.1-2.11.jar` for Spark 2.1.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您识别与您的Spark集群匹配的正确版本，例如，适用于Spark 2.1的`sparklyr-2.1-2.11.jar`。
- en: 'Then, from the terminal, run this:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从终端运行以下命令：
- en: '[PRE32]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The parameter `8880` represents the default port to use in `sparklyr`, while
    42 is the session number, which is a cryptographically secure number generated
    by `sparklyr`, but for troubleshooting purposes can be as simple as `42`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`8880`代表在`sparklyr`中使用的默认端口，而42是会话号码，它是由`sparklyr`生成的密码安全数字，但出于排查目的可以简单地是`42`。
- en: If this first connection step fails, it means that the cluster can’t accept
    the application. This usually means that there are not enough resources, or there
    are permission restrictions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果此第一连接步骤失败，则表示集群无法接受该应用程序。这通常意味着资源不足或存在权限限制。
- en: 'The second step is to connect from R as follows (notice that there is a 60-second
    timeout, so you’ll need to run the R command after running the terminal command;
    if needed, you can configure this timeout as described in [Chapter 9](ch09.html#tuning)):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是从R连接如下（请注意，有一个60秒的超时，因此您需要在运行终端命令后运行R命令；如果需要，您可以按照[第9章](ch09.html#tuning)中描述的配置此超时）：
- en: '[PRE34]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: If this second connection step fails, it usually means that there is a connectivity
    problem between R and the driver node. You can try using a different connection
    port, for instance.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果第二次连接尝试失败，通常意味着 R 和驱动节点之间存在连接问题。你可以尝试使用不同的连接端口。
- en: Windows
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Windows
- en: 'Connecting from Windows is, in most cases, as straightforward as connecting
    from Linux and macOS. However, there are a few common connection issues that you
    should be aware of:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，从 Windows 连接与从 Linux 和 macOS 连接一样简单。但是，有一些常见的连接问题需要注意：
- en: Firewalls and antivirus software might block ports for your connection. The
    default port used by `sparklyr` is `8880`; double-check that this port is not
    being blocked.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防火墙和防病毒软件可能会阻止连接端口。`sparklyr` 使用的默认端口是 `8880`；请确保此端口未被阻止。
- en: Long path names can cause issues, especially with older Windows systems like
    Windows 7\. When you’re using these systems, try connecting with Spark installed
    with all folders, using at most eight characters and no spaces in their names.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长路径名可能会导致问题，特别是在像 Windows 7 这样的旧系统中。当使用这些系统时，尝试使用最多八个字符且名称中不包含空格的所有文件夹安装的 Spark
    进行连接。
- en: Recap
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小结
- en: This chapter presented an overview of Spark’s architecture, connection concepts,
    and examples to connect in local mode, standalone, YARN, Mesos, Kubernetes, and
    Livy. It also presented edge nodes and their role while connecting to Spark clusters.
    This should have provided you with enough information to successfully connect
    to any Apache Spark cluster.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了 Spark 的架构、连接概念，并提供了在本地模式、独立模式、YARN、Mesos、Kubernetes 和 Livy 连接的示例。还介绍了边缘节点及其在连接到
    Spark 集群时的角色。这些信息应足以帮助你成功连接到任何 Apache Spark 集群。
- en: To troubleshoot connection problems beyond the techniques described in this
    chapter, we recommend that you search for the connection problem in Stack Overflow,
    the [`sparklyr` issues GitHub page](http://bit.ly/2Z72XWa), and, if needed, open
    a [new GitHub issue in `sparklyr`](http://bit.ly/2HasCmq) to assist further.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 若要解决本章未描述的连接问题，请建议在 Stack Overflow、[`sparklyr` 问题 GitHub 页面](http://bit.ly/2Z72XWa)，以及必要时在
    [新的 `sparklyr` GitHub 问题](http://bit.ly/2HasCmq) 中进一步协助。
- en: In [Chapter 8](ch08.html#data), we cover how to use Spark to read and write
    from a variety of data sources and formats, which allows you to be more agile
    when adding new data sources for data analysis. What used to take days, weeks,
    or even months, you now can complete in hours by embracing data lakes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 8 章](ch08.html#data) 中，我们介绍了如何使用 Spark 从各种数据源和格式读取和写入数据，这使得您在添加新数据源进行数据分析时更加灵活。过去可能需要花费几天、几周甚至几个月的工作，现在可以在几小时内完成。
