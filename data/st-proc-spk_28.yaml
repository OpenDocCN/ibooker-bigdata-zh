- en: Chapter 22\. Arbitrary Stateful Streaming Computation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第22章 任意的有状态流计算
- en: So far, we have seen how Spark Streaming can work on the incoming data independently
    of past records. In many applications, we are also interested in analyzing the
    evolution of the data that arrives with respect to older data points. We could
    also be interested in tracking changes generated by the received data points.
    That is, we might be interested in building a stateful representation of a system
    using the data that we have already seen.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到Spark Streaming如何独立于过去的记录处理传入数据。在许多应用中，我们还对分析与旧数据点相关的数据的演变感兴趣。我们也可能对跟踪由接收到的数据点生成的变化感兴趣。也就是说，我们可能有兴趣使用我们已经看到的数据来构建系统的有状态表示。
- en: Spark Streaming provides several functions that let us build and store knowledge
    about previously seen data as well as use that knowledge to transform new data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming提供了几个函数，让我们能够构建和存储关于先前看到的数据的知识，并使用该知识来转换新数据。
- en: Statefulness at the Scale of a Stream
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流的可扩展状态性
- en: Functional programmers like functions without statefulness. These functions
    return values that are independent of the state of the world outside their function
    definition, caring only about the value of their input.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式程序员喜欢没有状态性的函数。这些函数返回与其函数定义之外的世界状态无关的值，只关心其输入值的值。
- en: However, a function can be stateless, care only about its input, and yet maintain
    a notion of a managed value along with its computation, without breaking any rules
    about being functional. The idea is that this value, representing some intermediate
    state, is used in the traversal of one or several arguments of the computation,
    to keep some record simultaneously with the traversal of the argument’s structure.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个函数可以是无状态的，只关心它的输入，但同时保持与其计算相关的受管值的概念，而不违反任何有关功能性的规则。其思想是，表示某些中间状态的这个值，在计算的一个或多个参数的遍历中使用，以在遍历参数结构时同时保持一些记录。
- en: 'For example, the `reduce` operation discussed in [Chapter 17](ch17.xhtml#sps-programming-model)
    keeps one single value updated along the traversal of whichever RDD it was given
    as an argument:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[第17章](ch17.xhtml#sps-programming-model)讨论的`reduce`操作会沿着给定为参数的任何RDD更新一个单一值：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, the computation of the intermediate sums for each RDD along the input
    DStream is made by iterating over the elements of the RDD, from left to right,
    and keeping an accumulator variable updated—an operation specified thanks to the
    update operation that returns the new value of the accumulator (between brackets).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，计算每个RDD沿输入DStream的中间和是通过迭代RDD的元素从左到右进行的，并保持累加器变量更新——这是通过返回累加器新值的更新操作指定的（在括号内）。
- en: updateStateByKey
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: updateStateByKey
- en: 'Sometimes it is useful to compute some result that depends on the previous
    elements of the stream, which took place more than one batch before the current
    one. Examples of such cases include the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有时计算一些依赖于流的先前元素的结果是有用的，这些先前元素发生在当前批次之前超过一个批次。此类情况的示例包括以下内容：
- en: The running sum of all elements of the stream
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流的所有元素的运行总和
- en: The number of occurrences of a specific, marker value
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定标记值的出现次数
- en: The highest elements encountered in the stream, given a particular ordering
    of the elements of the stream
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流中遇到的最高元素，给定流元素的特定顺序
- en: 'This computation can often be thought of as the result of a big `reduce` operation,
    that would update some representation of the state of a computation, all along
    the traversal of the stream. In Spark Streaming, this is offered by the `updateStateByKey`
    function:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种计算通常可以被看作是一个大的`reduce`操作的结果，它会在整个流的遍历过程中更新某个计算状态的表示。在Spark Streaming中，这由`updateStateByKey`函数提供：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`updateStateBykey` is an operation that is defined only on DStreams of key–value
    pairs. It takes a state update function as an argument. This state update function
    should have the following type:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`updateStateBykey`是仅在键-值对DStream上定义的操作。它将一个状态更新函数作为参数。此状态更新函数应具有以下类型：'
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This type reflects how the update operation takes a set of new values for type
    `V`, which corresponds to all *values* for a given *key* that arrive during the
    operation of the current batch, and an optional state represented with type `S`.
    It then computes and returns a new value for the state `S` as `Some(state)` if
    there is one to return or `None` if there’s no new state, in which case, the stored
    state corresponding to this key is deleted from the internal representation:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此类型反映了更新操作如何接受一组新值类型`V`，这些值对应于当前批处理操作期间到达的给定键的所有*值*，以及类型`S`表示的可选状态。然后，如果有要返回的状态`S`，则计算并返回新状态值为`Some(state)`；如果没有新状态，则返回`None`，在这种情况下，删除与此键对应的存储状态的内部表示：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The update state function is called, on each batch, on all of the keys that
    the executor encounters since the beginning of processing this stream. In some
    cases, this is on a new key that was never previously seen. This is the case in
    which the second argument of the update function, the state, is `None`. In other
    cases, it will be on a key for which no new values have come in this batch, in
    which case the first argument of the update function, the new value, is `Nil`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 更新状态函数在每个批次上调用，在执行器处理此流的开始时遇到的所有键上。在某些情况下，这是在以前从未见过的新键上。这是更新函数的第二个参数（状态）为`None`的情况。在其他情况下，它将在此批次中未收到新值的键上，此时更新函数的第一个参数（新值）为`Nil`。
- en: 'Finally, the `updateStateByKey` function returns a value (that is, a snapshot
    of the new state for a particular key), only when the user’s updates mandate it
    should. This explains the `Option` in the return type of the function: in the
    previous example, we update the state only when we actually encounter integers
    greater than five. If a particular key encounters only values less than five,
    there will be no state created for this key, and correspondingly no update.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`updateStateByKey`函数仅在用户更新要求时返回值（即特定键的新状态快照）。这解释了函数返回类型中的`Option`：在前面的示例中，只有在实际遇到大于五的整数时，我们才会更新状态。如果特定键仅遇到小于五的值，则不会为此键创建状态，并相应地不进行更新。
- en: '[Figure 22-1](#update-state-by-key-dataflow) depicts the dynamics of the internal
    state preserved when using a stateful computation such as `updateStateByKey`.
    The intermediate state of the stream is kept in an internal state store. At each
    batch interval, the internal state is combined with the new data coming from the
    stream using the `updateFunc` function, producing a secondary stream with the
    current results of the stateful computation.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 22-1](#update-state-by-key-dataflow)描述了使用诸如`updateStateByKey`之类的状态计算时保留的内部状态动态。流的中间状态保留在内部状态存储中。在每个批次间隔中，内部状态与来自流的新数据使用`updateFunc`函数组合，产生一个当前状态计算结果的次级流。'
- en: '![spas 2201](Images/spas_2201.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2201](Images/spas_2201.png)'
- en: Figure 22-1\. The data flow produced by updateStateByKey
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 22-1\. 由`updateStateByKey`产生的数据流动态
- en: Mandatory Checkpointing on Stateful Streams
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 状态流的强制检查点
- en: 'Note that when starting a Spark Streaming Context for this application, Spark
    might output the following error:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在为此应用程序启动Spark Streaming Context时，Spark可能会输出以下错误：
- en: 'java.lang.IllegalArgumentException: requirement failed: The checkpoint directory
    has not been set. Please set it by `StreamingContext.checkpoint()`'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'java.lang.IllegalArgumentException: requirement failed: 未设置检查点目录。请通过`StreamingContext.checkpoint()`设置。'
- en: The reason for this is that the `StateStream` created under the hood by `updateStateByKey`
    has RDDs that inherently each have a dependency on the previous ones, meaning
    that the only way to reconstruct the chain of partial sums at each batch interval
    for each hashtag is to replay the entire stream. This does not play well with
    fault tolerance because we would need to preserve every record received to be
    able to recreate the state at some arbitrary point in time. Instead of preserving
    all records, we save the intermediary result of state to disk. In case one of
    the executors working on this stream crashes, it can recover from this intermediate
    state.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为由`updateStateByKey`在幕后创建的`StateStream`具有RDDs，这些RDDs本质上各自依赖于先前的RDD，这意味着在每个批次间隔内重新构建每个标签的部分总和链的唯一方法是重播整个流。这与容错性不兼容，因为我们需要保留接收到的每个记录以便能够在任意时间点重建状态。而不是保留所有记录，我们将状态的中间结果保存到磁盘中。如果处理此流的执行器之一崩溃，则可以从此中间状态中恢复。
- en: Luckily, the error tells us exactly how to do so, using `ssc.checkpoint("path/to/checkpoint/dir")`.
    Replace the content of the `String` passed as an argument with a directory in
    a shared filesystem, accessible by the driver and all executors that are part
    of the job.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，错误告诉我们如何使用 `ssc.checkpoint("path/to/checkpoint/dir")` 来做到这一点。用共享文件系统中的目录替换作为参数传递的
    `String` 的内容，该目录可由驱动程序和作业中的所有执行器访问。
- en: Limitation of updateStateByKey
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`updateStateByKey` 的限制'
- en: The `updateStateByKey` function that we have described so far allows us to do
    stateful programming using Spark Streaming. It allows us to encode, for example,
    the concept of user sessions—for which no particular batch interval is a clear
    match to the application at hand. However, there are two problems with this approach.
    Let’s look at those a bit more closely.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止描述的 `updateStateByKey` 函数使我们能够使用Spark Streaming进行有状态编程。例如，它允许我们对用户会话的概念进行编码——对于这种应用程序，没有特定的批次间隔与之匹配。然而，这种方法存在两个问题。让我们更仔细地看看这些问题。
- en: Performance
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: 'The first problem is related to performance: this `updateStateByKey` function
    is run on every single key encountered by the framework of the application since
    the beginning of the run. This is problematic because, even on a dataset that
    is somewhat sparse—provided there is a long tail to the variety of the data and
    in particular, in the variety of its keys—there is a clear argument that the total
    amount of data represented in memory grows indefinitely.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题与性能有关：自应用程序框架启动以来，`updateStateByKey` 函数在遇到的每个键上都运行。这是个问题，因为即使在数据集相对稀疏的情况下——只要数据的种类很多，并且特别是键的种类很多——可以清楚地证明内存中表示的数据总量是无限增长的。
- en: For example, if a key or a particular user is seen on a website at the beginning
    of the run of the application, what is the relevance of updating the state of
    that user to signify that we have not seen a session from this particular individual
    since the beginning of the application (e.g., since last month)? The benefits
    to the application are not clear.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果在应用程序运行的开始时看到网站上的某个键或特定用户，更新该用户的状态以表示我们自应用程序开始以来（例如，上个月以来）未见过此特定个体的会话的相关性是什么？这对应用程序的好处并不明显。
- en: Memory Usage
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存使用
- en: The second issue is that because the state should not grow indefinitely, the
    programmer must do the bookkeeping of memory by themselves—writing code for every
    key, to figure out whether it is still relevant to keep data in state for that
    particular element. This is a complexity that requires manual accounting for memory
    management.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是，因为状态不应无限增长，程序员必须自行管理内存的簿记——为每个键编写代码，以确定是否仍然有必要保留该特定元素的状态数据。这是一种需要手动进行内存管理的复杂性。
- en: 'Indeed, for most stateful computations, dealing with state is a simple operation:
    either a key is still relevant, such as a user who has visited a website within
    a certain time frame, or it hasn’t been refreshed in a while.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，对于大多数有状态计算，处理状态是一个简单的操作：要么一个键仍然相关，例如某个用户在一定时间范围内访问了网站，要么在一段时间内未刷新。
- en: Introducing Stateful Computation with mapwithState
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入带有 `mapwithState` 的有状态计算
- en: '`mapWithState` is a better model for stateful updates in Spark, which overcomes
    the two aforementioned shortcomings: updating every key and setting default timeouts
    to limit the size of the state objects created along with the computation. It
    was introduced in Spark 1.5:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapWithState` 是在Spark中进行有状态更新的更好模型，它克服了上述两个缺点：对每个键进行更新，并设置默认的超时时间以限制与计算一起创建的状态对象的大小。它是在Spark
    1.5中引入的：'
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`mapWithState` requires you to write a `StateSpec` function that operates on
    a state specification containing a key, an optional value, and a `State` object.
    Even though this is apparently more complex, because it involves a few explicit
    types, it simplifies a number of elements:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapWithState` 需要您编写一个 `StateSpec` 函数，该函数操作包含键、可选值和 `State` 对象的状态规范。尽管这显然更复杂，因为涉及到几种显式类型，但它简化了许多元素：'
- en: The programmer operates on values one by one rather than as a list
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 程序员逐个值操作，而不是作为列表
- en: The update function has access to the key itself
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新函数可以访问键本身
- en: This update is run only on keys that have a new value in the current batch
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此更新仅在当前批次中具有新值的键上运行
- en: Updating the state is an imperative call to a method of the state object, rather
    than the implicit act of producing an output
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新状态是对状态对象方法的命令式调用，而不是生成输出的隐式行为
- en: The programmer can now produce an output independently of state management
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 程序员现在可以独立于状态管理生成输出
- en: The function has an automatic timeout
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该函数具有自动超时功能
- en: '[Figure 22-2](#dataflow_mapwithstate) presents the data flow when using the
    `mapWithState` function.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 22-2](#dataflow_mapwithstate) 展示了使用 `mapWithState` 函数时的数据流程。'
- en: '![spas 2202](Images/spas_2202.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![spas 2202](Images/spas_2202.png)'
- en: Figure 22-2\. The dataflow produced by mapWithState
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 22-2\. 使用 `mapWithState` 产生的数据流
- en: If you want to see a snapshot of the state for each and every key on each and
    every batch interval, call the `.snapshots` function on your particular DStream
    created by `mapWithState`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望在每个批次间隔的每个键上看到状态的快照，请在通过 `mapWithState` 创建的特定 DStream 上调用 `.snapshots`
    函数。
- en: 'mapWithState and updateStateByKey: When to Use Which'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`mapWithState` 和 `updateStateByKey`：何时使用哪一个'
- en: '`mapWithState` is more performant and nicer to use than the `updateStateByKey`
    function, and the former is indeed a good default choice for stateful computation.
    However, one caveat is that the model for pushing data out of the state representation
    (aka flushing State data) is specifically a timeout, and no longer user controlled.
    As a consequence, `mapWithState` is particularly appropriate if you want to keep
    state under a freshness condition (e.g., the number of clicks on a time-bound
    session of a web user). We would prefer `updateStateByKey` for the niche cases
    for which we absolutely want to guarantee maintaining a small state over a long
    period of time.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapWithState` 的性能更高且使用更方便，比 `updateStateByKey` 函数更好，并且通常是处理有状态计算的一个很好的默认选择。然而，一个注意点是，将数据从状态表示中推出（即刷新状态数据）的模型是特定的超时机制，不再由用户控制。因此，如果您希望保持状态处于新鲜条件下（例如，网页用户时间限定会话的点击数），则
    `mapWithState` 尤为适合。对于那些绝对需要保证在长时间内保持小状态的特定情况，我们可以选择 `updateStateByKey`。'
- en: 'An example is flood watching: if we are dealing with sensors reporting water
    heights at specific locations near a river, and if we want to keep the largest
    values ever observed over the length of our observation, it might make sense to
    use `updateStateBykey` rather than `mapWithState`.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是洪水监控：如果我们处理的是在河流附近特定位置报告水位的传感器，并且我们想要在观察期间保持观察到的最大值，那么使用 `updateStateByKey`
    而不是 `mapWithState` 可能是有意义的。
- en: With `mapWithState`, the computation on elements of our stream can start as
    events are being received and, ideally, completes shortly after the last few structural
    events are received. We are going to see an example of this in the next few pages.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `mapWithState`，我们的流元素计算可以在接收到事件时开始，并在接收到最后几个结构事件后尽快完成。我们将在接下来的几页中看到一个示例。
- en: Using mapWithState
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `mapWithState`
- en: '`mapWithState` requires the user to provide a `StateSpec` object, which describes
    the workings of the state computation. The core piece of this is, naturally, the
    function that takes in new values for a given key and returns an output as well
    as updates the state for this key. In fact, the builder object for this `StateSpec`
    makes this function mandatory.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapWithState` 要求用户提供一个 `StateSpec` 对象，描述状态计算的工作原理。其中的核心部分是一个函数，该函数接受给定键的新值并返回一个输出，同时更新该键的状态。事实上，对于
    `StateSpec` 的构建对象来说，这个函数是必需的。'
- en: 'This `StateSpec` is parameterized by four types:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `StateSpec` 是由四种类型参数化的：
- en: Key type `K`
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键类型 `K`
- en: Value type `V`
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值类型 `V`
- en: State type `S`, representing the type used to store the state
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态类型 `S`，表示用于存储状态的类型
- en: Output type `U`
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出类型 `U`
- en: In its most general form, the `StateSpec` builder, `StateSpec.function`, requires
    a `(Time, K, Option[V], State[S]) => Option[U]` argument, or a `(K, Option[V],
    State[S]) => Option[U]` if you don’t need the timestamp of the batch that `mapWithState`
    comes with.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最一般的形式中，`StateSpec` 构建器 `StateSpec.function` 需要一个 `(Time, K, Option[V], State[S])
    => Option[U]` 参数，或者如果您不需要 `mapWithState` 附带的批处理时间戳，则可以使用 `(K, Option[V], State[S])
    => Option[U]`。
- en: 'The state type that intervenes in the definition of this function can be seen
    as a mutable cell with support for timeout. You can query it by using `state.exists()`
    and `state.get()`, or even treat it like an option with `state.getOption()`, you
    can check whether it’s timing out by using `state.isTimingOut()`, erase it using
    `state.remove()`, or update it using `state.update(newState: S)`.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个函数定义中涉及的状态类型可以看作是一个支持超时的可变单元。您可以使用 `state.exists()` 和 `state.get()` 进行查询，或者像处理选项一样使用
    `state.getOption()`，通过 `state.isTimingOut()` 检查是否超时，使用 `state.remove()` 进行擦除，或使用
    `state.update(newState: S)` 进行更新。'
- en: 'Let’s assume that we’re monitoring a plant with sensors and we want both the
    average temperature over the last batch and a straightforward way of detecting
    anomalous temperatures. For this exercise, let’s define an anomalous temperature
    to be higher than 80 degrees:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在监控一家带传感器的工厂，并且我们想要最后一批的平均温度以及检测异常温度的简单方法。对于这个练习，让我们定义异常温度为高于 80 度：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this function, we extract the old maximum value and run both an aggregation
    of the latest value with the mean and with the threshold value, routing the results
    to, correspondingly, the state update and our output value. This lets us exploit
    two streams:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们提取旧的最大值，并同时对最新值进行均值和阈值的聚合，将结果分别路由到状态更新和输出值。这使我们可以利用两个流：
- en: '`temperatureStream.mapWithState(highTempStateSpec)`, which tracks high temperatures
    as they occur'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperatureStream.mapWithState(highTempStateSpec)`，用于跟踪发生的高温'
- en: '`temperatureStream.mapWithState(highTempStateSpec).stateSnapshots()`, which
    tracks the mean temperatures for each sensor'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperatureStream.mapWithState(highTempStateSpec).stateSnapshots()`，用于跟踪每个传感器的平均温度'
- en: If a sensor stops emitting for 60 minutes, its state is removed automatically,
    preventing the state storage explosion that we feared. Note that we could have
    used the explicit `remove()` function for this, as well.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果传感器停止发射 60 分钟，则其状态会被自动移除，从而防止我们担心的状态存储爆炸。请注意，我们可以使用显式的 `remove()` 函数来实现这一点。
- en: 'There is, however, an issue with this: in the first few values of a sensor,
    we compare the sensor’s value to a low default, which might not be appropriate
    for each sensor. We might detect temperature spikes reading values that might
    be perfectly appropriate for this particular sensor, simply because we do not
    have values for it yet.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里存在一个问题：在传感器的前几个值中，我们将传感器的值与一个低默认值进行比较，这可能对每个传感器都不合适。我们可能会检测到温度峰值，读取可能对这个特定传感器来说是完全适当的值，只是因为我们还没有它的值。
- en: 'In this case, we have the opportunity to provide initial values for our sensor,
    using `highTempStateSpec.initialState(initialTemps: RDD[(String, Float)])`.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种情况下，我们有机会为我们的传感器提供初始值，使用 `highTempStateSpec.initialState(initialTemps: RDD[(String,
    Float)])`。'
- en: Event-Time Stream Computation Using mapWithState
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `mapWithState` 进行事件时间流计算
- en: An auxiliary benefit of `mapWithState` is that it lets us efficiently and explicitly
    store data about the past in its `State` object. This can be very useful in order
    to accomplish event-time computing.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapWithState` 的一个辅助好处是，它让我们能够有效且明确地在其 `State` 对象中存储过去的数据。这在执行事件时间计算时非常有用。'
- en: Indeed, elements seen “on the wire” in a streaming system can arrive out of
    order, be delayed, or even arrive exceptionally fast with respect to other elements.
    Because of this, the only way to be sure that we are dealing with data elements
    that have been generated at a very specific time is to timestamp them at the time
    of generation. A stream of temperatures in which we are trying to detect spikes,
    as in our previous example, could confuse temperature increases and temperature
    decreases if some events arrive in inverse order, for example.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在流系统中“在线”看到的元素可能会无序到达、延迟到达，甚至与其他元素相比异常地快。因此，确保我们处理的数据元素确实是在特定时间生成的唯一方法是在生成时为它们打上时间戳。例如，在我们之前的例子中，我们试图检测温度的增幅的温度流中，如果某些事件以相反的顺序到达，则可能会混淆温度的增加和温度的减少。
- en: Note
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although event-time computation is supported natively by Structured Streaming
    as we saw in [Chapter 12](ch12.xhtml#ss-event-time), you can programmatically
    implement it in Spark Streaming using the technique described here.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结构化流本地支持事件时间计算，就像我们在 [第 12 章](ch12.xhtml#ss-event-time) 中看到的那样，但是你可以使用这里描述的技术在
    Spark Streaming 中以编程方式实现它。
- en: 'However, if we aim to process events in order, we need to be able to detect
    and invert the misordering by reading the timestamps present on data elements
    seen on our stream. To perform this reordering, we need to have an idea of the
    order of magnitude of the delays (in one direction or another) that we can expect
    to see on our stream. Indeed, without this boundary on the scope of the reordering,
    we would need to wait indefinitely to be able to compute a final result on a particular
    period of time: we could always receive yet another element that would have been
    delayed.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们的目标是按顺序处理事件，我们需要能够通过读取流上数据元素的时间戳来检测和反转错序。为了进行这种重新排序，我们需要对我们的流上可能看到的延迟的数量级有一个概念（或者说上下限）。确实，如果没有这种重新排序范围的界限，我们将需要无限等待才能计算特定时间段的最终结果：我们始终可以收到另一个可能已被延迟的元素。
- en: To deal with this practically, we are going to define a *watermark*, or the
    highest duration of time for which we are going to wait for straggler elements.
    In the spirit of Spark Streaming’s notion of time, it should be a multiple of
    the batch interval. After this watermark, we are going to “seal” the result of
    the computation and ignore elements that are delayed by more than the watermark.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实际处理这个问题，我们将定义一个*水印*，即我们将等待滞后元素的最长时间。按照Spark Streaming时间概念，它应该是批处理间隔的倍数。超过这个水印后，我们将“封存”计算结果，并忽略延迟超过水印的元素。
- en: Caution
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'The spontaneous approach to dealing with this misordering could be windowed
    streams: define a window interval equal to the watermark, and make it slide by
    exactly one batch, defining a transformation that orders elements by their timestamp.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这种错序的自然方法可能是窗口化流：定义一个与水印相等的窗口间隔，并使其每次滑动一个批次，定义一个按时间戳排序元素的转换。
- en: This is correct insofar as it will result in a correct view of the ordered elements,
    as soon as they’re past the first watermark interval. However, it requires the
    user to accept an initial delay equal to the watermark to see the results of the
    computation. It’s plausible, however, to see a watermark that’s one order of magnitude
    higher than the batch interval, and such latency would not be acceptable for a
    system like Spark Streaming, which already incurs high latency because of its
    microbatching approach.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这在一定程度上是正确的，因为一旦超过第一个水印间隔，它将导致对排序元素的正确视图。然而，它要求用户接受等于水印的初始延迟才能看到计算结果。然而，对于像Spark
    Streaming这样已经因其微批处理方法而产生高延迟的系统来说，可能会看到一个比批处理间隔高一个数量级的水印延迟，这种延迟是不可接受的。
- en: A good event-time streaming solution would allow us to compute based on a provisional
    view of the events of the stream and then update this result if and when delayed
    elements arrive.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个良好的事件时间流处理解决方案将允许我们基于流事件的临时视图进行计算，然后在延迟元素到达时更新此结果。
- en: 'Suppose that we have a notion of a circular buffer, a fixed-size vector of
    size *k*, that contains the last `k` elements it has received:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个循环缓冲区的概念，即一个固定大小为*k*的向量，它包含它接收到的最近的`k`个元素：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This object keeps an inner vector of at least one, and at most `maxSize` of
    elements, selecting its most recent additions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个对象保持一个内部向量，至少有一个，最多有`maxSize`个元素，选择其中最近添加的元素。
- en: 'Let’s now assume that we are tracking the average temperature for the last
    four batches, assuming a batch interval of five milliseconds:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们正在跟踪最近四批次的平均温度，假设批处理间隔为五毫秒：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this function, our `State` is a set of four cells containing averages for
    each of the batches. Here we are using `mapWithState` in the variant that takes
    the current batch time as an argument. We use the `batch` function in order to
    make batch comparisons sensible, in that if `t1`, `t2` are within the same batch,
    we expect that `batch(t1) == batch(t2)`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们的`State`是包含每个批次平均值的四个单元格集合。这里我们使用`mapWithState`的变体，该变体将当前批次时间作为参数。我们使用`batch`函数以使批处理比较变得合理，即如果`t1`、`t2`在同一批次内，则我们期望`batch(t1)
    == batch(t2)`。
- en: We begin by examining our new value and its event time. If that event time’s
    batch is beyond the current batch time, there is an error in our wall clock or
    the event time. For this example, we return `None`, but we could log an error
    as well. If the event is in the past, we need to find which batch it belongs to.
    For that, we use Scala’s partition function on the batches of each cell of our
    `CircularBuffer` State and separate the elements coming from before our element’s
    batch from those coming from the same batch or after.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从检查我们的新值及其事件时间开始。如果事件时间的批次超出当前批次时间，那么我们的壁钟或事件时间存在错误。对于这个例子，我们返回`None`，但我们也可以记录一个错误。如果事件是过去的，我们需要找出它属于哪个批次。为此，我们在我们的`CircularBuffer`状态的每个单元格的批次上使用Scala的分区函数，将来自我们元素之前批次的元素与来自相同批次或之后批次的元素分开。
- en: We then look at whether there was already an average initialized for the batch
    of our event that we should find at the head of the latter list (thanks to our
    partition). If there is one, we add the new temperature to it; otherwise, we make
    an average out of our single element. Finally, we take the batches from before
    the current element’s time and add all the posterior batches to it in order. The
    `CircularBuffer` natively ensures that we retain only the latest elements if there
    are more than our threshold (four).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们查看是否已为我们的事件批次初始化了平均值，我们应该在后面列表的头部找到它（感谢我们的分区）。如果有的话，我们将新的温度添加到其中；否则，我们将我们的单个元素取平均值。最后，我们将当前元素时间之前的批次和所有后置批次按顺序添加到其中。`CircularBuffer`本身确保如果超过我们的阈值（四个），则仅保留最新的元素。
- en: As the last step, we look up the updated average on the cell that we updated
    with our new element (if there indeed was one, we might have updated a stale element),
    and we output the new average if so. As a consequence, the `mapWithState` stream
    we can create on an RDD of `(String, (Time, Float))` elements (with the sensor
    name as a key, and the timestamped temperature as a value) updates the averages
    for the last updates we received, from the very first batch.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们查找我们用新元素更新的单元格上的更新后的平均值（如果确实有的话，我们可能已经更新了一个过时的元素），如果是这样，则输出新的平均值。因此，我们可以在RDD的`(String,
    (Time, Float))`元素上创建`mapWithState`流（传感器名称作为键，时间戳温度作为值），更新我们收到的最后更新的平均值，从第一个批次开始。
- en: Naturally, it uses a linear time in processing the content of our `CircularBuffer`,
    a consequence of the simplicity that we wanted to reach through this example.
    Note, however, how we are dealing with a structure that is ordered by timestamp
    and how a different data structure such as a skip list would let us gain a lot
    in processing speed and let us make this scalable.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理我们的`CircularBuffer`内容时，自然地，它使用了线性时间，这是我们通过这个例子想要达到的简单性的结果。然而，请注意，我们正在处理一个按时间戳排序的结构，不同的数据结构比如跳表将使我们在处理速度上获得很大提升，并使其可扩展。
- en: In sum, `mapWithState`, with its powerful state update semantics, its parsimonious
    timeout semantics, and the versatility brought by `snapshots()`, gives us a powerful
    tool to represent basic event-time processing in a few lines of Scala.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，`mapWithState`凭借其强大的状态更新语义、简洁的超时语义以及`snapshots()`带来的多功能性，为我们提供了一个强大的工具，在几行Scala代码中表示基本的事件时间处理。
