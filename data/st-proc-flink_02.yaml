- en: Chapter 2\. Stream Processing Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 流处理基础
- en: So far, you have seen how stream processing addresses some of the limitations
    of traditional batch processing and how it enables new applications and architectures.
    You also know a little bit about the evolution of the open source stream processing
    space and what a Flink streaming application looks like. In this chapter, you
    will enter the streaming world for good.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到流处理如何解决传统批处理的一些局限性，并且它如何支持新的应用和架构。你也了解了开源流处理空间的演进和 Flink 流应用程序的外观。在本章中，你将真正进入流处理的世界。
- en: The goal of this chapter is to introduce the fundamental concepts of stream
    processing and the requirements of its frameworks. We hope that after reading
    this chapter, you will be able to evaluate the features of modern stream processing
    systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是介绍流处理的基本概念及其框架的需求。我们希望在阅读本章后，您能够评估现代流处理系统的特性。
- en: Introduction to Dataflow Programming
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据流编程介绍
- en: Before we delve into the fundamentals of stream processing, let’s look at the
    background on *dataflow* programming and the terminology we will use throughout
    this book.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究流处理基础之前，让我们看一下数据流编程的背景以及本书中将使用的术语。
- en: Dataflow Graphs
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据流图
- en: As the name suggests, a dataflow program describes how data flows between operations.
    Dataflow programs are commonly represented as directed graphs, where nodes are
    called operators and represent computations and edges represent data dependencies.
    Operators are the basic functional units of a dataflow application. They consume
    data from inputs, perform a computation on them, and produce data to outputs for
    further processing. Operators without input ports are called data sources and
    operators without output ports are called data sinks. A dataflow graph must have
    at least one data source and one data sink. [Figure 2-1](#fig-logical-dataflow) shows
    a dataflow program that extracts and counts hashtags from an input stream of tweets.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，数据流程序描述了数据在操作之间的流动方式。数据流程序通常表示为有向图，其中节点称为运算符，表示计算，边表示数据依赖关系。运算符是数据流应用程序的基本功能单元。它们从输入消耗数据，对其执行计算，并将数据产生到输出以供进一步处理。没有输入端口的运算符称为数据源，没有输出端口的运算符称为数据汇。数据流图必须至少有一个数据源和一个数据汇。[图 2-1](#fig-logical-dataflow)显示了一个从推文输入流中提取和计数标签的数据流程序。
- en: '![A logical dataflow graph](assets/spaf_0201.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑数据流图](assets/spaf_0201.png)'
- en: Figure 2-1\. A logical dataflow graph to continuously count hashtags (nodes
    represent operators and edges denote data dependencies)
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1 逻辑数据流图，持续计算标签数（节点表示运算符，边表示数据依赖关系）
- en: Dataflow graphs like the one in [Figure 2-1](#fig-logical-dataflow) are called
    logical because they convey a high-level view of the computation logic. In order
    to execute a dataflow program, its logical graph is converted into a physical
    dataflow graph, which specifies in detail how the program is executed. For instance,
    if we are using a distributed processing engine, each operator might have several
    parallel tasks running on different physical machines. [Figure 2-2](#fig-physical-dataflow) shows
    a physical dataflow graph for the logical graph of [Figure 2-1](#fig-logical-dataflow).
    While in the logical dataflow graph the nodes represent operators, in the physical
    dataflow, the nodes are tasks. The “Extract hashtags" and “Count" operators have
    two parallel operator tasks, each performing a computation on a subset of the
    input data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 像[图 2-1](#fig-logical-dataflow)中的数据流图被称为逻辑图，因为它们传达了计算逻辑的高层视图。为了执行数据流程序，其逻辑图被转换为物理数据流图，详细指定了程序的执行方式。例如，如果我们使用分布式处理引擎，每个运算符可能在不同的物理机器上运行多个并行任务。[图 2-2](#fig-physical-dataflow)展示了[图 2-1](#fig-logical-dataflow)的逻辑图的物理数据流图。在逻辑数据流图中，节点表示运算符；而在物理数据流图中，节点是任务。“提取标签”和“计数”运算符各有两个并行运算任务，每个任务在输入数据的子集上执行计算。
- en: '![A physical dataflow graph](assets/spaf_0202.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![物理数据流图](assets/spaf_0202.png)'
- en: Figure 2-2\. A physical dataflow plan for counting hashtags (nodes represent
    tasks)
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2 计算标签的物理数据流计划（节点表示任务）
- en: Data Parallelism and Task Parallelism
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据并行和任务并行
- en: You can exploit parallelism in dataflow graphs in different ways. First, you
    can partition your input data and have tasks of the same operation execute on
    the data subsets in parallel. This type of parallelism is called data parallelism.
    Data parallelism is useful because it allows for processing large volumes of data
    and spreading the computation load across several computing nodes. Second, you
    can have tasks from different operators performing computations on the same or
    different data in parallel. This type of parallelism is called task parallelism.
    Using task parallelism, you can better utilize the computing resources of a cluster.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以以不同方式利用数据流图中的并行性。首先，您可以对输入数据进行分区，并使同一操作的任务并行执行数据子集。这种并行性称为数据并行性。数据并行性非常有用，因为它允许处理大量数据并将计算负载分布到多个计算节点上。其次，您可以使来自不同运算符的任务并行执行相同或不同的数据。这种并行性称为任务并行性。使用任务并行性，您可以更好地利用集群的计算资源。
- en: Data Exchange Strategies
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据交换策略
- en: Data exchange strategies define how data items are assigned to tasks in a physical
    dataflow graph. Data exchange strategies can be automatically chosen by the execution
    engine depending on the semantics of the operators or explicitly imposed by the
    dataflow programmer. Here, we briefly review some common data exchange strategies,
    as shown in [Figure 2-3](#ch2-data-exchange).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据交换策略定义了如何在物理数据流图中将数据项分配给任务。数据交换策略可以根据运算符的语义由执行引擎自动选择，也可以由数据流程序员明确指定。在这里，我们简要回顾了一些常见的数据交换策略，如[图2-3](#ch2-data-exchange)所示。
- en: The *forward* strategy sends data from a task to a receiving task. If both tasks
    are located on the same physical machine (which is often ensured by task schedulers),
    this exchange strategy avoids network communication.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*前向*策略将数据从一个任务发送到接收任务。如果这两个任务位于同一物理机器上（这通常由任务调度器保证），这种交换策略可以避免网络通信。'
- en: The *broadcast* strategy sends every data item to all parallel tasks of an operator.
    Because this strategy replicates data and involves network communication, it is
    fairly expensive.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*广播*策略将每个数据项发送到运算符的所有并行任务。由于这种策略复制数据并涉及网络通信，因此成本相当高。'
- en: The *key-based* strategy partitions data by a key attribute and guarantees that
    data items having the same key will be processed by the same task. In [Figure 2-2](#fig-physical-dataflow),
    the output of the “Extract hashtags” operator is partitioned by the key (the hashtag),
    so that the count operator tasks can correctly compute the occurrences of each
    hashtag.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于键值*策略根据键属性对数据进行分区，并保证具有相同键的数据项将由同一个任务处理。在[图2-2](#fig-physical-dataflow)中，“提取标签”运算符的输出按键（标签）进行分区，以便计数运算符任务可以正确计算每个标签的出现次数。'
- en: The *random* strategy uniformly distributes data items to operator tasks in
    order to evenly distribute the load across computing tasks.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机*策略将数据项均匀分发到运算符任务中，以便在计算任务中均匀分配负载。'
- en: '![Data exchange strategies](assets/spaf_0203.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![数据交换策略](assets/spaf_0203.png)'
- en: Figure 2-3\. Data exchange strategies
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. 数据交换策略
- en: Processing Streams in Parallel
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行处理流
- en: 'Now that you are familiar with the basics of dataflow programming, it’s time
    to see how these concepts apply to processing data streams in parallel. But first,
    let’s define the term *data stream*: a data stream is a potentially unbounded
    sequence of events.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经熟悉了数据流编程的基础知识，是时候看看这些概念如何应用于并行处理数据流了。但首先，让我们定义一下 *数据流* 这个术语：数据流是一个潜在无界的事件序列。
- en: Events in a data stream can represent monitoring data, sensor measurements,
    credit card transactions, weather station observations, online user interactions,
    web searches, etc. In this section, you are going to learn how to process infinite
    streams in parallel, using the dataflow programming paradigm.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流中的事件可以表示监控数据、传感器测量、信用卡交易、气象站观测、在线用户互动、网络搜索等。在本节中，您将学习如何使用数据流编程范式并行处理无限流。
- en: Latency and Throughput
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 延迟和吞吐量
- en: In [Chapter 1](ch01.html#chap-1), you learned that streaming applications have
    different operational requirements than traditional batch programs. Requirements
    also differ when it comes to evaluating performance. For batch applications, we
    usually care about the total execution time of a job, or how long it takes for
    our processing engine to read the input, perform the computation, and write back
    the result. Since streaming applications run continuously and the input is potentially
    unbounded, there is no notion of total execution time in data stream processing.
    Instead, streaming applications must provide results for incoming data *as fast
    as possible* while being able to handle high *ingest rates* of events. We express
    these performance requirements in terms of *latency* and *throughput*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第一章](ch01.html#chap-1)中，你学习到流式应用程序与传统的批处理程序有不同的运行要求。在评估性能时，要求也不同。对于批处理应用程序，通常关心作业的总执行时间，或者处理引擎读取输入、执行计算并写回结果所需的时间。由于流式应用程序持续运行且输入可能是无界的，因此在数据流处理中没有总执行时间的概念。相反，流式应用程序必须尽可能快地为传入数据提供结果，同时能够处理高吞吐量的事件。我们用*延迟*和*吞吐量*来表达这些性能需求。
- en: Latency
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 延迟
- en: Latency indicates how long it takes for an event to be processed. Essentially,
    it is the time interval between receiving an event and seeing the effect of processing
    this event in the output. To understand latency intuitively, consider your daily
    visit to your favorite coffee shop. When you enter the coffee shop, there might
    be other customers inside already. Thus, you wait in line and when it is your
    turn you place an order. The cashier receives your payment and passes your order
    to the barista who prepares your beverage. Once your coffee is ready, the barista
    calls your name and you can pick up your coffee from the counter. The service
    latency is the time you spend in the coffee shop, from the moment you enter until
    you have your first sip of coffee.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟指事件被处理所需的时间长度。实质上，它是接收事件并在输出中看到处理效果之间的时间间隔。为了直观理解延迟，考虑你每天去你最喜欢的咖啡店的情景。当你进入咖啡店时，里面可能已经有其他顾客了。因此，你需要排队等候，当轮到你时你就点单。收银员收到你的支付并把订单传给咖啡师，后者准备你的饮料。当你的咖啡准备好时，咖啡师会叫你的名字，然后你可以从柜台取走你的咖啡。服务延迟就是你在咖啡店内的时间，从你进入到你第一口咖啡的时间。
- en: In data streaming, latency is measured in units of time, such as milliseconds.
    Depending on the application, you might care about *average* latency, *maximum*
    latency, or *percentile* latency. For example, an average latency value of 10
    ms means that events are processed within 10 ms on average. Alternately, a 95th-percentile
    latency value of 10 ms means that 95% of events are processed within 10 ms. Average
    values hide the true distribution of processing delays and might make it hard
    to detect problems. If the barista runs out of milk right before preparing your
    cappuccino, you will have to wait until they bring some from the supply room.
    While you might get annoyed by this delay, most other customers will still be
    happy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据流处理中，延迟以时间单位（例如毫秒）来衡量。根据应用程序的不同，你可能关心*平均*延迟、*最大*延迟或*百分位*延迟。例如，平均延迟值为10毫秒意味着事件平均在10毫秒内被处理。而95分位数的延迟值为10毫秒则表示95%的事件在10毫秒内被处理。平均值隐藏了处理延迟的真实分布，可能会使问题难以被察觉。如果咖啡师在准备你的卡布奇诺之前发现牛奶用完了，你就得等到他们从储藏室拿来。虽然这种延迟可能会让你感到恼火，但大多数其他顾客仍然会很满意。
- en: Ensuring low latency is critical for many streaming applications, such as fraud
    detection, system alarms, network monitoring, and offering services with strict
    service-level agreements. Low latency is a key characteristic of stream processing
    and it enables what we call *real-time* applications. Modern stream processors,
    like Apache Flink, can offer latencies as low as a few milliseconds. In contrast,
    traditional batch processing latencies typically range from a few minutes to several
    hours. In batch processing, you first need to gather the events in batches and
    only then can you process them. Thus, the latency is bounded by the arrival time
    of the last event in each batch and naturally depends on the batch size. True
    stream processing does not introduce such artificial delays and thus can achieve
    really low latencies. In a true streaming model, events can be processed as soon
    as they arrive in the system and latency more closely reflects the actual work
    that has to be performed on each event.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 确保低延迟对许多流应用程序至关重要，例如欺诈检测、系统警报、网络监控以及提供严格服务水平协议的服务。低延迟是流处理的关键特性，它使我们能够实现所谓的*实时*应用程序。现代流处理器，如Apache
    Flink，可以提供低至几毫秒的延迟。相比之下，传统的批处理处理延迟通常在几分钟到几小时之间。在批处理中，你首先需要批量收集事件，然后才能处理它们。因此，延迟由每批最后一个事件到达时间界定，并且自然取决于批量大小。真正的流处理不引入这种人为延迟，因此可以实现非常低的延迟。在真正的流模型中，事件一到达系统就可以被处理，延迟更接近于每个事件需要执行的实际工作量。
- en: Throughput
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 吞吐量
- en: Throughput is a measure of the system’s processing capacity—its *rate* of processing.
    That is, throughput tells us how many events the system can process per time unit.
    Revisiting the coffee shop example, if the shop is open from 7 a.m. to 7 p.m.
    and it serves 600 customers in one day, then its average throughput would be 50
    customers per hour. While you want latency to be as low as possible, you generally
    want throughput to be as high as possible.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Throughput 是系统处理能力的衡量标准—其*处理速率*。也就是说，通过put告诉我们系统每个时间单位可以处理多少事件。重新访问咖啡店的例子，如果店铺从早上7点到晚上7点开放，并且一天服务600位顾客，那么它的平均吞吐量将是每小时50位顾客。虽然你希望延迟尽可能低，但通常希望吞吐量尽可能高。
- en: Throughput is measured in events or operations per time unit. It is important
    to note that the rate of processing depends on the rate of arrival; low throughput
    does not necessarily indicate bad performance. In streaming systems you usually
    want to ensure that your system can handle the maximum expected rate of events.
    That is, you are primarily concerned with determining the *peak* throughput—the
    performance limit when your system is at its maximum load. To better understand
    the concept of peak throughput, let’s consider a stream processing application
    that does not receive any incoming data and thus does not consume any system resources.
    When the first event comes in, it will be immediately processed with the minimum
    latency possible. For example, if you are the first customer showing up at the
    coffee shop right after it opened its doors in the morning, you will be served
    immediately. Ideally, you would like this latency to remain constant and independent
    of the rate of the incoming events. However, once we reach a rate of incoming
    events such that the system resources are fully used, we will have to start buffering
    events. In the coffee shop example, you will probably see this happening right
    after lunch. Many people show up at the same time and have to wait in line. At
    this point, the system has reached its peak throughput and further increasing
    the event rate will only result in worse latency. If the system continues to receive
    data at a higher rate than it can handle, buffers might become unavailable and
    data might get lost. This situation is commonly known as *backpressure* and there
    are different strategies to deal with it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量是每个时间单位内的事件或操作数。重要的是要注意，处理速率取决于到达率；低吞吐量不一定意味着性能不佳。在流处理系统中，通常希望确保系统能够处理预期的最大事件速率。也就是说，你主要关心确定*峰值*吞吐量——系统在最大负载时的性能极限。为了更好地理解峰值吞吐量的概念，让我们考虑一个流处理应用程序，该应用程序没有接收任何传入数据，因此不消耗任何系统资源。当第一个事件进来时，它将立即被处理，延迟最小。例如，如果你是早上咖啡店开门后的第一个顾客，你将立即被服务。理想情况下，你希望这种延迟保持恒定，独立于传入事件的速率。然而，一旦达到传入事件的速率，使系统资源完全被使用，我们将不得不开始缓冲事件。在咖啡店的例子中，你可能会在午餐后看到这种情况发生。很多人同时出现并且不得不排队等候。在这一点上，系统已经达到了峰值吞吐量，进一步增加事件率只会导致更糟糕的延迟。如果系统继续以比其处理能力更高的速率接收数据，缓冲区可能会不可用，并且数据可能会丢失。这种情况通常被称为*反压*，并且有不同的策略来处理它。
- en: Latency Versus Throughput
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 延迟与吞吐量
- en: At this point, it should be clear that latency and throughput are not independent
    metrics. If events take a long time to travel in the data processing pipeline,
    we cannot easily ensure high throughput. Similarly, if a system’s capacity is
    small, events will be buffered and have to wait before they get processed.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，应该清楚延迟和吞吐量不是独立的度量标准。如果事件在数据处理管道中传播需要很长时间，我们就不能轻易确保高吞吐量。同样地，如果系统的容量较小，事件将被缓冲并且必须等待被处理。
- en: Let’s revisit the coffee shop example to clarify how latency and throughput
    affect each other. First, it should be clear that there is optimal latency when
    there is no load. That is, you will get the fastest service if you are the only
    customer in the coffee shop. However, during busy times, customers will have to
    wait in line and latency will increase. Another factor that affects latency and
    consequently throughput is the time it takes to process an event, or the time
    it takes for each customer to be served in the coffee shop. Imagine that during
    the Christmas holiday season, baristas have to draw a Santa Claus on the cup of
    each coffee they serve. This means the time needed to prepare a single beverage
    will increase, causing each person to spend more time in the coffees hop, thus
    lowering the overall throughput.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视咖啡店的例子，以澄清延迟和吞吐量如何相互影响。首先，应该清楚的是在没有负载的情况下存在最佳延迟。也就是说，如果你是咖啡店里唯一的顾客，你会得到最快的服务。然而，在繁忙时段，顾客将不得不排队等候，延迟会增加。影响延迟和因此影响吞吐量的另一个因素是处理事件所需的时间，或者在咖啡店里为每位顾客提供服务所需的时间。想象一下在圣诞节假期期间，咖啡师必须在每杯咖啡上画圣诞老人。这意味着准备一杯饮料所需的时间将增加，导致每个人在咖啡店里花费更多时间，从而降低总体吞吐量。
- en: So, can you get both low latency and high throughput or is this a hopeless endeavor?
    You may be able to lower the latency in our coffee shop example by hiring a more
    skilled barista—one that prepares coffees faster. At high load, this change will
    also increase throughput, because more customers will be served in the same amount
    of time. Another way to achieve the same result is to hire a second barista and
    exploit parallelism. The main takeaway here is that lowering latency increases
    throughput. Naturally, if a system can perform operations faster, it can perform
    more operations in the same amount of time. In fact, that’s what happens when
    you exploit parallelism in a stream processing pipeline. By processing several
    streams in parallel, you lower the latency while processing more events at the
    same time.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，您是否可以同时获得低延迟和高吞吐量，还是这是一个不切实际的努力？通过雇用一个更熟练的咖啡师——一个能更快地制作咖啡的人，也许您可以在我们咖啡店的例子中降低延迟。在高负载下，这种改变还将增加吞吐量，因为可以在同样的时间内为更多客户提供服务。实现同样结果的另一种方法是雇用第二个咖啡师并利用并行性。这里的主要要点是降低延迟会增加吞吐量。自然地，如果系统可以更快地执行操作，那么它在同样时间内可以执行更多操作。事实上，在流处理管道中利用并行性时，就会发生这种情况。通过并行处理多个流，您可以降低延迟，同时处理更多事件。
- en: Operations on Data Streams
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据流操作
- en: Stream processing engines usually provide a set of built-in operations to ingest,
    transform, and output streams. These operators can be combined into dataflow processing
    graphs to implement the logic of streaming applications. In this section, we describe
    the most common streaming operations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理引擎通常提供一组内置操作来摄入、转换和输出流。这些操作符可以组合成数据流处理图，实现流应用程序的逻辑。在本节中，我们描述最常见的流处理操作。
- en: Operations can be either *stateless* or *stateful*. Stateless operations do
    not maintain any internal state. That is, the processing of an event does not
    depend on any events seen in the past and no history is kept. Stateless operations
    are easy to parallelize, since events can be processed independently of each other
    and of their arriving order. Moreover, in the case of a failure, a stateless operator
    can be simply restarted and continue processing from where it left off. In contrast,
    stateful operators may maintain information about the events they have received
    before. This state can be updated by incoming events and can be used in the processing
    logic of future events. Stateful stream processing applications are more challenging
    to parallelize and operate in a fault-tolerant manner because state needs to be
    efficiently partitioned and reliably recovered in the case of failures. You will
    learn more about stateful stream processing, failure scenarios, and consistency
    at the end of this chapter.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 操作可以是*无状态*或*有状态*的。无状态操作不维护任何内部状态。也就是说，事件的处理不依赖于过去看到的任何事件，也不保留任何历史记录。无状态操作易于并行化，因为可以独立处理事件，而不考虑它们的顺序。此外，在发生故障时，可以简单地重新启动无状态操作符，并从离开的地方继续处理。相比之下，有状态操作符可能会维护有关它们之前接收的事件的信息。此状态可以通过传入事件更新，并可以在未来事件的处理逻辑中使用。有状态流处理应用程序更具挑战性，因为需要有效地分区状态，并在发生故障时可靠地恢复。您将在本章末了解更多关于有状态流处理、故障场景和一致性的内容。
- en: Data ingestion and data egress
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据摄入和数据出口
- en: Data ingestion and data egress operations allow the stream processor to communicate
    with external systems. Data *ingestion* is the operation of fetching raw data
    from external sources and converting it into a format suitable for processing.
    Operators that implement data ingestion logic are called *data sources*. A data
    source can ingest data from a TCP socket, a file, a Kafka topic, or a sensor data
    interface. Data *egress* is the operation of producing output in a form suitable
    for consumption by external systems. Operators that perform data egress are called
    *data sinks* and examples include files, databases, message queues, and monitoring
    interfaces.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄入和数据出口操作允许流处理器与外部系统通信。数据*摄入*是从外部源获取原始数据并转换为适合处理的格式的操作。实现数据摄入逻辑的操作符称为*数据源*。数据源可以从TCP套接字、文件、Kafka主题或传感器数据接口摄入数据。数据*出口*是生成适合外部系统消费的输出的操作。执行数据出口的操作符称为*数据接收器*，例如文件、数据库、消息队列和监控接口。
- en: Transformation operations
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换操作
- en: Transformation operations are single-pass operations that process each event
    independently. These operations consume one event after the other and apply some
    transformation to the event data, producing a new output stream. The transformation
    logic can be either integrated in the operator or provided by a user-defined function,
    as shown in [Figure 2-4](#fig-a-streaming-operator). Functions are written by
    the application programmer and implement custom computation logic.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 变换操作是单次遍历操作，它们独立处理每个事件。这些操作逐个事件消耗并应用一些转换到事件数据，产生一个新的输出流。转换逻辑可以集成在操作符中，也可以由用户定义的函数提供，如
    [图2-4](#fig-a-streaming-operator) 所示。函数由应用程序员编写并实现自定义计算逻辑。
- en: '![A transformation operator with a UDF](assets/spaf_0204.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![带有UDF的变换操作符](assets/spaf_0204.png)'
- en: Figure 2-4\. A streaming operator with a function that turns each incoming event
    into a darker event
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. 带有将每个传入事件转换为更暗事件的函数的流处理操作符
- en: Operators can accept multiple inputs and produce multiple output streams. They
    can also modify the structure of the dataflow graph by either splitting a stream
    into multiple streams or merging streams into a single flow. We discuss the semantics
    of all operators available in Flink in [Chapter 5](ch05.html#chap-5).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 操作符可以接受多个输入并产生多个输出流。它们还可以通过将流分割为多个流或将流合并为单一流来修改数据流图的结构。我们在 [第5章](ch05.html#chap-5)
    中讨论了Flink中所有可用操作符的语义。
- en: Rolling aggregations
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 滚动聚合
- en: A rolling aggregation is an aggregation, such as sum, minimum, and maximum,
    that is continuously updated for each input event. Aggregation operations are
    stateful and combine the current state with the incoming event to produce an updated
    aggregate value. Note that to be able to efficiently combine the current state
    with an event and produce a single value, the aggregation function must be associative
    and commutative. Otherwise, the operator would have to store the complete stream
    history. [Figure 2-5](#fig-rolling-minimum) shows a rolling minimum aggregation.
    The operator keeps the current minimum value and accordingly updates it for each
    incoming event.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动聚合是一种不断更新的聚合，如求和、最小值和最大值，它持续更新每个输入事件。聚合操作是有状态的，并结合当前状态与传入事件以产生更新后的聚合值。需要注意的是，为了能够有效地将当前状态与事件结合并产生单个值，聚合函数必须是可结合和可交换的。否则，操作符将不得不存储完整的流历史。[图2-5](#fig-rolling-minimum)
    展示了一个滚动最小聚合。操作符保持当前的最小值并相应地更新每个传入事件的值。
- en: '![A rolling minimum aggregation](assets/spaf_0205.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![一个滚动最小聚合](assets/spaf_0205.png)'
- en: Figure 2-5\. A rolling minimum aggregation operation
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. 滚动最小聚合操作
- en: Window operations
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 窗口操作
- en: Transformations and rolling aggregations process one event at a time to produce
    output events and potentially update state. However, some operations must collect
    and buffer records to compute their result. Consider, for example, a streaming
    join operation or a holistic aggregate, such as the median function. In order
    to evaluate such operations efficiently on unbounded streams, you need to limit
    the amount of data these operations maintain. In this section, we discuss window
    operations, which provide this service.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 变换和滚动聚合处理一个事件来产生输出事件，并可能更新状态。然而，有些操作必须收集和缓存记录以计算它们的结果。例如，考虑流连接操作或整体聚合，如中位数函数。为了有效评估这些操作在无界流上的表现，你需要限制这些操作维护的数据量。在本节中，我们讨论提供此服务的窗口操作。
- en: Apart from having a practical value, windows also enable semantically interesting
    queries on streams. You have seen how rolling aggregations encode the history
    of the whole stream in an aggregate value and provide us with a low-latency result
    for every event. This is fine for some applications, but what if you are only
    interested in the most recent data? Consider an application that provides real-time
    traffic information to drivers so that they can avoid congested routes. In this
    scenario, you want to know if there has been an accident in a certain location
    within the last few minutes. On the other hand, knowing about all accidents that
    have ever happened might not be so interesting in this case. What’s more, by reducing
    the stream history to a single aggregate, you lose the information about how your
    data varies over time. For instance, you might want to know how many vehicles
    cross an intersection every 5 minutes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 除了具有实际价值之外，窗口还使流上的语义查询变得有趣。您已经看到滚动聚合如何将整个流的历史编码为聚合值，并为每个事件提供低延迟的结果。这对某些应用程序很好，但如果您只对最新数据感兴趣怎么办？考虑一个应用程序，为司机提供实时交通信息，以便他们可以避开拥堵的路线。在这种情况下，您想知道最近几分钟内某个位置是否发生了事故。另一方面，仅知道曾经发生过的所有事故可能对此案例并不那么有趣。此外，通过将流历史减少到单个聚合，您失去了关于数据随时间变化的信息。例如，您可能想知道每5分钟有多少车辆通过一个十字路口。
- en: Window operations continuously create finite sets of events called buckets from
    an unbounded event stream and let us perform computations on these finite sets.
    Events are usually assigned to buckets based on data properties or based on time.
    To properly define window operator semantics we need to determine both how events
    are assigned to buckets and how often the window produces a result. The behavior
    of windows is defined by a set of policies. Window policies decide when new buckets
    are created, which events are assigned to which buckets, and when the contents
    of a bucket get evaluated. The latter decision is based on a trigger condition.
    When the trigger condition is met, the bucket contents are sent to an evaluation
    function that applies the computation logic on the bucket elements. Evaluation
    functions can be aggregations like sum or minimum or custom operations applied
    on the bucket’s collected elements. Policies can be based on time (e.g., events
    received in the last five seconds), on count (e.g., the last one hundred events),
    or on a data property. Next, we describe the semantics of common window types.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口操作不断从无界事件流中创建有限的事件集合，称为桶，并允许我们对这些有限集合进行计算。通常根据数据属性或时间将事件分配到桶中。为了准确定义窗口操作符的语义，我们需要确定事件如何分配到桶中以及窗口多久产生一个结果。窗口的行为由一组策略定义。窗口策略决定了何时创建新的桶，哪些事件分配到哪些桶中，以及何时评估桶的内容。后者的决定基于触发条件。当触发条件满足时，桶的内容被发送到一个评估函数上，该函数对桶元素应用计算逻辑。评估函数可以是诸如求和或最小值的聚合，也可以是应用于收集到的桶元素的自定义操作。策略可以基于时间（例如，最近五秒内接收到的事件）、计数（例如，最近一百个事件）或数据属性。接下来，我们描述常见窗口类型的语义。
- en: '*Tumbling* windows assign events into nonoverlapping buckets of fixed size.
    When the window border is passed, all the events are sent to an evaluation function
    for processing. Count-based tumbling windows define how many events are collected
    before triggering evaluation. [Figure 2-6](#fig-count-tumbling) shows a count-based
    tumbling window that discretizes the input stream into buckets of four elements.
    Time-based tumbling windows define a time interval during which events are buffered
    in the bucket. [Figure 2-7](#fig-time-based-tumbling) shows a time-based tumbling
    window that gathers events into buckets and triggers computation every 10 minutes.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*滚动* 窗口将事件分配到不重叠的固定大小的桶中。当窗口边界被越过时，所有事件都被发送到评估函数进行处理。基于计数的滚动窗口定义了在触发评估之前收集多少事件。图[2-6](#fig-count-tumbling)展示了一个将输入流离散化为四个元素桶的基于计数的滚动窗口。基于时间的滚动窗口定义了一个时间间隔，在此期间事件被缓冲到桶中。图[2-7](#fig-time-based-tumbling)展示了一个每10分钟将事件收集到桶中并触发计算的基于时间的滚动窗口。'
- en: '![Count-based tumbling window.](assets/spaf_0206.png)'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![基于计数的滚动窗口。](assets/spaf_0206.png)'
- en: Figure 2-6\. Count-based tumbling window
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6\. 基于计数的滚动窗口
- en: '![Time-based tumbling window.](assets/spaf_0207.png)'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![基于时间的滚动窗口。](assets/spaf_0207.png)'
- en: Figure 2-7\. Time-based tumbling window
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7\. 基于时间的滚动窗口
- en: '*Sliding * windows assign events into overlapping buckets of fixed size. Thus,
    an event might belong to multiple buckets. We define sliding windows by providing
    their length and their *slide*. The slide value defines the interval at which
    a new bucket is created. The sliding count-based window of [Figure 2-8](#fig-sliding-count-based)
    has a length of four events and slide of three events.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*滑动* 窗口将事件分配到重叠的固定大小的桶中。因此，一个事件可能属于多个桶。我们通过提供它们的长度和*滑动*来定义滑动窗口。滑动值定义了创建新桶的间隔。图 [2-8](#fig-sliding-count-based)
    中的滑动计数窗口具有四个事件的长度和三个事件的滑动。'
- en: '![Siding window.](assets/spaf_0208.png)'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![滑动窗口。](assets/spaf_0208.png)'
- en: Figure 2-8\. Sliding count-based window with a length of four events and a slide
    of three events
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-8\. 具有四个事件长度和三个事件滑动的滑动计数窗口
- en: '*Session* windows are useful in common real-world scenarios where neither tumbling
    nor sliding windows can be applied. Consider an application that analyzes online
    user behavior. In such applications, we would like to group together events that
    originate from the same period of user activity or *session*. Sessions are comprised
    of a series of events happening in adjacent times followed by a period of inactivity.
    For example, user interactions with a series of news articles one after the other
    could be considered a session. Since the length of a session is not defined beforehand
    but depends on the actual data, tumbling and sliding windows cannot be applied
    in this scenario. Instead, we need a window operation that assigns events belonging
    to the same session in the same bucket. Session windows group events in sessions
    based on a *session gap* value that defines the time of inactivity to consider
    a session closed. [Figure 2-9](#fig-session-window) shows a session window.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*会话* 窗口在常见的实际场景中非常有用，这些场景中既不能应用翻滚窗口也不能应用滑动窗口。考虑一个分析在线用户行为的应用程序。在这类应用中，我们希望将来自同一用户活动期间的事件分组在一起，形成一个*会话*。会话由一系列相邻时间内发生的事件以及随后的非活动期组成。例如，用户连续查看一系列新闻文章可以被视为一个会话。由于会话的长度事先未定义，而是取决于实际数据，因此在这种情况下无法应用翻滚和滑动窗口。相反，我们需要一个窗口操作，将属于同一会话的事件分配到同一个桶中。会话窗口根据*会话间隔*值将事件分组到会话中。图 [2-9](#fig-session-window)
    显示了一个会话窗口。'
- en: '![Session window.](assets/spaf_0209.png)'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![会话窗口。](assets/spaf_0209.png)'
- en: Figure 2-9\. Session window
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-9\. 会话窗口
- en: All the window types that you have seen so far are windows that operate on the
    full stream. But in practice you might want to partition a stream into multiple
    logical streams and define *parallel* windows. For instance, if you are receiving
    measurements from different sensors, you probably want to group the stream by
    sensor ID before applying a window computation. In parallel windows, each partition
    applies the window policies independently of other partitions. [Figure 2-10](#fig-parallel-count)
    shows a parallel count-based tumbling window of length 2 that is partitioned by
    event color.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您所看到的所有窗口类型都是在完整数据流上操作的窗口。但实际中，您可能希望将流分成多个逻辑流，并定义*并行*窗口。例如，如果您从不同传感器接收测量数据，可能希望在应用窗口计算之前按传感器ID对流进行分组。在并行窗口中，每个分区都独立地应用窗口策略，而不受其他分区的影响。图 [2-10](#fig-parallel-count)
    展示了一个按事件颜色分区的长度为2的并行计数翻滚窗口。
- en: '![Parallel tumbling window.](assets/spaf_0210.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![并行翻滚窗口。](assets/spaf_0210.png)'
- en: Figure 2-10\. A parallel count-based tumbling window of length 2
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-10\. 长度为2的并行计数翻滚窗口
- en: 'Window operations are closely related to two dominant concepts in stream processing:
    time semantics and state management. Time is perhaps the most important aspect
    of stream processing. Even though low latency is an attractive feature of stream
    processing, its true value is way beyond just fast analytics. Real-world systems,
    networks, and communication channels are far from perfect, and streaming data
    can often be delayed or arrive out of order. It is crucial to understand how to
    deliver accurate and deterministic results under such conditions. What’s more,
    streaming applications that process events as they are produced should also be
    able to process historical events in the same way, thus enabling offline analytics
    or even time travel analyses. Of course, none of this matters if your system cannot
    guard state against failures. All the window types that you have seen so far need
    to buffer data before producing a result. In fact, if you want to compute anything
    interesting in a streaming application, even a simple count, you need to maintain
    state. Considering that streaming applications might run for several days, months,
    or even years, you need to make sure that state can be reliably recovered under
    failures and that your system can guarantee accurate results even if things break.
    In the rest of this chapter, we are going to look deeper into the concepts of
    time and state guarantees under failures in data stream processing.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口操作与流处理中的两个主要概念密切相关：时间语义和状态管理。时间可能是流处理中最重要的方面。尽管低延迟是流处理的一大吸引特点，其真正的价值远不止于快速分析。现实世界中的系统、网络和通信通道远非完美，流数据通常会延迟或无序到达。在这种情况下，了解如何在准确和确定的条件下提供结果至关重要。更重要的是，能够处理实时产生的事件的流应用程序也应能以相同方式处理历史事件，从而实现离线分析甚至时间旅行分析。当然，如果系统不能在发生故障时保护状态，这一切都毫无意义。到目前为止，您所看到的所有窗口类型在生成结果之前都需要缓冲数据。事实上，即使是在流应用程序中计算任何有趣的事情，如简单的计数，也需要维护状态。考虑到流应用程序可能运行数天、数月甚至数年，您需要确保状态能够在发生故障时可靠地恢复，并且您的系统可以在出现故障时保证准确的结果。在本章的其余部分，我们将更深入地探讨数据流处理中关于时间和状态在故障条件下的保证概念。
- en: Time Semantics
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间语义
- en: In this section, we introduce time semantics and describe the different notions
    of time in streaming. We discuss how a stream processor can provide accurate results
    with out-of-order events and how you can perform historical event processing and
    time travel with streaming.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍时间语义，并描述流处理中不同的时间概念。我们讨论了流处理器如何处理无序事件并提供准确的结果，以及如何使用流进行历史事件处理和时间旅行。
- en: What Does One Minute Mean in Stream Processing?
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流处理中的一分钟到底意味着什么？
- en: When dealing with a potentially unbounded stream of continuously arriving events,
    time becomes a central aspect of applications. Let’s assume you want to compute
    results continuously, maybe every minute. What would *one minute* really mean
    in the context of our streaming application?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理连续到达的潜在无界事件流时，时间成为应用程序的核心方面。假设您希望连续计算结果，可能是每分钟一次。在我们的流应用程序背景下，“一分钟”到底意味着什么？
- en: Consider a program that analyzes events generated by users playing an online
    mobile game. Users are organized in teams and the application collects a team’s
    activity and provides rewards in the game, such as extra lives and level-ups,
    based on how fast the team’s members meet the game’s goals. For example, if all
    users in a team pop 500 bubbles within one minute, they get a level-up. Alice
    is a devoted player who plays the game every morning during her commute to work.
    The problem is that Alice lives in Berlin and takes the subway to work. And everyone
    knows that the mobile internet connection in the Berlin subway is lousy. Consider
    the case where Alice starts popping bubbles while her phone is connected to the
    network and sends events to the analysis application. Then suddenly the train
    enters a tunnel and her phone gets disconnected. Alice keeps on playing and the
    game events are buffered in her phone. When the train exits the tunnel, she comes
    back online, and pending events are sent to the application. What should the application
    do? What’s the meaning of one minute in this case? Does it include the time Alice
    was offline or not? [Figure 2-11](#online-subway) illustrates this problem.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个分析用户玩在线手机游戏生成事件的程序。用户被组织成团队，应用程序收集团队的活动，并根据团队成员完成游戏目标的速度提供奖励，例如额外生命和升级。例如，如果一个团队的所有用户在一分钟内弹出了500个气泡，他们就可以升级。艾丽斯是一个热爱游戏的玩家，每天早晨在上班路上都会玩游戏。问题在于，艾丽斯住在柏林，每天上班都乘坐地铁。大家都知道柏林地铁的移动互联网连接非常差。考虑艾丽斯开始在手机连接到网络时弹出气泡并向分析应用程序发送事件的情况。然后突然地铁进入隧道，她的手机断网了。艾丽斯继续玩游戏，并且游戏事件被缓存在她的手机中。当地铁驶出隧道时，她重新联网，待处理的事件被发送到应用程序。应用程序应该怎么做？在这种情况下一分钟的含义是什么？是否包括艾丽斯离线时的时间？
    [Figure 2-11](#online-subway) 描述了这个问题。
- en: '![Playing online mobile games in the subway.](assets/spaf_0211.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![在地铁上玩在线手机游戏。](assets/spaf_0211.png)'
- en: Figure 2-11\. An application receiving online mobile game events played on the
    subway would experience a gap when the network connection is lost, but events
    are buffered in the player’s phone and delivered when the connection is restored
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-11\. 接收在线手机游戏事件的应用程序在地铁上玩会遇到网络连接中断的间隙，但事件被缓存在玩家手机上，并在恢复连接时传送。
- en: Online gaming is a simple scenario showing how operator semantics should depend
    on the time when events actually happen and not the time when the application
    receives the events. In the case of a mobile game, consequences can be as bad
    as Alice and her team getting disappointed and never playing again. But there
    are much more time-critical applications whose semantics we need to guarantee.
    If we only consider how much data we receive within one minute, our results will
    vary and depend on the speed of the network connection or the speed of the processing.
    Instead, what really defines the amount of events in one minute is the time of
    the data itself.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在线游戏是一个简单的场景，展示了操作语义应该依赖事件实际发生的时间，而不是应用程序接收事件的时间。在移动游戏的情况下，后果可能很严重，例如艾丽斯和她的团队感到失望，从而再也不想玩了。但有更加时间关键的应用程序，我们需要保证其语义。如果我们只考虑在一分钟内接收到多少数据，结果将会因网络连接速度或处理速度的不同而有所不同。而真正定义一分钟内事件数量的是数据本身的时间。
- en: 'In Alice’s game example, the streaming application could operate with two different
    notions of time: processing time or event time. We describe both notions in the
    following sections.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在艾丽斯的游戏示例中，流应用程序可以使用两种不同的时间概念：处理时间或事件时间。我们将在接下来的章节中描述这两种概念。
- en: Processing Time
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理时间
- en: Processing time is the time of the local clock on the machine where the operator
    processing the stream is being executed. A processing-time window includes all
    events that happen to have arrived at the window operator within a time period,
    as measured by the wall clock of its machine. As shown in [Figure 2-12](#fig-alice-processing),
    in Alice’s case, a processing-time window would continue counting time when her
    phone gets disconnected, thus not accounting for her game activity during that
    time.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 处理时间是流处理操作员所在机器的本地时钟时间。处理时间窗口包括在某个时间段内到达窗口操作员的所有事件，这段时间由其机器的墙上时钟测量而得。如图 [Figure 2-12](#fig-alice-processing)
    所示，在艾丽斯的情况中，处理时间窗口会继续计算时间，即使她的手机断网了，因此在此期间不会计算她的游戏活动。
- en: '![A processing-time window in a mobile game application.](assets/spaf_0212.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![移动游戏应用程序中的处理时间窗口。](assets/spaf_0212.png)'
- en: Figure 2-12\. A processing-time window continues counting time even after Alice’s
    phone gets disconnected
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-12\. 在处理时间窗口中，即使爱丽丝的手机断开连接后，时间继续计数。
- en: Event Time
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事件时间
- en: Event time is the time when an event in the stream actually happened. Event
    time is based on a *timestamp* that is attached to the events of the stream. Timestamps
    usually exist inside the event data before they enter the processing pipeline
    (e.g., the event creation time). [Figure 2-13](#fig-alice-event) shows that an
    event-time window would correctly place events in a window, reflecting the reality
    of how things happened, even though some events were *delayed.*
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 事件时间是流中事件实际发生的时间。事件时间基于附加到流事件中的*时间戳*。时间戳通常存在于事件数据进入处理流水线之前（例如，事件创建时间）。[图 2-13](#fig-alice-event)显示，事件时间窗口可以正确地将事件放置在窗口中，反映了事情发生的真实情况，尽管一些事件*延迟*。
- en: '![An event-time window in a mobile game application.](assets/spaf_0213.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![一个移动游戏应用程序中的事件时间窗口。](assets/spaf_0213.png)'
- en: Figure 2-13\. Event time correctly places events in a window, reflecting the
    reality of how things happened
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-13\. 事件时间正确地将事件放置在窗口中，反映了事情发生的真实情况。
- en: Event time completely decouples the processing speed from the results. Operations
    based on event time are predictable and their results are deterministic. An event
    time window computation will yield the same result no matter how fast the stream
    is processed or when the events arrive at the operator.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 事件时间完全将处理速度与结果解耦。基于事件时间的操作是可预测的，其结果是确定性的。事件时间窗口计算将产生相同的结果，无论流处理的速度有多快或事件何时到达操作符。
- en: Handling delayed events is only one of the challenges that you can overcome
    with event time. The ubiquitous problem of out-of-order data can also be solved
    with it. Consider Bob, another player of the online mobile game, who happens to
    be on the same train as Alice. Bob and Alice play the same game but have different
    mobile providers. While Alice’s phone loses connection when inside the tunnel,
    Bob’s phone remains connected and delivers events to the gaming application.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 处理延迟事件只是使用事件时间可以克服的挑战之一。普遍存在的乱序数据问题也可以通过它解决。考虑一下鲍勃，是在线移动游戏的另一名玩家，恰好与爱丽丝同在一辆火车上。鲍勃和爱丽丝玩同一个游戏，但使用不同的移动服务提供商。当爱丽丝的手机在隧道内失去连接时，鲍勃的手机仍然保持连接并将事件传递给游戏应用程序。
- en: By relying on event time, we can guarantee result correctness even in cases
    of out-of-order data. What’s more, when combined with replayable streams, the
    determinism of timestamps gives you the ability to *fast forward* the past. That
    is, you can replay a stream and analyze historic data as if events are happening
    in real time. Additionally, you can fast forward the computation to the present
    so that once your program catches up with the events happening now, it can continue
    as a real-time application using exactly the same program logic.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 依靠事件时间，即使数据出现乱序，我们也能保证结果的正确性。更重要的是，当与可重放流相结合时，时间戳的确定性赋予你**快进**过去的能力。也就是说，你可以重放流并分析历史数据，就像事件是实时发生的一样。此外，你还可以将计算快进到当前时刻，使得一旦你的程序赶上当前事件，它就可以继续作为一个实时应用程序，完全使用相同的程序逻辑。
- en: Watermarks
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 水印
- en: 'In our discussion about event-time windows so far, we have overlooked one very
    important aspect: *how do we decide when to trigger an event-time window*? That
    is, how long do we have to wait before we can be certain that we have received
    all events that happened before a certain point of time? And how do we even know
    that data will be delayed? Given the unpredictable reality of distributed systems
    and arbitrary delays that might be caused by external components, there are no
    categorically correct answers to these questions. In this section, we will see
    how to use *watermarks* to configure event-time window behavior.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在讨论事件时间窗口时，忽略了一个非常重要的方面：*我们如何决定何时触发事件时间窗口*？也就是说，在我们可以确定在某个时间点之前收到了所有事件之前，我们需要等待多长时间？我们甚至如何知道数据会延迟？考虑到分布式系统的不可预测性和外部组件可能引起的任意延迟，这些问题没有一种绝对正确的答案。在本节中，我们将看到如何使用*水印*来配置事件时间窗口的行为。
- en: A watermark is a global progress metric that indicates the point in time when
    we are confident that no more delayed events will arrive. In essence, watermarks
    provide a logical clock that informs the system about the current event time.
    When an operator receives a watermark with time T, it can assume that no further
    events with timestamp less than T will be received. Watermarks are essential for
    both event-time windows and operators handling out-of-order events. Once a watermark
    has been received, operators are signaled that all timestamps for a certain time
    interval have been observed and either trigger computation or order received events.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 水印是全局进度指标，表示我们确信不会再有延迟事件到达的时间点。实质上，水印提供了一个逻辑时钟，告知系统当前事件时间。当操作器接收到时间为T的水印时，可以假设不会再接收到时间戳小于T的事件。水印对事件时间窗口和处理乱序事件的操作器至关重要。一旦接收到水印，操作器就会收到信号，表明已观察到某个时间间隔内的所有时间戳，并触发计算或排序接收到的事件。
- en: Watermarks provide a configurable tradeoff between results confidence and latency.
    *Eager* watermarks ensure low latency but provide lower confidence. In this case,
    late events might arrive after the watermark, and we should provide some code
    to handle them. On the other hand, if watermarks are too relaxed, you have high
    confidence but you might unnecessarily increase processing latency.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 水印提供了结果可信度和延迟之间的可配置权衡。*渴望的*水印确保低延迟但提供较低的可信度。在这种情况下，延迟事件可能会在水印之后到达，我们应提供一些代码来处理它们。另一方面，如果水印过于宽松，您将获得高可信度，但可能会不必要地增加处理延迟。
- en: In many real-world applications, the system does not have enough knowledge to
    perfectly determine watermarks. In the mobile gaming example, it is practically
    impossible to know how long a user might remain disconnected; they could be going
    through a tunnel, boarding a plane, or never playing again. No matter if watermarks
    are user defined or automatically generated, tracking global progress in a distributed
    system might be problematic in the presence of straggler tasks. Hence, simply
    relying on watermarks might not always be a good idea. Instead, it is crucial
    that the stream processing system provide some mechanism to deal with events that
    might arrive after the watermark. Depending on the application requirements, you
    might want to ignore such events, log them, or use them to correct previous results.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际应用中，系统无法完全确定水印。在移动游戏的例子中，几乎不可能知道用户可能断开连接多长时间；他们可能在隧道中、登机或永远不再玩游戏。无论水印是用户定义还是自动生成的，在存在滞后任务的情况下，在分布式系统中追踪全局进度可能会有问题。因此，仅仅依赖水印可能并不总是一个好主意。相反，流处理系统提供某些机制来处理可能在水印之后到达的事件非常关键。根据应用要求，您可能希望忽略这些事件、记录它们或使用它们来修正之前的结果。
- en: Processing Time Versus Event Time
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理时间与事件时间
- en: At this point, you might be wondering why we would even bother with processing
    time if event time solves all of our problems. The truth is, processing time can
    indeed be useful in some cases. Processing-time windows introduce the lowest latency
    possible. Since you do not take into consideration late events and out-of-order
    events, a window simply needs to buffer up events and immediately trigger computation
    once the specified time length is reached. Thus, for applications where speed
    is more important than accuracy, processing time comes in handy. Another case
    is when you need to periodically report results in real time, independently of
    their accuracy. An example application would be a real-time monitoring dashboard
    that displays event aggregates as they are received. Finally, processing-time
    windows offer a faithful representation of the streams themselves, which might
    be a desirable property for some use cases. For instance, you might be interested
    in observing the stream and counting the number of events per second to detect
    outages. To recap, processing time offers low latency but results depend on the
    speed of processing and are not deterministic. On the other hand, event time guarantees
    deterministic results and allows you to deal with events that are late or even
    out of order.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可能会想知道，如果事件时间能解决所有问题，为什么我们还要费心处理时间呢？事实是，在某些情况下，处理时间确实很有用处。处理时间窗口引入了可能的最低延迟。由于不考虑延迟事件和乱序事件，窗口只需缓冲事件并在达到指定时间长度后立即触发计算。因此，对于速度比准确性更重要的应用，处理时间非常方便。另一个案例是，当你需要定期实时报告结果时，与其准确性无关。一个示例应用是实时监控仪表板，在接收到事件后显示事件聚合数据。最后，处理时间窗口为流本身提供了忠实的表示，这可能对某些用例很重要。例如，你可能有兴趣观察流并计算每秒事件数量以检测故障。总结一下，处理时间提供低延迟，但结果取决于处理速度且不确定。另一方面，事件时间保证确定性结果，并允许处理延迟或乱序事件。
- en: State and Consistency Models
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 状态和一致性模型
- en: We now turn to another extremely important aspect of stream processing—state.
    State is ubiquitous in data processing. It is required by any nontrivial computation.
    To produce a result, a function accumulates state over a period of time or number
    of events (e.g., to compute an aggregation or detect a pattern). Stateful operators
    use both incoming events and internal state to compute their output. Take, for
    example, a rolling aggregation operator that outputs the current sum of all the
    events it has seen so far. The operator keeps the current value of the sum as
    its internal state and updates it every time it receives a new event. Similarly,
    consider an operator that raises an alert when it detects a “high temperature”
    event followed by a “smoke” event within 10 minutes. The operator needs to store
    the “high temperature” event in its internal state until it sees the “smoke” event
    or the until 10-minute time period expires.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向流处理的另一个极其重要的方面——状态。状态在数据处理中无处不在，它是任何非平凡计算所必需的。为了产生结果，函数会在一段时间或事件数量内积累状态（例如计算聚合或检测模式）。有状态的操作符使用传入的事件和内部状态来计算它们的输出。例如，考虑一个滚动聚合操作符，它输出到目前为止所有事件的当前总和。该操作符将总和的当前值作为其内部状态，并在接收到新事件时更新它。类似地，考虑一个操作符，在检测到“高温”事件后，如果在10分钟内再次检测到“烟雾”事件，则发出警报。该操作符需要将“高温”事件存储在其内部状态中，直到它看到“烟雾”事件或者10分钟时间段到期为止。
- en: The importance of state becomes even more evident if we consider the case of
    using a batch processing system to analyze an unbounded dataset. Before the rise
    of modern stream processors, a common approach to process unbounded data was to
    repeatedly schedule jobs over small batches of incoming events on a batch processing
    system. When a job finishes, the result is written to persistent storage, and
    all operator state is lost. Once a job is scheduled for execution on the next
    batch, it cannot access the state of the previous job. This problem is commonly
    solved by delegating state management to an external system, such as a database.
    In contrast, in continuously running streaming jobs, state is durable across events
    and we can expose it as a first-class citizen in the programming model. Arguably,
    we could use an external system to also manage streaming state, even though this
    design choice might introduce additional latency.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果考虑使用批处理系统来分析无界数据集的情况，则状态的重要性变得更加明显。在现代流处理器崛起之前，处理无界数据的常见方法是在批处理系统上重复调度小批量传入事件的作业。作业完成后，结果写入持久存储，并且所有操作符状态都丢失。一旦作业计划在下一批次上执行，它就无法访问前一个作业的状态。通常通过将状态管理委托给外部系统（如数据库）来解决此问题。相比之下，在持续运行的流作业中，状态跨事件是持久的，并且可以在编程模型中将其公开为一流对象。可以说，即使在流状态中使用外部系统管理，这种设计选择可能会引入额外的延迟。
- en: Since streaming operators process potentially unbounded data, caution should
    be taken to not allow internal state to grow indefinitely. To limit the state
    size, operators usually maintain some kind of summary or *synopsis* of the events
    seen so far. Such a summary can be a count, a sum, a sample of the events seen
    so far, a window buffer, or a custom data structure that preserves some property
    of interest to the running application.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于流式操作符处理潜在的无界数据，必须小心，以防止内部状态无限增长。为了限制状态大小，操作符通常会维护某种事件概要或*摘要*。这样的摘要可以是计数、求和、迄今为止看到的事件的样本、窗口缓冲区或保留某个应用程序感兴趣属性的自定义数据结构。
- en: 'As you might imagine, supporting stateful operators comes with a few implementation
    challenges:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可以想象的那样，支持有状态操作符存在一些实现挑战：
- en: State management
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 状态管理
- en: The system needs to efficiently manage the state and make sure it is protected
    from concurrent updates.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 系统需要高效地管理状态，并确保免受并发更新的影响。
- en: State partitioning
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 状态分区
- en: Parallelization gets complicated, since results depend on both the state and
    incoming events. Fortunately, in many cases, you can partition the state by a
    key and manage the state of each partition independently. For example, if you
    are processing a stream of measurements from a set of sensors, you can use a partitioned
    operator state to maintain state for each sensor independently.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化变得复杂，因为结果取决于状态和传入事件。幸运的是，在许多情况下，您可以按键对状态进行分区，并独立管理每个分区的状态。例如，如果您正在处理一组传感器的测量流，您可以使用分区操作状态来独立维护每个传感器的状态。
- en: State recovery
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 状态恢复
- en: The third and biggest challenge that comes with stateful operators is ensuring
    that state can be recovered and results will be correct even in the presence of
    failures.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有状态操作符的第三个和最大的挑战是确保状态可以在故障的情况下恢复，并且结果将正确。
- en: In the next section, we discuss task failures and result guarantees in detail.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将详细讨论任务失败和结果保证。
- en: Task Failures
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务失败
- en: Operator state in streaming jobs is very valuable and should be guarded against
    failures. If state gets lost during a failure, results will be incorrect after
    recovery. Streaming jobs run for long periods of time, and thus state might be
    collected over several days or even months. Reprocessing all input to reproduce
    lost state in the case of failures would be both very expensive and time-consuming.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 流作业中的操作符状态非常有价值，并且应当受到故障保护。如果在故障期间丢失状态，恢复后的结果将不正确。流作业长时间运行，因此状态可能会在几天甚至几个月内收集。在故障情况下，重新处理所有输入以重现丢失的状态将非常昂贵且耗时。
- en: In the beginning of this chapter, you saw how you can model streaming programs
    as dataflow graphs. Before execution, these are translated into physical dataflow
    graphs of connected parallel tasks, each running some operator logic, consuming
    input streams and producing output streams for other tasks. Typical real-world
    setups can easily have hundreds of such tasks running in parallel on many physical
    machines. In long-running, streaming jobs, each of these tasks can fail at any
    time. How can you ensure that such failures are handled transparently so that
    your streaming job can continue to run? In fact, you would like your stream processor
    to not only continue processing in the case of task failures, but also provide
    correctness guarantees about the result and operator state. We discuss all these
    matters in this section.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开头，您看到如何将流处理程序建模为数据流图。在执行之前，这些被转化为由连接的并行任务组成的物理数据流图，每个任务运行某些操作逻辑，消耗输入流并为其他任务生成输出流。典型的实际设置可以在许多物理机器上并行运行数百个这样的任务。在长时间运行的流作业中，每个任务可以随时失败。如何确保这些故障被透明处理，以便您的流作业可以继续运行？事实上，您希望您的流处理器不仅在任务失败的情况下继续处理，而且还提供有关结果和操作状态的正确性保证。我们在本节讨论所有这些问题。
- en: What is a task failure?
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是任务失败？
- en: 'For each event in the input stream, a task is a processing step that performs
    the following steps: (1) receives the event, storing it in a local buffer; (2)
    possibly updates internal state; and (3) produces an output record. A failure
    can occur during any of these steps and the system has to clearly define its behavior
    in a failure scenario. If the task fails during the first step, will the event
    get lost? If it fails after it has updated its internal state, will it update
    it again after it recovers? And in those cases, will the output be deterministic?'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入流中的每个事件，任务是执行以下步骤的处理步骤：（1）接收事件，将其存储在本地缓冲区中；（2）可能更新内部状态；和（3）生成一个输出记录。在任何这些步骤中都可能发生故障，系统必须在故障情况下明确定义其行为。如果任务在第一步失败，事件会丢失吗？如果在更新内部状态后失败，系统会在恢复后再次更新它吗？在这些情况下，输出是否是确定性的？
- en: Note
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We assume reliable network connections, and that no records are dropped or duplicated
    and all events are eventually delivered to their destination in FIFO order. Note
    that Flink uses TCP connections, and thus these requirements are guaranteed. We
    also assume perfect failure detectors and that no task will intentionally act
    maliciously, meaning all nonfailed tasks follow the above steps.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设网络连接是可靠的，并且没有记录会被丢弃或复制，所有事件最终按照 FIFO 顺序传递到其目的地。请注意，Flink 使用 TCP 连接，因此可以保证这些要求。我们还假设存在完美的故障检测器，并且没有任务会故意恶意行事，这意味着所有未失败的任务都遵循上述步骤。
- en: In a batch processing scenario, all these questions are answered because a batch
    job can be simply restarted from the beginning. Hence, no events are lost and
    the state is completely built up from scratch. In the streaming world, however,
    dealing with failures is not a trivial problem. Streaming systems define their
    behavior in the presence of failures by offering result guarantees. Next, we review
    the types of guarantees offered by modern stream processors and some of the mechanisms
    systems implement to achieve those guarantees.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在批处理场景中，所有这些问题都有答案，因为可以简单地从头重新启动批处理作业。因此，不会丢失任何事件，并且状态完全是从头构建起来的。然而，在流处理世界中，处理故障并非一个简单的问题。流处理系统通过提供结果保证来定义其在故障情况下的行为。接下来，我们将审查现代流处理器提供的保证类型以及系统实现这些保证所采用的一些机制。
- en: Result Guarantees
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果保证
- en: Before we describe the different types of guarantees, we need to clarify a few
    points that are often the source of confusion when discussing task failures in
    stream processors. In the rest of this chapter, when we talk about “result guarantees”
    we mean the consistency of the internal state of the stream processor. That is,
    we are concerned with what the application code sees as state value after recovering
    from a failure. Note that guaranteeing the consistency of an application’s state
    is not the same a guaranteeing consistency of its output. Once data has been emitted
    to a sink, it is difficult to guarantee result correctness, unless the sink system
    supports transactions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们描述不同类型的保证之前，我们需要澄清一些经常在讨论流处理器中任务失败时引起混淆的要点。在本章的其余部分中，当我们谈论“结果保证”时，我们指的是流处理器内部状态的一致性。也就是说，我们关心的是应用代码在从故障中恢复后看到的状态值的一致性。请注意，保证应用程序状态的一致性并不等同于保证其输出的一致性。一旦数据已经被发送到接收器，除非接收系统支持事务，否则很难保证结果的正确性。
- en: At-most-once
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**最多一次**'
- en: The simplest thing to do when a task fails is to do nothing to recover lost
    state and replay lost events. At-most-once is the trivial case that guarantees
    processing of each event at most once. In other words, events can be simply dropped
    and nothing is done to ensure result correctness. This type of guarantee is also
    known as “no guarantee” since even a system that drops every event can provide
    this guarantee. Having no guarantees whatsoever sounds like a terrible idea, but
    it might be fine if you can live with approximate results and all you care about
    is providing the lowest latency possible.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务失败时，最简单的做法是不采取任何措施来恢复丢失的状态，并重播丢失的事件。最多一次是保证每个事件最多处理一次的特例。换句话说，事件可以简单地被丢弃，而且没有任何措施来确保结果的正确性。这种保证也被称为“无保证”，因为即使是每个事件都被丢弃的系统也可以提供此保证。完全没有保证听起来像是一个糟糕的主意，但如果您可以接受近似结果，并且您所关心的只是提供尽可能低的延迟，那么这可能是可以接受的。
- en: At-least-once
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**至少一次**'
- en: In most real-world applications, the expectation is that events should not get
    lost. This type of guarantee is called at-least-once, and it means that all events
    will be processed, and there is a chance that some of them are processed more
    than once. Duplicate processing might be acceptable if application correctness
    only depends on the completeness of information. For example, determining whether
    a specific event occurs in the input stream can be correctly realized with at-least-once
    guarantees. In the worst case, you will locate the event more than once. However,
    counting how many times a specific event occurs in the input stream might return
    the wrong result under at-least-once guarantees.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实际应用程序中，事件不应丢失是期望。这种保证称为至少一次，意味着所有事件都将被处理，并且可能会有一些事件被处理多次。如果应用程序的正确性仅依赖于信息的完整性，则重复处理可能是可以接受的。例如，确定特定事件是否发生在输入流中可以通过至少一次保证正确实现。在最坏的情况下，您可能会定位到多次事件。然而，在至少一次保证下，计算特定事件在输入流中发生的次数可能会返回错误的结果。
- en: In order to ensure at-least-once result correctness, you need to have a way
    to replay events—either from the source or from some buffer. Persistent event
    logs write all events to durable storage, so that they can be replayed if a task
    fails. Another way to achieve equivalent functionality is using record acknowledgments.
    This method stores every event in a buffer until its processing has been acknowledged
    by all tasks in the pipeline, at which point the event can be discarded.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保至少一次的结果正确性，您需要一种重播事件的方法——无论是从源头还是从某个缓冲区。持久性事件日志将所有事件写入持久存储，以便在任务失败时可以重播。实现等效功能的另一种方法是使用记录确认。该方法将每个事件存储在缓冲区中，直到所有流水线中的任务都确认了其处理，此时可以丢弃该事件。
- en: Exactly-once
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**至少一次**'
- en: Exactly-once is the strictest guarantee and hard to achieve. Exactly-once means
    that not only will there be no event loss, but also updates on the internal state
    will be applied exactly once for each event. In essence, exactly-once guarantees
    mean that our application will provide the correct result, as though a failure
    never happened.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确一次**是最严格的保证，也是难以实现的。精确一次意味着不仅不会丢失事件，而且内部状态的更新将仅应用一次于每个事件。实质上，精确一次保证意味着我们的应用将提供正确的结果，就像从未发生过故障一样。'
- en: Providing exactly-once guarantees requires at-least-once guarantees, and thus
    a data replay mechanism is again necessary. Additionally, the stream processor
    needs to ensure internal state consistency. That is, after recovery, it should
    know whether an event update has already been reflected on the state or not. Transactional
    updates are one way to achieve this result, but they can incur substantial performance
    overhead. Instead, Flink uses a lightweight snapshotting mechanism to achieve
    exactly-once result guarantees. We discuss Flink’s fault-tolerance algorithm in
    [“Checkpoints, Savepoints, and State Recovery”](ch03.html#chap-3-checkpoints).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 提供精确一次保证需要至少一次保证，因此再次需要数据重放机制。此外，流处理器需要确保内部状态的一致性。也就是说，在恢复之后，它应该知道事件更新是否已经反映在状态上。事务更新是实现此结果的一种方式，但可能会带来相当大的性能开销。相反，Flink使用轻量级的快照机制来实现精确一次的结果保证。我们在[“检查点、保存点和状态恢复”](ch03.html#chap-3-checkpoints)中讨论了Flink的容错算法。
- en: End-to-end exactly-once
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 端到端精确一次
- en: The types of guarantees you have seen so far refer to the state of an application
    that is managed by the stream processor. In a real-world streaming application
    however, there will be at least one source and one sink apart from the stream
    processor. End-to-end guarantees refer to result correctness across the whole
    data processing pipeline. Each component provides its own guarantees and the end-to-end
    guarantee of the complete pipeline would be the weakest of each of its components.
    It is important to note that sometimes you can get stronger semantics with weaker
    guarantees. A common case is when a task performs idempotent operations, like
    maximum or minimum. In this case, you can achieve exactly-once semantics with
    at-least-once guarantees.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止您已经看到的保证类型是指由流处理器管理的应用程序状态。然而，在真实的流处理应用程序中，除了流处理器之外，至少还会有一个源和一个汇。端到端保证指的是整个数据处理流水线的结果正确性。每个组件都提供自己的保证，整个流水线的端到端保证将是每个组件中最弱的保证。重要的是要注意，有时候您可以通过更弱的保证获得更强的语义。一个常见的情况是，当任务执行幂等操作时，比如最大值或最小值。在这种情况下，您可以通过至少一次保证实现精确一次语义。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned the fundamentals of data stream processing. We
    looked at the dataflow programming model and learned how streaming applications
    can be expressed as distributed dataflow graphs. Next, you learned the requirements
    of processing infinite streams in parallel and saw the importance of latency and
    throughput for stream applications. We covered basic streaming operations and
    how to compute meaningful results on unbounded input data using windows. You learned
    the meaning of time in stream processing and compared the notions of event time
    and processing time. Finally, we learned why state is important in streaming applications
    and how to guard it against failures and guarantee correct results.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了数据流处理的基础知识。我们研究了数据流编程模型，并学习了如何将流处理应用程序表达为分布式数据流图。接下来，您学习了在并行处理无限流时的要求，并了解了流应用程序中延迟和吞吐量的重要性。我们介绍了基本的流操作，以及如何使用窗口在无界输入数据上计算有意义的结果。您学习了流处理中时间的含义，并比较了事件时间和处理时间的概念。最后，我们学习了在流处理应用程序中状态的重要性，以及如何保护它免受故障并保证正确的结果。
- en: Up to this point, we have considered streaming concepts independently of Apache
    Flink. In the rest of this book, we are going to see how Flink actually implements
    these concepts and how you can use its DataStream API to write applications that
    use all of the features we have introduced so far.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经独立于Apache Flink考虑了流式概念。在本书的其余部分，我们将看到Flink如何实际实现这些概念，以及如何使用其DataStream
    API编写应用程序，以利用我们迄今为止介绍的所有功能。
