- en: Chapter 10\. The Evolution of Large-Scale Data Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。大规模数据处理的演变
- en: You have now arrived at the final chapter in the book, you stoic literate, you.
    Your journey will soon be complete!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经到达了这本书的最后一章，你这位坚忍的有文化的人。你的旅程很快就要结束了！
- en: To wrap things up, I’d like you to join me on a brief stroll through history,
    starting back in the ancient days of large-scale data processing with MapReduce
    and touching upon some of the highlights over the ensuing decade and a half that
    have brought streaming systems to the point they’re at today. It’s a relatively
    lightweight chapter in which I make a few observations about important contributions
    from a number of well-known systems (and a couple maybe not-so-well known), refer
    you to a bunch of source material you can go read on your own should you want
    to learn more, all while attempting not to offend or inflame the folks responsible
    for systems whose truly impactful contributions I’m going to either oversimplify
    or ignore completely for the sake of space, focus, and a cohesive narrative. Should
    be a good time.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想邀请你和我一起进行一个简短的历史漫步，从MapReduce的大规模数据处理的古老时代开始，触及在随后的十五年中带来流式系统到今天这一点的一些亮点。这是一个相对轻松的章节，我在其中对一些著名系统（也许还有一些不太知名的系统）的重要贡献做了一些观察，并引用了一些你可以自己阅读的来源材料，如果你想了解更多，同时尽量不冒犯或激怒那些负责的系统的人，他们的真正有影响力的贡献我要么过于简化，要么完全忽略了，这是为了节省空间，专注和连贯的叙述。应该会很有趣。
- en: On that note, keep in mind as you read this chapter that we’re really just talking
    about specific pieces of the MapReduce/Hadoop family tree of large-scale data
    processing here. I’m not covering the SQL arena in any way shape or form¹; we’re
    not talking HPC/supercomputers, and so on. So as broad and expansive as the title
    of this chapter might sound, I’m really focusing on a specific vertical swath
    of the grand universe of large-scale data processing. Caveat literatus, and all
    that.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章时，请记住，我们实际上只是在谈论大规模数据处理中MapReduce/Hadoop家族的特定部分。我没有以任何方式涵盖SQL领域；我们不讨论HPC/超级计算机等。因此，尽管本章的标题听起来可能很广泛和广泛，但我实际上是在专注于大规模数据处理的宏大宇宙中的一个特定垂直领域。文学警告，等等。
- en: 'Also note that I’m covering a disproportionate amount of Google technologies
    here. You would be right in thinking that this might have something to do with
    the fact that I’ve worked at Google for more than a decade. But there are two
    other reasons for it: 1) big data has always been important for Google, so there
    have been a number of worthwhile contributions created there that merit discussing
    in detail, and 2) my experience has been that folks outside of Google generally
    seem to enjoy learning more about the things we’ve done, because we as a company
    have historically been somewhat tight-lipped in that regard. So indulge me a bit
    while I prattle on excessively about the stuff we’ve been working on behind closed
    doors.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意的是，我在这里涵盖了不成比例的谷歌技术。你会认为这可能与我在谷歌工作了十多年有关。但还有两个原因：1）大数据一直对谷歌很重要，因此在那里创造了一些值得详细讨论的有价值的贡献，2）我的经验是，谷歌以外的人似乎很喜欢了解我们所做的事情，因为我们公司在这方面历来有点守口如瓶。所以请容我多说一些我们在闭门造车方面的工作。
- en: To ground our travels in concrete chronology, we’ll be following the timeline
    in Figure 10-1, which shows rough dates of existence for the various systems I
    discuss.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的旅程有具体的时间线，我们将遵循图10-1中显示的时间线，该时间线显示了我讨论的各种系统的大致存在日期。
- en: '![](img/stsy_1001.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1001.png)'
- en: Figure 10-1\. Approximate timeline of systems discussed in this chapter
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1。本章讨论的系统的大致时间线
- en: At each stop, I give a brief history of the system as best I understand it and
    frame its contributions from the perspective of shaping streaming systems as we
    know them today. At the end, we recap all of the contributions to see how they’ve
    summed up to create the modern stream processing ecosystem of today.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个停留点，我都会简要介绍我对系统的历史的理解，并从塑造我们今天所知的流式系统的角度来框定它的贡献。最后，我们将总结所有的贡献，看看它们如何总结出今天的现代流处理生态系统。
- en: MapReduce
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce
- en: We begin the journey with MapReduce (Figure 10-2).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从MapReduce（图10-2）开始旅程。
- en: '![](img/stsy_1002.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1002.png)'
- en: 'Figure 10-2\. Timeline: MapReduce'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2。时间线：MapReduce
- en: 'I think it’s safe to say that large-scale data processing as we all know it
    today got its start with MapReduce way back in 2003.² At the time, engineers within
    Google were building all sorts of bespoke systems to tackle data processing challenges
    at the scale of the World Wide Web.³ As they did so, they noticed three things:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为可以说，今天我们所知道的大规模数据处理始于2003年的MapReduce。在当时，谷歌内部的工程师们正在构建各种定制系统，以解决全球网络规模的数据处理挑战。当他们这样做时，他们注意到了三件事：
- en: Data processing is hard
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理很难
- en: As the data scientists and engineers among us well know, you can build a career
    out of just focusing on the best ways to extract useful insights from raw data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们中的数据科学家和工程师所知，你可以通过专注于从原始数据中提取有用的见解来建立职业。
- en: Scalability is hard
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性很难
- en: Extracting useful insights over massive-scale data is even more difficult yet.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模数据上提取有用的见解更加困难。
- en: Fault-tolerance is hard
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 容错性很难
- en: Extracting useful insights from massive-scale data in a fault-tolerant, correct
    way on commodity hardware is brutal.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在商品硬件上以容错的正确方式从大规模数据中提取有用的见解是残酷的。
- en: After solving all three of these challenges in tandem across a number of use
    cases, they began to notice some similarities between the custom systems they’d
    built. And they came to the conclusion that if they could build a framework that
    took care of the latter two issues (scalability and fault-tolerance), it would
    make focusing on the first issue a heck of a lot simpler. Thus was born MapReduce.⁴
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在同时解决了这三个挑战之后，他们开始注意到他们构建的定制系统之间的一些相似之处。他们得出结论，如果他们能够构建一个框架来解决后两个问题（可扩展性和容错性），那么专注于第一个问题将会变得简单得多。于是MapReduce诞生了。⁴
- en: 'The basic idea with MapReduce was to provide a simple data processing API centered
    around two well-understand operations from the functional programming realm: map
    and reduce (Figure 10-3). Pipelines built with that API would then be executed
    on a distributed systems framework that took care of all the nasty scalability
    and fault-tolerance stuff that quickens the hearts of hardcore distributed-systems
    engineers and crushes the souls of the rest of us mere mortals.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce的基本思想是提供围绕函数式编程领域中两个广为人知的操作map和reduce（图10-3）的简单数据处理API。使用该API构建的流水线将在一个分布式系统框架上执行，该框架负责处理所有让核心分布式系统工程师激动不已并压垮我们这些凡人灵魂的可扩展性和容错性问题。
- en: '![](img/stsy_1003.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1003.png)'
- en: Figure 10-3\. Visualization of a MapReduce job
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3。MapReduce作业的可视化
- en: 'We already discussed the semantics of MapReduce in great detail back in Chapter 6,
    so we won’t dwell on them here. Simply recall that we broke things down into six
    discrete phases (MapRead, Map, MapWrite, ReduceRead, Reduce, ReduceWrite) as part
    of our streams and tables analysis, and we came to the conclusion in the end that
    there really wasn’t all that much different between the overall Map and Reduce
    phases; at a high-level, they both do the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第6章中已经非常详细地讨论了MapReduce的语义，所以我们不会在这里详细讨论。只需记住，我们将事情分解为六个离散阶段（MapRead、Map、MapWrite、ReduceRead、Reduce、ReduceWrite），作为我们的流和表分析的一部分，最终我们得出结论，总的来说，Map和Reduce阶段并没有太大的不同；在高层次上，它们都做以下工作：
- en: Convert a table to a stream
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将表转换为流
- en: Apply a user transformation to that stream to yield another stream
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对该流应用用户转换以产生另一个流
- en: Group that stream into a table
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将该流分组成表
- en: After it was placed into service within Google, MapReduce found such broad application
    across a variety of tasks that the team decided it was worth sharing its ideas
    with the rest of the world. The result was the [MapReduce paper](https://goo.gl/Rsqr3G),
    published at OSDI 2004 (see Figure 10-4).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在谷歌内部投入使用后，MapReduce在各种任务中得到了广泛的应用，团队决定值得与世界其他地方分享他们的想法。结果就是[MapReduce论文](https://goo.gl/Rsqr3G)，发表于OSDI
    2004（见图10-4）。
- en: '![](img/stsy_1004.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1004.png)'
- en: Figure 10-4\. The [MapReduce paper](https://goo.gl/Rsqr3G), published at OSDI
    2004
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4。[MapReduce论文](https://goo.gl/Rsqr3G)，发表于OSDI 2004
- en: In it, the team described in detail the history of the project, design of the
    API and implementation, and details about a number of different use cases to which
    MapReduce had been applied. Unfortunately, they provided no actual source code,
    so the best that folks outside of Google at the time could do was say, “Yes, that
    sounds very nice indeed,” and go back to building their bespoke systems.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中，团队详细描述了项目的历史，API的设计和实施，以及MapReduce应用的许多不同用例的细节。不幸的是，他们没有提供实际的源代码，因此当时谷歌以外的人们能做的就是说，“是的，这听起来确实很不错”，然后回去构建他们定制的系统。
- en: Over the course of the decade that followed, MapReduce continued to undergo
    heavy development within Google, with large amounts of time invested in making
    the system scale to unprecedented levels. For a more detailed account of some
    of the highlights along that journey, I recommend the post [“History of massive-scale
    sorting experiments at Google”](http://bit.ly/2LPvuVN) (Figure 10-5) written by
    our official MapReduce historian/scalability and performance wizard, Marián Dvorský.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的十年中，MapReduce在谷歌内部继续进行大量的开发，投入大量时间使系统扩展到前所未有的规模。对于这段旅程中的一些亮点的更详细描述，我推荐我们的官方MapReduce历史学家/可扩展性和性能专家Marián
    Dvorský撰写的文章[“Google大规模排序实验的历史”](http://bit.ly/2LPvuVN)（图10-5）。
- en: '![](img/stsy_1005.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1005.png)'
- en: Figure 10-5\. Marián Dvorský’s [“History of massive-scale sorting experiments”](http://bit.ly/2LPvuVN)
    blog post
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5。Marián Dvorský的[“大规模排序实验历史”](http://bit.ly/2LPvuVN)博客文章
- en: But for our purposes here, suffice it to say that nothing else yet has touched
    the magnitude of scale achieved by MapReduce, not even within Google. Considering
    how long MapReduce has been around, that’s saying something; 14 years is an eternity
    in our industry.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 但就我们在这里的目的而言，可以说迄今为止没有其他东西能够触及MapReduce所实现的规模，甚至在谷歌内部也是如此。考虑到MapReduce已经存在了这么长时间，这是一个值得注意的事实；在我们的行业中，14年就是一个漫长的时代。
- en: From a streaming systems perspective, the main takeaways I want to leave you
    with for MapReduce are *simplicity* and *scalability*. MapReduce took the first
    brave steps toward taming the unruly beast that is massive-scale data processing,
    exposing a simple and straightforward API for crafting powerful data processing
    pipelines, its austerity belying the complex distributed systems magic happening
    under the covers to allow those pipelines to run at scale on large clusters of
    commodity hardware.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从流系统的角度来看，我想要给MapReduce留下的主要印象是*简单*和*可扩展性*。MapReduce迈出了驯服大规模数据处理的第一步，提供了一个简单而直接的API，用于构建强大的数据处理流水线，其简约性掩盖了在幕后发生的复杂分布式系统魔术，使这些流水线能够在大规模的廉价硬件集群上运行。
- en: Hadoop
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop
- en: 'Next in our list is Hadoop (Figure 10-6). Fair warning: this is one of those
    times where I will grossly oversimplify the impact of a system for the sake of
    a focused narrative. The impact Hadoop has had on our industry and the world at
    large cannot be overstated, and it extends well beyond the relatively specific
    scope I discuss here.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是Hadoop（图10-6）。警告：这是我为了集中叙述而大大简化系统影响的情况之一。Hadoop对我们行业和整个世界的影响无法言喻，它远远超出了我在这里讨论的相对特定的范围。
- en: '![](img/stsy_1006.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1006.png)'
- en: 'Figure 10-6\. Timeline: Hadoop'
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6. 时间轴：Hadoop
- en: Hadoop came about in 2005, when Doug Cutting and Mike Cafarella decided that
    the ideas from the MapReduce paper were just the thing they needed as they built
    a distributed version of their Nutch webcrawler. They had already built their
    own version of Google’s distributed filesystem (originally called NDFS for Nutch
    Distributed File System, later renamed to HDFS, or Hadoop Distributed File System),
    so it was a natural next step to add a MapReduce layer on top after that paper
    was published. They called this layer Hadoop.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop诞生于2005年，当Doug Cutting和Mike Cafarella决定将MapReduce论文中的想法用于构建他们的Nutch网络爬虫的分布式版本时。他们已经构建了自己的谷歌分布式文件系统的版本（最初称为NDFS，后来更名为HDFS，或者Hadoop分布式文件系统），因此在该论文发表后，将MapReduce层添加到其上是一个自然的下一步。他们称这个层为Hadoop。
- en: The key difference between Hadoop and MapReduce was that Cutting and Cafarella
    made sure the source code for Hadoop was shared with the rest of the world by
    open sourcing it (along with the source for HDFS) as part of what would eventually
    become the Apache Hadoop project. Yahoo’s hiring of Cutting to help transition
    the Yahoo webcrawler architecture onto Hadoop gave the project an additional boost
    of validity and engineering oomph, and from there, an entire ecosystem of open
    source data processing tools grew. As with MapReduce, others have told the history
    of Hadoop in other fora far better than I can; one particularly good reference
    is Marko Bonaci’s [“The history of Hadoop,”](http://bit.ly/2Kjc4fZ) itself originally
    slated for inclusion in a print book (Figure 10-7).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop和MapReduce的主要区别在于，Cutting和Cafarella确保Hadoop的源代码通过开源方式与世界分享（以及HDFS的源代码），作为最终成为Apache
    Hadoop项目的一部分。雅虎聘请Cutting帮助将雅虎网络爬虫架构转移到Hadoop，为该项目增添了额外的有效性和工程能量，从那时起，一个完整的开源数据处理工具生态系统就开始成长。与MapReduce一样，其他人已经在其他地方更好地讲述了Hadoop的历史；其中一个特别好的参考资料是Marko
    Bonaci的“Hadoop的历史”，最初计划包括在一本印刷书籍中（图10-7）。
- en: '![](img/stsy_1007.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1007.png)'
- en: Figure 10-7\. Marko Bonaci’s [“The history of Hadoop”](http://bit.ly/2Kjc4fZ)
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-7. Marko Bonaci的“Hadoop的历史”
- en: The main point I want you to take away from this section is the massive impact
    the *open source ecosystem* that flowered around Hadoop had upon the industry
    as a whole. By creating an open community in which engineers could improve and
    extend the ideas from those early GFS and MapReduce papers, a thriving ecosystem
    was born, yielding dozens of useful tools like Pig, Hive, HBase, Crunch, and on
    and on. That openness was key to incubating the diversity of ideas that exist
    now across our industry, and it’s why I’m pigeonholing Hadoop’s open source ecosystem
    as its single most important contribution to the world of streaming systems as
    we know them today.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你从本节中得到的主要观点是，Hadoop周围蓬勃发展的开源生态系统对整个行业产生了巨大影响。通过创建一个开放的社区，工程师可以改进和扩展早期GFS和MapReduce论文中的想法，一个繁荣的生态系统诞生了，产生了许多有用的工具，如Pig、Hive、HBase、Crunch等等。这种开放性对于培育我们行业现在存在的多样化思想至关重要，这也是为什么我将Hadoop的开源生态系统定位为其对我们今天所知的流处理系统世界的最重要贡献。
- en: Flume
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flume
- en: 'We now return to Google territory to talk about the official successor to MapReduce
    within Google: Flume ([Figure 10-8] sometimes also called FlumeJava in reference
    to the original Java version of the system, and not to be confused with Apache
    Flume, which is an entirely different beast that just so happens to share the
    same name).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们回到谷歌领地，谈论谷歌内部MapReduce的官方继任者：Flume（图10-8有时也被称为FlumeJava，指的是系统的原始Java版本，并且不要与Apache
    Flume混淆，后者是一个完全不同的系统，只是碰巧有相同的名称）。
- en: '![](img/stsy_1008.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1008.png)'
- en: 'Figure 10-8\. Timeline: Flume'
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-8. 时间轴：Flume
- en: 'The Flume project was founded by Craig Chambers when the Google Seattle office
    opened in 2007\. It was motivated by a desire to solve some of the inherent shortcomings
    of MapReduce, which had become apparent over the first few years of its success.
    Many of these shortcomings revolved around MapReduce’s rigid Map → Shuffle → Reduce
    structure; though refreshingly simple, it carried with it some downsides:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Flume项目是由Craig Chambers在2007年谷歌西雅图办公室开设时创建的。它的动机是解决MapReduce的一些固有缺陷，这些缺陷在其成功的最初几年中变得明显。其中许多缺陷围绕着MapReduce的严格的Map→Shuffle→Reduce结构；尽管简单清晰，但它也带来了一些不利因素：
- en: Because many use cases cannot be served by the application of a single MapReduce,
    a number of bespoke *orchestration systems* began popping up across Google for
    coordinating sequences of MapReduce jobs. These systems all served essentially
    the same purpose (gluing together multiple MapReduce jobs to create a coherent
    pipeline solving a complex problem). However, having been developed independently,
    they were naturally incompatible and a textbook example of unnecessary duplication
    of effort.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为许多用例无法通过单个MapReduce的应用来解决，谷歌开始出现了许多定制的“编排系统”，用于协调一系列MapReduce作业。这些系统本质上都是为了相同的目的（将多个MapReduce作业粘合在一起，创建一个解决复杂问题的连贯管道）。然而，由于它们是独立开发的，它们自然是不兼容的，这是不必要的重复努力的典型例子。
- en: What’s worse, there were numerous cases in which a clearly written sequence
    of MapReduce jobs would introduce *inefficiencies* thanks to the rigid structure
    of the API. For example, one team might write a MapReduce that simply filtered
    out some number of elements; that is, a map-only job with an empty reducer. It
    might be followed up by another team’s map-only job doing some element-wise enrichment
    (with yet another empty reducer). The output from the second job might then finally
    be consumed by a final team’s MapReduce performing some grouping aggregation over
    the data. This pipeline, consisting of essentially a single chain of Map phases
    followed by a single Reduce phase, would require the orchestration of three completely
    independent jobs, each chained together by shuffle and output phases materializing
    the data. But that’s assuming you wanted to keep the codebase logical and clean,
    which leads to the final downside…
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更糟糕的是，有许多情况下，一个清晰的MapReduce作业序列会由于API的严格结构而引入*低效*。例如，一个团队可能会编写一个简单地过滤一些元素的MapReduce作业；也就是说，一个只有空reducer的仅映射作业。接着可能是另一个团队的仅映射作业，对元素进行逐个丰富（又是另一个空reducer）。然后第二个作业的输出可能最终被最后一个团队的MapReduce消费，对数据进行一些分组聚合。这个管道基本上由一个Map阶段的单一链条和一个Reduce阶段组成，需要通过完全独立的三个作业进行编排，每个作业通过洗牌和输出阶段将数据实现链在一起。但这是假设你想要保持代码库的逻辑和清晰，这导致了最终的缺点...
- en: In an effort to optimize away these inefficiencies in their MapReductions, engineers
    began introducing *manual optimizations* that would *obfuscate* the simple logic
    of the pipeline, increasing maintenance and debugging costs.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了消除这些MapReduce中的低效，工程师们开始引入*手动优化*，这将*混淆*管道的简单逻辑，增加了维护和调试成本。
- en: Flume addressed these issues by providing a composable, high-level API for describing
    data processing pipelines, essentially based around the same PCollection and PTransform
    concepts found in Beam, as illustrated in Figure 10-9.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Flume通过提供一个可组合的、高级的API来解决这些问题，用于描述数据处理管道，基本上是围绕Beam中发现的相同的PCollection和PTransform概念，如图10-9所示。
- en: '![](img/stsy_1009.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1009.png)'
- en: 'Figure 10-9\. High-level pipelines in Flume (image credit: Frances Perry)'
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-9。Flume中的高级管道（图片来源：Frances Perry）
- en: These pipelines, when launched, would be fed through an optimizer⁵ to generate
    a plan for an optimally efficient sequence of MapReduce jobs, the execution of
    which was then orchestrated by the framework, which you can see illustrated in
    Figure 10-10.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些管道在启动时将通过优化器⁵生成一个最优的MapReduce作业序列的计划，然后由框架进行编排，如图10-10所示。
- en: '![](img/stsy_1010.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1010.png)'
- en: Figure 10-10\. Optimization from a logical pipeline to a physical execution
    plan
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-10。从逻辑管道到物理执行计划的优化
- en: Perhaps the most important example of an automatic optimization that Flume can
    perform is fusion (which Reuven discussed a bit back in Chapter 5), in which two
    logically independent stages can be run in the same job either sequentially (consumer-producer
    fusion) or in parallel (sibling fusion), as depicted in Figure 10-11.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 也许Flume可以执行的最重要的自动优化示例是融合（Reuven在第5章中已经讨论过一些），在融合中，两个逻辑上独立的阶段可以在同一个作业中顺序运行（消费者-生产者融合）或并行运行（同级融合），如图10-11所示。
- en: '![](img/stsy_1011.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1011.png)'
- en: Figure 10-11\. Fusion optimizations combine successive or parallel operations
    together into the same physical operation
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-11。融合优化将连续或并行的操作合并到同一个物理操作中
- en: Fusing two stages together eliminates serialization/deserialization and network
    costs, which can be significant in pipelines processing large amounts of data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将两个阶段融合在一起可以消除序列化/反序列化和网络成本，这在处理大量数据的管道中可能是重要的。
- en: 'Another type of automatic optimization is *combiner lifting* (see Figure 10-12),
    the mechanics of which we already touched upon in Chapter 7 when we talked about
    incremental combining. Combiner lifting is simply the automatic application of
    multilevel combine logic that we discussed in that chapter: a combining operation
    (e.g., summation) that logically happens after a grouping operation is partially
    lifted into the stage preceding the group-by-key (which by definition requires
    a trip across the network to shuffle the data) so that it can perform partial
    combining before the grouping happens. In cases of very hot keys, this can greatly
    reduce the amount of data shuffled over the network, and also spread the load
    of computing the final aggregate more smoothly across multiple machines.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种自动优化类型是*组合器提升*（见图10-12），其机制我们在第7章已经提到，当时我们讨论了增量组合。组合器提升就是简单地应用我们在那一章讨论过的多级组合逻辑：在分组操作之后逻辑上发生的组合操作（例如求和）部分提升到分组之前的阶段（这根据定义需要通过网络进行数据洗牌），以便在分组发生之前进行部分组合。在非常热门的键的情况下，这可以大大减少在网络上传输的数据量，并且还可以更平稳地将最终聚合的负载分布在多台机器上。
- en: '![](img/stsy_1012.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1012.png)'
- en: Figure 10-12\. Combiner lifting applies partial aggregation on the sender side
    of a group-by-key operation before completing aggregation on the consumer side
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-12。组合器提升在消费者端的分组聚合之前对发送端进行部分聚合
- en: 'As a result of its cleaner API and automatic optimizations, Flume Java was
    an instant hit upon its introduction at Google in early 2009\. Following on the
    heels of that success, the team published the paper titled [“Flume Java: Easy,
    Efficient Data-Parallel Pipelines”](https://goo.gl/9e1nXf) (see Figure 10-13),
    itself an excellent resource for learning more about the system as it originally
    existed.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '由于其更清晰的API和自动优化，Flume Java在2009年初在Google推出后立即受到欢迎。在这一成功之后，团队发表了题为[“Flume Java:
    Easy, Efficient Data-Parallel Pipelines”](https://goo.gl/9e1nXf)的论文（见图10-13），这本身就是一个了解系统最初存在情况的绝佳资源。'
- en: '![](img/stsy_1013.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1013.png)'
- en: Figure 10-13\. [FlumeJava paper](https://goo.gl/9e1nXf)
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-13。[FlumeJava论文](https://goo.gl/9e1nXf)
- en: Flume C++ followed not too much later in 2011, and in early 2012 Flume was introduced
    into Noogler⁶ training provided to all new engineers at Google. That was the beginning
    of the end for MapReduce.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Flume C++在2011年不久后推出，2012年初，Flume被引入到Google为所有新工程师提供的Noogler⁶培训中。这标志着MapReduce的终结。
- en: Since then, Flume has been migrated to no longer use MapReduce as its execution
    engine; instead, it uses a custom execution engine, called Dax, built directly
    into the framework itself. By freeing Flume itself from the confines of the previously
    underlying Map → Shuffle → Reduce structure of MapReduce, Dax enabled new optimizations,
    such as the dynamic work rebalancing feature described in Eugene Kirpichov and
    Malo Denielou’s [“No shard left behind”](http://bit.ly/2JPaUnR) blog post (Figure 10-14).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自那时起，Flume已经迁移到不再使用MapReduce作为其执行引擎；相反，它使用了一个名为Dax的自定义执行引擎，直接构建在框架本身中。通过将Flume本身从以前的Map
    → Shuffle → Reduce结构的MapReduce的限制中解放出来，Dax实现了新的优化，例如Eugene Kirpichov和Malo Denielou在他们的[“No
    shard left behind”](http://bit.ly/2JPaUnR)博客文章中描述的动态工作再平衡功能（图10-14）。
- en: '![](img/stsy_1014.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1014.png)'
- en: Figure 10-14\. [“No shard left behind”](http://bit.ly/2JPaUnR) post
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-14。[“No shard left behind”](http://bit.ly/2JPaUnR)博客文章
- en: Though discussed in that post in the context of Cloud Dataflow, dynamic work
    rebalancing (or liquid sharding, as it’s colloquially known at Google) automatically
    rebalances extra work from straggler shards to other idle workers in the system
    as they complete their work early. By dynamically rebalancing the work distribution
    over time, it’s possible to come much closer to an optimal work distribution than
    even the best educated initial splits could ever achieve. It also allows for adapting
    to variations across the pool of workers, where a slow machine that might have
    otherwise held up the completion of a job is simply compensated for by moving
    most of its tasks to other workers. When liquid sharding was rolled out at Google,
    it recouped significant amounts of resources across the fleet.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在Cloud Dataflow的上下文中讨论，动态工作再平衡（或者在Google中俗称的液体分片）会自动将额外的工作从滞后的分片重新平衡到系统中其他空闲的工作者身上，因为他们提前完成了工作。通过随时间动态地重新平衡工作分配，可以更接近于最佳工作分配，甚至比最好的初始分配更接近。它还允许适应工作者池中的变化，其中一个慢速机器可能会延迟作业的完成，但通过将其大部分任务移交给其他工作者来进行补偿。当液体分片在Google推出时，它在整个系统中回收了大量资源。
- en: One last point on Flume is that it was also later extended to support streaming
    semantics. In addition to the batch Dax backend, Flume was extended to be able
    to execute pipelines on the MillWheel stream processing system (discussed in a
    moment). Most of the high-level streaming semantics concepts we’ve discussed in
    this book were first incorporated into Flume before later finding their way into
    Cloud Dataflow and eventually Apache Beam.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Flume的最后一点是，它后来还扩展到支持流处理语义。除了批处理的Dax后端外，Flume还扩展为能够在MillWheel流处理系统上执行管道（稍后将讨论）。我们在本书中讨论的大多数高级流处理语义概念最初都是在Flume中首次应用，然后才逐渐进入Cloud
    Dataflow，最终进入Apache Beam。
- en: All that said, the primary thing to take away from Flume in this section is
    the introduction of a notion of *high-level pipelines*, which enabled the *automatic
    optimization* of clearly written, logical pipelines. This enabled the creation
    of much larger and complex pipelines, without the need for manual orchestration
    or optimization, and all while keeping the code for those pipelines logical and
    clear.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在本节中从Flume中可以得出的主要观点是引入了“高级管道”的概念，这使得清晰编写的逻辑管道可以进行“自动优化”。这使得可以创建更大更复杂的管道，而无需手动编排或优化，并且同时保持这些管道的代码逻辑清晰。
- en: Storm
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm
- en: Next up is Apache Storm (Figure 10-15), the first real streaming system we cover.
    Storm most certainly wasn’t the first streaming system in existence, but I would
    argue it was the first streaming system to see truly broad adoption across the
    industry, and for that reason we give it a closer look here.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是Apache Storm（图10-15），这是我们涵盖的第一个真正的流处理系统。Storm肯定不是存在的第一个流处理系统，但我认为它是第一个在整个行业中得到广泛采用的流处理系统，因此我们在这里更仔细地研究它。
- en: '![](img/stsy_1015.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1015.png)'
- en: 'Figure 10-15\. Timeline: Storm'
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-15。时间轴：Storm
- en: '![](img/stsy_1016.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1016.png)'
- en: Figure 10-16\. [“History of Apache Storm and lessons learned”](http://bit.ly/2HLwSqd)
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-16。[“Apache Storm的历史和经验教训”](http://bit.ly/2HLwSqd)
- en: 'Storm was the brainchild of Nathan Marz, who later chronicled the history of
    its creation in a blog post titled [“History of Apache Storm and lessons learned”](http://bit.ly/2HLwSqd)
    (Figure 10-16). The TL;DR version of it is that Nathan’s team at the startup employing
    him then, BackType, had been attempting to process the Twitter firehose using
    a custom system of queues and workers. He came to essentially the same realization
    that the MapReduce folks had nearly a decade earlier: the actual data processing
    portion of their code was only a tiny amount of the system, and building those
    real-time data processing pipelines would be a lot easier if there were a framework
    doing all the distributed system’s dirty work under the covers. Out of that was
    born Storm.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Storm是Nathan Marz的创意，后来他在一篇名为[“Apache Storm的历史和经验教训”](http://bit.ly/2HLwSqd)的博客文章中详细记录了其创作历程（图10-16）。简而言之，Nathan所在的初创公司BackType一直在尝试使用自定义的队列和工作者系统来处理Twitter的数据流。他最终得出的结论与几乎十年前的MapReduce团队的结论基本相同：他们的代码中实际的数据处理部分只是系统的一小部分，如果有一个框架在幕后完成所有分布式系统的繁重工作，那么构建实时数据处理管道将会更容易。于是Storm诞生了。
- en: The interesting thing about Storm, in comparison to the rest of the systems
    we’ve talked about so far, is that the team chose to loosen the strong consistency
    guarantees found in all of the other systems we’ve talked about so far as a way
    of providing lower latency. By combining at-most once or at-least once semantics
    with per-record processing and no integrated (i.e., no consistent) notion of persistent
    state, Storm was able provide much lower latency in providing results than systems
    that executed over batches of data and guaranteed exactly-once correctness. And
    for a certain type of use cases, this was a very reasonable trade-off to make.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们迄今为止谈到的其他系统相比，Storm的有趣之处在于，团队选择放宽了所有其他系统中都有的强一致性保证，以提供更低的延迟。通过将最多一次或至少一次的语义与每条记录的处理和无集成（即无一致）的持久状态概念相结合，Storm能够以比执行数据批处理并保证一次性正确性的系统更低的延迟提供结果。对于某种类型的用例来说，这是一个非常合理的权衡。
- en: Unfortunately, it quickly became clear that people really wanted to have their
    cake and eat it, too. They didn’t just want to get their answers quickly, they
    wanted to have both low-latency results *and* eventual correctness. But such a
    thing was impossible with Storm alone. Enter the Lambda Architecture.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，很快就清楚地看到，人们确实希望既能快速得到答案，又能同时获得低延迟的结果和最终的正确性。但是仅凭Storm是不可能做到这一点的。于是Lambda架构应运而生。
- en: Given the limitations of Storm, shrewd engineers began running a weakly consistent
    Storm streaming pipeline alongside a strongly consistent Hadoop batch pipeline.
    The former produced low-latency, inexact results, whereas the latter produced
    high-latency, exact results, both of which would then be somehow merged together
    in the end to provide a single low-latency, eventually consistent view of the
    outputs. We learned back in Chapter 1 that the Lambda Architecture was Marz’s
    other brainchild, as detailed in his post titled [“How to beat the CAP theorem”](http://bit.ly/1ATyjbD)
    (Figure 10-17).⁷
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于Storm的局限性，精明的工程师开始在强一致的Hadoop批处理管道旁边运行一个弱一致的Storm流处理管道。前者产生低延迟、不精确的结果，而后者产生高延迟、精确的结果，然后这两者最终会以某种方式合并在一起，以提供单一的低延迟、最终一致的输出视图。我们在第1章中了解到Lambda架构是Marz的另一个创意，详细内容在他的帖子中介绍[“如何打败CAP定理”](http://bit.ly/1ATyjbD)（图10-17）。⁷
- en: '![](img/stsy_1017.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1017.png)'
- en: Figure 10-17\. [“How to beat the CAP theorem”](http://bit.ly/1ATyjbD)
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-17。[“如何打败CAP定理”](http://bit.ly/1ATyjbD)
- en: 'I’ve already spent a fair amount of time harping on the shortcomings of the
    Lambda Architecture, so I won’t belabor those points here. But I will reiterate
    this: the Lambda Architecture became quite popular, despite the costs and headaches
    associated with it, simply because it met a critical need that a great many businesses
    were otherwise having a difficult time fulfilling: that of getting low-latency,
    but eventually correct results out of their data processing pipelines.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经花了相当多的时间批评Lambda架构的缺点，所以我不会在这里再多加赘述。但我会重申一点：尽管成本高昂，带来了很多麻烦，Lambda架构仍然变得非常流行，仅仅是因为它满足了许多企业本来很难满足的关键需求：从数据处理管道中获得低延迟但最终正确的结果。
- en: From the perspective of the evolution of streaming systems, I argue that Storm
    was responsible for first bringing low-latency data processing to the masses.
    However, it did so at the cost of weak consistency, which in turn brought about
    the rise of the Lambda Architecture, and the years of dual-pipeline darkness that
    followed.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从流处理系统的演变角度来看，我认为Storm首次为大众带来了低延迟数据处理。然而，这是以弱一致性为代价的，这反过来导致了Lambda架构的兴起，以及随之而来的双管道黑暗时期。
- en: '![](img/stsy_1018.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1018.png)'
- en: Figure 10-18\. [Heron paper](http://bit.ly/2LNzOF4)
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-18。[Heron paper](http://bit.ly/2LNzOF4)
- en: But hyperbolic dramaticism aside, Storm was the system that gave the industry
    its first taste of low-latency data processing, and the impact of that is reflected
    in the broad interest in and adoption of streaming systems today.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，夸张的戏剧性除外，Storm是该行业首次尝试低延迟数据处理的系统，这一影响在今天对流处理系统的广泛兴趣和采用中得到体现。
- en: 'Before moving on, it’s also worth giving a shout out to Heron. In 2015, Twitter
    (the largest known user of Storm in the world, and the company that originally
    fostered the Storm project) surprised the industry by announcing it was abandoning
    the Storm execution engine in favor of a new system it had developed in house,
    called Heron. Heron aimed to address a number of performance and maintainability
    issues that had plagued Storm, while remaining API compatible, as detailed in
    the company’s paper titled [“Twitter Heron: Stream Processing at Scale”](http://bit.ly/2LNzOF4)
    (Figure 10-18). Heron itself was subsequently [open sourced](http://bit.ly/2MoOpYK)
    (with governance moved to its own independent foundation, not an existing one
    like Apache). Given the continued development on Storm, there are now two competing
    variants of the Storm lineage. Where things will end up is anyone’s guess, but
    it will be exciting to watch.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '在继续之前，也值得一提的是Heron。2015年，Twitter（全球已知最大的Storm用户，最初培育了Storm项目的公司）宣布放弃Storm执行引擎，转而采用公司内部开发的新系统Heron。Heron旨在解决困扰Storm的一些性能和可维护性问题，同时保持API兼容性，详细内容在公司的论文中介绍[“Twitter
    Heron: Stream Processing at Scale”](http://bit.ly/2LNzOF4)（图10-18）。Heron本身随后被[开源](http://bit.ly/2MoOpYK)（治理权转移到了自己独立的基金会，而不是像Apache那样的现有基金会）。鉴于Storm的持续发展，现在有两个竞争性的Storm变种。事情最终会如何发展，任何人都无法预测，但观察将会是令人兴奋的。'
- en: Spark
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark
- en: 'Moving on, we now come to Apache Spark (Figure 10-19). This is another section
    in which I’m going to greatly oversimplify the total impact that Spark has had
    on the industry by focusing on a specific portion of its contributions: those
    within the realm of stream processing. Apologies in advance.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 继续前进，我们现在来到Apache Spark（图10-19）。这是另一个部分，我将通过专注于其在流处理领域的贡献来大大简化Spark对行业的总体影响。提前道歉。
- en: '![](img/stsy_1019.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1019.png)'
- en: 'Figure 10-19\. Timeline: Spark'
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-19。时间轴：Spark
- en: Spark got its start at the now famous AMPLab in UC Berkeley around 2009\. The
    thing that initially fueled Spark’s fame was its ability to oftentimes perform
    the bulk of a pipeline’s calculations entirely in memory, without touching disk
    until the very end. Engineers achieved this via the Resilient Distributed Dataset
    (RDD) idea, which basically captured the full lineage of data at any given point
    in the pipeline, allowing intermediate results to be recalculated as needed on
    machine failure, under the assumptions that a) your inputs were always replayable,
    and b) your computations were deterministic. For many use cases, these preconditions
    were true, or at least true enough given the massive gains in performance users
    were able to realize over standard Hadoop jobs. From there, Spark gradually built
    up its eventual reputation as Hadoop’s de facto successor.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Spark最初是在2009年左右在加州大学伯克利分校的著名AMPLab开始的。最初推动Spark声名鹊起的是其能够在大多数情况下完全在内存中执行管道的大部分计算，直到最后才触及磁盘。工程师们通过弹性分布式数据集（RDD）的概念实现了这一点，基本上捕获了管道中任何给定点的数据完整谱系，允许根据需要在机器故障时重新计算中间结果，假设a）您的输入始终可以重放，b）您的计算是确定性的。对于许多使用案例，这些前提条件是真实的，或者至少在性能方面相对真实，用户能够实现与标准Hadoop作业相比的巨大性能提升。从那时起，Spark逐渐建立起其作为Hadoop事实上的继任者的声誉。
- en: 'A few years after Spark was created, Tathagata Das, then a graduate student
    in the AMPLab, came to the realization that: hey, we’ve got this fast batch processing
    engine, what if we just wired things up so we ran multiple batches one after another,
    and used that to process streaming data? From that bit of insight, Spark Streaming
    was born.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Spark创建几年后，当时是AMPLab的研究生的Tathagata Das意识到：嘿，我们有这个快速的批处理引擎，如果我们将多个批处理依次运行，并使用它来处理流数据怎么样？从这点洞察力出发，Spark
    Streaming诞生了。
- en: 'What was really fantastic about Spark Streaming was this: thanks to the strongly
    consistent batch engine powering things under the covers, the world now had a
    stream processing engine that could provide correct results all by itself without
    needing the help of an additional batch job. In other words, given the right use
    case, you could ditch your Lambda Architecture system and just use Spark Streaming.
    All hail Spark Streaming!'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming真正了不起的地方在于：由于在幕后支持一致性批处理引擎，世界现在拥有了一个流处理引擎，可以在不需要额外批处理作业的情况下自行提供正确的结果。换句话说，根据正确的使用案例，您可以放弃Lambda架构系统，只使用Spark
    Streaming。万岁Spark Streaming！
- en: 'The one major caveat here was the “right use case” part. The big downside to
    the original version of Spark Streaming (the 1.x variants) was that it provided
    support for only a specific flavor of stream processing: processing-time windowing.
    So any use case that cared about event time, needed to deal with late data, and
    so on, couldn’t be handled out of the box without a bunch of extra code being
    written by the user to implement some form of event-time handling on top of Spark’s
    processing-time windowing architecture. This meant that Spark Streaming was best
    suited for in-order data or event-time-agnostic computations. And, as I’ve reiterated
    throughout this book, those conditions are not as prevalent as you would hope
    when dealing with the large-scale, user-centric datasets common today.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个主要限制是“正确的使用案例”部分。 Spark Streaming的原始版本（1.x变体）的一个很大的缺点是它只支持特定类型的流处理：处理时间窗口。因此，任何关心事件时间、需要处理延迟数据等的使用案例，都需要用户编写大量额外的代码来实现某种形式的事件时间处理，以覆盖Spark的处理时间窗口架构。这意味着Spark
    Streaming最适合于顺序数据或事件时间不可知的计算。正如我在本书中一再强调的那样，这些条件在处理当今常见的大规模用户中心数据集时并不如你希望的那样普遍。
- en: Another interesting controversy that surrounds Spark Streaming is the age-old
    “microbatch versus true streaming” debate. Because Spark Streaming is built upon
    the idea of small, repeated runs of a batch processing engine, detractors claim
    that Spark Streaming is not a true streaming engine in the sense that progress
    in the system is gated by the global barriers of each batch. There’s some amount
    of truth there. Even though true streaming engines almost always utilize some
    sort of batching or bundling for the sake of throughput, they have the flexibility
    to do so at much finer-grained levels, down to individual keys. The fact that
    microbatch architectures process bundles at a global level means that it’s virtually
    impossible to have both low per-key latency and high overall throughput, and there
    are a number of benchmarks that have shown this to be more or less true. But at
    the same time, latency on the order of minutes or multiple seconds is still quite
    good. And there are very few use cases that demand exact correctness and such
    stringent latency capabilities. So in some sense, Spark was absolutely right to
    target the audience it did originally; most people fall in that category. But
    that hasn’t stopped its competitors from slamming this as a massive disadvantage
    for the platform. Personally, I see it as a minor complaint at best in most cases.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 围绕Spark Streaming的另一个有趣的争议是“微批处理与真正的流处理”的老问题。因为Spark Streaming建立在批处理引擎的小型重复运行的想法之上，批评者声称Spark
    Streaming不是真正的流处理引擎，因为系统中的进展受到每个批处理的全局障碍的限制。这里有一定的真相。尽管真正的流处理引擎几乎总是利用某种形式的批处理或捆绑以提高吞吐量，但它们有更高的灵活性，可以在更细粒度的级别进行，甚至可以到达单个键。微批处理架构在全局级别处理捆绑意味着几乎不可能同时具有低的每个键延迟和高的整体吞吐量，有许多基准测试表明这基本属实。但与此同时，以分钟或多秒为单位的延迟仍然相当不错。而且很少有使用案例要求确切的正确性和如此严格的延迟能力。因此，在某种程度上，Spark最初定位的受众是绝对正确的；大多数人都属于这一类别。但这并没有阻止竞争对手将其视为该平台的巨大劣势。就我个人而言，在大多数情况下，我认为这只是一个小小的抱怨。
- en: 'Shortcomings aside, Spark Streaming was a watershed moment for stream processing:
    the first publicly available, large-scale stream processing engine that could
    also provide the correctness guarantees of a batch system. And of course, as previously
    noted, streaming is only a very small part of Spark’s overall success story, with
    important contributions made in the space of iterative processing and machine
    learning, its native SQL integration, and the aforementioned lightning-fast in-memory
    performance, to name a few.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了缺点之外，Spark Streaming是流处理的一个分水岭时刻：首个公开可用的大规模流处理引擎，同时也能提供批处理系统的正确性保证。当然，正如之前提到的，流处理只是Spark整体成功故事的一小部分，它在迭代处理和机器学习领域做出了重要贡献，具有本地SQL集成，以及前面提到的快速内存性能等。
- en: If you’re curious to learn more about the details of the original Spark 1.x
    architecture, I highly recommend Matei Zaharia’s dissertation on the subject,
    [“An Architecture for Fast and General Data Processing on Large Clusters”](http://bit.ly/2y8rduN)
    (Figure 10-20). It’s 113 pages of Sparky goodness that’s well worth the investment.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对原始Spark 1.x架构的细节感兴趣，我强烈推荐马泰·扎哈里亚的论文，[“大规模集群上快速通用数据处理的架构”](http://bit.ly/2y8rduN)（图10-20）。这是113页的Spark精华，非常值得投资。
- en: '![](img/stsy_1020.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1020.png)'
- en: Figure 10-20\. [Spark dissertation](http://bit.ly/2y8rduN)
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-20\. [Spark论文](http://bit.ly/2y8rduN)
- en: As of today, the 2.x variants of Spark are greatly expanding upon the semantic
    capabilities of Spark Streaming, incorporating many parts of the model described
    in this book, while attempting to simplify some of the more complex pieces. And
    Spark is even pushing a new true streaming architecture, to try to shut down the
    microbatch naysayer arguments. But when it first came on the scene, the important
    contribution that Spark brought to the table was the fact that it was the *first
    publicly available stream processing engine with strong consistency semantics*,
    albeit only in the case of in-order data or event-time-agnostic computation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，Spark的2.x变体正在大大扩展Spark Streaming的语义能力，同时试图简化一些更复杂的部分。而且，Spark甚至正在推动一种新的真正的流处理架构，试图关闭微批处理的反对论点。但是当它首次出现时，Spark带来的重要贡献是，它是*第一个公开可用的具有强一致性语义的流处理引擎*，尽管只在有序数据或事件时间不可知的计算中。
- en: MillWheel
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MillWheel
- en: Next we discuss MillWheel, a project that I first dabbled with in my 20% time
    after joining Google in 2008, later joining the team full time in 2010 (Figure 10-21).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将讨论MillWheel，这是我在2008年加入谷歌后在我的20%工作时间中首次涉足的项目，后来在2010年全职加入了该团队（图10-21）。
- en: '![](img/stsy_1021.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1021.png)'
- en: 'Figure 10-21\. Timeline: MillWheel'
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-21\. 时间轴：MillWheel
- en: 'MillWheel is Google’s original, general-purpose stream processing architecture,
    and the project was founded by Paul Nordstrom around the time Google’s Seattle
    office opened. MillWheel’s success within Google has long centered on an ability
    to provide low-latency, strongly consistent processing of unbounded, out-of-order
    data. Over the course of this book, we’ve looked at most of the bits and pieces
    that came together in MillWheel to make this possible:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: MillWheel是谷歌最初的通用流处理架构，该项目是由保罗·诺德斯特罗姆在谷歌西雅图办公室开设之际创建的。MillWheel在谷歌内部的成功长期以来一直集中在能够提供无界、无序数据的低延迟、强一致性处理能力上。在本书的过程中，我们已经看到了MillWheel中几乎所有组件的组合，使这一切成为可能：
- en: Reuven discussed *exactly-once guarantees* in Chapter 5. Exactly-once guarantees
    are essential for correctness.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reuven在第5章讨论了*仅一次保证*。仅一次保证对于正确性至关重要。
- en: In Chapter 7 we looked at *persistent state*, the strongly consistent variations
    of which provide the foundation for maintaining that correctness in long-running
    pipelines executing on unreliable hardware.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第7章中，我们讨论了*持久状态*，其强一致性变体为在不可靠硬件上执行长时间运行的流水线维护正确性提供了基础。
- en: Slava talked about *watermarks* in Chapter 3. Watermarks provide a foundation
    for reasoning about disorder in input data.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Slava在第3章谈到了*水印*。水印为推理输入数据的混乱提供了基础。
- en: Also in Chapter 7, we looked at *persistent timers*, which provide the necessary
    link between watermarks and the pipeline’s business logic.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第7章中，我们还讨论了*持久定时器*，它提供了水印和流水线业务逻辑之间的必要联系。
- en: 'It’s perhaps somewhat surprising then to note that the MillWheel project was
    not initially focused on correctness. Paul’s original vision more closely targeted
    the niche that Storm later espoused: low-latency data processing with weak consistency.
    It was the initial MillWheel customers, one building sessions over search data
    and another performing anomaly detection on search queries (the Zeitgeist example
    from the MillWheel paper), who drove the project in the direction of correctness.
    Both had a strong need for consistent results: sessions were used to infer user
    behavior, and anomaly detection was used to infer trends in search queries; the
    utility of both decreased significantly if the data they provided were not reliable.
    As a result, MillWheel’s direction was steered toward one of strong consistency.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 也许有些令人惊讶的是，MillWheel项目最初并不关注正确性。保罗最初的愿景更接近Storm后来所倡导的：低延迟数据处理与弱一致性。最初的MillWheel客户，一个是在搜索数据上构建会话，另一个是在搜索查询上执行异常检测（MillWheel论文中的Zeitgeist示例），他们驱使项目朝着正确性的方向发展。两者都对一致的结果有强烈需求：会话用于推断用户行为，异常检测用于推断搜索查询的趋势；如果它们提供的数据不可靠，它们的效用将显著降低。因此，MillWheel的方向被引向了强一致性。
- en: 'Support for out-of-order processing, which is the other core aspect of robust
    streaming often attributed to MillWheel, was also motivated by customers. The
    Zeitgeist pipeline, as a true streaming use case, wanted to generate an output
    stream that identified anomalies in search query traffic, and only anomalies (i.e.,
    it was not practical for consumers of its analyses to poll all the keys in a materialized
    view output table waiting for an anomaly to be flagged; consumers needed a direct
    signal only when anomalies happened for specific keys). For anomalous spikes (i.e.,
    *increases* in query traffic), this is relatively straightforward: when the count
    for a given query exceeds the expected value in your model for that query by some
    statistically significant amount, you can signal an anomaly. But for anomalous
    dips (i.e., *decreases* in query traffic), the problem is a bit trickier. It’s
    not enough to simply see that the number of queries for a given search term has
    decreased, because for any period of time, the observed number always starts out
    at zero. What you really need to do in these cases is wait until you have reason
    to believe that you’ve seen a sufficiently representative portion of the input
    for a given time period, and only *then* compare the count against your model.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: MillWheel支持无序处理，这是强大流处理的另一个核心方面，也是MillWheel的常见特点，也是由客户的需求推动的。作为真正的流式使用案例，Zeitgeist管道希望生成一个输出流，用于识别搜索查询流量中的异常情况，仅限异常情况（即，对于其分析的消费者来说，轮询等待标记异常的材料化视图输出表中的所有键是不切实际的；消费者只在特定键发生异常时需要直接信号）。对于异常的峰值（即查询流量的增加），这相对简单：当给定查询的计数超过模型对该查询的预期值的一定统计显著量时，可以发出异常信号。但对于异常的下降（即查询流量的减少），问题就有点棘手了。仅仅看到给定搜索词的查询数量减少是不够的，因为在任何时间段内，观察到的数量总是从零开始。在这种情况下，您真正需要做的是等到您有理由相信您已经看到了足够代表性的输入部分，然后再与您的模型进行比较。
- en: The Zeitgeist pipeline first attempted to do this by inserting processing-time
    delays before the analysis logic that looked for dips. This would work reasonably
    decently when data arrived in order, but the pipeline’s authors discovered that
    data could, at times, be greatly delayed and thus arrive wildly out of order.
    In these cases, the processing-time delays they were using weren’t sufficient,
    because the pipeline would erroneously report a flurry of dip anomalies that didn’t
    actually exist. What they really needed was a way to wait until the input became
    complete.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Zeitgeist管道最初尝试在寻找下降的分析逻辑之前插入处理时间延迟来实现这一点。当数据按顺序到达时，这种方法可以工作得相当不错，但管道的作者发现数据有时会被大大延迟，因此到达时会出现严重的无序。在这些情况下，他们使用的处理时间延迟是不够的，因为管道会错误地报告一大堆实际上并不存在的下降异常。他们真正需要的是一种等待直到输入变得完整的方法。
- en: Watermarks were thus born out of this need for reasoning about input completeness
    in out-of-order data. As Slava described in Chapter 3, the basic idea was to track
    the known progress of the inputs being provided to the system, using as much or
    as little data available for the given type of data source, to construct a progress
    metric that could be used to quantify input completeness. For simpler input sources
    like a statically partitioned Kafka topic with each partition being written to
    in increasing event-time order (such as by web frontends logging events in real
    time), you can compute a perfect watermark. For more complex input sources like
    a dynamic set of input logs, a heuristic might be the best you can do. But either
    way, watermarks provide a distinct advantage over the alternative of using processing
    time to reason about event-time completeness, which experience has shown serves
    about as well as a map of London while trying to navigate the streets of Cairo.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，水印是出于对无序数据的输入完整性的推理的需要而产生的。正如Slava在第3章中所描述的，基本思想是跟踪系统提供的输入的已知进度，使用给定类型的数据源提供的尽可能多或尽可能少的数据，构建一个可以用来量化输入完整性的进度指标。对于像静态分区的Kafka主题这样的简单输入源，每个分区按照递增的事件时间顺序写入（例如，通过实时记录事件的Web前端），您可以计算出一个完美的水印。对于像动态输入日志这样的更复杂的输入源，启发式可能是您能做的最好的。但无论如何，水印相对于使用处理时间来推理事件时间完整性的替代方法具有明显的优势，经验表明，后者在试图导航开罗的街道时，效果与使用伦敦地图一样好。
- en: 'So thanks to the needs of its customers, MillWheel ended up as a system with
    the right set of features for supporting robust stream processing on out-of-order
    data. As a result, the paper titled [“MillWheel: Fault-Tolerant Stream Processing
    at Internet Scale”](http://bit.ly/2yab5ZH)⁸ (Figure 10-22) spends most of its
    time discussing the difficulties of providing correctness in a system like this,
    with consistency guarantees and watermarks being the main areas of focus. It’s
    well worth your time if you’re interested in the subject.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，由于客户的需求，MillWheel最终成为了一个具有支持无序数据的强大流处理功能的系统。因此，题为[“MillWheel: Fault-Tolerant
    Stream Processing at Internet Scale”](http://bit.ly/2yab5ZH)⁸（图10-22）的论文大部分时间都在讨论在这样的系统中提供正确性的困难，一致性保证和水印是主要关注的领域。如果您对这个主题感兴趣，这篇论文是非常值得一读的。'
- en: '![](img/stsy_1022.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1022.png)'
- en: Figure 10-22\. [MillWheel paper](http://bit.ly/2yab5ZH)
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-22。[MillWheel论文](http://bit.ly/2yab5ZH)
- en: Not long after the MillWheel paper was published, MillWheel was integrated as
    an alternative, streaming backend for Flume, together often referred to as Streaming
    Flume. Within Google today, MillWheel is in the process of being replaced by its
    successor, Windmill (the execution engine that also powers Cloud Dataflow, discussed
    in a moment), a ground-up rewrite that incorporates all the best ideas from MillWheel,
    along with a few new ones like better scheduling and dispatch, and a cleaner separation
    of user and system code.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在MillWheel论文发表后不久，MillWheel被集成为Flume的替代流后端，通常一起称为Streaming Flume。如今在Google内部，MillWheel正在被其后继者Windmill所取代（也是Cloud
    Dataflow的执行引擎，稍后将讨论），这是一个从头开始的重写，融合了MillWheel的所有最佳想法，以及一些新的想法，如更好的调度和分发，以及用户和系统代码的更清晰分离。
- en: 'However, the big takeaway for MillWheel is that the four concepts listed earlier
    (exactly-once, persistent state, watermarks, persistent timers) together provided
    the basis for a system that was finally able to deliver on the true promise of
    stream processing: robust, low-latency processing of out-of-order data, even on
    unreliable commodity hardware.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于MillWheel来说，最重要的是前面列出的四个概念（仅一次、持久状态、水印、持久定时器）共同为一个系统提供了基础，这个系统最终能够实现流处理的真正承诺：在不可靠的通用硬件上对无序数据进行稳健、低延迟的处理。
- en: Kafka
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka
- en: 'We now come to Kafka (Figure 10-23). Kafka is unique among the systems discussed
    in this chapter in that it’s not a data processing framework,⁹ but instead a transport
    layer. Make no mistake, however: Kafka has played one of the most influential
    roles in advancing stream processing out of all the system’s we’re discussing
    here.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来谈谈Kafka（图10-23）。Kafka在本章讨论的系统中是独一无二的，因为它不是一个数据处理框架，而是一个传输层。然而，毫无疑问，Kafka在推动流处理方面发挥了最有影响力的作用。
- en: '![](img/stsy_1023.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1023.png)'
- en: 'Figure 10-23\. Timeline: Kafka'
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-23。时间线：Kafka
- en: 'If you’re not familiar with it, Kafka is essentially a persistent streaming
    transport, implemented as a set of partitioned logs. It was developed originally
    at LinkedIn by such industry luminaries as Neha Narkhede and Jay Kreps, and its
    accolades include the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对它不熟悉，Kafka本质上是一个持久的流传输，实现为一组分区日志。它最初是在LinkedIn由Neha Narkhede和Jay Kreps等行业杰出人士开发的，它的荣誉包括以下内容：
- en: Providing a clean model of persistence that packaged that warm fuzzy feeling
    of *durable*, *replayable input sources* from the batch world in a streaming friendly
    interface.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供了一个清晰的持久性模型，将批处理世界中温暖的持久、可重放的输入源的感觉打包到了一个流处理友好的接口中。
- en: Providing an elastic *isolation layer* between producers and consumers.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产者和消费者之间提供一个弹性的*隔离层*。
- en: Embodying the relationship between *streams and tables* that we discussed in
    Chapter 6, revealing a foundational way of thinking about data processing in general
    while also providing a conceptual link to the rich and storied world of databases.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体现了我们在第6章讨论的*流和表*之间的关系，揭示了一种关于数据处理的基本思维方式，同时提供了与丰富而悠久的数据库世界的概念联系。
- en: As of side of effect of all of the above, not only becoming the *cornerstone*
    of a majority of stream processing installations across the industry, but also
    fostering the stream-processing-as-databases and microservices movements.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为上述所有内容的副作用，Kafka不仅成为了行业中大多数流处理安装的*基石*，还促进了流处理作为数据库和微服务运动。
- en: They must get up very early in the morning.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 他们一定是起得很早。
- en: Of those accolades, there are two that stand out most to me. The first is the
    application of durability and replayability to stream data. Prior to Kafka, most
    stream processing systems used some sort of ephemeral queuing system like Rabbit
    MQ or even plain-old TCP sockets to send data around. Durability might be provided
    to some degree via upstream backup in the producers (i.e., the ability for upstream
    producers of data to resend if the downstream workers crashed), but oftentimes
    the upstream data was stored ephemerally, as well. And most approaches entirely
    ignored the idea of being able to replay input data later in cases of backfills
    or for prototyping, development, and regression testing.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些荣誉中，有两个对我来说最突出。第一个是将持久性和可重放性应用于流数据。在Kafka之前，大多数流处理系统使用某种短暂的排队系统，如Rabbit MQ或甚至普通的TCP套接字来发送数据。持久性可能在生产者的上游备份中提供到一定程度（即，上游数据的生产者在下游工作人员崩溃时能够重新发送），但往往上游数据也是临时存储的。大多数方法完全忽略了在后续回填或用于原型设计、开发和回归测试时能够重放输入数据的想法。
- en: Kafka changed all that. By taking the battle-hardened concept of a durable log
    from the database world and applying it to the realm of stream processing, Kafka
    gave us all back that sense of safety and security we’d lost when moving from
    the durable input sources common in the Hadoop/batch world to the ephemeral sources
    prevalent at the time in the streaming world. With durability and replayability,
    stream processing took yet another step toward being a robust, reliable replacement
    for the ad hoc, continuous batch processing systems of yore that were still being
    applied to streaming use cases.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka改变了一切。通过将数据库世界中经受考验的持久日志的概念应用到流处理领域，Kafka让我们重新获得了从Hadoop/批处理世界中常见的持久输入源到当时流处理世界中普遍存在的短暂源时失去的安全感和安全性。具有耐用性和可重放性，流处理又迈出了一步，成为了对昔日临时批处理系统的强大、可靠替代品。
- en: As a streaming system developer, one of the more interesting visible artifacts
    of the impact that Kafka’s durability and replayability features have had on the
    industry is how many of the stream processing engines today have grown to fundamentally
    rely on that replayability to provide end-to-end exactly-once guarantees. Replayability
    is the foundation upon which end-to-end exactly-once guarantees in Apex, Flink,
    Kafka Streams, Spark, and Storm are all built. When executing in exactly-once
    mode, each of those systems assumes/requires that the input data source be able
    to rewind and replay all of the data up until the most recent checkpoint. When
    used with an input source that does not provide such ability (even if the source
    can guarantee reliable delivery via upstream backup), end-to-end exactly-once
    semantics fall apart. That sort of broad reliance on replayability (and the related
    aspect of durability) is a huge testament to the amount of impact those features
    have had across the industry.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 作为流处理系统开发人员，Kafka的耐用性和可重放性特性对行业产生的影响的一个更有趣的可见产物是，今天有多少流处理引擎在根本上依赖于可重放性来提供端到端的一次性保证。可重放性是Apex、Flink、Kafka
    Streams、Spark和Storm中端到端一次性保证的基础。在执行一次性模式时，这些系统中的每一个都假定/要求输入数据源能够倒带并重放直到最近的检查点。当与不能提供这种能力的输入源一起使用时（即使源可以通过上游备份保证可靠交付），端到端的一次性语义就会崩溃。对可重放性（以及相关的耐用性）的广泛依赖证明了这些特性在整个行业中产生的影响之大。
- en: The second noteworthy bullet from Kafka’s resume is the popularization of stream
    and table theory. We spent the entirety of Chapter 6 discussing streams and tables
    as well as much of Chapters 8 and 9. And for good reason. Streams and tables form
    the foundation of data processing, be it the MapReduce family tree of systems,
    the enormous legacy of SQL database systems, or what have you. Not all data processing
    approaches need speak directly in terms of streams and tables but conceptually
    speaking, that’s how they all operate. And as both users and developers of these
    systems, there’s great value in understanding the core underlying concepts that
    all of our systems build upon. We all owe a collective thanks to the folks in
    the Kafka community who helped shine a broader light on the streams-and-tables
    way of thinking.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka简历中的第二个值得注意的亮点是流和表理论的普及。我们在整个第6章中讨论了流和表，以及第8章和第9章的大部分内容。而且理由充分。流和表构成了数据处理的基础，无论是MapReduce系统家族、庞大的SQL数据库系统还是其他任何系统。并非所有的数据处理方法都需要直接使用流和表的术语，但从概念上讲，它们都是如此操作的。作为这些系统的用户和开发人员，了解我们所有系统都建立在的核心基本概念是非常有价值的。我们都应该对Kafka社区的人们感激不尽，他们帮助更广泛地了解了流和表的思维方式。
- en: '![](img/stsy_1024.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1024.png)'
- en: Figure 10-24\. I ❤ Logs
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-24。我❤日志
- en: If you’d like to learn more about Kafka and the foundations it’s built on, *I
    ❤ Logs* by Jay Kreps (O’Reilly; Figure 10-24) is an excellent resource.¹⁰ Additionally,
    as cited originally in Chapter 6, Kreps and Martin Kleppmann have a pair of articles
    (Figure 10-25) that I highly recommend for reading up on the origins of streams
    and table theory.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于Kafka及其基础的信息，《我❤日志》（O'Reilly；图10-24）是杰伊·克雷普斯的一本优秀资源。¹⁰ 此外，正如在第6章中引用的那样，克雷普斯和马丁·克莱普曼有一对文章（图10-25），我强烈推荐阅读，以了解流和表理论的起源。
- en: Kafka has made huge contributions to the world of stream processing, arguably
    more than any other single system out there. In particular, the application of
    durability and replayability to input and output streams played a big part in
    helping move stream processing out of the niche realm of approximation tools and
    into the big leagues of general data processing. Additionally, the theory of streams
    and tables, popularized by the Kafka community, provides deep insight into the
    underlying mechanics of data processing in general.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka对流处理领域做出了巨大的贡献，可以说比其他任何单一系统都要多。特别是，将耐用性和可重放性应用于输入和输出流在帮助将流处理从近似工具的小众领域推向了一般数据处理的大联盟中发挥了重要作用。此外，由Kafka社区推广的流和表的理论为我们提供了对数据处理基本机制的深刻洞察。
- en: '![](img/stsy_1025.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1025.png)'
- en: Figure 10-25\. [Martin’s post](https://www.confluent.io/blog/making-sense-of-stream-processing/)
    (left) and [Jay’s post](https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/)
    (right)
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-25。[马丁的文章](https://www.confluent.io/blog/making-sense-of-stream-processing/)（左）和[杰伊的文章](https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/)（右）
- en: Cloud Dataflow
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cloud Dataflow
- en: Cloud Dataflow (Figure 10-26) is Google’s fully managed, cloud-based data processing
    service. Dataflow launched to the world in August 2015\. It was built with the
    intent to take the decade-plus of experiences that had gone into building MapReduce,
    Flume, and MillWheel, and package them up into a serverless cloud experience.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 云数据流（图10-26）是谷歌的全面托管的基于云的数据处理服务。 Dataflow于2015年8月面向世界推出。它的构建意图是将十多年的MapReduce、Flume和MillWheel构建经验打包成一个无服务器的云体验。
- en: '![](img/stsy_1027.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1027.png)'
- en: 'Figure 10-26\. Timeline: Cloud Dataflow'
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-26。时间轴：Cloud Dataflow
- en: Although the serverless aspect of Cloud Dataflow is perhaps its most technically
    challenging and distinguishing factor from a systems perspective, the primary
    contribution to streaming systems that I want to discuss here is its unified batch
    plus streaming programming model. That’s all the transformations, windowing, watermarks,
    triggers, and accumulation goodness we’ve spent most of the book talking about.
    And all of them, of course, wrapped up the *what*/*where*/*when*/*how* way of
    thinking about things.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Cloud Dataflow的无服务器方面可能是从系统角度最具挑战性和区分性的因素，但我想在这里讨论的对流系统的主要贡献是其统一的批处理加流处理编程模型。这就是我们在大部分书中讨论的所有转换、窗口、水印、触发器和累积的好处。当然，它们都包含了关于*什么*/*在哪里*/*何时*/*如何*思考问题的方式。
- en: The model first arrived back in Flume, as we looked to incorporate the robust
    out-of-order processing support in MillWheel into the higher-level programming
    model Flume afforded. The combined batch and streaming approach available to Googlers
    internally with Flume was then the basis for the fully unified model included
    in Dataflow.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型最初出现在Flume中，当时我们试图将MillWheel中强大的无序处理支持纳入Flume提供的更高级别的编程模型中。随后，Flume在谷歌内部可用的综合批处理和流处理方法成为Dataflow中包含的完全统一模型的基础。
- en: 'The key insight in the unified model—the full extent of which none of us at
    the time even truly appreciated—is that under the covers, batch and streaming
    are really not that different: they’re both just minor variations on the streams
    and tables theme. As we learned in Chapter 6, the main difference really boils
    down to the ability to incrementally trigger tables into streams; everything else
    is conceptually the same.¹¹ By taking advantage of the underlying commonalities
    of the two approaches, it was possible to provide a single, nearly seamless experience
    that applied to both worlds. This was a big step forward in making stream processing
    more accessible.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 统一模型的关键洞察力——当时我们甚至没有真正意识到的全部内容——是，在幕后，批处理和流处理实际上并没有那么不同：它们只是流和表主题的微小变化。正如我们在第6章中所学到的，主要区别实际上归结为能够逐渐将表触发为流；其他一切在概念上都是相同的。¹¹通过利用这两种方法的共同点，可以提供一个几乎无缝的单一体验，适用于两个世界。这是在使流处理更易于访问方面迈出的一大步。
- en: 'In addition to taking advantage of the commonalities between batch and streaming,
    we took a long, hard look at the variety of use cases we’d encountered over the
    years at Google and used those to inform the pieces that went into the unified
    model. Key aspects we targeted included the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 除了利用批处理和流处理之间的共同点，我们还深入研究了多年来在谷歌遇到的各种用例，并利用这些信息来指导统一模型的组成部分。我们针对的关键方面包括以下内容：
- en: '*Unaligned, event-time windows* such as sessions, providing the ability to
    concisely express powerful analytic constructs and apply them to out-of-order
    data.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不对齐的事件时间窗口*，例如会话，提供了对无序数据进行简洁表达强大分析构造并将其应用的能力。'
- en: '*Custom windowing support*, because one (or even three or four) sizes rarely
    fit all.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自定义窗口支持*，因为一个（甚至三个或四个）大小很少适合所有情况。'
- en: '*Flexible triggering* and *accumulation modes*, providing the ability to shape
    the way data flow through the pipeline to match the correctness, latency, and
    cost needs of the given use case.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*灵活的触发*和*累积模式*，提供了塑造数据流通过管道的方式，以匹配给定用例的正确性、延迟和成本需求的能力。'
- en: The use of *watermarks* for reasoning about *input completeness*, which is critical
    for use cases like anomalous dip detection where the analysis depends upon an
    absence of data.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*水印*来推断*输入完整性*，这对于异常跌落检测等用例至关重要，其中分析取决于数据的缺失。
- en: '*Logical abstraction* of the underlying execution environment, be it batch,
    microbatch, or streaming, providing flexibility of choice in execution engine
    and avoiding system-level constructs (such as micro-batch size) from creeping
    into the logical API.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对底层执行环境的*逻辑抽象*，无论是批处理、微批处理还是流处理，都提供了执行引擎的选择灵活性，并避免系统级构造（如微批处理大小）渗入逻辑API。
- en: Taken together, these aspects provided the flexibility to balance the tensions
    between correctness, latency, and cost, allowing the model to be applied across
    a wide breadth of use cases.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这些方面提供了灵活性，以平衡正确性、延迟和成本之间的紧张关系，使该模型能够应用于广泛的用例。
- en: '![](img/stsy_1028.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1028.png)'
- en: Figure 10-27\. [Dataflow Model paper](http://bit.ly/2sXgVJ3)
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-27。[Dataflow模型论文](http://bit.ly/2sXgVJ3)
- en: Given that you’ve just read an entire book covering the finer points of the
    Dataflow/Beam Model, there’s little point in trying to retread any those concepts
    here. However, if you’re looking for a slightly more academic take on things as
    well as a nice overview of some of the motivating use cases alluded to earlier,
    you might find our 2015 [Dataflow Model paper](http://bit.ly/2sXgVJ3) worthwhile
    (Figure 10-27).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到您刚刚阅读了一本涵盖Dataflow/Beam模型细节的整本书，因此在这里重温这些概念没有太大意义。但是，如果您想对事情有稍微更学术化的看法，并且对之前提到的一些激励用例有一个很好的概述，您可能会发现我们2015年的[Dataflow模型论文](http://bit.ly/2sXgVJ3)有价值（图10-27）。
- en: Though there are many other compelling aspects to Cloud Dataflow, the important
    contribution from the perspective of this chapter is its *unified batch plus streaming
    programming model*. It brought the world a comprehensive approach to tackling
    unbounded, out-of-order datasets, and in a way that provided the flexibility to
    make the trade-offs necessary to balance the tensions between correctness, latency,
    and cost to match the requirements for a given use case.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Cloud Dataflow还有许多其他引人注目的方面，但从本章的角度来看，其重要贡献是其*统一的批处理加流处理编程模型*。它为解决无界、无序数据集提供了全面的方法，并以一种灵活的方式进行权衡，以满足给定用例的正确性、延迟和成本之间的紧张关系。
- en: Flink
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flink
- en: Flink (Figure 10-28) burst onto the scene in 2015, rapidly transforming itself
    from a system that almost no one had heard of into one of the powerhouses of the
    streaming world, seemingly overnight.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Flink（图10-28）于2015年突然出现，迅速从一个几乎没有人听说过的系统转变为流处理世界的强大力量之一。
- en: '![](img/stsy_1029.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1029.png)'
- en: 'Figure 10-28\. Timeline: Flink'
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-28。时间轴：Flink
- en: 'There were two main reasons for Flink’s rise to prominence:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Flink崭露头角的两个主要原因是：
- en: Its *rapid adoption of the Dataflow/Beam programming model*, which put it in
    the position of being the most semantically capable fully open source streaming
    system on the planet at the time.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它*快速采用Dataflow/Beam编程模型*，使其成为当时全球最具语义能力的完全开源流处理系统。
- en: 'Followed shortly thereafter by its *highly efficient snapshotting* implementation
    (derived from research in Chandy and Lamport’s original paper [“Distributed Snapshots:
    Determining Global States of Distributed Systems”](http://bit.ly/2JBCsRU) [Figure 10-29]),
    which gave it the strong consistency guarantees needed for correctness.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随后不久，Flink实现了*高效的快照*（源自Chandy和Lamport原始论文[“分布式快照：确定分布式系统的全局状态”](http://bit.ly/2JBCsRU)
    [图10-29]的研究），为其提供了所需的强一致性保证以确保正确性。
- en: '![](img/stsy_1030.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1030.png)'
- en: Figure 10-29\. [Chandy-Lamport snapshots](http://bit.ly/2JBCsRU)
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-29。[Chandy-Lamport快照](http://bit.ly/2JBCsRU)
- en: Reuven covered Flink’s consistency mechanism briefly in Chapter 5, but to reiterate,
    the basic idea is that periodic barriers are propagated along the communication
    paths between workers in the system. The barriers act as an alignment mechanism
    between the various distributed workers producing data upstream from a consumer.
    When the consumer receives a given barrier on all of its input channels (i.e.,
    from all of its upstream producers), it checkpoints its current progress for all
    active keys, at which point it is then safe to acknowledge processing of all data
    that came before the barrier. By tuning how frequently barriers are sent through
    the system, it’s possible to tune the frequency of checkpointing and thus trade
    off increased latency (due to the need for side effects to be materialized only
    at checkpoint times) in exchange for higher throughput.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Reuven在第5章简要介绍了Flink的一致性机制，但要重申的是，基本思想是定期障碍在系统中的工作人员之间的通信路径上传播。这些障碍充当各个分布式工作人员之间的数据生产者与消费者之间的对齐机制。当消费者在其所有输入通道（即来自其所有上游生产者）上接收到给定的障碍时，它会为所有活动键检查点其当前进度，然后可以安全地确认处理障碍之前的所有数据。通过调整障碍通过系统发送的频率，可以调整检查点的频率，从而在增加延迟（由于需要在检查点时间点上实现副作用）的情况下换取更高的吞吐量。
- en: '![](img/stsy_1031.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1031.png)'
- en: Figure 10-30\. [“Extending the Yahoo! Streaming Benchmark”](http://bit.ly/2LQvGnN)
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-30。[“扩展Yahoo！流式基准”](http://bit.ly/2LQvGnN)
- en: 'The simple fact that Flink now had the capability to provide exactly-once semantics
    along with native support for event-time processing was huge at the time. But
    it wasn’t until Jamie Grier published his article titled [“Extending the Yahoo!
    Streaming Benchmark”](http://bit.ly/2LQvGnN) (Figure 10-30) that it became clear
    just how performant Flink was. In that article, Jamie described two impressive
    achievements:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当时，Flink具有提供精确一次语义以及本地支持事件时间处理的能力是一个巨大的进步。但直到Jamie Grier发表了题为[“扩展Yahoo！流式基准”](http://bit.ly/2LQvGnN)（图10-30）的文章，才清楚地表明了Flink的性能。在那篇文章中，Jamie描述了两个令人印象深刻的成就：
- en: Building a prototype Flink pipeline that achieved greater accuracy than one
    of Twitter’s existing Storm pipelines (thanks to Flink’s exactly-once semantics)
    at 1% of the cost of the original.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建了一个原型Flink流水线，其准确性超过了Twitter现有的Storm流水线的1%成本。
- en: Updating the [Yahoo! Streaming Benchmark](http://bit.ly/2bhgMJd) to show Flink
    (with exactly-once) achieving 7.5 times the throughput of Storm (without exactly-once).
    Furthermore, Flink’s performance was shown to be limited due to network saturation;
    removing the network bottleneck allowed Flink to achieve almost 40 times the throughput
    of Storm.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新[雅虎！流式基准](http://bit.ly/2bhgMJd)，显示Flink（具有精确一次）的吞吐量是Storm（没有精确一次）的7.5倍。此外，由于网络饱和，Flink的性能受到限制；消除网络瓶颈使Flink的吞吐量几乎是Storm的40倍。
- en: Since then, numerous other projects (notably, Storm and Apex) have all adopted
    the same type of consistency mechanism.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，许多其他项目（特别是Storm和Apex）都采用了相同类型的一致性机制。
- en: '![](img/stsy_1032.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1032.png)'
- en: 'Figure 10-31\. [“Savepoints: Turning Back Time”](http://bit.ly/2JKCouO)'
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-31。[“保存点：时光倒流”](http://bit.ly/2JKCouO)
- en: 'With the addition of a snapshotting mechanism, Flink gained the strong consistency
    needed for end-to-end exactly-once. But to its credit, Flink went one step further,
    and used the global nature of its snapshots to provide the ability to restart
    an entire pipeline from any point in the past, a feature known as savepoints (described
    in the [“Savepoints: Turning Back Time”](http://bit.ly/2JKCouO) post by Fabian
    Hueske and Michael Winters [Figure 10-31]). The savepoints feature took the warm
    fuzziness of durable replay that Kafka had applied to the streaming transport
    layer and extended it to cover the breadth of an entire pipeline. Graceful evolution
    of a long-running streaming pipeline over time remains an important open problem
    in the field, with lots of room for improvement. But Flink’s savepoints feature
    stands as one of the first huge steps in the right direction, and one that remains
    unique across the industry as of this writing.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加快照机制，Flink获得了端到端精确一次所需的强一致性。但值得赞扬的是，Flink更进一步利用其快照的全局性质，提供了从过去任何时间点重新启动整个流水线的能力，这一功能称为保存点（由Fabian
    Hueske和Michael Winters在[“保存点：时光倒流”](http://bit.ly/2JKCouO)一文中描述 [图10-31]）。保存点功能将Kafka应用于流式传输层的持久重放的温暖特性扩展到整个流水线的广度。长时间运行的流式流水线随时间的优雅演变仍然是该领域的一个重要开放问题，有很大的改进空间。但截至目前，Flink的保存点功能是朝着正确方向迈出的第一步巨大进展之一，而且在整个行业中仍然是独一无二的。
- en: If you’re interested in learning more about the system constructs underlying
    Flink’s snapshots and savepoints, the paper [“State Management in Apache Flink”](http://bit.ly/2LLyr9O)
    (Figure 10-32) discusses the implementation in good detail.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对了解Flink快照和保存点的系统构造感兴趣，可以阅读论文[“Apache Flink中的状态管理”](http://bit.ly/2LLyr9O)（图10-32）对实现进行了详细讨论。
- en: '![](img/stsy_1033.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1033.png)'
- en: Figure 10-32\. [“State Management in Apache Flink”](http://bit.ly/2LLyr9O)
  id: totrans-186
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-32。[“Apache Flink中的状态管理”](http://bit.ly/2LLyr9O)
- en: Beyond savepoints, the Flink community has continued to innovate, including
    bringing the first practical streaming SQL API to market for a large-scale, distributed
    stream processing engine, as we discussed in Chapter 8.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 除了保存点之外，Flink社区继续创新，包括为大规模分布式流处理引擎推出了第一个实用的流式SQL API，正如我们在第8章中讨论的那样。
- en: 'In summary, Flink’s rapid rise to stream processing juggernaut can be attributed
    primarily to three characteristics of its approach: 1) incorporating the *best
    existing ideas* from across the industry (e.g., being the first open source adopter
    of the Dataflow/Beam Model), 2) *bringing its own innovations* to the table to
    push forward the state of the art (e.g., strong consistency via snapshots and
    savepoints, streaming SQL), and 3) doing both of those things *quickly* and *repeatedly*.
    Add in the fact that all of this is done in *open source*, and you can see why
    Flink has consistently continued to raise the bar for streaming processing across
    the industry.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，Flink迅速崛起为流处理巨头主要归功于其方法的三个特点：1）吸收了行业中*最好的现有思想*（例如，成为Dataflow/Beam模型的第一个开源采用者），2）*带来了自己的创新*，推动了技术的发展（例如，通过快照和保存点实现强一致性，流式SQL），以及3）快速且*反复*地做这两件事。再加上所有这些都是在*开源*中完成的，您就可以看到为什么Flink一直在整个行业中不断提高流处理的标准。
- en: Beam
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Beam
- en: 'The last system we talk about is Apache Beam (Figure 10-33). Beam differs from
    most of the other systems in this chapter in that it’s primarily a programming
    model, API, and portability layer, not a full stack with an execution engine underneath.
    But that’s exactly the point: just as SQL acts as a lingua franca for declarative
    data processing, Beam aims to be the lingua franca for programmatic data processing.
    Let’s explore how.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要讨论的最后一个系统是Apache Beam（图10-33）。Beam与本章中的大多数其他系统不同之处在于，它主要是一个编程模型、API和可移植性层，而不是具有执行引擎的完整堆栈。但这正是重点所在：正如SQL作为声明式数据处理的通用语言，Beam旨在成为编程式数据处理的通用语言。让我们来探讨一下。
- en: '![](img/stsy_1034.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1034.png)'
- en: 'Figure 10-33\. Timeline: Beam'
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-33\. 时间轴：Beam
- en: 'Concretely, Beam is composed a number of components:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，Beam由多个组件组成：
- en: A unified batch plus streaming *programming model*, inherited from Cloud Dataflow
    where it originated, and the finer points of which we’ve spent the majority of
    this book discussing. The model is independent of any language implementations
    or runtime systems. You can think of this as Beam’s equivalent to SQL’s relational
    algebra.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个统一的批处理加流式*编程模型*，继承自其起源地Cloud Dataflow，我们在本书的大部分内容中讨论了其细节。该模型独立于任何语言实现或运行时系统。您可以将其视为Beam对SQL的关系代数的等价物。
- en: A set of *SDKs (software development kits)* that implement that model, allowing
    pipelines to be expressed in terms of the model in idiomatic ways for a given
    language. Beam currently provides SDKs in Java, Python, and Go. You can think
    of these as Beam’s programmatic equivalents to the SQL language itself.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组*SDKs（软件开发工具包）*，实现了该模型，允许以特定语言的习惯方式表达流水线。Beam目前提供了Java、Python和Go的SDKs。您可以将这些视为Beam对SQL语言的编程等价物。
- en: A set of *DSLs (domain specific languages)* that build upon the SDKs, providing
    specialized interfaces that capture pieces of the model in unique ways. Whereas
    SDKs are required to surface all aspects of the model, DSLs can expose only those
    pieces that make sense for the specific domain a DSL is targeting. Beam currently
    provides a Scala DSL called Scio and an SQL DSL, both of which layer on top of
    the existing Java SDK.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组*DSLs（领域特定语言）*，建立在SDKs之上，提供专门的接口，以独特的方式捕捉模型的部分内容。而SDKs需要展示模型的所有方面，DSLs只能暴露那些对特定领域有意义的部分。Beam目前提供了一个名为Scio的Scala
    DSL和一个SQL DSL，两者都是建立在现有的Java SDK之上的。
- en: A set of *runners* that can execute Beam pipelines. Runners take the logical
    pipeline described in Beam SDK terms, and translate them as efficiently as possible
    into a physical pipeline that they can then execute. Beam runners exist currently
    for Apex, Flink, Spark, and Google Cloud Dataflow. In SQL terms, you can think
    of these runners as Beam’s equivalent to the various SQL database implementations,
    such as Postgres, MySQL, Oracle, and so on.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组可以执行Beam流水线的*运行器*。运行器以Beam SDK术语描述的逻辑流水线，并尽可能高效地将其转换为物理流水线，然后执行。目前存在的Beam运行器包括Apex、Flink、Spark和Google
    Cloud Dataflow。用SQL术语来说，您可以将这些运行器视为Beam对各种SQL数据库实现的等价物，如Postgres、MySQL、Oracle等。
- en: The core vision for Beam is built around its value as a portability layer, and
    one of the more compelling features in that realm is its planned support for full
    cross-language portability. Though not yet fully complete (but [landing imminently](http://bit.ly/2N0tPNL)),
    the plan is for Beam to provide sufficiently performant abstraction layers between
    SDKs and runners that will allow for a full cross-product of SDK × runner matchups.
    In such a world, a pipeline written in a JavaScript SDK could seamlessly execute
    on a runner written in Haskell, even if the Haskell runner itself had no native
    ability to execute JavaScript code.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Beam的核心愿景建立在其作为可移植性层的价值上，而在这个领域中更具吸引力的特性之一是其计划支持完全跨语言的可移植性。尽管尚未完全完成（但[即将到来](http://bit.ly/2N0tPNL)），计划是让Beam在SDK和运行器之间提供足够高效的抽象层，以实现完全的跨产品SDK
    × runner匹配。在这样的世界中，使用JavaScript SDK编写的流水线可以在Haskell运行器上无缝执行，即使Haskell运行器本身没有本地执行JavaScript代码的能力。
- en: 'As an abstraction layer, the way that Beam positions itself relative to its
    runners is critical to ensure that Beam actually brings value to the community,
    rather than introducing just an unnecessary layer of abstraction. The key point
    here is that Beam aims to never be just the intersection (lowest common denominator)
    or union (kitchen sink) of the features found in its runners. Instead, it aims
    to include only the best ideas across the data processing community at large.
    This allows for innovation in two dimensions:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个抽象层，Beam相对于其runners的定位对于确保Beam实际为社区带来价值至关重要，而不是引入一个不必要的抽象层。关键在于，Beam的目标是永远不只是其runners中发现的特性的交集（最低公共分母）或并集（厨房水槽）。相反，它的目标是仅包括整个数据处理社区中最好的想法。这允许在两个维度上进行创新：
- en: Innovation in Beam
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Beam中的创新
- en: '![](img/stsy_1035.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stsy_1035.png)'
- en: Figure 10-34\. [Powerful and modular I/O](http://bit.ly/2JQa7GJ)
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-34。强大和模块化的I/O
- en: Beam might include API support for runtime features that not all runners initially
    support. This is okay. Over time, we expect many runners will incorporate such
    features into future versions; those that don’t will be a less-attractive runner
    choice for use cases that need such features.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Beam可能会包括并非所有runners最初都支持的运行时特性的API支持。这没关系。随着时间的推移，我们期望许多runners将在未来版本中纳入这些特性；那些不这样做的将成为需要这些特性的用例的不太吸引人的runner选择。
- en: An example here is Beam’s SplittableDoFn API for writing composable, scalable
    sources (described by Eugene Kirpichov in his post [“Powerful and modular I/O
    connectors with Splittable DoFn in Apache Beam”](http://bit.ly/2JQa7GJ) [Figure 10-34]).
    It’s both unique and extremely powerful but also does not yet see broad support
    across all runners for some of the more innovative parts like dynamic work rebalancing.
    Given the value such features bring, however, we expect that will change over
    time.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个例子是Beam的SplittableDoFn API，用于编写可组合的可伸缩源（由Eugene Kirpichov在他的文章“在Apache
    Beam中使用Splittable DoFn编写强大和模块化的I/O连接器”中描述）。它既独特又非常强大，但对于一些更具创新性的部分，比如动态工作重新平衡，目前还没有得到所有runners的广泛支持。然而，考虑到这些特性带来的价值，我们预计随着时间的推移，情况将会改变。
- en: Innovation in runners
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: runners中的创新
- en: Runners might introduce runtime features for which Beam does not initially provide
    API support. This is okay. Over time, runtime features that have proven their
    usefulness will have API support incorporated into Beam.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Runners可能会引入运行时特性，而Beam最初并不提供API支持。这没关系。随着时间的推移，已经证明其有用性的运行时特性将被纳入Beam的API支持中。
- en: An example here is the state snapshotting mechanism in Flink, or savepoints,
    which we discussed earlier. Flink is still the only publicly available streaming
    system to support snapshots in this way, but there’s a proposal in Beam to provide
    an API around snapshots because we believe graceful evolution of pipelines over
    time is an important feature that will be valuable across the industry. If we
    were to magically push out such an API today, Flink would be the only runtime
    system to support it. But again, that’s okay. The point here is that the industry
    as a whole will begin to catch up over time as the value of these features becomes
    clear.¹² And that’s better for everyone.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个例子是Flink中的状态快照机制，或者之前我们讨论过的savepoints。Flink仍然是唯一公开可用的流处理系统，以这种方式支持快照，但Beam中有一个提案提供围绕快照的API，因为我们认为随着时间的推移，管道的优雅演进是一个重要的特性，将在整个行业中具有价值。如果我们今天神奇地推出这样的API，Flink将是唯一支持它的运行时系统。但同样，这没关系。这里的重点是，随着这些特性的价值变得清晰，整个行业将随着时间的推移开始赶上。¹²这对每个人都更好。
- en: By encouraging innovation within both Beam itself as well as runners, we hope
    to push forward the capabilities of the entire industry at a greater pace over
    time, without accepting compromises along the way. And by delivering on the promise
    of portability across runtime execution engines, we hope to establish Beam as
    the common language for expressing programmatic data processing pipelines, similar
    to how SQL exists today as the common currency of declarative data processing.
    It’s an ambitious goal, and as of writing, we’re still a ways off from seeing
    it fully realized, but we’ve also come a long way so far.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 通过鼓励Beam本身以及runners内的创新，我们希望随着时间的推移以更快的速度推动整个行业的能力，而不会在这一过程中接受妥协。通过实现跨运行时执行引擎的可移植性的承诺，我们希望建立Beam作为表达编程数据处理流水线的通用语言，类似于SQL如今作为声明式数据处理的通用货币存在。这是一个雄心勃勃的目标，截至目前，我们离完全实现它还有一段距离，但我们迄今为止也走了很长一段路。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'We just took a whirlwind tour through a decade and a half of advances in data
    processing technology, with a focus on the contributions that made streaming systems
    what they are today. To summarize one last time, the main takeaways for each system
    were:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚快速浏览了十五年来数据处理技术的进步，重点关注了使流式系统成为今天的样子的贡献。最后总结一下，每个系统的主要要点是：
- en: MapReduce—scalability and simplicity
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce - 可伸缩性和简单性
- en: By providing a simple set of abstractions for data processing on top of a robust
    and scalable execution engine, MapReduce allowed data engineers to focus on the
    business logic of their data processing needs rather than the gnarly details of
    building distributed systems resilient to the failure modes of commodity hardware.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在强大且可伸缩的执行引擎之上提供一组简单的数据处理抽象，MapReduce使数据工程师能够专注于其数据处理需求的业务逻辑，而不是构建分布式系统的棘手细节，以使其能够抵御商品硬件的故障模式。
- en: Hadoop—open source ecosystem
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop - 开源生态系统
- en: By building an open source platform on the ideas of MapReduce, Hadoop created
    a thriving ecosystem that expanded well beyond the scope of its progenitor and
    allowed a multitude of new ideas to flourish.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在MapReduce的思想基础上构建开源平台，Hadoop创造了一个蓬勃发展的生态系统，远远超出了其前身的范围，并允许大量新的想法蓬勃发展。
- en: Flume—pipelines, optimization
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Flume - 流水线，优化
- en: By coupling a high-level notion of logical pipeline operations with an intelligent
    optimizer, Flume made it possible to write clean and maintainable pipelines whose
    capabilities extended beyond the Map → Shuffle → Reduce confines of MapReduce,
    without sacrificing any of the performance theretofore gained by contorting the
    logical pipeline via hand-tuned manual optimizations.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将逻辑管道操作的高级概念与智能优化器相结合，Flume使得编写干净、可维护的管道成为可能，其能力超越了Map→Shuffle→Reduce的MapReduce的限制，而又不牺牲通过手工调优获得的性能。
- en: Storm—low latency with weak consistency
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Storm-低延迟与弱一致性
- en: By sacrificing correctness of results in favor of decreased latency, Storm brought
    stream processing to the masses and also ushered in the era of the Lambda Architecture,
    where weakly consistent stream processing engines were run alongside strongly
    consistent batch systems to realize the true business goal of low-latency, eventually
    consistent results.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 通过牺牲结果的正确性以换取延迟的降低，Storm将流处理带给了大众，并引领了Lambda架构的时代，其中弱一致性的流处理引擎与强一致性的批处理系统并行运行，实现了低延迟、最终一致的真正商业目标。
- en: Spark—strong consistency
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Spark-强一致性
- en: By utilizing repeated runs of a strongly consistent batch engine to provide
    continuous processing of unbounded datasets, Spark Streaming proved it possible
    to have both correctness and low-latency results, at least for in-order datasets.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用强一致性批处理引擎的重复运行来提供无界数据集的连续处理，Spark Streaming证明了在有序数据集中至少可以同时具有正确性和低延迟的结果是可能的。
- en: MillWheel—out-of-order processing
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: MillWheel-无序处理
- en: By coupling strong consistency and exactly-once processing with tools for reasoning
    about time like watermarks and timers, MillWheel conquered the challenge of robust
    stream processing over out-of-order data.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将强一致性和精确一次处理与水印和定时器等关于时间的推理工具相结合，MillWheel征服了对无序数据进行强大流处理的挑战。
- en: Kafka—durable streams, streams and tables
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka-持久流、流和表
- en: By applying the concept of a durable log to the problem of streaming transports,
    Kafka brought back the warm, fuzzy feeling of replayability that had been lost
    by ephemeral streaming transports like RabbitMQ and TCP sockets. And by popularizing
    the ideas of stream and table theory, it helped shed light on the conceptual underpinnings
    of data processing in general.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将持久日志的概念应用于流传输问题，Kafka重新带回了持久性流传输的温暖、踏实的感觉，这是由于像RabbitMQ和TCP套接字这样的短暂流传输而失去的。并通过推广流和表理论的思想，它帮助阐明了数据处理的概念基础。
- en: Cloud Dataflow—unified batch plus streaming
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Dataflow-统一批处理加流处理
- en: By melding the out-of-order stream processing concepts from MillWheel with the
    logical, automatically optimizable pipelines of Flume, Cloud Dataflow provided
    a unified model for batch plus streaming data processing that provided the flexibility
    to balance the tensions between correctness, latency, and cost to match any given
    use case.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将MillWheel的无序流处理概念与Flume的逻辑、自动可优化的管道相融合，Cloud Dataflow提供了一个统一的批处理加流处理数据模型，提供了灵活性，以平衡正确性、延迟和成本之间的紧张关系，以适应任何给定的用例。
- en: Flink—open source stream processing innovator
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Flink-开源流处理创新者
- en: By rapidly bringing the power of out-of-order processing to the world of open
    source and combining it with innovations of their own like distributed snapshots
    and its related savepoints features, Flink raised the bar for open source stream
    processing and helped lead the current charge of stream processing innovation
    across the industry.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 通过迅速将无序处理的能力带到开源世界，并将其与分布式快照和相关的保存点功能等创新相结合，Flink提高了开源流处理的标准，并帮助引领了行业内流处理创新的潮流。
- en: Beam—portability
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Beam-可移植性
- en: By providing a robust abstraction layer that incorporates the best ideas from
    across the industry, Beam provides a portability layer positioned as the programmatic
    equivalent to the declarative lingua franca provided by SQL, while also encouraging
    the adoption of innovative new ideas throughout the industry.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供一个强大的抽象层，结合了行业内最佳想法的Beam提供了一个可移植性层，被定位为与SQL提供的声明性共同语言在编程上等效的，同时也鼓励行业内创新思想的采纳。
- en: To be certain, these 10 projects and the sampling of their achievements that
    I’ve highlighted here do not remotely encompass the full breadth of the history
    that has led the industry to where it exists today. But they stand out to me as
    important and noteworthy milestones along the way, which taken together paint
    an informative picture of the evolution of stream processing over the past decade
    and a half. We’ve come a long way since the early days of MapReduce, with a number
    of ups, downs, twists, and turns along the way. Even so, there remains a long
    road of open problems ahead of us in the realm of streaming systems. I’m excited
    to see what the future holds.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 可以肯定的是，我在这里突出的这10个项目及其成就的抽样远远不能涵盖导致行业发展到今天的全部历史。但它们对我来说是重要和值得注意的里程碑，它们共同描绘了过去十五年中流处理的演变的信息图景。自MapReduce的早期以来，我们已经走了很长的路，沿途经历了许多起伏、曲折和转折。即便如此，在流系统领域仍然存在许多未解决的问题。我很期待未来会带来什么。
- en: ¹ Which means I’m skipping a ton of the academic literature around stream processing,
    because that’s where much of it started. If you’re really into hardcore academic
    papers on the topic, start from the references in [“The Dataflow Model” paper](http://bit.ly/2sXgVJ3)
    and work backward. You should be able to find your way pretty easily.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ¹这意味着我跳过了关于流处理的大量学术文献，因为那是它的起源。如果你真的对这个话题的学术论文很感兴趣，可以从[“数据流模型”论文](http://bit.ly/2sXgVJ3)的参考文献开始，然后往回看。你应该能够很容易地找到你的方向。
- en: ² Certainly, MapReduce itself was built upon many ideas that had been well known
    before, as is even explicitly stated in the MapReduce paper. That doesn’t change
    the fact that MapReduce was the system that tied those ideas together (along with
    some of its own) to create something practical that solved an important and emerging
    problem better than anyone else before ever had, and in a way that inspired generations
    of data-processing systems that followed.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ² 当然，MapReduce本身建立在许多早已知名的想法之上，甚至在MapReduce论文中明确说明了这一点。这并不改变MapReduce将这些想法（以及它自己的一些想法）结合起来，创造出一个实际解决重要且新兴问题的系统，比以往任何人都做得更好，并且激发了后来的数据处理系统的世代。
- en: ³ To be clear, Google was most certainly not the only company tackling data
    processing problems at this scale at the time. Google was just one among a number
    of companies involved in that first generation of attempts at taming massive-scale
    data processing.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 明确地说，谷歌当时绝对不是唯一一家在这个规模上解决数据处理问题的公司。谷歌只是第一代尝试解决大规模数据处理问题的众多公司之一。
- en: ⁴ And to be clear, MapReduce actually built upon the Google File System, GFS,
    which itself solved the scalability and fault-tolerance issues for a specific
    subset of the overall problem.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 明确地说，MapReduce实际上是建立在谷歌文件系统GFS之上的，GFS本身解决了特定子集的可伸缩性和容错性问题。
- en: ⁵ Not unlike the query optimizers long used in the database world.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵ 不完全像数据库世界长期使用的查询优化器。
- en: ⁶ Noogler == New + Googler == New hires at Google
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ⁶ Noogler == 新人 + Googler == Google的新员工
- en: ⁷ As an aside, I also highly recommend reading Martin Kleppmann’s [“A Critique
    of the CAP Theorem”](http://bit.ly/2ybJlnt) for very nice analysis of the shortcomings
    of the CAP theorem itself, as well as a more principled alternative way of looking
    at the same problem.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ⁷ 顺便说一句，我还强烈推荐阅读Martin Kleppmann的《CAP定理批评》（http://bit.ly/2ybJlnt），对CAP定理本身的缺点进行了很好的分析，以及更有原则的替代方法来看待同样的问题。
- en: ⁸ For the record, written primarily by Sam McVeety with help from Reuven and
    bits of input from the rest of us on the author list; we shouldn’t have alphabetized
    that author list, because everyone always assumes I’m the primary author on it,
    even though I wasn’t.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ⁸ 值得一提的是，这本书主要由Sam McVeety撰写，Reuven提供帮助，其他作者也有一些输入；我们不应该按字母顺序排列作者名单，因为每个人总是假设我是主要作者，即使我并不是。
- en: ⁹ Kafka Streams and now KSQL are of course changing that, but those are relatively
    recent developments, and I’ll be focusing primarily on the Kafka of yore.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ⁹ Kafka Streams和现在的KSQL当然正在改变这一点，但这些都是相对较新的发展，我将主要关注过去的Kafka。
- en: ¹⁰ While I recommend the book as the most comprehensive and cohesive resource,
    you can find much of the content from it scattered across O’Reilly’s website if
    you just search around for Kreps’ articles. Sorry, Jay...
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ¹⁰ 虽然我推荐这本书作为最全面和连贯的资源，但如果你在O'Reilly的网站上搜索Kreps的文章，你也可以找到其中的许多内容。抱歉，Jay...
- en: ¹¹ As with many broad generalizations, this one is true in a specific context,
    but belies the underlying complexity of reality. As I alluded to in Chapter 1,
    batch systems go to great lengths to optimize the cost and runtime of data processing
    pipelines over bounded datasets in ways that stream processing engines have yet
    to attempt to duplicate. To imply that modern batch and streaming systems only
    differ in one small way is a sizeable oversimplification in any realm beyond the
    purely conceptual.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹ 就像许多广泛的概括一样，这个概括在特定的背景下是正确的，但掩盖了现实的基本复杂性。正如我在第1章中所暗示的，批处理系统在优化有界数据集的数据处理管道的成本和运行时间方面做出了很大的努力，而流处理引擎尚未尝试复制。暗示现代批处理和流处理系统只在一个小方面上有所不同是一个相当简化的说法，在纯粹概念之外的任何领域都是如此。
- en: '¹² There’s an additional subtlety here that’s worth calling out: even as runners
    adopt new semantics and tick off feature checkboxes, it’s not the case that you
    can blindly choose any runner and have an identical experience. This is because
    the runners themselves can still vary greatly in their runtime and operational
    characteristics. Even for cases in which two given runners implement the same
    set of semantic features within the Beam Model, the way they go about executing
    those features at runtime is typically very different. As a result, when building
    a Beam pipeline, it’s important to do your homework regarding various runners,
    to ensure that you choose a runtime platform that serves your use case best.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ¹² 这里还有一个值得注意的微妙之处：即使运行器采用了新的语义并勾选了功能复选框，也不能盲目选择任何运行器并获得相同的体验。这是因为运行器本身在运行时和操作特性上仍然可能存在很大的差异。即使两个给定的运行器在Beam模型中实现了相同的语义特性，它们在运行时执行这些特性的方式通常也是非常不同的。因此，在构建Beam管道时，重要的是要对各种运行器进行功课，以确保选择最适合您用例的运行平台。
