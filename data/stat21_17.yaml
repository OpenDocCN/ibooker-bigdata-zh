- en: Chapter 16 Multivariate statistics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第16章 多元统计
- en: 原文：[https://statsthinking21.github.io/statsthinking21-core-site/multivariate.html](https://statsthinking21.github.io/statsthinking21-core-site/multivariate.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://statsthinking21.github.io/statsthinking21-core-site/multivariate.html](https://statsthinking21.github.io/statsthinking21-core-site/multivariate.html)
- en: The term *multivariate* refers to analyses that involve more than one random
    variable. Whereas we have seen previous examples where the model included multiple
    variables (as in linear regression), in those cases we were specifically interested
    in how the variation in a *dependent variable* could be explained in terms of
    one or more *independent variables* that are usually specified by the experimenter
    rather than measured. In a multivariate analysis we generally treat all of the
    variables as equals, and seek to understand how they relate to one another as
    a group.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*多元*指的是涉及多个随机变量的分析。虽然我们之前看到的模型包括多个变量（如线性回归），但在这些情况下，我们特别关注的是如何解释*因变量*的变化，这些变化通常由实验者而不是被测量的*自变量*来解释。在多元分析中，我们通常将所有变量视为平等，并试图理解它们如何作为一个群体相互关联。
- en: There are many different kinds of multivariate analysis, but we will focus on
    two major approaches in this chapter. First, we may simply want to understand
    and visualize the structure that exists in the data, by which we usually mean
    which variables or observations are related to which other. We would usually define
    “related” in terms of some measure that indexes the distance between the values
    across variables. One important method that fits under this category is known
    as *clustering*, which aims to find clusters of variables or observations that
    are similar across variables.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中有许多不同种类的多元分析，但我们将重点关注两种主要方法。首先，我们可能只是想要理解和可视化数据中存在的结构，通常指的是哪些变量或观察与其他变量或观察相关。我们通常会根据一些衡量指标来定义“相关”，这些指标可以衡量跨变量值之间的距离。属于这一类别的一个重要方法被称为*聚类*，旨在找到在变量或观察之间相似的聚类。
- en: Second, we may want to take a large number of variables and reduce them to a
    smaller number of variables in a way that retains as much information as possible.
    This is referred to as *dimensionality reduction*, where “dimensionality” refers
    to the number of variables in the dataset. We will discuss two techniques commonly
    used for dimensionality reduction, known as *principal component analysis* and
    *factor analysis*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们可能希望将大量变量减少到较少的变量，同时尽量保留尽可能多的信息。这被称为*降维*，其中“维度”指的是数据集中的变量数量。我们将讨论两种常用的降维技术，即*主成分分析*和*因子分析*。
- en: Clustering and dimensionality reduction are often classified as forms of *unsupervised
    learning*; this is in contrast to *supervised learning* which characterizes models
    such as linear regression that you’ve learned about so far. The reason that we
    consider linear regression to be “supervised” is that we know the value of thing
    that we are trying to predict (i.e. the dependent variable) and we are trying
    to find the model that best predicts those values. In unspervised learning, we
    don’t have a specific value that we are trying to predict; instead, we are trying
    to discover structure in the data that might be useful for understanding what’s
    going on, which generally requires some assumptions about what kind of structure
    we want to find.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类和降维通常被归类为*无监督学习*的形式；这与迄今为止学到的线性回归等*监督学习*形成对比。我们认为线性回归是“监督学习”的原因是，我们知道我们试图预测的事物的价值（即依赖变量），并且我们试图找到最佳预测这些值的模型。在无监督学习中，我们没有特定的值要预测；相反，我们试图发现数据中可能有用于理解情况的结构，这通常需要一些关于我们想要找到什么样的结构的假设。
- en: One thing that you will discover in this chapter is that whereas there is generally
    a “right” answer in supervised learning (once we have agreed upon how to determine
    the “best” model, such as the sum of squared errors), there is often not an agreed-upon
    “right” answer in unsupervised learning. Different unsupervised learning methods
    can give very different answers about the same data, and there is usually no way
    in principle to determine which of these is “correct”, as it depends on the goals
    of the analysis and the assumptions that one is willing to make about the mechanisms
    that give rise to the data. Some people find this frustrating, while others find
    it exhilarating; it will be up to you to figure out which of these camps you fall
    into.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将发现，虽然在监督学习中通常存在一个“正确”的答案（一旦我们已经同意如何确定“最佳”模型，例如平方误差的总和），但在无监督学习中通常没有一个一致的“正确”答案。不同的无监督学习方法可能会给出关于相同数据的非常不同的答案，通常原则上无法确定哪一个是“正确”的，因为这取决于分析的目标和对产生数据的机制愿意做出的假设。有些人会觉得这很沮丧，而其他人会觉得这很令人振奋；您将需要弄清楚自己属于哪个阵营。
- en: '16.1 Multivariate data: An example'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.1 多元数据：一个例子
- en: As an example of multivariate analysis, we will look at a dataset collected
    by my group and published by Eisenberg et al. ([**Eisenberg:2019um?**](#ref-Eisenberg:2019um)).
    This dataset is useful both because it has a large number of interesting variables
    collected on a relatively large number of individuals, and because it is freely
    available online, so that you can explore it on your own.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作为多元分析的一个例子，我们将看一下由我的团队收集并由Eisenberg等人发布的数据集。([**Eisenberg:2019um?**](#ref-Eisenberg:2019um))。这个数据集很有用，因为它收集了大量有趣的变量，并且涉及相对较多的个体，并且可以在网上免费获取，因此您可以自行探索。
- en: This study was performed because we were interested in understanding how several
    different aspects of psychological function are related to one another, focusing
    particularly on psychological measures of self-control and related concepts. Participants
    performed a ten-hour long battery of cognitive tests and surveys over the course
    of a week; in this first example we will focus on variables related to two specific
    aspects of self-control. *Response inhibition* is defined as the ability to quickly
    stop an action, and in this study was measured using a set of tasks known as *stop-signal
    tasks*. The variable of interest for these tasks is an estimate of how long it
    takes a person to stop themself, known as the *stop-signal reaction time* (*SSRT*),
    of which there are four different measures in the dataset. *Impulsivity* is defined
    as the tendency to make decisions on impulse, without regard to potential consequences
    and longer-term goals. The study included a number of different surveys measuring
    impulsivity, but we will focus on the *UPPS-P* survey, which assesses five different
    facets of impulsivity.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 进行这项研究是因为我们对了解心理功能的几个不同方面如何相互关联感兴趣，特别关注自我控制和相关概念的心理测量。参与者在一周内进行了长达十小时的认知测试和调查；在这个第一个例子中，我们将关注与自我控制的两个特定方面相关的变量。*反应抑制*被定义为迅速停止行动的能力，在这项研究中使用了一组称为*停止信号任务*的任务来衡量。这些任务的感兴趣变量是一个估计一个人停止自己所需的时间，称为*停止信号反应时间*（*SSRT*），数据集中有四种不同的测量。*冲动性*被定义为倾向于冲动决策，不考虑潜在后果和长期目标。研究包括了许多不同的调查来衡量冲动性，但我们将关注*UPPS-P*调查，该调查评估了冲动性的五个不同方面。
- en: After these scores have been computed for each of the 522 participants in Eisenberg’s
    study, we end up with 9 numbers for each individual. While multivariate data can
    sometimes have thousands or even millions of variables, it’s useful to see how
    the methods work with a small number of variables first.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在为艾森伯格的研究中的522名参与者计算了这些分数之后，我们得到了每个个体的9个数字。虽然多变量数据有时可能有数千甚至数百万个变量，但首先了解这些方法如何处理少量变量是有用的。
- en: 16.2 Visualizing multivariate data
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.2 可视化多变量数据
- en: A fundamental challenge of multivariate data is that the human eye and brain
    are simply not equipped to visualize data with more than three dimensions. There
    are various tools that we can use to try to visualize multivariate data, but all
    of them break down as the number of variables grows. Once the number of variables
    becomes too large to directly visualize, one approach is to first reduce the number
    of dimensions (as discussed further below), and then visualize that reduced dataset.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量数据的一个基本挑战是，人眼和大脑只能够可视化三维以上的数据。我们可以使用各种工具来尝试可视化多变量数据，但随着变量数量的增加，所有这些工具都会失效。一种方法是首先减少维度（如下文所述），然后可视化减少后的数据集。
- en: 16.2.1 Scatterplot of matrices
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.1 矩阵散点图
- en: One useful way to visualize a small number of variables is to plot each pair
    of variables against one another, sometimes known as a “scatterplot of matrices”;
    An example is shown in Figure [16.1](multivariate.html#fig:pairpanel). Each row/column
    in the panel refers to a single variable – in this case one of our psychological
    variables from the earlier example. The diagonal elements on the plot show the
    distribution of each variable as a histogram. The elements below the diagonal
    show a scatterplot for each pair of matrices, overlaid with a regression line
    describing the relationship between the variables. The elements above the diagonal
    show the correlation coefficient for each pair of variables. When the number of
    variables is relatively small (about 10 or less) this can be a useful way to get
    good insight into a multivariate dataset.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化少量变量的一种有用方法是将每对变量相互绘制，有时被称为“矩阵散点图”；图[16.1](multivariate.html#fig:pairpanel)中显示了一个例子。面板中的每行/列都指代一个单一变量
    - 在这种情况下是我们之前例子中的心理变量之一。图中的对角元素显示了每个变量的分布情况，即直方图。对角线以下的元素显示了每对矩阵的散点图，并叠加了描述变量关系的回归线。对角线以上的元素显示了每对变量的相关系数。当变量数量相对较少（大约10个或更少）时，这可以是一种有用的方式来深入了解多变量数据集。
- en: '![Scatterplot of matrices for the nine variables in the self-control dataset.  The
    diagonal elements in the matrix show the histogram for each of the individual
    variables.  The lower left panels show scatterplots of the relationship between
    each pair of variables, and the upper right panel shows the correlation coefficient
    for each pair of variables.](../Images/d5f9cc7a80ae423a207f9a6b8141fbaf.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![自我控制数据集中九个变量的矩阵散点图。矩阵中的对角元素显示了每个单独变量的直方图。左下方的面板显示了每对变量之间的关系散点图，右上方的面板显示了每对变量的相关系数。](../Images/d5f9cc7a80ae423a207f9a6b8141fbaf.png)'
- en: 'Figure 16.1: Scatterplot of matrices for the nine variables in the self-control
    dataset. The diagonal elements in the matrix show the histogram for each of the
    individual variables. The lower left panels show scatterplots of the relationship
    between each pair of variables, and the upper right panel shows the correlation
    coefficient for each pair of variables.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1：自我控制数据集中九个变量的矩阵散点图。矩阵中的对角元素显示了每个单独变量的直方图。左下方的面板显示了每对变量之间的关系散点图，右上方的面板显示了每对变量的相关系数。
- en: 16.2.2 Heatmap
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.2 热力图
- en: In some cases we wish to visualize the relationships between a large number
    of variables at once, usually focusing on the correlation coefficient. A useful
    way to do this can be to plot the correlation values as a *heatmap*, in which
    the color of the map relates to the value of the correlation. Figure [16.2](multivariate.html#fig:hmap)
    shows an example with a relatively small number of variables, using our psychological
    example from above. In this case, the heatmap helps the structure of the data
    “pop out” at us; we see that there are strong intercorrelations within the SSRT
    variables and within the UPPS variables, with relatively little correlation between
    the two sets of variables.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们希望一次可视化大量变量之间的关系，通常关注相关系数。这样做的一个有用方式是将相关值绘制成*热图*，其中地图的颜色与相关性的值相关。图[16.2](multivariate.html#fig:hmap)显示了一个相对较少变量的示例，使用了上面的心理学示例。在这种情况下，热图帮助我们看到数据的结构；我们看到SSRT变量内部和UPPS变量内部之间存在强相关，而两组变量之间的相关性相对较小。
- en: '![Heatmap of the correlation matrix for the nine self-control variables.  The
    brighter yellow areas in the top left and bottom right highlight the higher correlations
    within the two subsets of variables.](../Images/45aeac2fd48ca934450b64e5ecee309e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![九个自我控制变量的相关矩阵热图。左上角和右下角的较亮的黄色区域突出了两个变量子集内部的更高相关性。](../Images/45aeac2fd48ca934450b64e5ecee309e.png)'
- en: 'Figure 16.2: Heatmap of the correlation matrix for the nine self-control variables.
    The brighter yellow areas in the top left and bottom right highlight the higher
    correlations within the two subsets of variables.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2：九个自我控制变量的相关矩阵热图。左上角和右下角的较亮的黄色区域突出了两个变量子集内部的更高相关性。
- en: Heatmaps become particularly useful for visualizing correlations between large
    numbers of variables. We can use brain imaging data as an example. It is common
    for neuroscience researchers to collect data about brain function from a large
    number of locations in the brain using functional magnetic resonance imaging (fMRI),
    and then to assess the correlation between those locations, to measure “function
    connectivity” between the regions. For example, Figure [16.3](multivariate.html#fig:parcelheatmap)
    shows a heatmap for a large correlation matrix, based on activity in over 300
    regions in the brain of a single individual (yours truly). The presence of clear
    structure in the data pops out simply by looking at the heatmap. In particular
    we see that there are large sets of brain regions whose activity is highly correlated
    with each other (visible in the large yellow blocks along the diagonal of the
    correlation matrix), whereas these blocks are also strongly negatively correlated
    with other blocks (visible in the large blue blocks seen off the diagonal). Heatmaps
    are a powerful tool for easily visualizing large data matrices.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 热图特别适用于可视化大量变量之间的相关性。我们可以以脑成像数据为例。神经科学研究人员通常使用功能磁共振成像（fMRI）从大脑的许多位置收集关于脑功能的数据，然后评估这些位置之间的相关性，以测量区域之间的“功能连接”。例如，图[16.3](multivariate.html#fig:parcelheatmap)显示了一个大的相关矩阵的热图，基于单个个体（即我自己）大脑中300多个区域的活动。通过查看热图，数据中的清晰结构显而易见。特别是，我们看到有大量脑区域的活动彼此高度相关（在相关矩阵对角线上的大黄色块中可见），而这些块也与其他块强烈负相关（在对角线外的大蓝色块中可见）。热图是一种强大的工具，可以轻松可视化大型数据矩阵。
- en: '![A heatmap showing the correlation coefficient of brain activity between 316
    regions in the left hemisphere of a single individiual.  Cells in yellow reflect
    strong positive correlation, whereas cells in blue reflect strong negative correlation.
    The large blocks of positive correlation along the diagonal of the matrix correspond
    to the major connected networks in the brain](../Images/e51fc0e7ac99558e67e0a7aa473ea1fa.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![显示单个个体左半球316个脑区活动之间的相关系数的热图。黄色的单元格反映了强正相关，而蓝色的单元格反映了强负相关。矩阵对角线上的大块正相关对应于大脑中的主要连接网络](../Images/e51fc0e7ac99558e67e0a7aa473ea1fa.png)'
- en: 'Figure 16.3: A heatmap showing the correlation coefficient of brain activity
    between 316 regions in the left hemisphere of a single individiual. Cells in yellow
    reflect strong positive correlation, whereas cells in blue reflect strong negative
    correlation. The large blocks of positive correlation along the diagonal of the
    matrix correspond to the major connected networks in the brain'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3：显示单个个体左半球316个脑区活动之间的相关系数的热图。黄色的单元格反映了强正相关，而蓝色的单元格反映了强负相关。矩阵对角线上的大块正相关对应于大脑中的主要连接网络
- en: 16.3 Clustering
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.3 聚类
- en: Clustering refers to a set of methods for identifying groups of related observations
    or variables within a dataset, based on the similarity of the values of the observations.
    Usually this similarity will be quantified in terms of some measure of the *distance*
    between multivariate values. The clustering method then finds the set of groups
    that have the lowest distance between their members.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是指一组方法，根据观测值的相似性在数据集中识别相关观测或变量的群组。通常，这种相似性将以某种多变量值的*距离*度量来量化。然后，聚类方法找到成员之间距离最小的一组群组。
- en: One commonly used measure of distance for clustering is the *Euclidean distance*,
    which is basically the length of the line that connects two data points. Figure
    [16.4](multivariate.html#fig:eucdist) shows an example of a dataset with two data
    points and two dimensions (X and Y). The Euclidean distance between these two
    points is the length of the dotted line that connects the points in space.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类中常用的距离度量是*欧氏距离*，基本上是连接两个数据点的线的长度。图[16.4](multivariate.html#fig:eucdist)显示了一个具有两个数据点和两个维度（X和Y）的数据集的示例。这两个点之间的欧氏距离是空间中连接点的虚线的长度。
- en: '![A depiction of the Euclidean distance between two points, (1,2) and (4,3).  The
    two points differ by 3 along the X axis and by 1 along the Y axis.](../Images/d6ac8af8444a8496a425ed1c63b32aae.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![两点之间的欧几里德距离的描绘，(1,2)和(4,3)。这两点在X轴上相差3，在Y轴上相差1。](../Images/d6ac8af8444a8496a425ed1c63b32aae.png)'
- en: 'Figure 16.4: A depiction of the Euclidean distance between two points, (1,2)
    and (4,3). The two points differ by 3 along the X axis and by 1 along the Y axis.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4：两点之间的欧几里德距离的描绘，(1,2)和(4,3)。这两点在X轴上相差3，在Y轴上相差1。
- en: 'The Euclidean distance is computed by squaring the differences in the locations
    of the points in each dimension, adding these squared differences, and then taking
    the square root. When there are two dimensions \(x\) and \(y\), this would be
    computed as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里德距离是通过平方每个维度中点的位置的差异，将这些平方差异相加，然后取平方根来计算的。当有两个维度\(x\)和\(y\)时，这将被计算为：
- en: \[ d(x, y) = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2} \]
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: \[ d(x, y) = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2} \]
- en: 'Plugging in the values from our example data:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们示例数据的值代入公式：
- en: \[ d(x, y) = \sqrt{(1 - 4)^2 + (2 - 3)^2} = 3.16 \]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: \[ d(x, y) = \sqrt{(1 - 4)^2 + (2 - 3)^2} = 3.16 \]
- en: If the formula for Euclidean distance seems slightly familiar, this is because
    it is identical to the *Pythagorean theorem* that most of us learned in geometry
    class, which computes the length of the hypotenuse of a right triangle based on
    the lengths of the two sides. In this case, the length of the sides of the triangle
    correspond to the distance between the points along each of the two dimensions.
    While this example was in two dimensions, we will often work with data that have
    many more than two dimensions, but the same idea extends to arbitrary numbers
    of dimensions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果欧几里德距离的公式看起来有点熟悉，那是因为它与大多数人在几何课上学到的*毕达哥拉斯定理*是相同的，该定理根据两边的长度计算直角三角形的斜边长度。在这种情况下，三角形的两边的长度对应于沿着两个维度之一的点之间的距离。虽然这个例子是在两个维度上，但我们经常处理的数据的维度远远超过两个，但是相同的思想可以扩展到任意数量的维度。
- en: One important feature of the Euclidean distance is that it is sensitive to the
    overall mean and variability of the data. In this sense it is unlike the correlation
    coefficient, which measures the linear relationship between variables in a way
    that is insensitive to the overall mean or variability. For this reason, it is
    common to *scale* the data prior to computing the Euclidean distance, which is
    equivalent to converting each variable into its Z-scored version.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里德距离的一个重要特征是它对数据的整体均值和变异性敏感。在这个意义上，它不像相关系数，后者测量变量之间的线性关系，对整体均值或变异性不敏感。因此，通常在计算欧几里德距离之前对数据进行*缩放*，这相当于将每个变量转换为其Z得分版本。
- en: 16.3.1 K-means clustering
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.1 K均值聚类
- en: One commonly used method for clustering data is *K-means clustering*. This technique
    identifies a set of cluster centers, and then assigns each data point to the cluster
    whose center is the closest (that is, has the lowest Euclidean distance) from
    the data point. As an example, let’s take the latitude and longitude of a number
    of countries around the world as our data points, and see whether K-means clustering
    can effectively identify the continents of the world.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常用的聚类数据的方法是*K均值聚类*。这种技术识别一组聚类中心，然后将每个数据点分配给离该数据点最近的聚类（即欧几里德距离最小的聚类）。举个例子，让我们以世界各国的纬度和经度作为我们的数据点，并看看K均值聚类是否能有效地识别世界各大洲。
- en: Most statistical software packages have a built-in function for performing K-means
    clustering using a single command, but it’s useful to understand how it works
    step by step. We must first decide on a specific value for *K*, the number of
    clusters to be found in the data. It’s important to point out that there is no
    unique “correct” value for the number of clusters; there are various techniques
    that one can use to try to determine which solution is “best” but they can often
    give different answers, as they incorporate different assumptions or tradeoffs.
    Nonetheless, clustering techniques such as K-means are an important tool for understanding
    the structure of data, especially as they become high-dimensional.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数统计软件包都有一个内置函数，可以使用单个命令执行K均值聚类，但了解它是如何一步一步工作的是很有用的。我们必须首先决定*K*的具体值，即要在数据中找到的聚类数。重要的是要指出，聚类数没有唯一的“正确”值；有各种技术可以尝试确定哪个解决方案是“最佳”的，但它们通常会给出不同的答案，因为它们包含不同的假设或权衡。尽管如此，聚类技术如K均值对于理解数据的结构是一种重要工具，特别是当它们变得高维时。
- en: Having selected the number of clusters (*K*) that we wish to find, we must come
    up with K locations that will be our starting guesses for the centers of our clusters
    (since we don’t initially know where the centers are). One simple way to start
    is to choose K of the actual data points at random and use those as our starting
    points, which are referred to as *centroids*. We then compute the Euclidean distance
    of each data point to each of the centroids, and assign each point to a cluster
    based on its closest centroid. Using these new cluster assignments, we recompute
    the centroid of each cluster by averaging the location of all of the points assigned
    to that cluster. This process is then repeated until a stable solution is found;
    we refer to this as an *iterative* processes, because it iterates until the answer
    doesn’t change, or until some other kind of limit is reached, such as a maximum
    number of possible iterations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择我们希望找到的聚类数（*K*）之后，我们必须想出K个位置，这些位置将成为我们聚类中心的起始猜测（因为我们最初不知道中心在哪里）。一个简单的开始方法是随机选择K个实际数据点，并将它们用作我们的起始点，这些点被称为*质心*。然后，我们计算每个数据点到每个质心的欧几里德距离，并根据最接近的质心将每个点分配到一个聚类中。使用这些新的聚类分配，我们通过对分配给该聚类的所有点的位置进行平均来重新计算每个聚类的质心。然后重复这个过程，直到找到一个稳定的解决方案；我们将这称为*迭代*过程，因为它迭代直到答案不再改变，或者直到达到其他种类的限制，比如可能的最大迭代次数。
- en: '![A two-dimensional depiction of clustering on the latitude and longitude of
    countries across the world.  The square black symbols show the starting centroids
    for each cluster, and the lines show the movement of the centroid for that cluster
    across the iterations of the algorithm.](../Images/2125754cdca385da7519cbd510b6a0fa.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![对世界各国的纬度和经度进行聚类的二维描述。方形黑色符号显示了每个簇的起始质心，线条显示了该簇在算法迭代中的移动。](../Images/2125754cdca385da7519cbd510b6a0fa.png)'
- en: 'Figure 16.5: A two-dimensional depiction of clustering on the latitude and
    longitude of countries across the world. The square black symbols show the starting
    centroids for each cluster, and the lines show the movement of the centroid for
    that cluster across the iterations of the algorithm.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5：对世界各国的纬度和经度进行聚类的二维描述。方形黑色符号显示了每个簇的起始质心，线条显示了该簇在算法迭代中的移动。
- en: Applying K-means clustering to the latitude/longitude data (Figure [16.5](multivariate.html#fig:kmeans)),
    we see that there is a reasonable match between the resulting clusters and the
    continents, though none of the continents is perfectly matched to any of the clusters.
    We can further examine this by plotting a table that compares the membership of
    each cluster to the actual continents for each country; this kind of table is
    often called a *confusion matrix*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 将K均值聚类应用于纬度/经度数据（图[16.5](multivariate.html#fig:kmeans)），我们看到结果簇与大洲之间有合理的匹配，尽管没有一个大洲完全匹配任何一个簇。我们可以通过绘制一个表格来进一步检查这一点，该表格比较了每个国家的每个簇的成员资格与实际大洲；这种表格通常被称为*混淆矩阵*。
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Cluster 1 contains all European countries, as well as countries from northern
    Africa and Asia.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 簇1包含所有欧洲国家，以及来自北非和亚洲的国家。
- en: Cluster 2 contains contains Asian countries as well as several African countries.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 簇2包含亚洲国家以及一些非洲国家。
- en: Cluster 3 contains countries from the southern part of South America.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 簇3包含南美洲南部的国家。
- en: Cluster 4 contains all of the North American countries as well as northern South
    American countries.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 簇4包含所有北美国家以及南美洲北部国家。
- en: Cluster 5 contains Oceania as well as several Asian countries
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 簇5包含大洋洲以及一些亚洲国家
- en: Cluster 6 contains all of the remaining African countries.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 簇6包含所有剩余的非洲国家。
- en: Although in this example we know the actual clusters (that is, the continents
    of the world), in general we don’t actually know the ground truth for unsupervised
    learning problems, so we simply have to trust that the clustering method has found
    useful structure in the data. However, one important point about K-means clustering,
    and iterative procedures in general, is that they are not guaranteed to give the
    same answer each time they are run. The use of random numbers to determine the
    starting points means that the starting points can differ each time, and depending
    on the data this can sometimes lead to different solutions being found. For this
    example, K-means clustering will sometimes find a single cluster encompassing
    both North and South America, and sometimes find two clusters (as it did for the
    specific choice of random seed used here). Whenever using a method that involves
    an iterative solution, it is important to rerun the method a number of times using
    different random seeds, to make sure that the answers don’t diverge too greatly
    between runs. If they do, then one should avoid making strong conclusions based
    on the unstable results. In fact, it’s probably a good idea to avoid strong conclusions
    on the basis of clustering results more generally; they are primarily useful for
    getting intuition about the structure that might be present in a dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这个例子中我们知道实际的簇（也就是世界各大洲），但通常我们并不知道无监督学习问题的真实情况，所以我们只能相信聚类方法在数据中找到了有用的结构。然而，关于K均值聚类和迭代过程的一个重要点是，它们不能保证每次运行时都会得到相同的答案。使用随机数确定起始点意味着起始点每次可能不同，而且根据数据的不同，有时可能会导致找到不同的解决方案。对于这个例子，K均值聚类有时会找到一个包含北美和南美的单一簇，有时会找到两个簇（就像在这里使用的特定随机种子的选择一样）。每当使用涉及迭代解决方案的方法时，重要的是使用不同的随机种子多次重新运行该方法，以确保答案在运行之间不会有太大的分歧。如果有的话，那么就不应该基于不稳定的结果得出坚定的结论。实际上，基于聚类结果得出坚定的结论可能是一个好主意；它们主要用于对可能存在于数据集中的结构有直观感觉。
- en: '![A visualization of the clustering results from 10 runs of the K-means clustering
    algorithm with K=3\. Each row in the figure represents a different run of the
    clustering algorithm (with different random starting points), and variables sharing
    the same color are members of the same cluster.](../Images/a1b7979a567b468198cedc6fd0694a7d.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![K=3的K均值聚类算法的10次运行结果的可视化。图中的每一行代表聚类算法的不同运行（使用不同的随机起始点），颜色相同的变量属于同一簇。](../Images/a1b7979a567b468198cedc6fd0694a7d.png)'
- en: 'Figure 16.6: A visualization of the clustering results from 10 runs of the
    K-means clustering algorithm with K=3\. Each row in the figure represents a different
    run of the clustering algorithm (with different random starting points), and variables
    sharing the same color are members of the same cluster.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6：K=3的K均值聚类算法的10次运行结果的可视化。图中的每一行代表聚类算法的不同运行（使用不同的随机起始点），颜色相同的变量属于同一簇。
- en: We can apply K-means clustering to the self-control variables in order to determine
    which variables are most closely related to one another. For K=2, the K-means
    algorithm consistently picks out one cluster containing the SSRT variables and
    one containing the impulsivity variables. With higher values of K, the results
    are less consistent; for example, with K=3 the algorithm sometimes identifies
    a third cluster containing only the UPPS sensation seeking variable, whereas in
    other cases it splits the SSRT variables into two separate clusters (as seen in
    Figure [16.6](multivariate.html#fig:kmeansSro)). The stability of the clusters
    with K=2 suggests that this is probably the most robust clustering for these data,
    but these results also highlight the importance of running the algorithm multiple
    times to determine whether any particular clustering result is stable.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对自我控制变量应用K均值聚类，以确定哪些变量彼此之间最密切相关。对于K=2，K均值算法始终选择包含SSRT变量和包含冲动性变量的一个聚类。对于较高的K值，结果不太一致；例如，对于K=3，该算法有时会识别出一个仅包含UPPS感觉寻求变量的第三个聚类，而在其他情况下，它将SSRT变量分成两个单独的聚类（如图[16.6](multivariate.html#fig:kmeansSro)所示）。K=2时聚类的稳定性表明，这可能是这些数据的最稳健的聚类，但这些结果也突显了多次运行算法以确定任何特定聚类结果是否稳定的重要性。
- en: 16.3.2 Hierarchical clustering
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.2 层次聚类
- en: Another useful method for examining the structure of a multivariate dataset
    is known as *hierarchical clustering*. This technique also uses the distances
    between data points to determine clusters, but it also provides a way to visualize
    the relationships between data points in terms of a tree-like structure known
    as a *dendrogram*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种检查多元数据集结构的有用方法被称为*层次聚类*。这种技术也利用数据点之间的距离来确定聚类，但它还提供了一种可视化数据点之间关系的方式，即树状结构，称为*树状图*。
- en: The most commonly used hierarchical clustering procedure is known as *agglomerative
    clustering*. This procedure starts by treating each data point as its own cluster,
    and then progressively creates new clusters by combining the two clusters with
    the least distance between them. It continues to do this until there is only a
    single cluster. This requires computing the distance between clusters, and there
    are numerous ways to do this; in this example we will use the *average linkage*
    method, which simply takes the average of all of the distances between each each
    data point in each of two clusters. As an example, we will examine the relationship
    between the self-control variables that were described above.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的层次聚类程序被称为*聚合聚类*。该程序首先将每个数据点视为自己的一个聚类，然后通过合并两个距离最小的聚类来逐渐创建新的聚类。它继续这样做，直到只剩下一个单一的聚类。这需要计算聚类之间的距离，有许多方法可以做到这一点；在这个例子中，我们将使用*平均链接*方法，它简单地取两个聚类中每个数据点之间的所有距离的平均值。例如，我们将检查上面描述的自我控制变量之间的关系。
- en: '![A dendrogram depicting the relative similarity of the nine self-control variables.  The
    three colored vertical lines represent three different cutoffs, resulting in either
    two (blue line), three (green line), or four (red line) clusters.](../Images/fb8d0596ffea683634723dbfe434c4cf.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![树状图显示了九个自我控制变量的相对相似性。三条彩色垂直线代表三个不同的截断点，分别得到两个（蓝线）、三个（绿线）或四个（红线）聚类。](../Images/fb8d0596ffea683634723dbfe434c4cf.png)'
- en: 'Figure 16.7: A dendrogram depicting the relative similarity of the nine self-control
    variables. The three colored vertical lines represent three different cutoffs,
    resulting in either two (blue line), three (green line), or four (red line) clusters.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7：树状图显示了九个自我控制变量的相对相似性。三条彩色垂直线代表三个不同的截断点，分别得到两个（蓝线）、三个（绿线）或四个（红线）聚类。
- en: 'Figure [16.7](multivariate.html#fig:dendro) shows the dendrogram generated
    from the self-regulation dataset. Here we see that there is structure in the relationships
    between variables that can be understood at various levels by “cutting” the tree
    to create different numbers of clusters: if we cut the tree at 25, we get two
    clusters; if we cut it at 20 we get three clusters, and at 19 we get four clusters.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图[16.7](multivariate.html#fig:dendro)显示了从自我调节数据集生成的树状图。在这里，我们看到变量之间的关系具有结构，可以通过“剪切”树来在不同层次上理解：如果我们在25处剪切树，我们得到两个聚类；如果我们在20处剪切，我们得到三个聚类，而在19处我们得到四个聚类。
- en: Interestingly, the solution found by the hierarchical clustering analysis of
    the self-control data is identical to the solution found in the majority of the
    K-means clustering runs, which is comforting.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，对自我控制数据进行层次聚类分析找到的解与大多数K均值聚类运行找到的解相同，这令人欣慰。
- en: Our interpretation of this analysis would be that there is a high degree of
    similarity within each of the variable sets (SSRT and UPPS) compared to between
    the sets. Within the UPPS variables, it seems that the sensation seeking variable
    stands separately from the others, which are much more similar to one another.
    Within the SSRT variables, it seems that the stimulus selective SSRT variable
    is distinct from the other three, which are more similar. These are the kinds
    of conclusions that can be drawn from a clustering analysis. It is again important
    to point out that there is no single “right” number of clusters; different methods
    rely upon different assumptions or heuristics, and can give different results
    and interpretations. In general, it’s good to present the data clustered at several
    different levels, and make sure that this doesn’t drastically change the interpretation
    of the data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这个分析的解释是，每个变量集合（SSRT和UPPS）内部之间存在高度相似性，而与集合之间相比则相对较少。在UPPS变量中，似乎寻求感觉变量与其他变量有所不同，其他变量之间更相似。在SSRT变量中，似乎刺激选择性SSRT变量与其他三个变量有所不同，其他三个变量更相似。这些是可以从聚类分析中得出的结论。重要的是要指出，没有单一“正确”的聚类数量；不同的方法依赖于不同的假设或启发式方法，可能会给出不同的结果和解释。一般来说，最好以几个不同的层次呈现聚类数据，并确保这不会大幅改变数据的解释。
- en: 16.4 Dimensionality reduction
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4 降维
- en: It is often the case with multivariate data that many of the variables will
    be highly similar to one another, such that they are largely measuring the same
    thing. One way to think of this is that while the data have a particular number
    of variables, which we call its *dimensionality*, in reality there are not as
    many underlying sources of information as there are variables. The idea behind
    *dimensionality reduction* is to reduce the number of variables in order to create
    composite variables that reflect underlying signals in the data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在多变量数据中，往往许多变量之间会高度相似，它们在很大程度上测量相同的事物。一种思考方式是，虽然数据具有特定数量的变量，我们称之为*维度*，但实际上信息源并不像变量那么多。*降维*的想法是减少变量数量，以创建反映数据中潜在信号的复合变量。
- en: 16.4.1 Principal component analysis
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.4.1 主成分分析
- en: The idea behind principal component analysis is to find a lower-dimensional
    description of a set of variables that accounts for the maximum possible amount
    of information in the full dataset. A deep understanding of principal component
    analysis requires an understanding of linear algebra, which is beyond the scope
    of this book; see the resources at the end of this chapter for helpful guides
    to this topic. In this section I will outline the concept and hopefully whet your
    appetite to learn more.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析的想法是找到一组变量的低维描述，以解释完整数据集中可能的最大信息量。对主成分分析的深入理解需要对线性代数有深刻的理解，这超出了本书的范围；请参阅本章末尾的资源，了解有关此主题的有用指南。在本节中，我将概述这个概念，并希望激发您学习更多的兴趣。
- en: We will start with a simple example with just two variables in order to give
    an intuition for how it works. First we generate some synthetic data for variables
    X and Y, with a correlation of 0.7 between the two variables. The goal of principal
    component analysis is to find a linear combination of the observed variables in
    the dataset that will explain the maximum amount of variance; the idea here is
    that the variance in the data is a combination of signal and noise, and we want
    to find the strongest common signal between the variables. The first principal
    component is the combination that explains the maximum variance. The second component
    is the the one that explains the maximum remaining variance, while also being
    uncorrelated with the first component. With more variables we can continue this
    process to obtain as many components as there are variables (assuming that there
    are more observations than there are variables), though in practice we usually
    hope to find a small number of components that can explain a large portion of
    the variance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的例子开始，只有两个变量，以便直观地理解它是如何工作的。首先，我们为变量X和Y生成一些合成数据，两个变量之间的相关性为0.7。主成分分析的目标是找到数据集中观察变量的线性组合，以解释最大量的方差；这里的想法是数据中的方差是信号和噪音的组合，我们希望找到变量之间最强的共同信号。第一个主成分是解释最大方差的组合。第二个成分是解释剩余最大方差的组合，同时与第一个成分不相关。对于更多的变量，我们可以继续这个过程，获得与变量数量相同的成分（假设观察次数多于变量数量），尽管在实践中，我们通常希望找到能解释大部分方差的少数成分。
- en: In the case of our two-dimensional example, we can compute the principal components
    and plot them over the data (Figure [16.8](multivariate.html#fig:pcaPlot)). What
    we see is that the first principal component (shown in green) follows the direction
    of the greatest variance. This line is similar, though not identical, to the linear
    regression line; while the linear regression solution minimizes the distance between
    each data point and the regression line at the same X value (i.e. the vertical
    distance), the principal component minimizes the Euclidean distance between the
    data points and the line representing the component (i.e. the distance perpendicular
    to the component). The second component points in a direction that is perpendicular
    to the first component (which is equivalent to being uncorrelated).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的二维示例中，我们可以计算主成分并将它们绘制在数据上（图[16.8](multivariate.html#fig:pcaPlot)）。我们看到第一个主成分（显示为绿色）沿着最大方差的方向。这条线与线性回归线相似，尽管不完全相同；虽然线性回归解决方案最小化了每个数据点与回归线在相同X值（即垂直距离）的距离，但主成分最小化了数据点与表示该成分的线之间的欧几里得距离（即垂直于成分的距离）。第二个成分指向与第一个成分垂直的方向（相当于不相关）。
- en: '![A plot of synthetic data, with the first principal component plotted in green
    and the second in red.](../Images/2b415bfc50034df93eb63e26c320f2ac.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![合成数据的绘图，第一个主成分以绿色绘制，第二个以红色绘制。](../Images/2b415bfc50034df93eb63e26c320f2ac.png)'
- en: 'Figure 16.8: A plot of synthetic data, with the first principal component plotted
    in green and the second in red.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8：合成数据的绘图，第一个主成分以绿色绘制，第二个以红色绘制。
- en: It’s common to use PCA to reduce the dimensionality of a more complex dataset.
    For example, let’s say that we would like to know whether performance on all four
    of the stop signal task variables in the earlier dataset are related to the five
    impulsivity survey variables. We can perform PCA on each of those datasets separately,
    and examine how much of the variance in the data is accounted for by the first
    principal component, which will serve as our summary of the data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用PCA来降低更复杂数据集的维度。例如，假设我们想知道早期数据集中所有四个停止信号任务变量的表现是否与五个冲动性调查变量相关。我们可以分别对这些数据集执行PCA，并检查数据中多少方差由第一个主成分解释，这将作为我们对数据的摘要。
- en: '![A plot of the variance accounted for (or *scree plot*) for PCA applied separately
    to the response inhibition and impulsivity variables from the Eisenberg dataset.](../Images/6d3367d9804f2ff9beaacf55e105dfc0.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![在Eisenberg数据集中分别应用于反应抑制和冲动性变量的PCA的方差解释（或*屏幕图*）的绘图。](../Images/6d3367d9804f2ff9beaacf55e105dfc0.png)'
- en: 'Figure 16.9: A plot of the variance accounted for (or *scree plot*) for PCA
    applied separately to the response inhibition and impulsivity variables from the
    Eisenberg dataset.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9：Eisenberg数据集中应用PCA分别应用于反应抑制和冲动性变量的方差解释（或*屏幕图*）的绘图。
- en: We see in Figure [16.9](multivariate.html#fig:VAF) that for the stop signal
    variables, the first principal component accounts for about 60% of the variance
    in the data, whereas for the UPPS it accounts for about 55% of the variance. We
    can then compute the correlation between the scores obtained using the first principal
    component from each set of variables, in order to ask whether the there is a relation
    between the two sets of variables. The correlation of -0.014 between the two summary
    variables suggests that there is no overall relationship between response inhibition
    and impulsivity in this dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[16.9](multivariate.html#fig:VAF)中看到，对于停止信号变量，第一个主成分解释了数据中约60%的方差，而对于UPPS，它解释了约55%的方差。然后，我们可以计算使用每组变量的第一个主成分获得的分数之间的相关性，以了解两组变量之间是否存在关系。两个摘要变量之间的-0.014的相关性表明，在这个数据集中，反应抑制和冲动性之间没有总体关系。
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We could also perform PCA on all of these variables at once. Looking at the
    plot of variance accounted for (also known as a *scree plot) in Figure [16.7](multivariate.html#fig:dendro),
    we can see that the first two components account for a substantial amount of the
    variance in the data. We can then look at the loadings of each of the individual
    variables on these two components to understand how each specific variable is
    associated with the different components.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以同时对所有这些变量进行PCA。查看图[16.7](multivariate.html#fig:dendro)中解释的方差的绘图（也称为*屏幕图*），我们可以看到前两个成分解释了数据中相当多的方差。然后，我们可以查看每个单独变量在这两个成分上的载荷，以了解每个特定变量与不同成分的关联。
- en: '![Plot of variance accounted for by PCA components computed on the full set
    of self-control variables.](../Images/c4a70eda55ae16c836c43d150e0b9a3c.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![在包括所有自控变量的PCA解决方案中计算的PCA成分解释的方差的绘图。每个变量都显示为其在两个成分中的载荷；分别反映在两行中。](../Images/c4a70eda55ae16c836c43d150e0b9a3c.png)'
- en: (#fig:imp_pc_scree)Plot of variance accounted for by PCA components computed
    on the full set of self-control variables.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: （#fig:imp_pc_scree）绘制了在全套自控变量上计算的PCA成分解释的方差。
- en: '![Plot of variable loadings in PCA solution including all self-control variables.
    Each variable is shown in terms of its loadings on each of the two components;
    reflected in the two rows respectively.](../Images/19bbc832aeee932e86d3fb18e157a916.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![在包括所有自控变量的PCA解决方案中计算的PCA成分解释的方差的绘图。](../Images/19bbc832aeee932e86d3fb18e157a916.png)'
- en: 'Figure 16.10: Plot of variable loadings in PCA solution including all self-control
    variables. Each variable is shown in terms of its loadings on each of the two
    components; reflected in the two rows respectively.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10：在包括所有自控变量的PCA解决方案中计算的PCA成分的变量载荷的绘图。每个变量都显示为其在两个成分中的载荷；分别反映在两行中。
- en: Doing this for the impulsivity dataset (Figure [16.10](multivariate.html#fig:pcaVarPlot)),
    we see that the first component (in the first row of the figure) has nonzero loadings
    for most of the UPPS variables, and nearly zero loadings for each of the SSRT
    variables, whereas the opposite is true of the second principal component, which
    loads primarily on the SSRT variables. This tells us that the first principal
    component captured mostly variance related to the impulsivity measures, while
    the second component captured mostly variance related to the response inhibition
    measures. You might notice that the loadings are actually negative for most of
    these variables; the sign of the loadings is arbitrary, so we should make sure
    to look at large positive and negative loadings.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对冲动性数据集进行这样的操作（图[16.10](multivariate.html#fig:pcaVarPlot)），我们看到第一个成分（在图的第一行）对大多数UPPS变量具有非零载荷，对每个SSRT变量几乎没有载荷，而第二主成分的情况正好相反，它主要对SSRT变量进行载荷。这告诉我们，第一个主成分主要捕获了与冲动性测量相关的方差，而第二个主成分主要捕获了与反应抑制测量相关的方差。您可能会注意到这些变量的载荷实际上是负的；载荷的符号是任意的，因此我们应该确保查看大的正载荷和负载荷。
- en: 16.4.2 Factor analysis
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.4.2 因子分析
- en: While principal component analysis can be useful for reducing a dataset to a
    smaller number of composite variables, the standard methods for PCA have some
    limitations. Most importantly, it ensures that the components are uncorrelated;
    while this may sometimes be useful, there are often cases where we want to extract
    dimensions that may be correlated with one another. A second limitation is that
    PCA doesn’t account for measurement error in the variables that are being analyzed,
    which can lead to difficult in interpreting the resulting loadings on the components.
    Although there are modifications of PCA that can address these issues, it is more
    common in some fields (such as psychology) to use a technique called *exploratory
    factor analysis* (or EFA) to reduce the dimensionality of a dataset.[¹](#fn1)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然主成分分析对于将数据集减少到较少数量的复合变量可能是有用的，但是标准的PCA方法有一些局限性。最重要的是，它确保组件之间不相关；虽然这有时可能是有用的，但通常存在我们希望提取可能彼此相关的维度的情况。第二个限制是PCA不考虑正在分析的变量中的测量误差，这可能导致难以解释结果的载荷。虽然有修改PCA可以解决这些问题，但在某些领域（如心理学）中更常见的是使用一种称为*探索性因子分析*（或EFA）的技术来降低数据集的维度。[¹](#fn1)
- en: The idea behind EFA is that each observed variable is created through a combination
    of contributions from a set of *latent variables* – that is, variables that cannot
    be observed directly – along with some amount of measurement error for each variable.
    For this reason, EFA models are often referred to as belonging to a class of statistical
    models known as *latent variable models*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: EFA的理念是每个观察变量都是通过一组*潜在变量*的贡献组合而成的，即不能直接观察到的变量，以及每个变量的一定量的测量误差。因此，EFA模型通常被称为属于一类称为*潜在变量模型*的统计模型。
- en: 'For example, let’s say that we want to understand how measures of several different
    variables relate to the underlying factors that give rise to those measurements.
    We will first generate a synthetic dataset to show how this might work. We will
    generate a set of individuals for whom we will pretend that we know the values
    of several latent psychological variables: impulsivity, working memory capacity,
    and fluid reasoning We will assume that working memory capacity and fluid reasoning
    are correlated with one another, but that neither is correlated with impulsivity.
    From these latent variables we will then generate a set of eight observed variables
    for each indivdiual, which are simply linear combinations of the latent variables
    along with random noise added to simulate measurement error.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想了解几个不同变量的测量与产生这些测量的潜在因素之间的关系。我们将首先生成一个合成数据集，以展示这可能是如何工作的。我们将生成一组个体，假装我们知道几个潜在心理变量的值：冲动性、工作记忆能力和流体推理。我们假设工作记忆能力和流体推理彼此相关，但两者都与冲动性不相关。然后，我们将从这些潜在变量中为每个个体生成一组八个观察变量，这些变量只是潜在变量的线性组合，同时加入随机噪声以模拟测量误差。
- en: We can further examine the data by displaying a heatmap of the correlation matrix
    relating all of these variables (Figure [16.7](multivariate.html#fig:dendro).
    We see from this that there are three clusters of variables corresponding to our
    three latent variables, which is as it should be.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过显示与所有这些变量相关的相关矩阵的热图来进一步检查数据（图[16.7](multivariate.html#fig:dendro)）。从中我们可以看到，有三个变量簇对应于我们的三个潜在变量，这正是应该的。
- en: '![A heatmap showing the correlations between the variables generated from the
    three underlying latent variables.](../Images/99edd0ff49e158d2a961f5ee5379d5ca.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![显示从三个潜在潜在变量生成的变量之间的相关性的热图。](../Images/99edd0ff49e158d2a961f5ee5379d5ca.png)'
- en: (#fig:efa_cor_hmap)A heatmap showing the correlations between the variables
    generated from the three underlying latent variables.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: （#fig:efa_cor_hmap）显示从三个潜在潜在变量生成的变量之间的相关性的热图。
- en: We can think of EFA as estimating the parameters of a set of linear models all
    at once, where each model relates each of the observed variables to the latent
    variables. For our example, these equations would look as follows. In these equations,
    the \(\beta\) characters have two subscripts, one that refers to the task and
    the other than refers to the latent variable, and a variable \(\epsilon\) that
    refers to the error. Here we will assume that everything has a mean of zero, so
    that we don’t need to include an extra intercept term for each equation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将EFA视为一次性估计一组线性模型的参数，其中每个模型将每个观察变量与潜在变量相关联。对于我们的示例，这些方程将如下所示。在这些方程中，\(\beta\)字符有两个下标，一个是指任务，另一个是指潜在变量，还有一个变量\(\epsilon\)指的是误差。在这里，我们假设一切都有零的平均值，因此我们不需要为每个方程包括额外的截距项。
- en: \[ \begin{array}{lcl} nback & = &beta_{[1, 1]} * WM + \beta_{[1, 2]} * FR +
    \beta_{[1, 3]} * IMP + \epsilon \\ dspan & = &beta_{[2, 1]} * WM + \beta_{[2,
    2]} * FR + \beta_{[2, 3]} * IMP + \epsilon \\ ospan & = &beta_{[3, 1]} * WM +
    \beta_{[3, 2]} * FR + \beta_{[3, 3]} * IMP + \epsilon \\ ravens & = &beta_{[4,
    1]} * WM + \beta_{[4, 2]} * FR + \beta_{[4, 3]} * IMP + \epsilon \\ crt & = &beta_{[5,
    1]} * WM + \beta_{[5, 2]} * FR + \beta_{[5, 3]} * IMP + \epsilon \\ UPPS & = &beta_{[6,
    1]} * WM + \beta_{[6, 2]} * FR + \beta_{[6, 3]} * IMP + \epsilon \\ BIS11 & =
    &beta_{[7, 1]} * WM + \beta_{[7, 2]} * FR + \beta_{[7, 3]} * IMP + \epsilon \\
    dickman & = &beta_{[8, 1]} * WM + \beta_{[8, 2]} * FR + \beta_{[8, 3]} * IMP +
    \epsilon \\ \end{array} \]
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{array}{lcl} nback & = &beta_{[1, 1]} * WM + \beta_{[1, 2]} * FR +
    \beta_{[1, 3]} * IMP + \epsilon \\ dspan & = &beta_{[2, 1]} * WM + \beta_{[2,
    2]} * FR + \beta_{[2, 3]} * IMP + \epsilon \\ ospan & = &beta_{[3, 1]} * WM +
    \beta_{[3, 2]} * FR + \beta_{[3, 3]} * IMP + \epsilon \\ ravens & = &beta_{[4,
    1]} * WM + \beta_{[4, 2]} * FR + \beta_{[4, 3]} * IMP + \epsilon \\ crt & = &beta_{[5,
    1]} * WM + \beta_{[5, 2]} * FR + \beta_{[5, 3]} * IMP + \epsilon \\ UPPS & = &beta_{[6,
    1]} * WM + \beta_{[6, 2]} * FR + \beta_{[6, 3]} * IMP + \epsilon \\ BIS11 & =
    &beta_{[7, 1]} * WM + \beta_{[7, 2]} * FR + \beta_{[7, 3]} * IMP + \epsilon \\
    dickman & = &beta_{[8, 1]} * WM + \beta_{[8, 2]} * FR + \beta_{[8, 3]} * IMP +
    \epsilon \\ \end{array} \]
- en: In effect what we want to do using EFA is to estimate the *matrix* of coefficients
    (betas) that map the latent variables into the observed variables. For the data
    that we are generating, we know that most of the betas in this matrix are zero
    because we created them that way; for each task, only one of the weights is set
    to 1, which means that each task is a noisy measurement of a single latent variable.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们使用EFA想要做的是估计将潜在变量映射到观察变量的系数（beta）*矩阵*。对于我们生成的数据，我们知道这个矩阵中的大多数beta都是零，因为我们是这样创建的；对于每个任务，只有一个权重被设置为1，这意味着每个任务是单个潜在变量的嘈杂测量。
- en: We can apply EFA to our synthetic dataset to estimate these parameters. We won’t
    go into the details of how EFA is actually performed, other than to mention one
    important point. Most of the previous analyses in this book have relied upon methods
    that try to minimize the difference between the observed data values and the values
    predicted by the model. The methods that are used to estimate the parameters for
    EFA instead attempt to minimize the difference between the observed *covariances*
    between the observed variables, and the covariances implied by the model parameters.
    For this reason, these methods are often referred to as *covariance structure
    models*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将EFA应用于我们的合成数据集以估计这些参数。我们不会详细介绍EFA是如何实际执行的，只是提到一个重要的点。本书中大多数先前的分析都依赖于试图最小化观察数据值与模型预测值之间的差异的方法。用于估计EFA参数的方法反而试图最小化观察变量之间的*协方差*与模型参数暗示的协方差之间的差异。因此，这些方法通常被称为*协方差结构模型*。
- en: 'Let’s apply an exploratory factor analysis to our synthetic data. As with clustering
    methods, we need to first determine how many latent factors we want to include
    in our model. In this case we know there are three factors, so let’s start with
    that; later on we will examine ways to estimate the number of factors directly
    from the data. Here is the output from our statistical software for this model:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将探索性因子分析应用到我们的合成数据上。与聚类方法一样，我们首先需要确定我们的模型中要包含多少个潜在因子。在这种情况下，我们知道有三个因子，所以让我们从这个开始；稍后我们将研究直接从数据中估计因子数量的方法。这是我们统计软件对这个模型的输出：
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: One question that we would like to ask is how well our model actually fits the
    data. There is no single way to answer this question; rather, researchers have
    developed a number of different methods that provide some insight into how well
    the model fits the data. For example, one commonly used criterion is based on
    the *root mean square error of approximation* (RMSEA) statistic, which quantifies
    how far the predicted covariances are from the actual covariances; a value of
    RMSEA less than 0.08 is often considered to reflect an adequately fitting model.
    In the example here, the RMSEA value is 0.026, which suggests a model that fits
    quite well.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要问的一个问题是我们的模型实际上对数据拟合得有多好。没有单一的方法来回答这个问题；相反，研究人员已经开发了许多不同的方法，这些方法可以提供一些关于模型对数据拟合程度的见解。例如，一个常用的标准是基于*均方根逼近误差*（RMSEA）统计量，它量化了预测的协方差与实际协方差之间的距离；RMSEA小于0.08的值通常被认为反映了一个适当拟合的模型。在这个例子中，RMSEA值为0.026，这表明模型拟合得相当好。
- en: We can also examine the parameter estimates in order to see whether the model
    has appropriately identified the structure in the data. It’s common to plot this
    as a graph, with arrows from the latent variables (represented as ellipses) pointing
    to the observed variables (represented as rectangles), where an arrow represents
    a substantial loading of the observed variable on the latent variable; this kind
    of graph is often referred to as a *path diagram* since it reflects the paths
    relating the variables. This is shown in Figure [16.11](multivariate.html#fig:faDiagram).
    In this case the EFA procedure correctly identified the structure present in the
    data, both in terms of which observed variables are related to each of the latent
    variables, and in terms of the correlations between latent variables.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检查参数估计，以查看模型是否适当地识别了数据中的结构。通常将这个作为图表，用箭头从潜在变量（表示为椭圆）指向观察变量（表示为矩形），其中箭头表示观察变量对潜在变量的实质性载荷；这种图通常被称为*路径图*，因为它反映了变量之间的路径关系。这在图[16.11](multivariate.html#fig:faDiagram)中显示。在这种情况下，EFA过程正确地识别了数据中存在的结构，无论是哪些观察变量与每个潜在变量相关，还是潜在变量之间的相关性。
- en: '![Path diagram for the exploratory factor analysis model.](../Images/b3a3953f9563abf955c07d81f586fec7.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![探索性因子分析模型的路径图。](../Images/b3a3953f9563abf955c07d81f586fec7.png)'
- en: 'Figure 16.11: Path diagram for the exploratory factor analysis model.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.11：探索性因子分析模型的路径图。
- en: 16.4.3 Determining the number of factors
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.4.3 确定因子的数量
- en: One of the main challenges in applying EFA is to determine the number of factors.
    A common way to do this is to examine the fit of the model while varying the number
    of factors, and then selecting the model that gives the best fit. This is not
    foolproof, and there are multiple ways to quantify the fit of the model that can
    sometimes give different answers.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 应用EFA的主要挑战之一是确定因子的数量。一个常见的做法是在改变因子数量的同时检查模型的拟合情况，然后选择给出最佳拟合的模型。这并不是绝对可靠的，有多种方法来量化模型的拟合程度，有时会得出不同的答案。
- en: One might think that we could simply look at how well the model fits and pick
    the number of factors with the best fit, but this won’t work, because a more complex
    model will always fit the data better (as we saw in our earlier discussion of
    overfitting). For this reason, we need to use a metric of model fit that penalizes
    for the number of parameters in the model. For the purposes of this example we
    will select one of the common methods for quantifying model fit, which is known
    as the *sample-size adjusted Bayesian information criterion* (or *SABIC*). This
    measure quantifies how well the model fits the data, while also taking into account
    the number of parameters in the model (which in this case is related to the number
    of factors) as well as the sample size. While the absolute value of the SABIC
    is not interpretable, when using the same data and same kind of model we can compare
    models using SABIC to determine which is most appropriate for the data. One important
    thing to know about SABIC and other measures like it (known as *information criteria*)
    is that lower values represent better fit of the model, so in this case we want
    to find the number of factors with the lowest SABIC. In Figure [16.12](multivariate.html#fig:sabicPlot)
    we see that the model with the lowest SABIC has three factors, which shows that
    this approach was able to accurately determine the number of factors used to generate
    the data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会认为我们可以简单地看模型拟合得有多好，然后选择最拟合的因素数量，但这不起作用，因为更复杂的模型总是会更好地拟合数据（正如我们在之前关于过度拟合的讨论中看到的）。因此，我们需要使用一个惩罚模型参数数量的模型拟合度量。在这个例子中，我们将选择一种常见的量化模型拟合度的方法，即*样本大小调整的贝叶斯信息准则*（或*SABIC*）。这个度量量化了模型对数据的拟合程度，同时还考虑了模型中的参数数量（在这种情况下与因素数量有关）以及样本大小。虽然SABIC的绝对值是不可解释的，但是当使用相同的数据和相同类型的模型时，我们可以使用SABIC来比较模型，以确定哪个对数据最合适。关于SABIC和其他类似的度量（称为*信息准则*）的一个重要事实是，较低的值代表模型拟合得更好，因此在这种情况下，我们希望找到具有最低SABIC的因素数量。在图[16.12](multivariate.html#fig:sabicPlot)中，我们看到具有最低SABIC的模型有三个因素，这表明这种方法能够准确确定用于生成数据的因素数量。
- en: '![Plot of SABIC for varying numbers of factors.](../Images/d67dbe7592c780f202b2156caeb7ee34.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![不同因素数量的SABIC图。](../Images/d67dbe7592c780f202b2156caeb7ee34.png)'
- en: 'Figure 16.12: Plot of SABIC for varying numbers of factors.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.12：不同因素数量的SABIC图。
- en: Now let’s see what happens when we apply this model to real data from the Eisenberg
    et al. dataset, which contains measurements of all of the eight variables that
    were simulated in the above example. The model with three factors also has the
    lowest SABIC for these real data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看当我们将这个模型应用到Eisenberg等人数据集的真实数据时会发生什么，该数据集包含了上面示例中模拟的所有八个变量的测量值。三因素模型在这些真实数据中的SABIC也是最低的。
- en: '![Path diagram for the three-factor model on the Eisenberg et al. data.](../Images/9d4a403893943ca85c277c918944282e.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![Eisenberg等人数据上三因素模型的路径图。](../Images/9d4a403893943ca85c277c918944282e.png)'
- en: 'Figure 16.13: Path diagram for the three-factor model on the Eisenberg et al. data.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.13：Eisenberg等人数据上三因素模型的路径图。
- en: Plotting the path diagram (Figure [16.13](multivariate.html#fig:faDiagramSro))
    we see that the real data demonstrate a factor structure that is very similar
    to what we saw with the simulated data. This should not be surprising, since the
    simulated data were generated based on knowledge of these different tasks, but
    it’s comforting to know that human behavior is systematic enough that we can reliably
    identify these kinds of relationships. The main difference is that the correlation
    between the working memory factor (MR3) and the fluid reasoning factor (MR1) is
    even higher than it was in the simulated data. This result is scientifically useful
    because it shows us that while working memory and fluid reasoning are closely
    related, there is utility in separately modeling them.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制路径图（图[16.13](multivariate.html#fig:faDiagramSro)），我们看到真实数据展示了一个与我们在模拟数据中看到的非常相似的因素结构。这并不奇怪，因为模拟数据是基于对这些不同任务的知识生成的，但是知道人类行为是有系统性的，我们可以可靠地识别这些关系是令人欣慰的。主要的区别是工作记忆因素（MR3）和流体推理因素（MR1）之间的相关性甚至比模拟数据中更高。这个结果在科学上是有用的，因为它向我们展示了，虽然工作记忆和流体推理密切相关，但分别对它们进行建模是有用的。
- en: 16.5 Learning objectives
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.5 学习目标
- en: 'Having read this chapter, you should be able to:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，您应该能够：
- en: Describe the distinction between supervised and unsupervised learning.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述监督学习和无监督学习之间的区别。
- en: Employ visualization techniques including heatmaps to visualize the structure
    of multivariate data.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可视化技术，包括热图，来可视化多变量数据的结构。
- en: Understand the concept of clustering and how it can be used to identify structure
    in data.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解聚类的概念以及如何用它来识别数据中的结构。
- en: Understand the concept of dimensionality reduction.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解降维的概念。
- en: Describe how principal component analysis and factor analysis can be used to
    perform dimensionality reduction.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述主成分分析和因素分析如何用于进行降维。
- en: 16.6 Suggested readings
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.6 建议阅读
- en: '*The Geometry of Multivariate Statistics*, by Thomas Wickens'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多元统计的几何学*，Thomas Wickens'
- en: '*No Bullshit Guide to Linear Algebra*, by Ivan Savov'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*线性代数的无废话指南*，Ivan Savov'
- en: '* * *'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: There is another application of factor analysis known as *confirmatory factor
    analysis* (or CFA) that we will not discuss here; In practice its application
    can be problematic, and recent work has started to move towards modifications
    of EFA that can answer the questions often addressed using CFA.([**Marsh:2014th?**](#ref-Marsh:2014th))[↩︎](multivariate.html#fnref1)
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还有另一种因素分析的应用，称为*验证性因素分析*（或CFA），我们在这里不讨论；在实践中，它的应用可能存在问题，最近的研究已经开始转向修改EFA以回答通常使用CFA解决的问题。([**Marsh:2014th?**](#ref-Marsh:2014th))[↩︎](multivariate.html#fnref1)
