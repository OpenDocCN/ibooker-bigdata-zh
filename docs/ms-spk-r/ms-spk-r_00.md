# 序言

Apache Spark 是一个建立在可扩展性之上的分布式计算平台：Spark 的 API 使得从多个数据源获取输入并使用不同的编程语言和算法进行处理来构建数据应用变得容易。R 语言是数据科学和统计学中最强大的语言之一，因此将 R 与 Spark 连接起来非常有意义。幸运的是，R 丰富的语言特性使得从 R 调用 Spark 的 API 变得简单，看起来就像在本地数据源上运行 R 一样。只需了解这两个系统的一些背景知识，你就能够在 Spark 中调用大规模计算，或者从你喜爱的 R 编程环境中并行运行你的 R 代码。

本书详细探讨了如何使用 Spark 进行 R 编程，重点介绍了`sparklyr`包，该包支持`dplyr`和其他为 R 社区所熟知的包。它详细介绍了所有主要的使用案例，从使用 Spark 引擎查询数据到探索性数据分析、机器学习、R 代码的并行执行以及流处理。它还提供了一个自包含的介绍，讲述了如何运行 Spark 并监控作业的执行情况。书中的作者——Javier、Kevin 和 Edgar，自项目开始以来一直参与了`sparklyr`的开发。我对他们如何精心组织这本关于在 R 中使用 Spark 的清晰而专注的指南感到非常兴奋。

希望你喜欢这本书，并用它来扩展你的 R 工作负载，并将其连接到更广泛的 Spark 生态系统的能力中去。因为这里所有的基础设施都是开源的，所以请毫不犹豫地向开发人员提供关于改进这些工具的反馈。

[Matei Zaharia](https://en.wikipedia.org/wiki/Matei_Zaharia)

斯坦福大学助理教授，

Databricks 的首席技术专家，

和 Apache Spark 的原创者
