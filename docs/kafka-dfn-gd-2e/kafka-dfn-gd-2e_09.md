# 第七章：可靠数据传递

可靠性是系统的属性，而不是单个组件的属性，因此当我们谈论 Apache Kafka 的可靠性保证时，我们需要牢记整个系统及其用例。在可靠性方面，与 Kafka 集成的系统和 Kafka 本身一样重要。由于可靠性是一个系统问题，它不能仅仅是一个人的责任。每个人——Kafka 管理员、Linux 管理员、网络和存储管理员以及应用程序开发人员——都必须共同努力构建一个可靠的系统。

Apache Kafka 在可靠数据传递方面非常灵活。我们知道 Kafka 有许多用例，从跟踪网站上的点击到信用卡支付。一些用例要求最高的可靠性，而其他一些则将速度和简单性置于可靠性之上。Kafka 被设计为足够可配置，其客户端 API 足够灵活，以允许各种可靠性权衡。

由于其灵活性，使用 Kafka 时也很容易不小心自食其果，认为我们的系统是可靠的，而实际上并非如此。在本章中，我们将首先讨论可靠性的不同类型及其在 Apache Kafka 环境中的含义。然后我们将讨论 Kafka 的复制机制以及它如何促进系统的可靠性。接下来我们将讨论 Kafka 的代理和主题以及它们在不同用例中的配置方式。然后我们将讨论客户端、生产者和消费者，以及它们在不同可靠性场景中的使用方式。最后，我们将讨论验证系统可靠性的话题，因为仅仅相信系统是可靠的是不够的——假设必须经过彻底的测试。

# 可靠性保证

当我们谈论可靠性时，通常是以*保证*的术语来谈论的，这些保证是系统在不同情况下保证保持的行为。

可能最为人熟知的可靠性保证是 ACID，这是关系数据库普遍支持的标准可靠性保证。ACID 代表*原子性*、*一致性*、*隔离性*和*持久性*。当供应商解释他们的数据库符合 ACID 时，这意味着数据库保证了关于事务行为的某些行为。

这些保证是人们信任关系数据库的原因，他们知道系统承诺了什么以及在不同条件下它将如何行为。他们理解这些保证，并且可以依靠这些保证编写安全的应用程序。

理解 Kafka 提供的保证对于那些希望构建可靠应用程序的人来说至关重要。这种理解使系统的开发人员能够弄清在不同的故障条件下它将如何行为。那么，Apache Kafka 提供了什么样的保证呢？

+   Kafka 提供了分区中消息的顺序保证。如果消息 B 是在消息 A 之后使用相同的生产者在同一分区中写入的，那么 Kafka 保证消息 B 的偏移量将高于消息 A，并且消费者将在消息 A 之后读取消息 B。

+   当消息被写入所有其同步副本的分区时（但不一定刷新到磁盘），产生的消息被视为“已提交”。生产者可以选择在消息完全提交时、在消息被写入领导者时或在消息被发送到网络时接收发送消息的确认。

+   只要至少有一个副本保持存活，已提交的消息就不会丢失。

+   消费者只能读取已提交的消息。

这些基本保证可以在构建可靠系统时使用，但它们本身并不能使系统完全可靠。在构建可靠系统时涉及到权衡，Kafka 被设计为允许管理员和开发人员通过提供配置参数来控制这些权衡，从而决定他们需要多少可靠性。这些权衡通常涉及到可靠和一致地存储消息的重要性与其他重要考虑因素之间的权衡，比如可用性、高吞吐量、低延迟和硬件成本。

接下来我们将回顾 Kafka 的复制机制，介绍术语，并讨论可靠性是如何内置到 Kafka 中的。之后，我们将讨论刚才提到的配置参数。

# 复制

Kafka 的复制机制，以及每个分区的多个副本，是 Kafka 所有可靠性保证的核心。在多个副本中写入消息是 Kafka 在发生崩溃时提供消息持久性的方式。

我们在第六章中深入解释了 Kafka 的复制机制，但让我们在这里回顾一下要点。

每个 Kafka 主题都被分解成*分区*，这是基本的数据构建块。一个分区存储在一个磁盘上。Kafka 保证分区内事件的顺序，并且一个分区可以是在线的（可用的）或离线的（不可用的）。每个分区可以有多个副本，其中一个是指定的领导者。所有事件都是由领导者副本产生的，并且通常也是从领导者副本消费的。其他副本只需要与领导者保持同步，并及时复制所有最近的事件。如果领导者不可用，一个同步的副本将成为新的领导者（这条规则有一个例外，我们在第六章中讨论过）。

如果一个副本是分区的领导者，或者是一个从者，它被认为是同步的，如果它在最后 10 秒内：

+   与 ZooKeeper 有一个活动会话——意味着它在最近 6 秒内向 ZooKeeper 发送了心跳（可配置）。

+   在最后 10 秒内从领导者获取的消息（可配置）。

+   从领导者那里获取了最近的消息。也就是说，从者仍然从领导者那里获取消息是不够的；它必须在最近的 10 秒内至少一次没有滞后（可配置）。

如果一个副本失去与 ZooKeeper 的连接，停止获取新消息，或者落后并且无法在 10 秒内赶上，那么该副本被认为是失步的。当失步的副本再次连接到 ZooKeeper 并赶上最近写入领导者的消息时，它就会重新同步。这通常在暂时的网络故障修复后很快发生，但如果存储副本的代理服务器长时间宕机，可能需要一段时间才能发生。

# 失步的副本

在较旧版本的 Kafka 中，看到一个或多个副本在同步和不同步状态之间迅速切换并不罕见。这是集群出现问题的明显迹象。一个相对常见的原因是较大的最大请求大小和大的 JVM 堆，需要调整以防止长时间的垃圾回收暂停，导致经纪人暂时断开与 ZooKeeper 的连接。如今，这个问题非常罕见，特别是在使用 Apache Kafka 2.5.0 及更高版本时，其默认配置用于 ZooKeeper 连接超时和最大副本滞后。使用 JVM 8 及以上版本（现在是 Kafka 支持的最低版本）与[G1 垃圾收集器](https://oreil.ly/oDL86)有助于遏制这个问题，尽管对于大消息可能仍需要调整。一般来说，自第一版书籍出版以来，Kafka 的复制协议在这些年里变得更加可靠。有关 Kafka 复制协议演变的详细信息，请参考 Jason Gustafson 的出色演讲[“Hardening Apache Kafka Replication”](https://oreil.ly/Z1R1w)，以及 Gwen Shapira 对 Kafka 改进的概述[“Please Upgrade Apache Kafka Now”](https://oreil.ly/vKnVl)。

稍有滞后的同步副本会减慢生产者和消费者的速度——因为它们等待所有同步副本收到消息后才会*提交*。一旦副本不再同步，我们就不再等待它接收消息。它仍然滞后，但现在没有性能影响。问题在于，同步副本越少，分区的有效复制因子就越低，因此停机或数据丢失的风险就越高。

在下一节中，我们将看看这在实践中意味着什么。

# 经纪人配置

经纪人中有三个配置参数，它们改变了 Kafka 关于可靠消息存储的行为。像许多经纪人配置变量一样，这些可以应用于经纪人级别，控制系统中所有主题的配置，也可以应用于主题级别，控制特定主题的行为。

能够在主题级别控制可靠性权衡意味着同一个 Kafka 集群可以用来托管可靠和不可靠的主题。例如，在银行，管理员可能希望为整个集群设置非常可靠的默认值，但对于存储客户投诉的主题做出例外，其中一些数据丢失是可以接受的。

让我们逐一查看这些配置参数，看看它们如何影响 Kafka 中消息存储的可靠性以及涉及的权衡考虑。

## 复制因子

主题级别的配置是`replication.factor`。在经纪人级别，我们控制自动创建主题的`default.replication.factor`。

在本书的这一部分，我们假设主题的复制因子为 3，这意味着每个分区在三个不同的经纪人上被复制三次。这是一个合理的假设，因为这是 Kafka 的默认设置，但这是用户可以修改的配置。即使主题存在后，我们也可以选择添加或删除副本，从而使用 Kafka 的副本分配工具修改复制因子。

*N*的复制因子允许我们失去*N*-1 个经纪人，同时仍能够读写数据到主题。因此，更高的复制因子会导致更高的可用性、更高的可靠性和更少的灾难。另一方面，对于*N*的复制因子，我们将需要至少*N*个经纪人，并且我们将存储*N*份数据，这意味着我们将需要*N*倍的磁盘空间。基本上，我们在可用性和硬件之间进行交易。

那么，我们如何确定主题的正确副本数量呢？有一些关键考虑因素：

可用性

只有一个副本的分区即使在单个经纪人的例行重启期间也将变得不可用。我们拥有的副本越多，我们就可以期望更高的可用性。

耐久性

每个副本都是分区中所有数据的副本。如果一个分区只有一个副本，并且由于任何原因磁盘变得无法使用，我们将丢失分区中的所有数据。拥有更多副本，特别是在不同的存储设备上，减少了丢失所有副本的可能性。

吞吐量

每增加一个副本，我们就会增加代理之间的流量。如果我们以每秒 10MB 的速率向分区生成数据，那么单个副本将不会产生任何复制流量。如果我们有 2 个副本，那么我们将有 10MBps 的复制流量，有 3 个副本则为 20MBps，有 5 个副本则为 40MBps。在规划集群大小和容量时，我们需要考虑这一点。

端到端延迟

每个生成的记录在可供消费者使用之前必须被复制到所有同步的副本中。理论上，拥有更多副本会增加其中一个副本速度较慢的概率，因此会减慢消费者的速度。实际上，如果一个代理因任何原因变慢，它将减慢每个尝试使用它的客户端的速度，无论复制因子如何。

成本

这是使用非关键数据的复制因子低于 3 的最常见原因。我们拥有的数据副本越多，存储和网络成本就越高。由于许多存储系统已经将每个块复制 3 次，因此通过配置 Kafka 的复制因子为 2 来降低成本有时是有意义的。请注意，与复制因子为 3 相比，这仍会降低可用性，但存储设备将保证耐久性。

副本的放置也非常重要。Kafka 将始终确保分区的每个副本位于不同的代理上。在某些情况下，这还不够安全。如果分区的所有副本都放置在同一机架上的代理上，并且顶部交换机出现故障，我们将失去分区的可用性，无论复制因子如何。为了防止机架级别的不幸，我们建议将代理放置在多个机架上，并使用`broker.rack`代理配置参数为每个代理配置机架名称。如果配置了机架名称，Kafka 将确保分区的副本分布在多个机架上，以确保更高的可用性。在云环境中运行 Kafka 时，通常将可用性区域视为单独的机架。在第六章中，我们提供了有关 Kafka 如何在代理和机架上放置副本的详细信息。

## 不洁净的领导者选举

此配置仅在代理（实际上是在整个集群范围内）级别可用。参数名称为`unclean.leader.election.enable`，默认设置为`false`。

如前所述，当分区的领导者不再可用时，将选择一个同步的副本作为新的领导者。这种领导者选举是“干净的”，因为它保证没有丢失已提交的数据——根据定义，已提交的数据存在于所有同步的副本上。

但是当除了刚刚变得不可用的领导者之外，没有同步的副本存在时，我们该怎么办呢？

这种情况可能发生在以下两种情况之一：

+   分区有三个副本，两个跟随者变得不可用（假设两个代理崩溃）。在这种情况下，当生产者继续写入领导者时，所有消息都被确认和提交（因为领导者是唯一的同步副本）。现在假设领导者不再可用（哎呀，又一个代理崩溃了）。在这种情况下，如果一个不同步的跟随者首先启动，我们将有一个不同步的副本作为分区的唯一可用副本。

+   分区有三个副本，由于网络问题，两个跟随者落后，即使它们已经上线并复制，它们也不再同步。领导者继续接受消息作为唯一的同步副本。现在，如果领导者不可用，只有不同步的副本可以成为领导者。

在这两种情况下，我们需要做出一个困难的决定：

+   如果我们不允许不同步的副本成为新的领导者，分区将保持离线，直到我们将旧领导者（和最后一个同步副本）重新上线。在某些情况下（例如，内存芯片需要更换），这可能需要很多小时。

+   如果我们允许不同步的副本成为新的领导者，我们将丢失所有在旧领导者不同步时写入的消息，并且还会导致一些消费者的不一致。为什么？想象一下，当副本 0 和 1 不可用时，我们将偏移量为 100-200 的消息写入副本 2（然后成为领导者）。现在副本 2 不可用，副本 0 重新上线。副本 0 只有消息 0-100，而没有 100-200。如果我们允许副本 0 成为新的领导者，它将允许生产者写入新消息，并允许消费者读取它们。因此，现在新的领导者完全有新的消息 100-200。首先，让我们注意到一些消费者可能已经读取了旧消息 100-200，一些消费者得到了新的 100-200，一些得到了混合的。当涉及到下游报告等事项时，这可能会导致非常糟糕的后果。此外，副本 2 将重新上线并成为新领导者的跟随者。在那时，它将删除任何它得到的但在当前领导者上不存在的消息。这些消息将不会对未来的任何消费者可用。

总之，如果我们允许不同步的副本成为领导者，我们就面临着数据丢失和不一致的风险。如果我们不允许它们成为领导者，我们将面临较低的可用性，因为我们必须等待原始领导者恢复正常，分区才能重新上线。

默认情况下，`unclean.leader.election.enable`被设置为 false，这将不允许不同步的副本成为领导者。这是最安全的选择，因为它提供了最好的保证来防止数据丢失。这意味着在我们之前描述的极端不可用的情况下，一些分区将保持不可用，直到手动恢复。管理员始终可以查看情况，决定接受数据丢失以使分区可用，并在启动集群之前将此配置切换为 true。只是不要忘记在集群恢复后将其切换回 false。

## 最小同步副本

主题和经纪人级别的配置都称为`min.insync.replicas`。

正如我们所看到的，有些情况下，即使我们配置了一个主题有三个副本，我们可能只剩下一个同步副本。如果这个副本不可用，我们可能不得不在可用性和一致性之间做出选择。这从来都不是一个容易的选择。请注意，问题的一部分是，根据 Kafka 的可靠性保证，数据被认为是已提交的，当它被写入所有同步副本时，即使“所有”只意味着一个副本，如果该副本不可用，数据可能会丢失。

当我们想要确保提交的数据被写入多个副本时，我们需要将最小同步副本的数量设置为更高的值。如果一个主题有三个副本，我们将`min.insync.replicas`设置为`2`，那么生产者只能在至少两个副本中有同步的情况下写入主题中的分区。

当三个副本都同步时，一切都会正常进行。如果其中一个副本不可用，情况也是如此。但是，如果三个副本中有两个不可用，经纪人将不再接受生产请求。相反，试图发送数据的生产者将收到`NotEnoughReplicasException`。消费者可以继续读取现有数据。实际上，使用这种配置，一个单独的同步副本将变为只读。这可以防止产生和消费数据，然后在发生不干净的选举时消失的不良情况。为了从这种只读状态中恢复，我们必须使两个不可用的分区中的一个再次可用（可能重新启动经纪人），并等待它赶上并同步。

## 保持副本同步

如前所述，不同步的副本会降低整体可靠性，因此尽量避免这种情况非常重要。我们还解释了副本可能以两种方式之一变得不同步：要么失去与 ZooKeeper 的连接，要么无法跟上领导者并积累复制延迟。Kafka 有两个经纪人配置，用于控制集群对这两种情况的敏感度。

`zookeeper.session.timeout.ms`是 Kafka 经纪人可以在此期间停止向 ZooKeeper 发送心跳而 ZooKeeper 不认为经纪人已死并将其从集群中移除的时间间隔。在 2.5.0 版本中，这个值从 6 秒增加到 18 秒，以增加云环境中 Kafka 集群的稳定性，其中网络延迟显示更高的方差。通常，我们希望这个时间足够长，以避免由垃圾收集或网络条件引起的随机波动，但仍然足够低，以确保实际上被冻结的经纪人将及时被检测到。

如果副本未从领导者那里获取消息，或者未赶上领导者的最新消息时间超过`replica.lag.time.max.ms`，它将变得不同步。在 2.5.0 版本中，这个值从 10 秒增加到 30 秒，以提高集群的弹性并避免不必要的波动。请注意，这个更高的值也会影响消费者的最大延迟——使用更高的值可能需要长达 30 秒的时间，直到所有副本都收到消息并允许消费。

## 持久化到磁盘

我们已经多次提到，Kafka 将确认未持久化到磁盘的消息，仅取决于接收消息的副本数量。Kafka 将在旋转段（默认大小为 1GB）之前和重新启动之前将消息刷新到磁盘，但在其他情况下，将依赖于 Linux 页面缓存在其变满时刷新消息。其背后的想法是，在分开的机架或可用区域中有三台机器，每台机器都有数据的副本，比在领导者上将消息写入磁盘更安全，因为两个不同机架或区域的同时故障是如此不太可能。但是，也可以配置经纪人更频繁地将消息持久化到磁盘。配置参数`flush.messages`允许我们控制未同步到磁盘的最大消息数量，`flush.ms`允许我们控制同步到磁盘的频率。在使用此功能之前，值得阅读[“`fsync`如何影响 Kafka 的吞吐量以及如何减轻其缺点”](https://oreil.ly/Ai1hl)。

# 在可靠系统中使用生产者

即使我们将经纪人配置为可能的最可靠配置，如果我们不配置生产者也是可靠的，整个系统仍然可能会丢失数据。

以下是两个示例场景，以演示这一点：

+   我们使用三个副本配置了代理，并且禁用了不洁净的领导者选举。因此，我们不应该丢失提交到 Kafka 集群的任何单个消息。然而，我们配置了生产者使用`acks=1`发送消息。我们从生产者发送了一条消息，它被写入了领导者，但尚未被写入同步副本。领导者向生产者发送了一个响应，表示“消息已成功写入”，然后立即在数据被复制到其他副本之前崩溃。其他副本仍然被认为是同步的（记住，在我们宣布副本不同步之前需要一段时间），其中一个将成为领导者。由于消息未被写入副本，因此丢失了。但生产应用程序认为它已成功写入。系统是一致的，因为没有消费者看到消息（因为副本从未收到消息而未提交），但从生产者的角度来看，消息丢失了。

+   我们使用三个副本配置了代理，并且禁用了不洁净的领导者选举。我们从错误中吸取教训，并开始使用`acks=all`来生产消息。假设我们试图向 Kafka 写入消息，但我们要写入的分区的领导者刚刚崩溃，新的领导者仍在选举中。Kafka 将回复“领导者不可用”。此时，如果生产者没有正确处理错误并重试直到写入成功，消息可能会丢失。再次强调，这不是代理可靠性问题，因为代理从未收到消息；也不是一致性问题，因为消费者也从未收到消息。但如果生产者没有正确处理错误，可能会导致消息丢失。

正如示例所示，每个编写生产到 Kafka 的应用程序的人都必须注意两件重要的事情：

+   使用正确的`acks`配置以满足可靠性要求

+   在配置和代码中正确处理错误

我们在第三章中深入讨论了生产者配置，但让我们再次重点介绍一些重要的内容。

## 发送确认

生产者可以在三种不同的确认模式之间进行选择：

`acks=0`

这意味着如果生产者成功将消息发送到网络上，那么消息被认为已成功写入 Kafka。如果我们发送的对象无法序列化，或者网络卡失败，我们仍会收到错误，但如果分区脱机，领导者选举正在进行，甚至整个 Kafka 集群不可用，我们将不会收到任何错误。使用`acks=0`可以降低生产延迟（这就是为什么我们看到很多基准测试使用这种配置），但它不会改善端到端延迟（记住，消费者在所有可用副本中复制之前将看不到消息）。

`acks=1`

这意味着领导者在收到消息并将其写入分区数据文件时（但不一定同步到磁盘），将发送确认或错误。如果领导者关闭或崩溃，并且在崩溃之前成功写入领导者并得到确认的一些消息未被复制到跟随者，我们可能会丢失数据。使用这种配置，还可能会比领导者更快地写入领导者，导致副本不足，因为领导者在复制消息之前将从生产者那里确认消息。

`acks=all`

这意味着领导者将等待直到所有同步副本收到消息，然后才发送确认或错误。结合代理上的`min.insync.replicas`配置，这让我们控制在消息被确认之前有多少副本收到消息。这是最安全的选项——生产者在消息完全提交之前不会停止尝试发送消息。这也是生产者延迟最长的选项——生产者等待所有同步副本收到所有消息，然后才能将消息批次标记为“完成”并继续进行。

## 配置生产者重试

在生产者中处理错误有两个部分：生产者自动处理的错误和我们作为使用生产者库的开发人员必须处理的错误。

生产者可以处理*可重试*错误。当生产者向代理发送消息时，代理可以返回成功或错误代码。这些错误代码属于两类——可以在重试后解决的错误和不会解决的错误。例如，如果代理返回错误代码`LEADER_NOT_AVAILABLE`，生产者可以尝试再次发送消息——也许新的代理被选举出来，第二次尝试会成功。这意味着`LEADER_NOT_AVAILABLE`是一个*可重试*错误。另一方面，如果代理返回`INVALID_CONFIG`异常，再次尝试发送相同的消息不会改变配置。这是一个*不可重试*错误的例子。

总的来说，当我们的目标是永远不丢失消息时，我们最好的方法是配置生产者在遇到可重试错误时继续尝试发送消息。而在第三章中推荐的重试最佳方法是将重试次数保持在当前默认值（`MAX_INT`，或者实际上是无限）并使用`delivery.timout.ms`来配置我们愿意等待放弃发送消息的最长时间——生产者将在此时间间隔内尽可能多次地重试发送消息。

重试发送失败消息包括一个风险，即两条消息都成功写入代理，导致重复。重试和谨慎的错误处理可以保证每条消息将被存储*至少一次*，但不是*确切一次*。使用`enable.idempotence=true`将导致生产者在其记录中包含额外的信息，代理将使用这些信息来跳过由重试引起的重复消息。在第八章中，我们详细讨论了这是如何工作的。

## 额外的错误处理

使用内置的生产者重试是一种正确处理各种错误而不丢失消息的简单方法，但作为开发人员，我们仍然必须能够处理其他类型的错误。这些包括：

+   不可重试的代理错误，例如有关消息大小、授权错误等的错误。

+   在消息发送到代理之前发生的错误，例如序列化错误

+   当生产者耗尽所有重试尝试或由于使用所有内存来存储消息而填满生产者可用内存时发生的错误

+   超时

在第三章中，我们讨论了如何为同步和异步发送消息的方法编写错误处理程序。这些错误处理程序的内容是特定于应用程序及其目标的——我们要丢弃“坏消息”吗？记录错误？停止从源系统读取消息？对源系统施加反压力，暂停发送消息一段时间？将这些消息存储在本地磁盘上的目录中？这些决定取决于架构和产品要求。只需注意，如果错误处理程序所做的只是重试发送消息，那么我们最好依赖生产者的重试功能。

# 在可靠系统中使用消费者

现在我们已经学会了如何在考虑 Kafka 的可靠性保证的情况下生成数据，是时候看看如何消费数据了。

正如我们在本章的第一部分中所看到的，数据只有在提交到 Kafka 之后才对消费者可用——这意味着它已被写入到所有的同步副本。这意味着消费者获取的数据是保证一致的。消费者唯一需要做的就是确保他们跟踪已经读取的消息和尚未读取的消息。这对于在消费消息时不丢失消息至关重要。

当从分区中读取数据时，消费者会获取一批消息，检查批中的最后偏移量，然后请求从上次接收到的偏移量开始的另一批消息。这保证了 Kafka 消费者始终以正确的顺序获取新数据，而不会错过任何消息。

当一个消费者停止时，另一个消费者需要知道从哪里开始工作——前一个消费者在停止之前处理的最后偏移量是多少？“其他”消费者甚至可以是重新启动后的原始消费者。这并不重要——某个消费者将从该分区开始消费，并且需要知道从哪个偏移量开始。这就是为什么消费者需要“提交”它们的偏移量。对于它正在消费的每个分区，消费者都会存储其当前位置，因此它或其他消费者将知道在重新启动后从哪里继续。消费者可能丢失消息的主要方式是在提交已读取但尚未完全处理的事件的偏移量时。这样，当另一个消费者接管工作时，它将跳过这些消息，它们将永远不会被处理。这就是为什么仔细关注偏移量何时以及如何提交是至关重要的。

# 已提交的消息与已提交的偏移量

这与*已提交的消息*不同，如之前讨论的，*已提交的消息*是写入所有同步副本并对消费者可用的消息。*已提交的偏移量*是消费者发送给 Kafka 以确认它已接收并处理了分区中到特定偏移量的所有消息。

在第四章中，我们详细讨论了消费者 API，并涵盖了提交偏移量的许多方法。在这里，我们将介绍一些重要的考虑和选择，但是有关使用 API 的详细信息，请参考第四章。

## 可靠处理的重要消费者配置属性

有四个消费者配置属性对于理解如何配置我们的消费者以获得所需的可靠性行为是重要的。

第一个是`group.id`，如第四章中详细解释的那样。基本思想是，如果两个消费者具有相同的组 ID 并订阅相同的主题，每个消费者将被分配主题中分区的一个子集，因此每个消费者将单独读取一部分消息（但整个组将读取所有消息）。如果我们需要一个消费者能够独立地看到其订阅的主题中的每条消息，它将需要一个唯一的`group.id`。

第二个相关的配置是`auto.offset.reset`。该参数控制当没有提交偏移量时消费者将会做什么（例如，当消费者首次启动时），或者当消费者请求在代理中不存在的偏移量时（第四章解释了这种情况）。这里只有两个选项。如果我们选择`earliest`，消费者将从分区的开头开始，每当它没有有效的偏移量时。这可能导致消费者处理很多消息两次，但它保证了最小化数据丢失。如果我们选择`latest`，消费者将从分区的末尾开始。这最小化了消费者的重复处理，但几乎肯定会导致一些消息被消费者错过。

第三个相关配置是`enable.auto.commit`。这是一个重大决定：我们是否打算让消费者根据计划为我们提交偏移量，还是打算在我们的代码中手动提交偏移量？自动偏移量提交的主要好处是在我们的应用程序中使用消费者时，这是一件少了的事情需要担心。当我们在消费者轮询循环内处理所有消费记录时，自动偏移量提交可以保证我们永远不会意外提交我们没有处理的偏移量。自动偏移量提交的主要缺点是，我们无法控制应用程序可能处理的重复记录数量，因为它在处理一些记录后停止，但在自动提交生效之前。当应用程序有更复杂的处理，例如将记录传递到另一个线程在后台处理时，除了使用手动偏移量提交外别无选择，因为自动提交可能会提交消费者已读取但可能尚未处理的记录的偏移量。

第四个相关配置`auto.commit.``interval.ms`与第三个相关。如果我们选择自动提交偏移量，这个配置让我们配置它们的提交频率。默认值是每五秒一次。一般来说，更频繁地提交会增加开销，但会减少消费者停止时可能发生的重复数量。

虽然与可靠的数据处理没有直接关系，但如果消费者经常停止消费以进行重新平衡，很难认为它是可靠的。第四章包括如何配置消费者以最小化不必要的重新平衡和在重新平衡时最小化暂停的建议。

## 在消费者中明确提交偏移量

如果我们决定需要更多控制并选择手动提交偏移量，我们需要关注正确性和性能影响。

我们不会在这里详细介绍提交偏移量涉及的机制和 API，因为它们在第四章中已经深入讨论过。相反，我们将回顾在开发消费者处理数据时的重要考虑因素。我们将从简单而明显的观点开始，然后转向更复杂的模式。

### 在处理消息后始终提交偏移量

如果我们在轮询循环内进行所有处理，并且在轮询循环之间不保持状态（例如，用于聚合），这应该很容易。我们可以使用自动提交配置，在轮询循环结束时提交偏移量，或者在循环内以平衡开销和缺乏重复处理的要求提交偏移量。如果涉及额外的线程或有状态的处理，这将变得更加复杂，特别是因为消费者对象不是线程安全的。在第四章中，我们讨论了如何做到这一点，并提供了更多示例的参考。

### 提交频率是性能和在发生崩溃时重复事件数量之间的权衡。

即使在最简单的情况下，我们在轮询循环内进行所有处理，并且在轮询循环之间不保持状态（例如，用于聚合），我们可以选择在循环内多次提交或者选择每隔几个循环才提交一次。提交会带来显著的性能开销。这类似于使用`acks=all`进行生产，但单个消费者组的所有偏移量提交都会发送到同一个代理，这可能会导致负载过重。提交频率必须平衡性能要求和重复处理的要求。在非常低吞吐量的主题上，应该只在每条消息之后才进行提交。

### 在正确的时间提交正确的偏移量

在轮询循环中提交时的一个常见陷阱是在轮询时意外提交了最后读取的偏移量，而不是最后处理的偏移量之后的偏移量。请记住，始终要为处理后的消息提交偏移量——提交读取但未处理的消息的偏移量可能导致消费者丢失消息。第四章中有示例，展示了如何做到这一点。

### 重新平衡

在设计应用程序时，我们需要记住消费者重新平衡会发生，并且需要正确处理它们。第四章包含一些示例。通常这涉及在分区被撤销之前提交偏移量，并在分配新分区时清理应用程序维护的任何状态。

### 消费者可能需要重试

在某些情况下，在调用轮询并处理记录后，一些记录可能没有完全处理，需要稍后处理。例如，我们可能尝试将 Kafka 中的记录写入数据库，但发现数据库此时不可用，需要稍后重试。请注意，与传统的发布/订阅消息系统不同，Kafka 消费者提交偏移量而不是“确认”单个消息。这意味着如果我们未能处理记录＃30 并成功处理记录＃31，我们不应该提交偏移量＃31——这将导致标记为已处理所有记录直到＃31，包括＃30，这通常不是我们想要的。相反，尝试遵循以下两种模式之一。

当我们遇到可重试错误时的一个选择是提交我们成功处理的最后一条记录。然后，我们将仍需要处理的记录存储在缓冲区中（以便下一次轮询不会覆盖它们），使用消费者的`pause()`方法确保额外的轮询不会返回数据，并继续尝试处理记录。

遇到可重试错误时的第二个选择是将其写入到一个单独的主题中并继续。可以使用单独的消费者组来处理重试主题中的重试，或者一个消费者可以订阅主题和重试主题，但在重试之间暂停重试主题。这种模式类似于许多消息系统中使用的死信队列系统。

### 消费者可能需要维护状态

在一些应用程序中，我们需要在多次轮询之间保持状态。例如，如果我们想要计算移动平均值，我们将希望在每次轮询 Kafka 获取新消息后更新平均值。如果我们的进程重新启动，我们不仅需要从最后的偏移量开始消费，还需要恢复匹配的移动平均值。一种方法是在应用程序提交偏移量的同时将最新累积值写入“结果”主题。这意味着当线程启动时，它可以在启动时获取最新的累积值，并从上次离开的地方继续。在第八章中，我们讨论了应用程序如何在单个事务中写入结果和提交偏移量。一般来说，这是一个相当复杂的问题，我们建议查看像 Kafka Streams 或 Flink 这样的库，它们提供了高级 DSL 样式的 API，用于聚合、连接、窗口和其他复杂的分析。

# 验证系统可靠性

一旦我们经历了确定我们的可靠性要求、配置代理、配置客户端以及以最佳方式使用 API 来满足我们的用例的过程，我们就可以放心地在生产环境中运行一切，确信不会错过任何事件，对吗？

我们建议首先进行一些验证，并建议三层验证：验证配置、验证应用程序，并在生产中监视应用程序。让我们看看每个步骤，并了解我们需要验证什么以及如何验证。

## 验证配置

从应用逻辑中隔离出来测试代理和客户端配置很容易，也建议出于两个原因这样做：

+   测试我们选择的配置是否能满足我们的要求是有帮助的。

+   推理系统的预期行为是一个很好的练习。

Kafka 包括两个重要的工具来帮助进行验证。`org.apache.kafka.tools`包括`VerifiableProducer`和`VerifiableConsumer`类。这些可以作为命令行工具运行，也可以嵌入到自动化测试框架中。

这个想法是，可验证的生产者产生一个包含从 1 到我们选择的值的数字序列的消息。我们可以像配置自己的生产者一样配置可验证的生产者，设置正确数量的`ack`、`retries`、`delivery.timeout.ms`，以及消息产生的速率。当我们运行它时，它将根据接收到的`ack`为每条发送到代理的消息打印成功或错误。可验证的消费者执行补充检查。它消费事件（通常是可验证的生产者产生的事件），并按顺序打印出它消费的事件。它还打印有关提交和重新平衡的信息。

重要的是要考虑我们想要运行哪些测试。例如：

+   领导者选举：如果我们杀死领导者会发生什么？生产者和消费者需要多长时间才能像往常一样开始工作？

+   控制器选举：系统在控制器重启后需要多长时间才能恢复？

+   滚动重启：我们能否逐个重启代理而不丢失任何消息？

+   非干净的领导者选举测试：当我们逐个杀死一个分区的所有副本（以确保每个副本都不同步），然后启动一个不同步的代理时会发生什么？为了恢复操作需要发生什么？这是可以接受的吗？

然后我们选择一个场景，启动可验证的生产者，启动可验证的消费者，并运行该场景——例如，杀死我们正在生产数据的分区的领导者。如果我们期望有一个短暂的暂停，然后一切都能正常恢复而没有消息丢失，我们需要确保生产者产生的消息数量和消费者消费的消息数量匹配。

Apache Kafka 源代码库包括一个广泛的测试套件。套件中的许多测试都基于相同的原则，并使用可验证的生产者和消费者来确保滚动升级正常工作。

## 验证应用程序

一旦我们确定代理和客户端配置符合我们的要求，就是测试应用程序是否提供我们需要的保证的时候了。这将检查诸如自定义错误处理代码、偏移提交、重新平衡监听器以及应用程序逻辑与 Kafka 客户端库交互的类似位置。

自然地，由于应用逻辑可能会有很大的变化，我们只能提供有限的关于如何测试的指导。我们建议将应用程序作为任何开发过程的一部分进行集成测试，并建议在各种故障条件下运行测试：

+   客户端失去与其中一个代理的连接

+   客户端和代理之间的高延迟

+   磁盘已满

+   挂起的磁盘（也称为“停电”）

+   领导者选举

+   代理的滚动重启

+   消费者的滚动重启

+   生产者的滚动重启

有许多工具可用于引入网络和磁盘故障，其中许多都非常出色，因此我们不会尝试提出具体建议。Apache Kafka 本身包括[Trogdor 测试框架](https://oreil.ly/P3ai1)用于故障注入。对于每种情况，我们将有*预期行为*，这是我们在开发应用程序时计划看到的情况。然后我们运行测试，看看实际发生了什么。例如，当计划对消费者进行滚动重启时，我们计划进行短暂暂停，因为消费者会重新平衡，然后继续消费，最多不超过 1,000 个重复值。我们的测试将显示应用程序提交偏移量和处理重新平衡的方式是否真的如此运行。

## 在生产环境中监控可靠性

测试应用程序很重要，但这并不能取代持续监控生产系统以确保数据流如预期般顺畅。第十二章将详细介绍如何监控 Kafka 集群，但除了监控集群的健康状况外，还要监控客户端和数据流通过系统的情况。

Kafka 的 Java 客户端包括 JMX 指标，允许监控客户端状态和事件。对于生产者来说，可靠性最重要的两个指标是每条记录的错误率和重试率（汇总）。要密切关注这些指标，因为错误率或重试率的上升可能表明系统存在问题。还要监控生产者日志，查看在发送事件时记录为`WARN`级别的错误，内容类似于“在主题-分区 [topic-1,3] 上使用相关 ID 5689 产生错误的响应，正在重试（还剩两次尝试）。错误：…”当我们看到剩余尝试次数为 0 的事件时，表示生产者的重试次数已用尽。在第三章中，我们讨论了如何配置`delivery.timeout.ms`和`retries`以改进生产者的错误处理，并避免过早用尽重试次数。当然，解决导致错误的问题才是更好的选择。生产者的`ERROR`级别日志消息可能表明由于不可重试的错误、用尽重试次数的可重试错误或超时而完全发送消息失败。在适用的情况下，经纪人的确切错误也将被记录。

在消费者方面，最重要的指标是消费者滞后。该指标表示消费者距离经纪人分区上最新提交的消息有多远。理想情况下，滞后应始终为零，消费者将始终读取最新的消息。实际上，因为调用`poll()`会返回多条消息，然后消费者会花时间处理它们，然后再获取更多消息，滞后值会有所波动。重要的是确保消费者最终能够赶上，而不是越来越落后。由于消费者滞后的预期波动，设置传统的警报指标可能会有挑战性。[Burrow](https://oreil.ly/supY1)是 LinkedIn 开发的消费者滞后检查工具，可以简化这一过程。

监控数据流也意味着确保所有生成的数据及时被消费（“及时”通常基于业务需求）。为了确保数据及时被消费，我们需要知道数据是何时生成的。Kafka 在这方面提供了帮助：从 0.10.0 版本开始，所有消息都包括一个时间戳，指示事件生成的时间（尽管请注意，如果应用程序发送事件或经纪人自身配置为这样做，时间戳可以被覆盖）。

为了确保所有生成的消息在合理的时间内被消耗，我们需要应用程序记录生成的事件数量（通常以每秒事件数的形式）。消费者需要记录每个时间单位消耗的事件数量，以及使用事件时间戳记录事件产生和消耗之间的滞后时间。然后，我们需要一个系统来协调生产者和消费者的每秒事件数量（以确保消息在传输过程中没有丢失），并确保生产时间和消费时间之间的间隔是合理的。这种端到端的监控系统可能具有挑战性，并且实施起来可能耗时。据我们所知，目前没有开源实现这种类型系统的，但 Confluent 作为[Confluent Control Center](https://oreil.ly/KnvVV)的一部分提供了商业实现。

除了监控客户端和端到端数据流之外，Kafka 代理还包括指示从代理发送到客户端的错误响应速率的指标。我们建议收集`kafka.server:type=BrokerTopicMetrics,​name=FailedProduceRequestsPerSec`和`kafka.server:type=BrokerTopic​Met⁠rics,name=FailedFetchRequestsPerSec`。有时，预期会出现一定级别的错误响应，例如，如果我们关闭代理进行维护，并在另一个代理上选举新的领导者，那么预期生产者将收到`NOT_LEADER_FOR_PARTITION`错误，这将导致它们在继续正常生产事件之前请求更新的元数据。无法解释的失败请求增加应该始终进行调查。为了帮助进行此类调查，失败请求指标附带了代理发送的具体错误响应标记。

# 总结

正如我们在本章开头所说的，可靠性不仅仅是特定 Kafka 功能的问题。我们需要构建一个完整可靠的系统，包括应用程序架构、应用程序使用生产者和消费者 API 的方式、生产者和消费者配置、主题配置和代理配置。使系统更加可靠总是会在应用程序复杂性、性能、可用性或磁盘空间使用等方面进行权衡。通过了解所有选项和常见模式，并了解每种用例的要求，我们可以就应用程序和 Kafka 部署需要多可靠以及哪些权衡是合理的做出明智的决策。
